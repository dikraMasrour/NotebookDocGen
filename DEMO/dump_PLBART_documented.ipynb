{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2705728c",
   "metadata": {},
   "source": [
    "ðŸª„This is a utility function that will be used in Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input/word2vec-nlp-tutorial/\"))\n",
    "print(os.listdir(\"../input/movie-review/\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "* First we import important packages like pandas,nltk,re,os \n",
    "* we use pandas to handle our dataset it is used to take input of test and training data then \n",
    "* we import stopwords to remove usnecessary words like is,are,names etc from the dataset we use re to keep only words\n",
    "* i will explain this in details where we use re. then we import os for setting directory\n",
    "* some worldcloud and barplot visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa579840ecd2215db88bdb24a4d2f2046265202d"
   },
   "source": [
    "***if you dont have any of these files then you can download these files from command prompt  pip install module name\n",
    "for pandas --- pip install pandas for nltk ---- pip install nltk then you have to download stopwords by going to python editor and import nltk then nltk.download() select all from gui or you can make custom download i suggest you to download all. Rest are inbuilt in python(excluding keras i explained thoses below) just import and enjoy.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63c4e913f7d55ab04cf228251a96fa199de4e283"
   },
   "source": [
    "## First use pandas pd.read_csv() for reading these tabulated files and our basic process will be like \n",
    "* importing data \n",
    "* cleaning them \n",
    "* visualizing them \n",
    "* our stack models for deep leaning with "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d766b",
   "metadata": {},
   "source": [
    "ðŸª„Reads the training data from the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "12fd8960945a69a5a2e0ca9eeed7edfecfe57465"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/word2vec-nlp-tutorial/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa37a9",
   "metadata": {},
   "source": [
    "ðŸª„Reads the training data from the IDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "182cb20c31f72e6b217a999a5e72fffce5c44ecf"
   },
   "outputs": [],
   "source": [
    "df_train1=pd.read_csv(\"../input/movie-review/imdb_master.csv\",encoding=\"latin-1\")\n",
    "df_train1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7fe4d99d32acad6eb1169861341a62d70aebcc90"
   },
   "source": [
    "## dropping unecessary columns from additional dataset and combine as one ,\n",
    "**step 1 done\n",
    "now after this cleaning will startdown**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96151be5",
   "metadata": {},
   "source": [
    "ðŸª„drop axis labels of type 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94c5be8eb27dc73ef5602b474b8a8016a44a38ea"
   },
   "outputs": [],
   "source": [
    "df_train1=df_train1.drop([\"type\",'file'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c512585",
   "metadata": {},
   "source": [
    "ðŸª„Rename the columns of the train1 table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d627e7e23a3f08e53b1e9ce982c08e574ce4188a"
   },
   "outputs": [],
   "source": [
    "df_train1.rename(columns={'label':'sentiment',\n",
    "                          'Unnamed: 0':'id',\n",
    "                          'review':'review'}, \n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53a115",
   "metadata": {},
   "source": [
    "ðŸª„train1 = > train2 = > train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "284d280f2031b444988ed54c86a2d1098d1e8341"
   },
   "outputs": [],
   "source": [
    "df_train1 = df_train1[df_train1.sentiment != 'unsup']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46668c79",
   "metadata": {},
   "source": [
    "ðŸª„Map the string value of pos to neg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "884b0639c3a0fb7490e9ad0187a4aa384e7e2a26"
   },
   "outputs": [],
   "source": [
    "maping = {'pos': 1, 'neg': 0}\n",
    "df_train1['sentiment'] = df_train1['sentiment'].map(maping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4adfc9",
   "metadata": {},
   "source": [
    "ðŸª„Concatenate two data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62c8d83d7b9080a78b0cc664253eeca410673127"
   },
   "outputs": [],
   "source": [
    "new_train=pd.concat([df_train,df_train1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae58612",
   "metadata": {},
   "source": [
    "ðŸª„Read test data from a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ca2291a3b8a9d4d2d0e52029e3e0454e3a6a096"
   },
   "outputs": [],
   "source": [
    "df_test=pd.read_csv(\"../input/word2vec-nlp-tutorial/testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98665d",
   "metadata": {},
   "source": [
    "ðŸª„head of the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e06aba08a708f8894c2b691b88965aeb656944e3"
   },
   "outputs": [],
   "source": [
    "new_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb7e13675b4479124c7642330e62f3111217d7b5"
   },
   "source": [
    "# So for now files importing has been done \n",
    "# Step 2 begins that is data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13241b65d5aee459ab29e54a74fea16fa40c844b"
   },
   "source": [
    "# making function for filtering the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "667a5142a21ed739f9531edeb220f5c918e7a6e9"
   },
   "source": [
    "*  removing html tags \n",
    "*  re.sub(\"[^a-zA-Z]\",\" \", raw_review)\" \n",
    "* in this line we will keep all the alphabetical words which are present in the file name raw_review all special characters are replaced by a space.\n",
    "* spliting of words with normalizing it \n",
    "* taking stopwords into account \n",
    "* checking words alphanumeric or not \n",
    "* then after checking we stopwords finding and removing them \n",
    "* joinning meaningful words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc697d",
   "metadata": {},
   "source": [
    "ðŸª„Converts the review text into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b6f9554cee6f44fb975343c7a751868ffe1098c2"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def review_to_words( raw_review ):\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review, 'lxml').get_text() \n",
    "    \n",
    "    # 2. Remove non-letters with regex\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                           \n",
    "    \n",
    "    # 4. Create set of stopwords\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   \n",
    "\n",
    "new_train['review']=new_train['review'].apply(review_to_words)\n",
    "df_test[\"review\"]=df_test[\"review\"].apply(review_to_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d0cd280c64a2dbd9c840a2562a69b21ce28b5b53"
   },
   "source": [
    "# using above function and store the filter things in array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff7718c",
   "metadata": {},
   "source": [
    "ðŸª„Shows a wordcloud of the words in the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9985185187475a01a65ace549cac2cf37a3d54c0"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='black',\n",
    "        stopwords=stopwords,\n",
    "        max_words=200,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    ").generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "show_wordcloud(new_train[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f66df8",
   "metadata": {},
   "source": [
    "ðŸª„Check if the nullity is in the data of the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dcefb2635886e91f717c414a870c936b228edeca"
   },
   "outputs": [],
   "source": [
    "# checking nullity in the data of train and test\n",
    "new_train.isnull().sum(),df_test.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d07ca3579ecdbf8f6b5131e1883b3d21cd8516fe"
   },
   "source": [
    "# importing keras files \n",
    "## tokenizer and padding is necessary so that long and short reviews must be of same length\n",
    "## what is stacking idea , first to create word2vec dictionary , word embbeding \n",
    "## apply cnn with maxpooling to find out features of neg and positive sentiment \n",
    "## apply lstm bi directional unit to for need of good memory \n",
    "\n",
    "Background of the Techniques\n",
    "Convolution Neural Networks (CNN):\n",
    "\n",
    "CNNâ€™s are efficient for sentence classification tasks as the convolution layers can extract features horizontally from multiple words . These characteristics are essential for classification tasks as it is tricky to find clues about class memberships especially when these clues can appear in different orders in the input.  CNN has also been used for document topic classifications where a single local phrase could aid in establishing the topic regardless of the position where it appears in the document. They found that CNN is powerful enough to find these local indicators due to the powerful combination of the convolution and pooling layers.\n",
    "Long Short-Term Memory (LSTM):\n",
    "\n",
    "An example of LSTMâ€™s effectiveness is its ability to capture changing sentiment in a tweet. A sentence such as â€œThe movie was fine but not to my expectationâ€ contains words with conflicting sentiments which is not able to be inferred accurately by a typical neural network. However, LSTM will learn that the sentiments expressed towards the end of the sentence would carry more important context compared to the words at the start.\n",
    "\n",
    "CNNâ€Šâ€”â€ŠLSTM Model:\n",
    "\n",
    "The final model architecture is . We initialized the model with Kerasâ€™ Sequential layer and added the embedding layer as the first layer. By using the embedding layer, the positive integers is turned into a dense vector of fixed size and this new representations will be passed to the CNN layer. Each filter in the CNN will detect specific features or patterns and then it will be pooled to a smaller dimension in the max-pooling layer. These features are then passed into a single LSTM layer of 100 units. Then, the LSTM outputs are then fed to a Fully Connected Layer (FCL) which was built using Kerasâ€™s Dense layer. As there are five labels to be predicted, a softmax activation function was used at the output layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce8248",
   "metadata": {},
   "source": [
    "ðŸª„ine keras preprocessing layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63fd01b6e37210eb565871b5ede1cc1f4a2d7352"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c342b083",
   "metadata": {},
   "source": [
    "ðŸª„Lists the classes of the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "990ecfb1fc12d223d1be3038229a3472bf2f1de1"
   },
   "outputs": [],
   "source": [
    "list_classes = [\"sentiment\"]\n",
    "y = new_train[list_classes].values\n",
    "list_sentences_train = new_train[\"review\"]\n",
    "list_sentences_test = df_test[\"review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "20fa4fe7c9257f5fae24f7f013b08cc922e33e3e"
   },
   "source": [
    "# tokenize upto max 6000 words \n",
    "# then using keras function of preprocessing of tokenizing and padding \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd9d5b",
   "metadata": {},
   "source": [
    "ðŸª„Gets a feature matrix for a given number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "447280bb00b9034b45b00a5e2caf79d7b9aa026b"
   },
   "outputs": [],
   "source": [
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6803eee46ecea0d4012253ba95e8e10f5890f574"
   },
   "source": [
    "# checking distribution of word length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246cc2e",
   "metadata": {},
   "source": [
    "ðŸª„Create a histogram plot of the number of words vs the number of comments for one_comment in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "251de7329c03dab7b00d8c3e737667a01f850ce2"
   },
   "outputs": [],
   "source": [
    "totalNumWords = [len(one_comment) for one_comment in list_tokenized_train]\n",
    "plt.hist(totalNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\n",
    "plt.xlabel(\"Distribution of comment\")\n",
    "plt.ylabel(\"no of comments\")\n",
    "plt.title(\"no of comments vs no of words distribution \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dafd7ae8697d6e474e966e3f6004eff207193e23"
   },
   "source": [
    "# maxlen of review is 400 words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e4b9e",
   "metadata": {},
   "source": [
    "ðŸª„pad sequences with 370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e0a99b17da5ced34dc0e375fffaa3b2df75dd4b5"
   },
   "outputs": [],
   "source": [
    "maxlen = 370\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109331fa",
   "metadata": {},
   "source": [
    "ðŸª„Input placeholder for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e627b8995ea0bf793644cef12013f8cbc5fe6a48"
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen, ))\n",
    "embed_size = 128\n",
    "x = Embedding(max_features, embed_size)(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24865b6",
   "metadata": {},
   "source": [
    "ðŸª„Gets the global max pool length and sequences for the LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec18d958a3977e75e8592d0e5e50ee7d3f473933"
   },
   "outputs": [],
   "source": [
    "x = LSTM(60, return_sequences=True,name='lstm_layer')(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff16d4ec",
   "metadata": {},
   "source": [
    "ðŸª„Fit a model to a single epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0299da888f1ec80bb081d67464f33f98166a67fd"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b00494",
   "metadata": {},
   "source": [
    "ðŸª„Predict class label for a prediction of two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72ac8d10ec366afc8cea5d9281ee997f80c267bf"
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(X_te)\n",
    "y_pred = (prediction > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0da3e",
   "metadata": {},
   "source": [
    "ðŸª„Map a test sentiment to a specific value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "865e2aa5fc1d2ef9e675050cbbf991d91e81081e"
   },
   "outputs": [],
   "source": [
    "df_test[\"sentiment\"] = df_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\n",
    "y_test = df_test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cccf7e",
   "metadata": {},
   "source": [
    "ðŸª„Prints the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f3c5114419d1b2922f07be77321348e777fdbda"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "print('F1-score: {0}'.format(f1_score(y_pred, y_test)))\n",
    "print('Confusion matrix:')\n",
    "confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f0015",
   "metadata": {},
   "source": [
    "ðŸª„Function to get the OGput ouput of a file submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af0ba814cb29ec32bc972c41d6e02f3af643df15"
   },
   "outputs": [],
   "source": [
    "# ouput submission file \n",
    "df_test = df_test[['id','sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f7b14",
   "metadata": {},
   "source": [
    "ðŸª„Convert a test dataframe to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17b0ef978a1eddf5d8d64e6756bea94dab4f71da"
   },
   "outputs": [],
   "source": [
    "df_test.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a393c140",
   "metadata": {},
   "source": [
    "ðŸª„Get the current state of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "50542a12cb92b637082ac457c148cdc3a28634a7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
