markdown,code,title
linear algebra,import numpy as np ,1-house-prices-solution-top-1.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,1-house-prices-solution-top-1.ipynb
"For example, running this by clicking run or pressing Shift Enter will list the files in the input directory",from datetime import datetime ,1-house-prices-solution-top-1.ipynb
for some statistics,from scipy.stats import skew ,1-house-prices-solution-top-1.ipynb
Load data,"train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')
test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')
print (""Data is loaded!"")",1-house-prices-solution-top-1.ipynb
"There are 1460 instances of training data and 1460 of test data. Total number of attributes equals 81, of which 36 is quantitative, 43 categorical Id and SalePrice.Quantitative: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSoldQualitative: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities,","quantitative = [f for f in train.columns if train.dtypes[f] != 'object']
quantitative.remove('SalePrice')
quantitative.remove('Id')
qualitative = [f for f in train.columns if train.dtypes[f] == 'object']",1-house-prices-solution-top-1.ipynb
"19 attributes have missing values, 5 over 50 of all data. Most of times NA means lack of subject described by attribute, like missing pool, fence, no garage and basement.","y = train['SalePrice']
plt.figure(1); plt.title('Johnson SU')
sns.distplot(y, kde=False, fit=stats.johnsonsu)
plt.figure(2); plt.title('Normal')
sns.distplot(y, kde=False, fit=stats.norm)
plt.figure(3); plt.title('Log Normal')
sns.distplot(y, kde=False, fit=stats.lognorm)",1-house-prices-solution-top-1.ipynb
"It is apparent that SalePrice doesn t follow normal distribution, so before performing regression it has to be transformed. While log transformation does pretty good job, best fit is unbounded Johnson distribution.","test_normality = lambda x: stats.shapiro(x.fillna(0))[1] < 0.01
normal = pd.DataFrame(train[quantitative])
normal = normal.apply(test_normality)
print(not normal.any())",1-house-prices-solution-top-1.ipynb
"Spearman correlation is better to work with in this case because it picks up relationships between variables even when they are nonlinear. OverallQual is main criterion in establishing house price. Neighborhood has big influence, partially it has some intrisinc value in itself, but also houses in certain regions tend to share same characteristics confunding what causes similar valuations.","def encode(frame, feature):
 ordering = pd.DataFrame()
 ordering['val'] = frame[feature].unique()
 ordering.index = ordering.val
 ordering['spmean'] = frame[[feature, 'SalePrice']].groupby(feature).mean()['SalePrice']
 ordering = ordering.sort_values('spmean')
 ordering['ordering'] = range(1, ordering.shape[0]+1)
 ordering = ordering['ordering'].to_dict()
 
 for cat, o in ordering.items():
 frame.loc[frame[feature] == cat, feature+'_E'] = o
 
qual_encoded = []
for q in qualitative: 
 encode(train, q)
 qual_encoded.append(q+'_E')
print(qual_encoded)",1-house-prices-solution-top-1.ipynb
"spearman train, features ","plt.figure(1)
corr = train[quantitative+['SalePrice']].corr()
sns.heatmap(corr)
plt.figure(2)
corr = train[qual_encoded+['SalePrice']].corr()
sns.heatmap(corr)
plt.figure(3)
corr = pd.DataFrame(np.zeros([len(quantitative)+1, len(qual_encoded)+1]), index=quantitative+['SalePrice'], columns=qual_encoded+['SalePrice'])
for q1 in quantitative+['SalePrice']:
 for q2 in qual_encoded+['SalePrice']:
 corr.loc[q1, q2] = train[q1].corr(train[q2])
sns.heatmap(corr)",1-house-prices-solution-top-1.ipynb
Simple clustering,"features = quantitative + qual_encoded
model = TSNE(n_components=2, random_state=0, perplexity=50)
X = train[features].fillna(0.).values
tsne = model.fit_transform(X)

std = StandardScaler()
s = std.fit_transform(X)
pca = PCA(n_components=30)
pca.fit(s)
pc = pca.transform(s)
kmeans = KMeans(n_clusters=5)
kmeans.fit(pc)

fr = pd.DataFrame({'tsne1': tsne[:,0], 'tsne2': tsne[:, 1], 'cluster': kmeans.labels_})
sns.lmplot(data=fr, x='tsne1', y='tsne2', hue='cluster', fit_reg=False)
print(np.sum(pca.explained_variance_ratio_))",1-house-prices-solution-top-1.ipynb
Data processing,"train.drop(['Id'], axis=1, inplace=True)
test.drop(['Id'], axis=1, inplace=True)",1-house-prices-solution-top-1.ipynb
Features,"train_features = train.drop(['SalePrice'], axis=1)
test_features = test
features = pd.concat([train_features, test_features]).reset_index(drop=True)",1-house-prices-solution-top-1.ipynb
Blending Models,"def blend_models_predict(X):
 return ((0.1 * elastic_model_full_data.predict(X)) + \
 (0.05 * lasso_model_full_data.predict(X)) + \
 (0.1 * ridge_model_full_data.predict(X)) + \
 (0.1 * svr_model_full_data.predict(X)) + \
 (0.1 * gbr_model_full_data.predict(X)) + \
 (0.15 * xgb_model_full_data.predict(X)) + \
 (0.1 * lgb_model_full_data.predict(X)) + \
 (0.3 * stack_gen_model.predict(np.array(X))))",1-house-prices-solution-top-1.ipynb
Submission,"q1 = submission['SalePrice'].quantile(0.005)
q2 = submission['SalePrice'].quantile(0.995)
submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)
submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)
submission.to_csv(""submission.csv"", index=False)",1-house-prices-solution-top-1.ipynb
Based on: ,from datetime import datetime ,1-house-prices-solution-top-1.ipynb
for some statistics,from scipy.stats import skew ,1-house-prices-solution-top-1.ipynb
Based on ,train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv') ,1-house-prices-solution-top-1.ipynb
Now drop the Id colum since it s unnecessary for the prediction process.,"train.drop (['Id'], axis = 1 , inplace = True) ",1-house-prices-solution-top-1.ipynb
Deleting outliers,train = train[train.GrLivArea < 4500] ,1-house-prices-solution-top-1.ipynb
We use the numpy fuction log1p which applies log 1 x to all elements of the column,"train[""SalePrice""]= np.log1p(train[""SalePrice""]) ",1-house-prices-solution-top-1.ipynb
Some of the non numeric predictors are stored as numbers we convert them into strings,features['MSSubClass']= features['MSSubClass']. apply(str) ,1-house-prices-solution-top-1.ipynb
Filling in the rest of the NA s,"numeric_dtypes =['int16' , 'int32' , 'int64' , 'float16' , 'float32' , 'float64'] ",1-house-prices-solution-top-1.ipynb
let s up it by mixing with the top kernels,"print('Blend with Top Kernals submissions' , datetime.now (),) ",1-house-prices-solution-top-1.ipynb
Brutal approach to deal with predictions close to outer range,q1 = submission['SalePrice']. quantile(0.0045) ,1-house-prices-solution-top-1.ipynb
UsageLet s use the titanic dataset to demonstrate the capabilities of the versatile python profiler.,"import pandas as pd
import pandas_profiling 

titanic_df = pd.read_csv('/kaggle/input/titanic/train.csv')",10-simple-hacks-to-speed-up-your-data-analysis.ipynb
"To display the report in a Jupyter notebook, run the following code. This single line of code is all that you need to display the data profiling report in a Jupyter notebook. The report is pretty detailed including charts wherever necessary.", titanic_df.profile_report(),10-simple-hacks-to-speed-up-your-data-analysis.ipynb
The report can also be exported into an interactive HTML file with the following code.,"profile = titanic_df.profile_report(title='Pandas Profiling Report')
profile.to_file(output_file=""Titanic data profiling.html"")",10-simple-hacks-to-speed-up-your-data-analysis.ipynb
importing Pandas,import pandas as pd ,10-simple-hacks-to-speed-up-your-data-analysis.ipynb
importing plotly and cufflinks in offline mode,import cufflinks as cf ,10-simple-hacks-to-speed-up-your-data-analysis.ipynb
 Magic Commands Magic commands are a set of convenient functions in Jupyter Notebooks that are designed to solve some of the common problems in standard data analysis. You can see all available magics with the help of lsmagic. ,%lsmagic,10-simple-hacks-to-speed-up-your-data-analysis.ipynb
"matplotlib notebookThe matplotlib inline function is used to render the static matplotlib plots within the Jupyter notebook. Try replacing the inline part with notebook to get zoom able resize able plots, easily. Make sure the function is called before importing the matplotlib library.","%matplotlib notebook
import matplotlib.pyplot as plt
plt.plot([[0,0],[1,1]], linewidth=2)
plt.show()",10-simple-hacks-to-speed-up-your-data-analysis.ipynb
writefile writefile writes the contents of a cell to a file. Here the code will be written to a file named foo.py and saved in the current directory.,"%%writefile foo.py
x = int(input('Enter a number: '))
print(x*x)",10-simple-hacks-to-speed-up-your-data-analysis.ipynb
latexThe latex function renders the cell contents as LaTeX. It is useful for writing mathematical formulae and equations in a cell.,"%%latex
\begin{align}
a = \frac{1}{2} && b = \frac{1}{3}\\
\end{align}",10-simple-hacks-to-speed-up-your-data-analysis.ipynb
" Finding and Eliminating ErrorsThe interactive debugger is also a magic function but I have given it a category of its own. If you get an exception while running the code cell, type debug in a new line and run it. This opens an interactive debugging environment which brings you to the position where the exception has occurred. You can also check for values of variables assigned in the program and also perform operations here. To exit the debugger hit q. ","x = [1,2,3]
y = 2
z = 5

result = y+z
print(result)
result2 = x+y
print(result2)",10-simple-hacks-to-speed-up-your-data-analysis.ipynb
with pretty print,import pprint ,10-simple-hacks-to-speed-up-your-data-analysis.ipynb
" Printing all the outputs of a cellIt is a normal property of the cell that only the last output gets printed and for the others, we need to add the print function. Well, it turns out that we can print all the outputs just by adding the following snippet at the top of the notebook.from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast node interactivity all To revert to the original setting : InteractiveShell.ast node interactivity last expr ","10+5
11+6
12+7",10-simple-hacks-to-speed-up-your-data-analysis.ipynb
"In this kernel we will use the dataset in a special way to get better performance and lower loss without any data augmentation. I have read in one of the forums that the dataset is actually collected by merging 2 datasets together, the first one contains 7000 samples with 8 features 4 keypoints for each image, the second one contains 2000 images that actually belongs to the first dataset but with 30 features 15 keypoints . Now the question is, how to handle this sneaky dataset to get better results and lower loss without data augmentation . you can try 3 approaches for this one: Drop any sample that doesn t contain the full 15 key points, in this approach you simply ignore the first dataset, you will get an even smaller dataset with 2140 samples, eventually after training and submitting, you will get almost 3.0 loss. Fill any missing point with the previous available one, in this approach you will end up with 7000 samples, but most of the features are filled and not accurate, surprisingly this approach will get almost 2.4 loss which is better than the first one, a reasonable explanation for this result is providing the model with 5000 more samples with 4 accurate keypoints and 11 inaccurate filled keypoints lower the loss a bit. Enhance the 1st approach by using the ignored dataset 1st dataset to train a separate model to predict only 4 key points. Why would we do that?, Obviously this model four keypoints model will produce more accurate predictions for those specific key points as the training set contains 7000 samples with accurate labels rather than only 2000 samples notice that those 4 keypoints are just subset of the 15 keypoints . In this case, we have 2 models, fifteen keypoints model which produces 30 dim vector for each sample, and four keypoints model 8 dim vector for each sample which produces more accurate values for certain four key points , then you should replace the predictions of the four keypoints model with the corresponding predictions of the fifteen keypoints model. This approach will lower loss to almost 2.1, this simply because we got more accurate predictions for 8 features. I think with alittle bit of data augmentation after splitting the dataset you can get more decent loss, also i encourage you to take a look at Ole Gee s solution which got 1.28 loss and achieved the 1st place. The code is simple, concise and fully commented. Feel free to ask for help or more info or more explanation in the comments, i will be more than happy to help. Finally if this kernel helps you somehow, kindly don t forget to leave a little upvote up there. Hope you enjoy. ","import keras
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from tqdm import tqdm
%matplotlib inline ",2-15-loss-simple-split-trick.ipynb
Set some directories.,train_zip_path = '/kaggle/input/facial-keypoints-detection/training.zip' ,2-15-loss-simple-split-trick.ipynb
Unzip train csv file to extracted files path .,import zipfile ,2-15-loss-simple-split-trick.ipynb
Unzip test csv file to extracted files path .,"with zipfile.ZipFile(test_zip_path , 'r')as zip_ref : ",2-15-loss-simple-split-trick.ipynb
Read train csv file.,train_csv = pd.read_csv(extracted_files_path + '/training.csv') ,2-15-loss-simple-split-trick.ipynb
Read test csv file.,test_csv = pd.read_csv(extracted_files_path + '/test.csv') ,2-15-loss-simple-split-trick.ipynb
Read IdLookUpTable csv file.,looktable_csv = pd.read_csv(Id_table_path) ,2-15-loss-simple-split-trick.ipynb
"7000 samples, 8 features.",train_8_csv.info () ,2-15-loss-simple-split-trick.ipynb
"2410 samples, 30 features.",train_30_csv.info () ,2-15-loss-simple-split-trick.ipynb
Wrap train data and labels into numpy arrays.,X_train_30 = str_to_array(train_30_csv['Image']) ,2-15-loss-simple-split-trick.ipynb
Wrap test data and labels into numpy arrays.,X_train_8 = str_to_array(train_8_csv['Image']) ,2-15-loss-simple-split-trick.ipynb
Display samples of the dataset.,"fig = plt.figure(figsize =(10 , 7)) ",2-15-loss-simple-split-trick.ipynb
Prepare 2 models to handle 2 different datasets.,model_30 = create_model(output_n = 30) ,2-15-loss-simple-split-trick.ipynb
Prepare callbacks,"LR_callback = keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss' , patience = 4 , verbose = 10 , factor = .4 , min_lr = .00001) ",2-15-loss-simple-split-trick.ipynb
Train the model with 30 features.,"history = model_30.fit(X_train_30 , y_train_30 , validation_split = .1 , batch_size = 64 , epochs = 100 , callbacks =[LR_callback , EarlyStop_callback]) ",2-15-loss-simple-split-trick.ipynb
Plot the loss and accuracy curves for training and validation,"fig , ax = plt.subplots(2 , 1) ",2-15-loss-simple-split-trick.ipynb
Train the model with 8 features.,"history = model_8.fit(X_train_8 , y_train_8 , validation_split = .1 , batch_size = 64 , epochs = 100 , callbacks =[LR_callback , EarlyStop_callback]) ",2-15-loss-simple-split-trick.ipynb
Plot the loss and accuracy curves for training and validation,"fig , ax = plt.subplots(2 , 1) ",2-15-loss-simple-split-trick.ipynb
Wrap test images into 3d array.,X_test = str_to_array(test_csv['Image']) ,2-15-loss-simple-split-trick.ipynb
Pridect points for each image using 2 different model.,y_hat_30 = model_30.predict(X_test) ,2-15-loss-simple-split-trick.ipynb
Merge 2 predictions arrya into one by replacing each column in y hat 8 with the corresponding column in y hat 30.,"feature_8_ind =[0 , 1 , 2 , 3 , 20 , 21 , 28 , 29] ",2-15-loss-simple-split-trick.ipynb
Merge 2 prediction from y hat 30 and y hat 8.,for i in range(8): ,2-15-loss-simple-split-trick.ipynb
Display samples of the dataset.,"fig = plt.figure(figsize =(10 , 7)) ",2-15-loss-simple-split-trick.ipynb
All required features in order.,required_features = list(looktable_csv['FeatureName']) ,2-15-loss-simple-split-trick.ipynb
All images nmber in order.,imageID = list(looktable_csv['ImageId']- 1) ,2-15-loss-simple-split-trick.ipynb
Generate Directory to map feature name Str into int from 0 to 29.,"feature_to_num = dict(zip(required_features[0 : 30], range(30))) ",2-15-loss-simple-split-trick.ipynb
Generate list of required features encoded into ints.,feature_ind = [] ,2-15-loss-simple-split-trick.ipynb
Pick only the required predictions from y hat 30 filteration .,required_pred = [] ,2-15-loss-simple-split-trick.ipynb
Submit,rowid = looktable_csv['RowId'] ,2-15-loss-simple-split-trick.ipynb
LOAD LIBRARIES,import pandas as pd ,25-million-images-0-99757-mnist.ipynb
LOAD THE DATA,"train = pd.read_csv(""../input/train.csv"") ",25-million-images-0-99757-mnist.ipynb
PREPARE DATA FOR NEURAL NETWORK,"Y_train = train[""label""] ",25-million-images-0-99757-mnist.ipynb
PREVIEW IMAGES,"plt.figure(figsize =(15 , 4.5)) ",25-million-images-0-99757-mnist.ipynb
PREVIEW AUGMENTED IMAGES,"X_train3 = X_train[9 ,]. reshape(( 1 , 28 , 28 , 1)) ",25-million-images-0-99757-mnist.ipynb
BUILD CONVOLUTIONAL NEURAL NETWORKS,nets = 15 ,25-million-images-0-99757-mnist.ipynb
COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST," model[j]. compile(optimizer = ""adam"" , loss = ""categorical_crossentropy"" , metrics =[""accuracy""]) ",25-million-images-0-99757-mnist.ipynb
DECREASE LEARNING RATE EACH EPOCH,annealer = LearningRateScheduler(lambda x : 1e-3 * 0.95 ** x) ,25-million-images-0-99757-mnist.ipynb
TRAIN NETWORKS,history =[0]* nets ,25-million-images-0-99757-mnist.ipynb
ENSEMBLE PREDICTIONS AND SUBMIT,"results = np.zeros(( X_test.shape[0], 10)) ",25-million-images-0-99757-mnist.ipynb
PREVIEW PREDICTIONS,"plt.figure(figsize =(15 , 6)) ",25-million-images-0-99757-mnist.ipynb
the labeled training data,import pandas as pd ,93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
Import the stop word list,from nltk.corpus import stopwords ,93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
1. Remove HTML," review_text = BeautifulSoup(raw_review ,). get_text () ",93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
2. Remove non letters," letters_only = re.sub(""[^a-zA-Z]"" , "" "" , review_text) ",93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
"3. Convert to lower case, split into individual words", words = letters_only.lower (). split () ,93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
"a list, so convert the stop words to a set"," stops = set(stopwords.words(""english"")) ",93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
5. Remove stop words, meaningful_words =[w for w in words if not w in stops] ,93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
and return the result.," return("" "".join(meaningful_words)) ",93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
Get the number of reviews based on the dataframe column size,"num_reviews = training[""review""]. size ",93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
Initialize an empty list to hold the clean reviews,clean_train_reviews = [] ,93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
of the movie review list,"print(""Cleaning and parsing the training set movie reviews...\n"") ",93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
"If the index is evenly divisible by 10000, print a message", if(( i + 1)% 10000 == 0): ,93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
Creating Features from a Bag of Words Using scikit learn ,"print(""Creating the bag of words...\n"") ",93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
Creating a WordCloud,"from wordcloud import WordCloud , STOPWORDS ",93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
splitting dataset into training and testing data,from sklearn.model_selection import train_test_split ,93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
Initialize a Random Forest classifier with 100 trees,forest = RandomForestClassifier(n_estimators = 100) ,93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
This may take a few minutes to run,"forest = forest.fit(x_train , y_train) ",93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
Use the random forest to make sentiment label predictions,result = forest.predict(x_test) ,93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
checking accuracy score,"accuracy = accuracy_score(y_test , result) ",93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
checking confusion matrix,"cm = confusion_matrix(y_test , result) ",93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
checking f1 score,"f1 = f1_score(y_test , result) ",93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
Import the stop word list,from nltk.corpus import stopwords ,93-f-score-bag-of-words-m-bags-of-popcorn-with-rf.ipynb
"Introduction Let me start by saying, this is not the best way to classify digits! This notebook is rather meant to be for someone who might not know where to start. As an ml beginner myself, I find it helpful to play with these sorts of commented kernels. Any suggestions for improvement or comments on poor coding practices are appreciated!","import pandas as pd
import matplotlib.pyplot as plt, matplotlib.image as mpimg
from sklearn.model_selection import train_test_split
from sklearn import svm
%matplotlib inline",a-beginner-s-approach-to-classification.ipynb
"Loading the data We use panda s read csv 1 to read train.csv into a dataframe 2 . Then we separate our images and labels for supervised learning. We also do a train test split 3 to break our data into two sets, one for training and one for testing. This let s us measure how well our model was trained by later inputting some known test data. For the sake of time, we re only using 5000 images. You should increase or decrease this number to see how it affects model training.","labeled_images = pd.read_csv('../input/train.csv')
images = labeled_images.iloc[0:5000,1:]
labels = labeled_images.iloc[0:5000,:1]
train_images, test_images,train_labels, test_labels = train_test_split(images, labels, train_size=0.8, random_state=0)",a-beginner-s-approach-to-classification.ipynb
"Viewing an Image Since the image is currently one dimension, we load it into a numpy array 1 and reshape 2 it so that it is two dimensional 28x28 pixels Then, we plot the image and label with matplotlib You can change the value of variable i to check out other images and labels.","i=1
img=train_images.iloc[i].as_matrix()
img=img.reshape((28,28))
plt.imshow(img,cmap='gray')
plt.title(train_labels.iloc[i,0])",a-beginner-s-approach-to-classification.ipynb
"Examining the Pixel Values Note that these images aren t actually black and white 0,1 . They are gray scale 0 255 . A histogram 1 of this image s pixel values shows the range. ",plt.hist(train_images.iloc[i]),a-beginner-s-approach-to-classification.ipynb
"Training our model First, we use the sklearn.svm 1 module to create a vector classifier 2 . Next, we pass our training images and labels to the classifier s fit 3 method, which trains our model. Finally, the test images and labels are passed to the score 4 method to see how well we trained our model. Fit will return a float between 0 1 indicating our accuracy on the test data set Try playing with the parameters of svm.SVC to see how the results change.","clf = svm.SVC()
clf.fit(train_images, train_labels.values.ravel())
clf.score(test_images,test_labels)",a-beginner-s-approach-to-classification.ipynb
"How did our model do? You should have gotten around 0.10, or 10 accuracy. This is terrible. 10 accuracy is what get if you randomly guess a number. There are many ways to improve this, including not using a vector classifier, but here s a simple one to start. Let s just simplify our images by making them true black and white. To make this easy, any pixel with a value simply becomes 1 and everything else remains 0. We ll plot the same image again to see how it looks now that it s black and white. Look at the histogram now. ","test_images[test_images>0]=1
train_images[train_images>0]=1

img=train_images.iloc[i].as_matrix().reshape((28,28))
plt.imshow(img,cmap='binary')
plt.title(train_labels.iloc[i])",a-beginner-s-approach-to-classification.ipynb
"Retraining our model We follow the same procedure as before, but now our training and test sets are black and white instead of gray scale. Our score still isn t great, but it s a huge improvement.","clf = svm.SVC()
clf.fit(train_images, train_labels.values.ravel())
clf.score(test_images,test_labels)",a-beginner-s-approach-to-classification.ipynb
"Labelling the test data Now for those making competition submissions, we can load and predict the unlabeled data from test.csv. Again, for time we re just using the first 5000 images. We then output this data to a results.csv for competition submission.","test_data=pd.read_csv('../input/test.csv')
test_data[test_data>0]=1
results=clf.predict(test_data[0:5000])",a-beginner-s-approach-to-classification.ipynb
Example: Spaceship Titanic Survival PredictionCompetition: ,"import os
import numpy as np
import pandas as pd

import seaborn as sn
import matplotlib.pyplot as plt

from sklearn.impute import KNNImputer
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_validate
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier",a-complete-guide-to-decision-trees-ensembles.ipynb
Import Data,"df_train = pd.read_csv('../input/spaceship-titanic/train.csv')
df_test = pd.read_csv('../input/spaceship-titanic/test.csv')",a-complete-guide-to-decision-trees-ensembles.ipynb
Data Preparation,"df_train['Transported'] = df_train['Transported'].map({True:1, False:0})

df = pd.concat([df_train, df_test], axis=0).copy()
df1 = df.copy()
df1 = df1.drop(['Cabin','Name'], axis=1)
",a-complete-guide-to-decision-trees-ensembles.ipynb
feature engineering,df2 = df1.copy () ,a-complete-guide-to-decision-trees-ensembles.ipynb
standardize data,cat_missing_features = df2.columns[: 5]. to_list () ,a-complete-guide-to-decision-trees-ensembles.ipynb
deal with missing values,df4 = df3.copy () ,a-complete-guide-to-decision-trees-ensembles.ipynb
encode categorical variables,df6 = df5.copy () ,a-complete-guide-to-decision-trees-ensembles.ipynb
Split Data,"X_train = df6.iloc[:df_train.shape[0],:].copy()
X_train = X_train.drop(['Transported'], axis=1).copy()
y_train = df_train['Transported'].copy()
X_test = df6.iloc[df_train.shape[0]:,:]",a-complete-guide-to-decision-trees-ensembles.ipynb
Decision Tree,"DT = DecisionTreeClassifier()

DT_param = {'max_depth': np.arange(1,30),
 'max_features': np.arange(2,10),
 'min_samples_leaf': np.arange(1,20),
 'min_samples_split': np.arange(2,20),
 'criterion': ['gini', 'entropy']}

DT_cv = RandomizedSearchCV(DT, param_distributions=DT_param, cv=5, n_iter=20, random_state=0)
DT_cv.fit(X_train, y_train)
DT_cv.best_params_",a-complete-guide-to-decision-trees-ensembles.ipynb
Random Forest,"RF = RandomForestClassifier(random_state=0)

RF_param = {'n_estimators': [20,50,100,150,200],
 'max_depth': np.arange(1,30),
 'max_features': np.arange(2,10),
 'min_samples_leaf': np.arange(1,20),
 'min_samples_split': np.arange(2,20),
 'criterion': ['gini', 'entropy']}

RF_cv = RandomizedSearchCV(RF, param_distributions=RF_param, cv=5, n_iter=20, random_state=0, verbose=3)
RF_cv.fit(X_train, y_train)
RF_cv.best_params_",a-complete-guide-to-decision-trees-ensembles.ipynb
Graident Boosting,gbc = GradientBoostingClassifier(random_state = 0) ,a-complete-guide-to-decision-trees-ensembles.ipynb
Example: House Price PredictionThanks to the great Notebook: ,"import numpy as np
import pandas as pd

import seaborn as sns
import matplotlib.pyplot as plt

from scipy import stats",a-complete-guide-to-linear-regression.ipynb
Import Data,"df_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')
df_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')",a-complete-guide-to-linear-regression.ipynb
Q Q Plot,fig = plt.figure () ,a-complete-guide-to-linear-regression.ipynb
log transform the target variable,"df_train[""SalePrice""]= np.log1p(df_train[""SalePrice""]) ",a-complete-guide-to-linear-regression.ipynb
Data Preparation,"y_train= df_train['SalePrice']

df = pd.concat((df_train, df_test)).reset_index(drop=True)
df = df.drop(['Id','SalePrice'], axis=1)",a-complete-guide-to-linear-regression.ipynb
impute missing values,df_1 = df.copy () ,a-complete-guide-to-linear-regression.ipynb
encode categorical variables,df_7 = df_6.copy () ,a-complete-guide-to-linear-regression.ipynb
standardize numerical varaibles,from sklearn.preprocessing import StandardScaler ,a-complete-guide-to-linear-regression.ipynb
split the dataset back to train and test sets,df_train = df_8[: df_train.shape[0]] ,a-complete-guide-to-linear-regression.ipynb
build and train a linear regression model,from sklearn.linear_model import LinearRegression ,a-complete-guide-to-linear-regression.ipynb
"Example: Handwritten Digits RecognitionSupport Vector Machine is one of the strongest methods in solving handwriting optical character recognition problems.This is because the kernel trick allows SVM to learn the complicated realtionships not just between individual pixels of an imgae but also between larger conglomerations of pixels.Therefore, as an example here we use SVM to perform handwritten digits recognition. The dataset comes from Kaggle: ","import numpy as np
import pandas as pd
import seaborn as sb
import matplotlib.pyplot as plt
from sklearn import svm, metrics
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, confusion_matrix",a-complete-guide-to-support-vector-machine.ipynb
Import Data,"df_train = pd.read_csv('../input/mnist-in-csv/mnist_train.csv')
df_test = pd.read_csv('../input/mnist-in-csv/mnist_test.csv')",a-complete-guide-to-support-vector-machine.ipynb
have a look at an example of the output variable,instance = 25 ,a-complete-guide-to-support-vector-machine.ipynb
Split Data,"X_train = df_train[df_train.columns[1:]]
y_train = df_train['label'] 
X_test = df_test[df_test.columns[1:]] 
y_test = df_test['label']",a-complete-guide-to-support-vector-machine.ipynb
Data Preparation,"scaler = MinMaxScaler(feature_range=(-1,1)).fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)",a-complete-guide-to-support-vector-machine.ipynb
train a SVM with the RBF kernel,rbf_svm = svm.SVC(kernel = 'rbf') ,a-complete-guide-to-support-vector-machine.ipynb
Make Predictions,y_pred = rbf_svm.predict(X_test),a-complete-guide-to-support-vector-machine.ipynb
compute the confusion matrix,"cm = confusion_matrix(y_pred , y_test) ",a-complete-guide-to-support-vector-machine.ipynb
the accuracy score of the model,"accuracy_score(y_pred , y_test) ",a-complete-guide-to-support-vector-machine.ipynb
have a look at a case that the model correctly classified,for i in range(len(y_test)) : ,a-complete-guide-to-support-vector-machine.ipynb
have a look at a case that the model misclassified,for i in range(len(y_test)) : ,a-complete-guide-to-support-vector-machine.ipynb
access to system parameters ,import sys ,a-data-science-framework-to-achieve-99-accuracy.ipynb
collection of functions for data processing and analysis modeled after R dataframes with SQL like features,import pandas as pd ,a-data-science-framework-to-achieve-99-accuracy.ipynb
collection of functions for scientific and publication ready visualization,import matplotlib ,a-data-science-framework-to-achieve-99-accuracy.ipynb
foundational package for scientific computing,import numpy as np ,a-data-science-framework-to-achieve-99-accuracy.ipynb
collection of functions for scientific computing and advance mathematics,import scipy as sp ,a-data-science-framework-to-achieve-99-accuracy.ipynb
pretty printing of dataframes in Jupyter notebook,from IPython import display ,a-data-science-framework-to-achieve-99-accuracy.ipynb
collection of machine learning algorithms,import sklearn ,a-data-science-framework-to-achieve-99-accuracy.ipynb
measure execution of code snippets: ,import timeit as t ,a-data-science-framework-to-achieve-99-accuracy.ipynb
Ignore warnings,import warnings ,a-data-science-framework-to-achieve-99-accuracy.ipynb
"For example, running this by clicking run or pressing Shift Enter will list the files in the input directory",from subprocess import check_output ,a-data-science-framework-to-achieve-99-accuracy.ipynb
Common Model Algorithms,"from sklearn import svm , tree , linear_model , neighbors , naive_bayes , ensemble , discriminant_analysis , gaussian_process ",a-data-science-framework-to-achieve-99-accuracy.ipynb
Common Model Helpers,"from sklearn.preprocessing import Imputer , Normalizer , scale , OneHotEncoder , LabelEncoder ",a-data-science-framework-to-achieve-99-accuracy.ipynb
Visualization,import matplotlib as mpl ,a-data-science-framework-to-achieve-99-accuracy.ipynb
 matplotlib inline show plots in Jupyter Notebook browser,% matplotlib inline ,a-data-science-framework-to-achieve-99-accuracy.ipynb
load as dataframe,data_raw = pd.read_csv('../input/train.csv') ,a-data-science-framework-to-achieve-99-accuracy.ipynb
"We will create real test data in a later section, so we can evaluate our model before competition submission",validation_raw = pd.read_csv('../input/test.csv') ,a-data-science-framework-to-achieve-99-accuracy.ipynb
preview data,print(data_raw.info ()) ,a-data-science-framework-to-achieve-99-accuracy.ipynb
data raw.tail ,data_raw.sample(10) ,a-data-science-framework-to-achieve-99-accuracy.ipynb
Quantitative Descriptive Statistics,print(data_raw.isnull (). sum ()) ,a-data-science-framework-to-achieve-99-accuracy.ipynb
Qualitative Descriptive Statistics,print(data_raw['Sex']. value_counts ()) ,a-data-science-framework-to-achieve-99-accuracy.ipynb
"remember python assignment or equal passes by reference vs values, so we use the copy function: ",data1 = data_raw.copy(deep = True) ,a-data-science-framework-to-achieve-99-accuracy.ipynb
cleanup age with median,"data1['Age']. fillna(data1['Age']. median (), inplace = True) ",a-data-science-framework-to-achieve-99-accuracy.ipynb
preview data again,print(data1.isnull (). sum ()) ,a-data-science-framework-to-achieve-99-accuracy.ipynb
cleanup embarked with mode,"data1['Embarked']. fillna(data1['Embarked']. mode ()[ 0], inplace = True) ",a-data-science-framework-to-achieve-99-accuracy.ipynb
preview data again,print(data1.isnull (). sum ()) ,a-data-science-framework-to-achieve-99-accuracy.ipynb
delete the cabin feature column and others previously stated to exclude,"drop_column =['Cabin' , 'PassengerId' , 'Name' , 'Ticket'] ",a-data-science-framework-to-achieve-99-accuracy.ipynb
preview data again,print(data1.isnull (). sum ()) ,a-data-science-framework-to-achieve-99-accuracy.ipynb
data1 Embarked data1 Embarked .astype category ,"print(""Original Features: "" , list(data1.columns), '\n') ",a-data-science-framework-to-achieve-99-accuracy.ipynb
Quantitative Descriptive Statistics,print(data1.info ()) ,a-data-science-framework-to-achieve-99-accuracy.ipynb
Qualitative Descriptive Statistics,print(data1.Sex.value_counts ()) ,a-data-science-framework-to-achieve-99-accuracy.ipynb
define x and y variables for original features aka feature selection,"data1_x =['Pclass' , 'Sex' , 'Age' , 'SibSp' , 'Parch' , 'Fare' , 'Embarked'] ",a-data-science-framework-to-achieve-99-accuracy.ipynb
define x and y variables for dummy features aka feature selection,"data1_dummy_x = data1_dummy.iloc[: , 1 :]. columns.tolist () ",a-data-science-framework-to-achieve-99-accuracy.ipynb
split train and test data with function defaults,"train1_x , test1_x , train1_y , test1_y = train_test_split(data1[data1_x], data1[data1_y]) ",a-data-science-framework-to-achieve-99-accuracy.ipynb
using group by ,for x in data1_x : ,a-data-science-framework-to-achieve-99-accuracy.ipynb
Family Size,data1['FamilySize']= data1['SibSp']+ data1['Parch']+ 1 ,a-data-science-framework-to-achieve-99-accuracy.ipynb
graph distribution of quantitative data,"plt.figure(figsize =[16 , 12]) ",a-data-science-framework-to-achieve-99-accuracy.ipynb
barplot using ,"sns.barplot(x = 'MLA Test Accuracy' , y = 'MLA Name' , data = MLA_compare , color = 'm') ",a-data-science-framework-to-achieve-99-accuracy.ipynb
prettify using pyplot: ,plt.title('Machine Learning Algorithm Accuracy Score \n') ,a-data-science-framework-to-achieve-99-accuracy.ipynb
create a 2nd copy of our data,data2 = data_raw.copy(deep = True) ,a-data-science-framework-to-achieve-99-accuracy.ipynb
Note: we will not do any imputing missing data at this time,"data2['FareBin']= pd.qcut(data1['Fare'], 4) ",a-data-science-framework-to-achieve-99-accuracy.ipynb
"Iterate over DataFrame rows as index, Series pairs: ","for index , row in data2.iterrows (): ",a-data-science-framework-to-achieve-99-accuracy.ipynb
"Random float x, 0.0 x 1.0", if random.random ()> .5 : ,a-data-science-framework-to-achieve-99-accuracy.ipynb
predict survived 1," data2.set_value(index , 'Random_Predict' , 1) ",a-data-science-framework-to-achieve-99-accuracy.ipynb
predict died 0," data2.set_value(index , 'Random_Predict' , 0) ",a-data-science-framework-to-achieve-99-accuracy.ipynb
assume prediction wrong,data2['Random_Score']= 0 ,a-data-science-framework-to-achieve-99-accuracy.ipynb
set to 1 for correct prediction,"data2.loc[( data2['Survived']== data2['Random_Predict']) , 'Random_Score']= 1 ",a-data-science-framework-to-achieve-99-accuracy.ipynb
we can also use scikit s accuracy score function to save us a few lines of code,"print('Coin Flip Model Accuracy w/SciKit: {:.2f}%'.format(metrics.accuracy_score(data2['Survived'], data2['Random_Predict']) * 100)) ",a-data-science-framework-to-achieve-99-accuracy.ipynb
group by or pivot table: ,"pivot_female = data2[data2.Sex == 'female']. groupby (['Sex' , 'Pclass' , 'Embarked' , 'FareBin'])['Survived']. mean () ",a-data-science-framework-to-achieve-99-accuracy.ipynb
Question 1: Were you on the Titanic majority died,data2['Tree_Predict']= 0 ,a-data-science-framework-to-achieve-99-accuracy.ipynb
Question 2: Are you female majority survived,"data2.loc[( data2['Sex']== 'female'), 'Tree_Predict']= 1 ",a-data-science-framework-to-achieve-99-accuracy.ipynb
Ignore the warnings,import warnings ,a-detailed-explanation-of-keras-embedding-layer.ipynb
data visualisation and manipulation,import numpy as np ,a-detailed-explanation-of-keras-embedding-layer.ipynb
sets matplotlib to inline and displays graphs below the corressponding cell.,% matplotlib inline ,a-detailed-explanation-of-keras-embedding-layer.ipynb
nltk,import nltk ,a-detailed-explanation-of-keras-embedding-layer.ipynb
stop words,from nltk.corpus import stopwords ,a-detailed-explanation-of-keras-embedding-layer.ipynb
tokenizing,"from nltk import word_tokenize , sent_tokenize ",a-detailed-explanation-of-keras-embedding-layer.ipynb
keras,import keras ,a-detailed-explanation-of-keras-embedding-layer.ipynb
CREATING SAMPLE CORPUS OF DOCUMENTS ie TEXTS,"sample_text_1=""bitty bought a bit of butter""
sample_text_2=""but the bit of butter was a bit bitter""
sample_text_3=""so she bought some better butter to make the bitter butter better""

corp=[sample_text_1,sample_text_2,sample_text_3]
no_docs=len(corp)
",a-detailed-explanation-of-keras-embedding-layer.ipynb
After this all the unique words will be reprsented by an integer. For this we are using one hot function from the Keras. Note that the vocab size is specified large enough so as to ensure unique integer encoding for each and every word.Note one important thing that the integer encoding for the word remains same in different docs. eg butter is denoted by 31 in each and every document.,"vocab_size=50 
encod_corp=[]
for i,doc in enumerate(corp):
 encod_corp.append(one_hot(doc,50))
 print(""The encoding for document"",i+1,"" is : "",one_hot(doc,50))",a-detailed-explanation-of-keras-embedding-layer.ipynb
length of maximum document. will be nedded whenever create embeddings for the words,maxlen = - 1 ,a-detailed-explanation-of-keras-embedding-layer.ipynb
now to create embeddings all of our docs need to be of same length. hence we can pad the docs with zeros.,"pad_corp = pad_sequences(encod_corp , maxlen = maxlen , padding = 'post' , value = 0.0) ",a-detailed-explanation-of-keras-embedding-layer.ipynb
specifying the input shape,"input = Input(shape =(no_docs , maxlen), dtype = 'float64') ",a-detailed-explanation-of-keras-embedding-layer.ipynb
PARAMETERS OF THE EMBEDDING LAYER input dim the vocab size that we will choose. In other words it is the number of unique words in the vocab. output dim the number of dimensions we wish to embed into. Each word will be represented by a vector of this much dimensions. input length lenght of the maximum document. which is stored in maxlen variable in our case.,"embed_model.compile(optimizer = keras.optimizers.Adam(lr = 1e-3), loss = 'binary_crossentropy' , metrics =['acc']) ",a-detailed-explanation-of-keras-embedding-layer.ipynb
compiling the model. parameters can be tuned as always.,"print(type(word_embedding))
print(word_embedding)",a-detailed-explanation-of-keras-embedding-layer.ipynb
summary of the model,print(embed_model.summary ()) ,a-detailed-explanation-of-keras-embedding-layer.ipynb
finally getting the embeddings.,embeddings = embed_model.predict(pad_corp) ,a-detailed-explanation-of-keras-embedding-layer.ipynb
GETTING ENCODING FOR A PARTICULAR WORD IN A SPECIFIC DOCUMENT,"for i,doc in enumerate(embeddings):
 for j,word in enumerate(doc):
 print(""The encoding for "",j+1,""th word"",""in"",i+1,""th document is : \n\n"",word)",a-detailed-explanation-of-keras-embedding-layer.ipynb
pandas,import pandas as pd ,a-journey-through-titanic.ipynb
"numpy, matplotlib, seaborn",import numpy as np ,a-journey-through-titanic.ipynb
machine learning,from sklearn.linear_model import LogisticRegression ,a-journey-through-titanic.ipynb
get titanic test csv files as a DataFrame,"titanic_df = pd.read_csv(""../input/train.csv"") ",a-journey-through-titanic.ipynb
preview the data,titanic_df.head () ,a-journey-through-titanic.ipynb
"drop unnecessary columns, these columns won t be useful in analysis and prediction","titanic_df = titanic_df.drop (['PassengerId' , 'Name' , 'Ticket'], axis = 1) ",a-journey-through-titanic.ipynb
"only in titanic df, fill the two missing values with the most occurred value, which is S .","titanic_df[""Embarked""]= titanic_df[""Embarked""]. fillna(""S"") ",a-journey-through-titanic.ipynb
plot,"sns.factorplot('Embarked' , 'Survived' , data = titanic_df , size = 4 , aspect = 3) ",a-journey-through-titanic.ipynb
"sns.factorplot Survived ,hue Embarked ,data titanic df,kind count ,order 1,0 ,ax axis2 ","sns.countplot(x = 'Embarked' , data = titanic_df , ax = axis1) ",a-journey-through-titanic.ipynb
"group by embarked, and get the mean for survived passengers for each value in Embarked","embark_perc = titanic_df[[ ""Embarked"" , ""Survived""]].groupby (['Embarked'], as_index = False). mean () ",a-journey-through-titanic.ipynb
"because logically, Embarked doesn t seem to be useful in prediction.",embark_dummies_titanic = pd.get_dummies(titanic_df['Embarked']) ,a-journey-through-titanic.ipynb
"only for test df, since there is a missing Fare values","test_df[""Fare""]. fillna(test_df[""Fare""]. median (), inplace = True) ",a-journey-through-titanic.ipynb
convert from float to int,titanic_df['Fare']= titanic_df['Fare']. astype(int) ,a-journey-through-titanic.ipynb
get fare for survived didn t survive passengers,"fare_not_survived = titanic_df[""Fare""][ titanic_df[""Survived""]== 0] ",a-journey-through-titanic.ipynb
get average and std for fare of survived not survived passengers,"avgerage_fare = DataFrame ([fare_not_survived.mean (), fare_survived.mean()]) ",a-journey-through-titanic.ipynb
plot,"titanic_df['Fare']. plot(kind = 'hist' , figsize =(15 , 3), bins = 100 , xlim =(0 , 50)) ",a-journey-through-titanic.ipynb
Age,"fig ,(axis1 , axis2)= plt.subplots(1 , 2 , figsize =(15 , 4)) ",a-journey-through-titanic.ipynb
"get average, std, and number of NaN values in titanic df","average_age_titanic = titanic_df[""Age""]. mean () ",a-journey-through-titanic.ipynb
"get average, std, and number of NaN values in test df","average_age_test = test_df[""Age""]. mean () ",a-journey-through-titanic.ipynb
generate random numbers between mean std mean std ,"rand_1 = np.random.randint(average_age_titanic - std_age_titanic , average_age_titanic + std_age_titanic , size = count_nan_age_titanic) ",a-journey-through-titanic.ipynb
"NOTE: drop all null values, and convert to int","titanic_df['Age']. dropna (). astype(int). hist(bins = 70 , ax = axis1) ",a-journey-through-titanic.ipynb
fill NaN values in Age column with random values generated,"titanic_df[""Age""][ np.isnan(titanic_df[""Age""])]= rand_1 ",a-journey-through-titanic.ipynb
convert from float to int,titanic_df['Age']= titanic_df['Age']. astype(int) ,a-journey-through-titanic.ipynb
plot new Age Values,"titanic_df['Age']. hist(bins = 70 , ax = axis2) ",a-journey-through-titanic.ipynb
peaks for survived not survived passengers by their age,"facet = sns.FacetGrid(titanic_df , hue = ""Survived"" , aspect = 4) ",a-journey-through-titanic.ipynb
average survived passengers by age,"fig , axis1 = plt.subplots(1 , 1 , figsize =(18 , 4)) ",a-journey-through-titanic.ipynb
"It has a lot of NaN values, so it won t cause a remarkable impact on prediction","titanic_df.drop(""Cabin"" , axis = 1 , inplace = True) ",a-journey-through-titanic.ipynb
"Meaning, if having any family member whether parent, brother, ...etc will increase chances of Survival or not.","titanic_df['Family']= titanic_df[""Parch""]+ titanic_df[""SibSp""] ",a-journey-through-titanic.ipynb
drop Parch SibSp,"titanic_df = titanic_df.drop (['SibSp' , 'Parch'], axis = 1) ",a-journey-through-titanic.ipynb
plot,"fig ,(axis1 , axis2)= plt.subplots(1 , 2 , sharex = True , figsize =(10 , 5)) ",a-journey-through-titanic.ipynb
"sns.factorplot Family ,data titanic df,kind count ,ax axis1 ","sns.countplot(x = 'Family' , data = titanic_df , order =[1 , 0], ax = axis1) ",a-journey-through-titanic.ipynb
average of survived for those who had didn t have any family member,"family_perc = titanic_df[[ ""Family"" , ""Survived""]].groupby (['Family'], as_index = False). mean () ",a-journey-through-titanic.ipynb
"So, we can classify passengers as males, females, and child",def get_person(passenger): ,a-journey-through-titanic.ipynb
No need to use Sex column since we created Person column,"titanic_df.drop (['Sex'], axis = 1 , inplace = True) ",a-journey-through-titanic.ipynb
"create dummy variables for Person column, drop Male as it has the lowest average of survived passengers",person_dummies_titanic = pd.get_dummies(titanic_df['Person']) ,a-journey-through-titanic.ipynb
"sns.factorplot Person ,data titanic df,kind count ,ax axis1 ","sns.countplot(x = 'Person' , data = titanic_df , ax = axis1) ",a-journey-through-titanic.ipynb
"average of survived for each Person male, female, or child ","person_perc = titanic_df[[ ""Person"" , ""Survived""]].groupby (['Person'], as_index = False). mean () ",a-journey-through-titanic.ipynb
"sns.factorplot Pclass ,data titanic df,kind count ,order 1,2,3 ","sns.factorplot('Pclass' , 'Survived' , order =[1 , 2 , 3], data = titanic_df , size = 5) ",a-journey-through-titanic.ipynb
"create dummy variables for Pclass column, drop 3rd class as it has the lowest average of survived passengers",pclass_dummies_titanic = pd.get_dummies(titanic_df['Pclass']) ,a-journey-through-titanic.ipynb
define training and testing sets,"X_train = titanic_df.drop(""Survived"" , axis = 1) ",a-journey-through-titanic.ipynb
Logistic Regression,logreg = LogisticRegression () ,a-journey-through-titanic.ipynb
Random Forests,random_forest = RandomForestClassifier(n_estimators = 100) ,a-journey-through-titanic.ipynb
get Correlation Coefficient for each feature using Logistic Regression,coeff_df = DataFrame(titanic_df.columns.delete(0)) ,a-journey-through-titanic.ipynb
preview,coeff_df ,a-journey-through-titanic.ipynb
"Disclaimer: Just like the titanic kernel, the purpose of this kernel is not to let everyone get the perfect score, but to let everyone know how some participants got their perfect scores, so you can focus on learning ML rather than wondering how you can end up at the top of the LB.I ve checked the origin of the dataset look for Disasters on social media provided in the overview page and found the dataset that holds ground truth for the test set.If I can discover this so easily, I am sure it s just a matter of time before someone else does the same.My point is, ignore the LB and just focus on learning from all the great kernels that are being shared.",import pandas as pd,a-real-disaster-leaked-label.ipynb
The holy grail of a perfect ML model prediction,"subm_df.to_csv('submission.csv' , index = False) ",a-real-disaster-leaked-label.ipynb
"A Simple TF 2.2 notebookThis is intended as a simple, short introduction to the operations competitors will need to perform with TPUs.","import tensorflow as tf
from kaggle_datasets import KaggleDatasets
import numpy as np

print(""Tensorflow version "" + tf.__version__)",a-simple-petals-tf-2-2-notebook.ipynb
"Detect hardware, return appropriate distribution strategy",try : ,a-simple-petals-tf-2-2-notebook.ipynb
TPU detection. No parameters necessary if TPU NAME environment variable is set. On Kaggle this is always the case., tpu = tf.distribute.cluster_resolver.TPUClusterResolver () ,a-simple-petals-tf-2-2-notebook.ipynb
default distribution strategy in Tensorflow. Works on CPU and single GPU., strategy = tf.distribute.get_strategy () ,a-simple-petals-tf-2-2-notebook.ipynb
you can list the bucket with !gsutil ls GCS DS PATH ,GCS_DS_PATH = KaggleDatasets (). get_gcs_path () ,a-simple-petals-tf-2-2-notebook.ipynb
"at this size, a GPU will run out of memory. Use the TPU","IMAGE_SIZE =[192 , 192] ",a-simple-petals-tf-2-2-notebook.ipynb
Load my dataThis data is loaded from Kaggle and automatically sharded to maximize parallelization.,def decode_image(image_data): ,a-simple-petals-tf-2-2-notebook.ipynb
"convert image to floats in 0, 1 range"," image = tf.cast(image , tf.float32)/ 255.0 ",a-simple-petals-tf-2-2-notebook.ipynb
explicit size needed for TPU," image = tf.reshape(image ,[* IMAGE_SIZE , 3]) ",a-simple-petals-tf-2-2-notebook.ipynb
"Build a model on TPU or GPU, or CPU... with Tensorflow 2.1!",with strategy.scope (): ,a-simple-petals-tf-2-2-notebook.ipynb
tramsfer learning, pretrained_model.trainable = False ,a-simple-petals-tf-2-2-notebook.ipynb
"since we are splitting the dataset and iterating separately on images and ids, order matters.",test_ds = get_test_dataset(ordered = True) ,a-simple-petals-tf-2-2-notebook.ipynb
all in one batch,test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))). numpy (). astype('U') ,a-simple-petals-tf-2-2-notebook.ipynb
Import necessary modules,import pandas as pd ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
importing warnings library.,import warnings ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Ignore warning,warnings.filterwarnings('ignore') ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
imporing os,import os ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Importing the datasets,"train = pd.read_csv(""../input/titanic/train.csv"") ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"You are probably wondering why two datasets? Also, Why have I named it train and test ? To explain that I am going to give you an overall picture of the supervised machine learning process. Machine Learning is simply Machine and Learning . Nothing more and nothing less. In a supervised machine learning process, we are giving machine computer models specific inputs or data text number image audio to learn from aka we are training the machine to learn certain aspects based on the data and the output. Now, how can we determine that machine is actually learning what we are try to teach? That is where the test set comes to play. We withhold part of the data where we know the output result of each datapoints, and we use this data to test the trained models. We then compare the outcomes to determine the performance of the algorithms. If you are a bit confused thats okay. I will explain more as we keep reading. Let s take a look at sample datasets.",train.head(),a-statistical-analysis-ml-workflow-of-titanic.ipynb
Train Set,"%%time
train.sample(5)",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Take a look at the overview of the dataset.,% timeit test.sample(5) ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
This is a sample of train and test dataset. Lets find out a bit more about the train and test dataset. ,"print (""The shape of the train data is (row, column):""+ str(train.shape))
print (train.info())
print (""The shape of the test data is (row, column):""+ str(test.shape))
print (test.info())",a-statistical-analysis-ml-workflow-of-titanic.ipynb
1e. Tableau Visualization of the Data I have incorporated a tableau visualization below of the training data. This visualization... is for us to have an overview and play around with the dataset. is done without making any changes including Null values to any features of the dataset. Let s get a better perspective of the dataset through this visualization.,"%%HTML
<div class='tableauPlaceholder' id='viz1516349898238' style='position: relative'><noscript><a href='#'><img alt='An Overview of Titanic Training Dataset ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz' style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='Titanic_data_mining&#47;Dashboard1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div> <script type='text/javascript'> var divElement = document.getElementById('viz1516349898238'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement); </script>",a-statistical-analysis-ml-workflow-of-titanic.ipynb
saving passenger id in advance in order to submit later.,passengerid = test.PassengerId ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
"test.drop PassengerId , axis 1, inplace True ",print(train.info ()) ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Let s write a functin to print the total percentage of the missing values. this can be a good exercise for beginners to try to write simple functions like this. ,def missing_percentage(df): ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
setting the number of runs r and or loops n ,% timeit - r2 - n10 missing_percentage(train) ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Missing values in test set.,"%%timeit -r2 -n10 
missing_percentage(test)",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Embarked feature ,"def percent_value_counts(df , feature): ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
creating a df with th," total = pd.DataFrame(df.loc[: , feature]. value_counts(dropna = False)) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
concating percent and total dataframe," total.columns =[""Total""] ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"It looks like there are only two null values 0.22 in the Embarked feature, we can replace these with the mode value S . However, let s dig a little deeper. Let s see what are those two null values",train[train.Embarked.isnull()],a-statistical-analysis-ml-workflow-of-titanic.ipynb
"We may be able to solve these two missing values by looking at other independent variables of the two raws. Both passengers paid a fare of 80, are of Pclass 1 and female Sex. Let s see how the Fare is distributed among all Pclass and Embarked feature values",import seaborn as sns ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
legs 2 .set text Lower ,fig.show () ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Replacing the null values in the Embarked column with the mode.,"train.Embarked.fillna(""C"" , inplace = True) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Cabin Feature ,"print(""Train Cabin missing: "" + str(train.Cabin.isnull().sum()/len(train.Cabin)))
print(""Test Cabin missing: "" + str(test.Cabin.isnull().sum()/len(test.Cabin)))",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Concat train and test into a variable all data ,survivers = train.Survived ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Assign all the null values to N,"all_data.Cabin.fillna(""N"" , inplace = True) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"All the cabin names start with an English alphabet following by multiple digits. It seems like there are some passengers that had booked multiple cabin rooms in their name. This is because many of them travelled with family. However, they all seem to book under the same letter followed by different numbers. It seems like there is a significance with the letters rather than the numbers. Therefore, we can group these cabins according to the letter of the cabin name. ",all_data.Cabin = [i[0] for i in all_data.Cabin],a-statistical-analysis-ml-workflow-of-titanic.ipynb
Now let s look at the value counts of the cabin features and see how it looks. ,"percent_value_counts(all_data, ""Cabin"")",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"So, We still haven t done any effective work to replace the null values. Let s stop for a second here and think through how we can take advantage of some of the other features here. We can use the average of the fare column We can use pythons groupby function to get the mean fare of each cabin letter. ","all_data.groupby(""Cabin"")['Fare'].mean().sort_values()",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"Now, these means can help us determine the unknown cabins, if we compare each unknown cabin rows with the given mean s above. Let s write a simple function so that we can give cabin names based on the means. ","def cabin_estimator(i):
 """"""Grouping cabin feature by the first letter""""""
 a = 0
 if i<16:
 a = ""G""
 elif i>=16 and i<27:
 a = ""F""
 elif i>=27 and i<38:
 a = ""T""
 elif i>=38 and i<47:
 a = ""A""
 elif i>= 47 and i<53:
 a = ""E""
 elif i>= 53 and i<54:
 a = ""D""
 elif i>=54 and i<116:
 a = 'C'
 else:
 a = ""B""
 return a
 ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Let s apply cabin estimator function in each unknown cabins cabin with null values . Once that is done we will separate our train and test to continue towards machine learning modeling. ,"with_N = all_data[all_data.Cabin == ""N""]

without_N = all_data[all_data.Cabin != ""N""]",a-statistical-analysis-ml-workflow-of-titanic.ipynb
applying cabin estimator function.,with_N['Cabin']= with_N.Fare.apply(lambda x : cabin_estimator(x)) ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
getting back train.,"all_data = pd.concat ([with_N , without_N], axis = 0) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
PassengerId helps us separate train and test.,"all_data.sort_values(by = 'PassengerId' , inplace = True) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Separating train and test from all data.,train = all_data[: 891] ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
adding saved target variable with train.,train['Survived']= survivers ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
"Fare Feature If you have paid attention so far, you know that there is only one missing value in the fare column. Let s have it. ",test[test.Fare.isnull()],a-statistical-analysis-ml-workflow-of-titanic.ipynb
"Age Feature We know that the feature Age is the one with most missing values, let s see it in terms of percentage. ","print (""Train age missing value: "" + str((train.Age.isnull().sum()/len(train))*100)+str(""%""))
print (""Test age missing value: "" + str((test.Age.isnull().sum()/len(test))*100)+str(""%""))",a-statistical-analysis-ml-workflow-of-titanic.ipynb
3a. Gender and Survived ,"import seaborn as sns
pal = {'male':""green"", 'female':""Pink""}
sns.set(style=""darkgrid"")
plt.subplots(figsize = (15,8))
ax = sns.barplot(x = ""Sex"", 
 y = ""Survived"", 
 data=train, 
 palette = pal,
 linewidth=5,
 order = ['female','male'],
 capsize = .05,

 )

plt.title(""Survived/Non-Survived Passenger Gender Distribution"", fontsize = 25,loc = 'center', pad = 40)
plt.ylabel(""% of passenger survived"", fontsize = 15, )
plt.xlabel(""Sex"",fontsize = 15);

",a-statistical-analysis-ml-workflow-of-titanic.ipynb
This bar plot above shows the distribution of female and male survived. The x label represents Sex feature while the y label represents the of passenger survived. This bar plot shows that 74 female passenger survived while only 19 male passenger survived.,"pal = { 1 : ""seagreen"" , 0 : ""gray"" } ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
3b. Pclass and Survived ,"temp = train[[ 'Pclass' , 'Survived' , 'PassengerId']].groupby (['Pclass' , 'Survived']).count (). reset_index () ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Plotting,"plt.subplots(figsize =(15 , 10)) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Create green Bars,"plt.bar(r , No_s , color = 'Red' , edgecolor = 'white' , width = barWidth) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Create orange Bars,"plt.bar(r , Yes_s , bottom = No_s , color = 'Green' , edgecolor = 'white' , width = barWidth) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Custom x axis,"plt.xticks(r , names) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Show graphic,plt.show () ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Kernel Density Plot,"fig = plt.figure(figsize =(15 , 8),) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Kernel Density Plot,"fig = plt.figure(figsize =(15 , 8),) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"This plot shows something impressive.. The spike in the plot under 100 dollar represents that a lot of passengers who bought the ticket within that range did not survive. When fare is approximately more than 280 dollars, there is no gray shade which means, either everyone passed that fare point survived or maybe there is an outlier that clouds our judgment. Let s check...",train[train.Fare > 280],a-statistical-analysis-ml-workflow-of-titanic.ipynb
Kernel Density Plot,"fig = plt.figure(figsize =(15 , 8),) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"3e. Combined Feature Relations In this section, we are going to discover more than two feature relations in a single graph. I will try my best to illustrate most of the feature relations. Let s get to it. ","pal = {1:""seagreen"", 0:""gray""}
g = sns.FacetGrid(train,size=5, col=""Sex"", row=""Survived"", margin_titles=True, hue = ""Survived"",
 palette=pal)
g = g.map(plt.hist, ""Age"", edgecolor = 'white');
g.fig.suptitle(""Survived by Sex and Age"", size = 25)
plt.subplots_adjust(top=0.90)
",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"Facetgrid is a great way to visualize multiple variables and their relationships at once. From the chart in section 3a we have a intuation that female passengers had better prority than males during the tragedy. However, from this facet grid, we can also understand which age range groups survived more than others or were not so lucky","g = sns.FacetGrid(train,size=5, col=""Sex"", row=""Embarked"", margin_titles=True, hue = ""Survived"",
 palette = pal
 )
g = g.map(plt.hist, ""Age"", edgecolor = 'white').add_legend();
g.fig.suptitle(""Survived by Sex and Age"", size = 25)
plt.subplots_adjust(top=0.90)",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"This is another compelling facet grid illustrating four features relationship at once. They are Embarked, Age, Survived Sex. The color illustrates passengers survival status green represents survived, gray represents not survived The column represents Sex left being male, right stands for female The row represents Embarked from top to bottom: S, C, Q Now that I have steered out the apparent let s see if we can get some insights that are not so obvious as we look at the data. Most passengers seem to be boarded on Southampton S . More than 60 of the passengers died boarded on Southampton. More than 60 of the passengers lived boarded on Cherbourg C . Pretty much every male that boarded on Queenstown Q did not survive. There were very few females boarded on Queenstown, however, most of them survived. ","g = sns.FacetGrid(train, size=5,hue=""Survived"", col =""Sex"", margin_titles=True,
 palette=pal,)
g.map(plt.scatter, ""Fare"", ""Age"",edgecolor=""w"").add_legend()
g.fig.suptitle(""Survived by Sex, Fare and Age"", size = 25)
plt.subplots_adjust(top=0.85)",a-statistical-analysis-ml-workflow-of-titanic.ipynb
dropping the three outliers where Fare is over 500,train = train[train.Fare < 500] ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
factor plot,"sns.factorplot(x = ""Parch"" , y = ""Survived"" , data = train , kind = ""point"" , size = 8) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Passenger who traveled in big groups with parents children had less survival rate than other passengers.,"sns.factorplot(x = ""SibSp"", y = ""Survived"", data = train,kind = ""point"",size = 8)
plt.title('Factorplot of Sibilings/Spouses survived', fontsize = 25)
plt.subplots_adjust(top=0.85)",a-statistical-analysis-ml-workflow-of-titanic.ipynb
1 for male in the Sex column.,"train['Sex']= train.Sex.apply(lambda x : 0 if x == ""female"" else 1) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Train info,train.describe(),a-statistical-analysis-ml-workflow-of-titanic.ipynb
Overview Survived vs non survied ,"survived_summary = train.groupby(""Survived"") ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
4a. Correlation Matrix and Heatmap Correlations,pd.DataFrame(abs(train.corr()['Survived']).sort_values(ascending = False)),a-statistical-analysis-ml-workflow-of-titanic.ipynb
get the most important variables.,corr = train.corr ()** 2 ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Generate a mask for the upper triangle taken from seaborn example gallery ,import numpy as np ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
"Hypothesis testing for Titanic Formulating a well developed researched question: Regarding this dataset, we can formulate the null hypothesis and alternative hypothesis by asking the following questions. Is there a significant difference in the mean sex between the passenger who survived and passenger who did not survive?. Is there a substantial difference in the survival rate between the male and female passengers? The Null Hypothesis and The Alternative Hypothesis: We can formulate our hypothesis by asking questions differently. However, it is essential to understand what our end goal is. Here our dependent variable or target variable is Survived. Therefore, we say Null Hypothesis : There is no difference in the survival rate between the male and female passengers. or the mean difference between male and female passenger in the survival rate is zero. Alternative Hypothesis : There is a difference in the survival rate between the male and female passengers. or the mean difference in the survival rate between male and female is not zero. Onc thing we can do is try to set up the Null and Alternative Hypothesis in such way that, when we do our t test, we can choose to do one tailed test. According to this article, one tailed tests are more powerful than two tailed test. In addition to that, this video is also quite helpful understanding these topics. with this in mind we can update modify our null and alternative hypothesis. Let s see how we can rewrite this.. Null Hypothesis H0 : male mean is greater or equal to female mean. Alternative Hypothesis H1 : male mean is less than female mean. Determine the test statistics: This will be a two tailed test since the difference between male and female passenger in the survival rate could be higher or lower than 0. Since we do not know the standard deviation and n is small, we will use the t distribution. Specify the significance level: Specifying a significance level is an important step of the hypothesis test. It is an ultimate balance between type 1 error and type 2 error. We will discuss more in depth about those in another lesson. For now, we have decided to make our significance level 0.05. So, our confidence interval or non rejection region would be 1 1 0.05 95 . Computing T statistics and P value: Let s take a random sample and see the difference.","male_mean = train[train['Sex'] == 1].Survived.mean()

female_mean = train[train['Sex'] == 0].Survived.mean()
print (""Male survival mean: "" + str(male_mean))
print (""female survival mean: "" + str(female_mean))

print (""The mean difference between male and female survival rate: "" + str(female_mean - male_mean))",a-statistical-analysis-ml-workflow-of-titanic.ipynb
separating male and female dataframe.,import random ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
empty list for storing mean sample,m_mean_samples = [] ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Print them out,"print(f""Male mean sample mean: {round(np.mean(m_mean_samples),2)}"") ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Creating a new colomn with a,train['name_length']=[len(i)for i in train.Name] ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
get the title from the name,"train[""title""]=[i.split('.')[ 0]for i in train.Name] ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"Whenever we split like that, there is a good change that we will end up with while space around our string values. Let s check that.",print(train.title.unique()),a-statistical-analysis-ml-workflow-of-titanic.ipynb
Let s fix that,train.title = train.title.apply(lambda x : x.strip ()) ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
We can also combile all three lines above for test set here,"test['title']=[i.split('.')[ 0]. split(',')[ 1]. strip ()for i in test.Name] ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
train Data,"train[""title""]=[i.replace('Ms' , 'Miss')for i in train.title] ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
we are writing a function that can help us modify title column,def name_converted(feature): ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Family size seems like a good feature to create,train['family_size']= train.SibSp + train.Parch + 1 ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
bin the family size.,def family_group(size): ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
apply the family group function in family size,train['family_group']= train['family_size']. map(family_group) ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
is alone,"train['is_alone'] = [1 if i<2 else 0 for i in train.family_size]
test['is_alone'] = [1 if i<2 else 0 for i in test.family_size]",a-statistical-analysis-ml-workflow-of-titanic.ipynb
ticket,train.Ticket.value_counts().sample(10),a-statistical-analysis-ml-workflow-of-titanic.ipynb
"I have yet to figureout how to best manage ticket feature. So, any suggestion would be truly appreciated. For now, I will get rid off the ticket feature.","train.drop(['Ticket'], axis=1, inplace=True)

test.drop(['Ticket'], axis=1, inplace=True)",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Calculating fare based on family size.,train['calculated_fare']= train.Fare / train.family_size ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
fare group,def fare_group(fare): ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
It seems like PassengerId column only works as an id in this dataset without any significant effect on the dataset. Let s drop it.,"train.drop(['PassengerId'], axis=1, inplace=True)

test.drop(['PassengerId'], axis=1, inplace=True)",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"Creating dummy variablesYou might be wondering what is a dummy variable? Dummy variable is an important prepocessing machine learning step. Often times Categorical variables are an important features, which can be the difference between a good model and a great model. While working with a dataset, having meaningful value for example, male or female instead of 0 s and 1 s is more intuitive for us. However, machines do not understand the value of categorical values, for example, in this dataset we have gender male or female, algorithms do not accept categorical variables as input. In order to feed data in a machine learning model, we ","
train = pd.get_dummies(train, columns=['title',""Pclass"", 'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)
test = pd.get_dummies(test, columns=['title',""Pclass"",'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)
train.drop(['family_size','Name', 'Fare','name_length'], axis=1, inplace=True)
test.drop(['Name','family_size',""Fare"",'name_length'], axis=1, inplace=True)",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"As I promised before, we are going to use Random forest regressor in this section to predict the missing age values. Let s do it",train.head(),a-statistical-analysis-ml-workflow-of-titanic.ipynb
rearranging the columns so that I can easily use the dataframe to predict the missing age values.,"train = pd.concat ([train[[ ""Survived"" , ""Age"" , ""Sex"" , ""SibSp"" , ""Parch""]] , train.loc[: , ""is_alone"" :]] , axis = 1) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Importing RandomForestRegressor,from sklearn.ensemble import RandomForestRegressor ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
writing a function that takes a dataframe with missing values and outputs it by filling the missing values.,def completing_age(df): ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
gettting all the features except survived," age_df = df.loc[: , ""Age"" :] ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
df with age values, temp_train = age_df.loc[age_df.Age.notnull()] ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
df without age values, temp_test = age_df.loc[age_df.Age.isnull()] ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
setting target variables age in y, y = temp_train.Age.values ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Implementing the completing age function in both train and test dataset.,completing_age(train) ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Let s look at the his,"plt.subplots(figsize =(22 , 10),) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
create bins for age,def age_group_fun(age): ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
separating our independent and dependent variable,"X = train.drop (['Survived'], axis = 1) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"6b. Splitting the training data There are multiple ways of splitting data. They are... train test split. cross validation. We have separated dependent and independent features We have separated train and test data. So, why do we still have to split our training data? If you are curious about that, I have the answer. For this competition, when we train the machine learning algorithms, we use part of the training set usually two thirds of the train data. Once we train our algorithm using 2 3 of the train data, we start to test our algorithms using the remaining data. If the model performs well we dump our test data in the algorithms to predict and submit the competition. The code below, basically splits the train data into 4 parts, X train, X test, y train, y test. X train and y train first used to train the algorithm. then, X test is used in that trained algorithms to predict outcomes. Once we get the outcomes, we compare it with y test By comparing the outcome of the model with y test, we can determine whether our algorithms are performing well or not. As we compare we use confusion matrix to determine different aspects of model performance.P.S. When we use cross validation it is important to remember not to use X train, X test, y train and y test, rather we will use X and y. I will discuss more on that. ","from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .33, random_state=0)",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"6c. Feature Scaling Feature scaling is an important concept of machine learning models. Often times a dataset contain features highly varying in magnitude and unit. For some machine learning models, it is not a problem. However, for many other ones, its quite a problem. Many machine learning algorithms uses euclidian distances to calculate the distance between two points, it is quite a problem. Let s again look at a the sample of the train dataset below.",train.sample(5),a-statistical-analysis-ml-workflow-of-titanic.ipynb
Before Scaling,"headers = X_train.columns 

X_train.head()",a-statistical-analysis-ml-workflow-of-titanic.ipynb
We will be using standardscaler to transform,from sklearn.preprocessing import StandardScaler ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
transforming train x ,X_train = st_scale.fit_transform(X_train) ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
transforming test x ,X_test = st_scale.transform(X_test) ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
After Scaling,"pd.DataFrame(X_train, columns=headers).head()",a-statistical-analysis-ml-workflow-of-titanic.ipynb
import LogisticRegression model in python.,from sklearn.linear_model import LogisticRegression ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
"Evaluating a classification modelThere are multiple ways to evaluate a classification model. Confusion Matrix. ROC Curve AUC Curve. Confusion Matrix Confusion matrix, a table that describes the performance of a classification model. Confusion Matrix tells us how many our model predicted correctly and incorrectly in terms of binary multiple outcome classes by comparing actual and predicted cases. For example, in terms of this dataset, our model is a binary one and we are trying to classify whether the passenger survived or not survived. we have fit the model using X train and y train and predicted the outcome of X test in the variable y pred. So, now we will use a confusion matrix to compare between y test and y pred. Let s do the confusion matrix. ","from sklearn.metrics import classification_report , confusion_matrix ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"True Positive TP : values that the model predicted as yes survived and is actually yes survived . True Negative TN : values that model predicted as no not survived and is actually no not survived False Positive or Type I error : values that model predicted as yes survived but actually no not survived False Negative or Type II error : values that model predicted as no not survived but actually yes survived For this dataset, whenever the model is predicting something as yes, it means the model is predicting that the passenger survived and for cases when the model predicting no it means the passenger did not survive. Let s determine the value of all these terminologies above. True Positive TP :87 True Negative TN :149 False Positive FP :28 False Negative FN :30 From these four terminologies, we can compute many other rates that are used to evaluate a binary classifier. Accuracy: Accuracy is the measure of how often the model is correct. TP TN total 87 149 294 .8027We can also calculate accuracy score using scikit learn. ","from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_pred)",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Misclassification Rate: Misclassification Rate is the measure of how often the model is wrong Misclassification Rate and Accuracy are opposite of each other. Missclassification is equivalent to 1 minus Accuracy. Misclassification Rate is also known as Error Rate . FP FN Total 28 30 294 0.19 True Positive Rate Recall Sensitivity: How often the model predicts yes survived when it s actually yes survived ? TP TP FN 87 87 30 0.7435897435897436 ,"from sklearn.metrics import recall_score
recall_score(y_test, y_pred)",a-statistical-analysis-ml-workflow-of-titanic.ipynb
False Positive Rate: How often the model predicts yes survived when it s actually no not survived ? FP FP TN 28 28 149 0.15819209039548024 True Negative Rate Specificity: How often the model predicts no not survived when it s actually no not survived ? True Negative Rate is equivalent to 1 minus False Positive Rate. TN TN FP 149 149 28 0.8418079096045198 Precision: How often is it correct when the model predicts yes. TP TP FP 87 87 28 0.7565217391304347 ,"from sklearn.metrics import precision_score
precision_score(y_test, y_pred)",a-statistical-analysis-ml-workflow-of-titanic.ipynb
we have our confusion matrix. How about we give it a little more character. ,from sklearn.utils.multiclass import unique_labels ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
AUC ROC Curve,"from sklearn.metrics import roc_curve , auc ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
plt.style.use seaborn pastel ,y_score = logreg.decision_function(X_test) ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Using Cross validation: Pros: Helps reduce variance. Expends models predictability. ,sc = st_scale,a-statistical-analysis-ml-workflow-of-titanic.ipynb
"We can use KFold, StratifiedShuffleSplit, StratiriedKFold or ShuffleSplit, They are all close cousins. look at sklearn userguide for more info.","from sklearn.model_selection import StratifiedShuffleSplit , cross_val_score ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
run model 10x with 60 30 split intentionally leaving out 10 ,"cv = StratifiedShuffleSplit(n_splits = 10 , test_size = .25 , random_state = 0) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
saving the feature names for decision tree display,column_names = X.columns ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
"Grid Search on Logistic Regression What is grid search? What are the pros and cons? Gridsearch is a simple concept but effective technique in Machine Learning. The word GridSearch stands for the fact that we are searching for optimal parameter parameters over a grid. These optimal parameters are also known as Hyperparameters. The Hyperparameters are model parameters that are set before fitting the model and determine the behavior of the model.. For example, when we choose to use linear regression, we may decide to add a penalty to the loss function such as Ridge or Lasso. These penalties require specific alpha the strength of the regularization technique to set beforehand. The higher the value of alpha, the more penalty is being added. GridSearch finds the optimal value of alpha among a range of values provided by us, and then we go on and use that optimal value to fit the model and get sweet results. It is essential to understand those model parameters are different from models outcomes, for example, coefficients or model evaluation metrics such as accuracy score or mean squared error are model outcomes and different than hyperparameters.This part of the kernel is a working progress. Please check back again for future updates.","from sklearn.model_selection import GridSearchCV , StratifiedKFold ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
remember effective alpha scores are 0 alpha infinity,"C_vals =[0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 12 , 13 , 14 , 15 , 16 , 16.5 , 17 , 17.5 , 18] ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Choosing penalties Lasso l1 or Ridge l2 ,"penalties =['l1' , 'l2'] ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Choose a cross validation strategy.,"cv = StratifiedShuffleSplit(n_splits = 10 , test_size = .25) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
setting param for param grid in GridSearchCV.,"param = { 'penalty' : penalties , 'C' : C_vals } ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Getting the best of everything.,print(grid.best_score_) ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Using the best parameters from the grid search.,logreg_grid = grid.best_estimator_ ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Importing the model.,from sklearn.neighbors import KNeighborsClassifier ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
calling on the model oject.,"knn = KNeighborsClassifier(metric = 'minkowski' , p = 2) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
doing 10 fold staratified shuffle split cross validation,"cv = StratifiedShuffleSplit(n_splits = 10 , test_size = .25 , random_state = 2) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Search for an optimal value of k for KNN.,"k_range = range(1 , 31) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Grid search on KNN classifier,from sklearn.model_selection import GridSearchCV ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
trying out multiple values for k,"k_range = range(1 , 31) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Using startifiedShufflesplit.,"cv = StratifiedShuffleSplit(n_splits = 10 , test_size = .30 , random_state = 15) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"estimator knn, param grid param, n jobs 1 to instruct scikit learn to use all available processors.","grid = GridSearchCV(KNeighborsClassifier (), param , cv = cv , verbose = False , n_jobs = - 1) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Fitting the model.,"grid.fit(X , y) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Using the best parameters from the grid search.,knn_grid = grid.best_estimator_ ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Using RandomizedSearchCV Randomized search is a close cousin of grid search. It doesn t always provide the best result but its fast. ,from sklearn.model_selection import RandomizedSearchCV ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
trying out multiple values for k,"k_range = range(1 , 31) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Using startifiedShufflesplit.,"cv = StratifiedShuffleSplit(n_splits = 10 , test_size = .30) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"for RandomizedSearchCV,","grid = RandomizedSearchCV(KNeighborsClassifier (), param , cv = cv , verbose = False , n_jobs = - 1 , n_iter = 40) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Fitting the model.,"grid.fit(X , y) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Using the best parameters from the grid search.,knn_ran_grid = grid.best_estimator_ ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Gaussian Naive Bayes,from sklearn.naive_bayes import GaussianNB ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Support Vector Machines SVM ,from sklearn.svm import SVC ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
penalty parameter C for the error term.,"Cs =[0.001 , 0.01 , 0.1 , 1 , 1.5 , 2 , 2.5 , 3 , 4 , 5 , 10] ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
 rbf stands for gaussian kernel,"grid_search = GridSearchCV(SVC(kernel = 'rbf' , probability = True), param_grid , cv = cv) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
using the best found hyper paremeters to get the score.,svm_grid = grid_search.best_estimator_ ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
"Decision Tree ClassifierDecision tree works by breaking down the dataset into small subsets. This breaking down process is done by asking questions about the features of the datasets. The idea is to unmix the labels by asking fewer questions necessary. As we ask questions, we are breaking down the dataset into more subsets. Once we have a subgroup with only the unique type of labels, we end the tree in that node. If you would like to get a detailed understanding of Decision tree classifier, please take a look at this kernel. ","from sklearn.tree import DecisionTreeClassifier
max_depth = range(1,30)
max_feature = [21,22,23,24,25,26,28,29,30,'auto']
criterion=[""entropy"", ""gini""]

param = {'max_depth':max_depth, 
 'max_features':max_feature, 
 'criterion': criterion}
grid = GridSearchCV(DecisionTreeClassifier(), 
 param_grid = param, 
 verbose=False, 
 cv=StratifiedKFold(n_splits=20, random_state=15, shuffle=True),
 n_jobs = -1)
grid.fit(X, y) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
using the best found hyper paremeters to get the score.,"dectree_grid.score(X , y) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"I admire working with decision trees because of the potential and basics they provide towards building a more complex model like Random Forest RF . RF is an ensemble method combination of many decision trees which is where the forest part comes in. One crucial details about Random Forest is that while using a forest of decision trees, RF model takes random subsets of the original dataset bootstrapped and random subsets of the variables features columns . Using this method, the RF model creates 100 s 1000 s the amount can be menually determined of a wide variety of decision trees. This variety makes the RF model more effective and accurate. We then run each test data point through all of these 100 s to 1000 s of decision trees or the RF model and take a vote on the output. ","from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit
from sklearn.ensemble import RandomForestClassifier
n_estimators = [140,145,150,155,160];
max_depth = range(1,10);
criterions = ['gini', 'entropy'];
cv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)


parameters = {'n_estimators':n_estimators,
 'max_depth':max_depth,
 'criterion': criterions
 
 }
grid = GridSearchCV(estimator=RandomForestClassifier(max_features='auto'),
 param_grid=parameters,
 cv=cv,
 n_jobs = -1)
grid.fit(X,y) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Print classification report for y test,"print(classification_report(y_test , y_pred , labels = rf_grid.classes_)) ",a-statistical-analysis-ml-workflow-of-titanic.ipynb
"Bagging Classifier Bootstrap Aggregating is the ensemble method that involves manipulating the training set by resampling and running algorithms on it. Let s do a quick review: Bagging classifier uses a process called bootstrapped dataset to create multiple datasets from one original dataset and runs algorithm on each one of them. Here is an image to show how bootstrapped dataset works. Resampling from original dataset to bootstrapped datasets Source: After running a learning algorithm on each one of the bootstrapped datasets, all models are combined by taking their average. the test data new data then go through this averaged classifier combined classifier and predict the output. Here is an image to make it clear on how bagging works, Source: Please check out this kernel if you want to find out more about bagging classifier. ",from sklearn.ensemble import BaggingClassifier ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
"7h. AdaBoost Classifier AdaBoost is another ensemble model and is quite different than Bagging. Let s point out the core concepts. AdaBoost combines a lot of weak learners they are also called stump a tree with only one node and two leaves to make classifications. This base model fitting is an iterative process where each stump is chained one after the other It cannot run in parallel. Some stumps get more say in the final classifications than others. The models use weights that are assigned to each data point raw indicating their importance. Samples with higher weight have a higher influence on the total error of the next model and gets more priority. The first stump starts with uniformly distributed weight which means, in the beginning, every datapoint have an equal amount of weights. Each stump is made by talking the previous stump s mistakes into account. After each iteration weights gets re calculated in order to take the errors misclassifications from the last stump into consideration. The final prediction is typically constructed by a weighted vote where weights for each base model depends on their training errors or misclassification rates. To illustrate what we have talked about so far let s look at the following visualization. Source: Diogo Medium Let s dive into each one of the nitty gritty stuff about AdaBoost: First, we determine the best feature to split the dataset using Gini index basics from decision tree . The feature with the lowest Gini index becomes the first stump in the AdaBoost stump chain the lower the Gini index is, the better unmixed the label is, therefore, better split . Secondly, we need to determine how much say a stump will have in the final classification and how we can calculate that. We learn how much say a stump has in the final classification by calculating how well it classified the samples aka calculate the total error of the weight . The Total Error for a stump is the sum of the weights associated with the incorrectly classified samples. For example, lets say, we start a stump with 10 datasets. The first stump will uniformly distribute an weight amoung all the datapoints. Which means each data point will have 1 10 weight. Let s say once the weight is distributed we run the model and find 2 incorrect predicitons. In order to calculate the total erorr we add up all the misclassified weights. Here we get 1 10 1 10 2 10 or 1 5. This is our total error. We can also think about it Since the weight is uniformly distributed all add up to 1 among all data points, the total error will always be between 0 perfect stump and 1 horrible stump . We use the total error to determine the amount of say a stump has in the final classification using the following formula Where is the misclassification rate for the current classifier:Here... Amount of Say Total errorWe can draw a graph to determine the amount of say using the value of total error 0 to 1 Source: Chris McCormick The blue line tells us the amount of say for Total Error Error rate between 0 and 1. When the stump does a reasonably good job, and the total error is minimal, then the amount of say Alpha is relatively large, and the alpha value is positive. When the stump does an average job similar to a coin flip the ratio of getting correct and incorrect 50 50 , then the total error is 0.5. In this case the amount of say is 0. When the error rate is high let s say close to 1, then the amount of say will be negative, which means if the stump outputs a value as survived the included weight will turn that value into not survived. P.S. If the Total Error is 1 or 0, then this equation will freak out. A small amount of error is added to prevent this from happening. Third, We need to learn how to modify the weights so that the next stump will take the errors that the current stump made into account. The pseducode for calculating the new sample weight is as follows. Here the can be positive or negative depending whether the sample was correctly classified or misclassified by the current stump. We want to increase the sample weight of the misclassified samples hinting the next stump to put more emphasize on those. Inversely, we want to decrease the sample weight of the correctly classified samples hinting the next stump to put less emphasize on those. The following equation help us to do this calculation. Here, New Sample Weight. Current Sample weight. Amount of Say, alpha value, this is the coefficient that gets updated in each iteration and place holder for 1 if stump correctly classified, 1 if misclassified. Finally, we put together the combined classifier, which is Here, is the classification predictions for using predictor matrix is the set of weak learners is the contribution weight for weak learner is the prediction of weak learner and is binary with values 1 and 1P.S. Since the stump barely captures essential specs about the dataset, the model is highly biased in the beginning. However, as the chain of stumps continues and at the end of the process, AdaBoost becomes a strong tree and reduces both bias and variance.Resources: Statquest Principles of Machine Learning AdaBoost Video ",from sklearn.ensemble import AdaBoostClassifier ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Gradient Boosting Classifier,from sklearn.ensemble import GradientBoostingClassifier ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
7k. Extra Trees Classifier ,"from sklearn.ensemble import ExtraTreesClassifier
ExtraTreesClassifier = ExtraTreesClassifier()
ExtraTreesClassifier.fit(X, y)
y_pred = ExtraTreesClassifier.predict(X_test)
extraTree_accy = round(accuracy_score(y_pred, y_test), 3)
print(extraTree_accy)
",a-statistical-analysis-ml-workflow-of-titanic.ipynb
7l. Gaussian Process Classifier ,"from sklearn.gaussian_process import GaussianProcessClassifier
GaussianProcessClassifier = GaussianProcessClassifier()
GaussianProcessClassifier.fit(X, y)
y_pred = GaussianProcessClassifier.predict(X_test)
gau_pro_accy = round(accuracy_score(y_pred, y_test), 3)
print(gau_pro_accy)",a-statistical-analysis-ml-workflow-of-titanic.ipynb
7m. Voting Classifier ,from sklearn.ensemble import VotingClassifier ,a-statistical-analysis-ml-workflow-of-titanic.ipynb
Part 8: Submit test predictions ,"all_models = [logreg_grid,
 knn_grid, 
 knn_ran_grid,
 svm_grid,
 dectree_grid,
 rf_grid,
 bagging_grid,
 adaBoost_grid,
 voting_classifier]

c = {}
for i in all_models:
 a = i.predict(X_test)
 b = accuracy_score(a, y_test)
 c[i] = b
 
",a-statistical-analysis-ml-workflow-of-titanic.ipynb
Imports,import pandas as pd ,a-study-on-regression-applied-to-the-ames-dataset.ipynb
Definitions,"pd.set_option('display.float_format' , lambda x : '%.3f' % x) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Get data,"train = pd.read_csv(""../input/train.csv"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Check for duplicates,idsUnique = len(set(train.Id)) ,a-study-on-regression-applied-to-the-ames-dataset.ipynb
Drop Id column,"train.drop(""Id"" , axis = 1 , inplace = True) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
"Looking for outliers, as indicated in ","plt.scatter(train.GrLivArea , train.SalePrice , c = ""blue"" , marker = ""s"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Log transform the target for official scoring,train.SalePrice = np.log1p(train.SalePrice) ,a-study-on-regression-applied-to-the-ames-dataset.ipynb
Alley : data description says NA means no alley access ,"train.loc[: , ""Alley""]= train.loc[: , ""Alley""]. fillna(""None"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
BedroomAbvGr : NA most likely means 0,"train.loc[: , ""BedroomAbvGr""]= train.loc[: , ""BedroomAbvGr""]. fillna(0) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
BsmtQual etc : data description says NA for basement features is no basement ,"train.loc[: , ""BsmtQual""]= train.loc[: , ""BsmtQual""]. fillna(""No"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
CentralAir : NA most likely means No,"train.loc[: , ""CentralAir""]= train.loc[: , ""CentralAir""]. fillna(""N"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Condition : NA most likely means Normal,"train.loc[: , ""Condition1""]= train.loc[: , ""Condition1""]. fillna(""Norm"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
EnclosedPorch : NA most likely means no enclosed porch,"train.loc[: , ""EnclosedPorch""]= train.loc[: , ""EnclosedPorch""]. fillna(0) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
External stuff : NA most likely means average,"train.loc[: , ""ExterCond""]= train.loc[: , ""ExterCond""]. fillna(""TA"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Fence : data description says NA means no fence ,"train.loc[: , ""Fence""]= train.loc[: , ""Fence""]. fillna(""No"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
FireplaceQu : data description says NA means no fireplace ,"train.loc[: , ""FireplaceQu""]= train.loc[: , ""FireplaceQu""]. fillna(""No"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Functional : data description says NA means typical,"train.loc[: , ""Functional""]= train.loc[: , ""Functional""]. fillna(""Typ"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
GarageType etc : data description says NA for garage features is no garage ,"train.loc[: , ""GarageType""]= train.loc[: , ""GarageType""]. fillna(""No"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
HalfBath : NA most likely means no half baths above grade,"train.loc[: , ""HalfBath""]= train.loc[: , ""HalfBath""]. fillna(0) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
HeatingQC : NA most likely means typical,"train.loc[: , ""HeatingQC""]= train.loc[: , ""HeatingQC""]. fillna(""TA"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
KitchenAbvGr : NA most likely means 0,"train.loc[: , ""KitchenAbvGr""]= train.loc[: , ""KitchenAbvGr""]. fillna(0) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
KitchenQual : NA most likely means typical,"train.loc[: , ""KitchenQual""]= train.loc[: , ""KitchenQual""]. fillna(""TA"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
LotFrontage : NA most likely means no lot frontage,"train.loc[: , ""LotFrontage""]= train.loc[: , ""LotFrontage""]. fillna(0) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
LotShape : NA most likely means regular,"train.loc[: , ""LotShape""]= train.loc[: , ""LotShape""]. fillna(""Reg"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
MasVnrType : NA most likely means no veneer,"train.loc[: , ""MasVnrType""]= train.loc[: , ""MasVnrType""]. fillna(""None"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
MiscFeature : data description says NA means no misc feature ,"train.loc[: , ""MiscFeature""]= train.loc[: , ""MiscFeature""]. fillna(""No"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
OpenPorchSF : NA most likely means no open porch,"train.loc[: , ""OpenPorchSF""]= train.loc[: , ""OpenPorchSF""]. fillna(0) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
PavedDrive : NA most likely means not paved,"train.loc[: , ""PavedDrive""]= train.loc[: , ""PavedDrive""]. fillna(""N"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
PoolQC : data description says NA means no pool ,"train.loc[: , ""PoolQC""]= train.loc[: , ""PoolQC""]. fillna(""No"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
SaleCondition : NA most likely means normal sale,"train.loc[: , ""SaleCondition""]= train.loc[: , ""SaleCondition""]. fillna(""Normal"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
ScreenPorch : NA most likely means no screen porch,"train.loc[: , ""ScreenPorch""]= train.loc[: , ""ScreenPorch""]. fillna(0) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
TotRmsAbvGrd : NA most likely means 0,"train.loc[: , ""TotRmsAbvGrd""]= train.loc[: , ""TotRmsAbvGrd""]. fillna(0) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Utilities : NA most likely means all public utilities,"train.loc[: , ""Utilities""]= train.loc[: , ""Utilities""]. fillna(""AllPub"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
WoodDeckSF : NA most likely means no wood deck,"train.loc[: , ""WoodDeckSF""]= train.loc[: , ""WoodDeckSF""]. fillna(0) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Find most important features relative to target,"print(""Find most important features relative to target"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
3 Polynomials on the top 10 existing features,"train[""OverallQual-s2""]= train[""OverallQual""]** 2 ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Differentiate numerical features minus the target and categorical features,"categorical_features = train.select_dtypes(include =[""object""]).columns ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Handle remaining missing values for numerical features by using median as replacement,"print(""NAs for numerical features in train : "" + str(train_num.isnull (). values.sum ())) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
"As a general rule of thumb, a skewness with an absolute value 0.5 is considered at least moderately skewed",skewness = train_num.apply(lambda x : skew(x)) ,a-study-on-regression-applied-to-the-ames-dataset.ipynb
Create dummy features for categorical values via one hot encoding,"print(""NAs for categorical features in train : "" + str(train_cat.isnull (). values.sum ())) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Join categorical and numerical features,"train = pd.concat ([train_num , train_cat], axis = 1) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Partition the dataset in train validation sets,"X_train , X_test , y_train , y_test = train_test_split(train , y , test_size = 0.3 , random_state = 0) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Standardize numerical features,stdSc = StandardScaler () ,a-study-on-regression-applied-to-the-ames-dataset.ipynb
Define error measure for official scoring : RMSE,"scorer = make_scorer(mean_squared_error , greater_is_better = False) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Linear Regression,lr = LinearRegression () ,a-study-on-regression-applied-to-the-ames-dataset.ipynb
Look at predictions on training and validation set,"print(""RMSE on Training set :"" , rmse_cv_train(lr). mean ()) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Plot residuals,"plt.scatter(y_train_pred , y_train_pred - y_train , c = ""blue"" , marker = ""s"" , label = ""Training data"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
Plot predictions,"plt.scatter(y_train_pred , y_train , c = ""blue"" , marker = ""s"" , label = ""Training data"") ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
2 Ridge,"ridge = RidgeCV(alphas =[0.01 , 0.03 , 0.06 , 0.1 , 0.3 , 0.6 , 1 , 3 , 6 , 10 , 30 , 60]) ",a-study-on-regression-applied-to-the-ames-dataset.ipynb
import packages,import pandas as pd ,advanced-analysis-to-infinity-and-beyond.ipynb
setting up options,import warnings ,advanced-analysis-to-infinity-and-beyond.ipynb
read datasets,train_df = pd.read_csv('../input/spaceship-titanic/train.csv') ,advanced-analysis-to-infinity-and-beyond.ipynb
back to top 3.1.1 Quick view,train_df.head(),advanced-analysis-to-infinity-and-beyond.ipynb
back to top 3.1.2 Data types,train_df.dtypes,advanced-analysis-to-infinity-and-beyond.ipynb
"back to top 3.1.3 Basic Statistics Below is the basic statistics for each variables which contain information on count, mean, standard deviation, minimum, 1st quartile, median, 3rd quartile and maximum.",train_df.describe(),advanced-analysis-to-infinity-and-beyond.ipynb
back to top 3.1.4 Target Column,"print('Target column basic statistics:')
train_df['Transported'].describe()",advanced-analysis-to-infinity-and-beyond.ipynb
back to top 3.2.1 Quick view,test_df.head(),advanced-analysis-to-infinity-and-beyond.ipynb
back to top 3.2.2 Data types,test_df.dtypes,advanced-analysis-to-infinity-and-beyond.ipynb
"back to top 3.2.3 Basic Statistics Below is the basic statistics for each variables which contain information on count, mean, standard deviation, minimum, 1st quartile, median, 3rd quartile and maximum.",test_df.describe(),advanced-analysis-to-infinity-and-beyond.ipynb
back to top 3.3 Submission Below is the first 5 rows of submission file:,ssub.head(),advanced-analysis-to-infinity-and-beyond.ipynb
"back to top 4. Explore Data Analisys 4.1 TransportedTarget value is binary, 49.64 passengers were transported to an alternate dimension!","plt.subplots(figsize=(25, 10), facecolor='#f6f5f5')
plt.pie(train_df.Transported.value_counts(), startangle=90, wedgeprops={'width':0.3}, colors=['#F5C63C', '#7A5197'] )
plt.title('Target Balance Pie Chart', loc='center', fontsize=24, color='#7A5197', fontweight='bold')
plt.text(0, 0, f""{train_df.Transported.value_counts()[0] / train_df.Transported.count() * 100:.2f}%"", ha='center', va='center', fontweight='bold', fontsize=42, color='#7A5197')
plt.legend(train_df.Transported.value_counts().index, ncol=2, facecolor='#f6f5f5', edgecolor='#f6f5f5', loc='lower center', fontsize=16)
plt.show()",advanced-analysis-to-infinity-and-beyond.ipynb
back to top 4.2 HomePlanetThe most part of passengers from Earth. Also as we can see below residents from Europa was transported more then others. 57.6 mistransported passengers from Earth in relation to fellow countrymans Probably residents Earth is not able to get safely cabins. 34.1 mistransported passengers from Europe in relation to fellow countrymans Reverse effect. 31.2 residents Earth wasnt transported in relation to all. 54.2 residents Earth. 25.1 residents from Europe. 20.7 residents from Mars. ,facecolor = '#f6f5f5' ,advanced-analysis-to-infinity-and-beyond.ipynb
print val ," ax.text(0 , ind , formatter(val), fontweight = ""bold"" , color = facecolor , fontsize = 14) ",advanced-analysis-to-infinity-and-beyond.ipynb
back to top 4.3 CryoSleep 30.7 passengers from Earth with option CryoSleep in relation to fellow countrymans. The hypothesis that Earth residents live poorer than those from Mars and Europe is look true. 43.9 passengers from Europe with option CryoSleep in relation to fellow countrymans. 16.3 residents Earth with option CryoSleep in relation to all. ,facecolor = '#f6f5f5' ,advanced-analysis-to-infinity-and-beyond.ipynb
print val ," ax.text(0 , ind , formatter(val), fontweight = ""bold"" , color = facecolor , fontsize = 14) ",advanced-analysis-to-infinity-and-beyond.ipynb
"back to top 4.4 Cabins CryoSleep option is positive affect to be transported. Residents Earth have worst Deck and probability to be transported. Residents Mars have high dispersion of probability to be transported. I have computed difference between transported probabilities within cryosleep option. The more this values, the options to be transported of this Deck for different classes of HomePlanet. D, E, F within cryoSleep is optimal to be transported. B is optimal in general. D and F optimal for Mars residents. S Side more optimal to be transported. ","train_df = train_df.dropna()
train_df['CabinDeck'] = train_df.Cabin.map(lambda x: x.split('/')[0])
train_df['CabinNum'] = train_df.Cabin.map(lambda x: x.split('/')[1])
train_df['CabinSide'] = train_df.Cabin.map(lambda x: x.split('/')[2])",advanced-analysis-to-infinity-and-beyond.ipynb
"ax.set title target col t val , fontsize 12, fontweight medium "," ax.tick_params(left = False , bottom = False , right = False , top = False , labelleft =(i <= 1)) ",advanced-analysis-to-infinity-and-beyond.ipynb
print val ," ax.text(0 , ind , formatter(val), fontweight = ""bold"" , color = facecolor , fontsize = 14) ",advanced-analysis-to-infinity-and-beyond.ipynb
"pd.pivot table index CabinSide , data train df, values Transported , aggfunc count , sum , mean ","_pivot_1 = pd.pivot_table(index =['CryoSleep' , 'HomePlanet' , 'CabinDeck'], data = train_df , values = 'Transported' , aggfunc =['mean'])['mean']. style.background_gradient(cmap = 'BuPu') ",advanced-analysis-to-infinity-and-beyond.ipynb
"back to top 4.5 Destination 55 Cancri e is optimal probablility to be transported than other. TRAPPIST 1e Earth: 0.38 probability to be transported. if CryoSleep option include, this given more probability to be transported. ","_pivot_1 = pd.pivot_table(index=['Destination'], data=train_df, values='Transported', aggfunc=['mean'])['mean'].style.background_gradient(cmap='BuPu')
_pivot_2 = pd.pivot_table(index=['Destination'], columns=['HomePlanet'], data=train_df, values='Transported', aggfunc=['mean'])['mean'].style.background_gradient(cmap='BuPu', axis=0)
_pivot_3 = pd.pivot_table(index=['Destination', 'CryoSleep'], columns=['HomePlanet'], data=train_df, values='Transported', aggfunc=['mean'])['mean'].style.background_gradient(cmap='BuPu')
_pivot_4 = pd.pivot_table(index=['Destination', 'CabinDeck'], columns=['HomePlanet'], data=train_df, values='Transported', aggfunc=['mean'])['mean'].style.background_gradient(cmap='BuPu')

multi_table([_pivot_1, _pivot_2, _pivot_3, _pivot_4])",advanced-analysis-to-infinity-and-beyond.ipynb
back to top 4.6 Age Age under 10 is 0.72 probablity to be transported. Earthlings over 20 years old suffered the most. Passengers in CabinDeck e and f over 20 years old suffered the most. ,"train_df['AgeBins']= pd.cut(train_df['Age'], bins = 8 , labels =['>10' , '10-20' , '20-30' , '30-40' , '40-50' , '50-60' , '60-70' , '70-80']) ",advanced-analysis-to-infinity-and-beyond.ipynb
" target col False: Without CryoSleep , True: CryoSleep "," ax.set_title(t_val , fontsize = 12 , fontweight = ""medium"") ",advanced-analysis-to-infinity-and-beyond.ipynb
print val ," ax.text(0 , ind , formatter(val), fontweight = ""bold"" , color = facecolor , fontsize = 14) ",advanced-analysis-to-infinity-and-beyond.ipynb
print val ," ax.text(0 , ind , formatter(val), fontweight = ""bold"" , color = facecolor , fontsize = 14) ",advanced-analysis-to-infinity-and-beyond.ipynb
print val ," ax.text(0 , ind , formatter(val), fontweight = ""bold"" , color = facecolor , fontsize = 14) ",advanced-analysis-to-infinity-and-beyond.ipynb
back to top 4.7 Vip VIP status does not affect transported chance ,"plt.subplots(figsize =(25 , 10), facecolor = '#f6f5f5') ",advanced-analysis-to-infinity-and-beyond.ipynb
"plt.legend True , , ncol 2, facecolor f6f5f5 , edgecolor f6f5f5 , loc lower center , fontsize 16 ",plt.show () ,advanced-analysis-to-infinity-and-beyond.ipynb
back to top 4.8 RoomService RoomService option is popular for residers of Mars and Europe and whose destination of Trappist and Canci. VIP passengers is also used this option more than other. Young passengers more used this option then older. ,"facecolor = '#f6f5f5'
fig = plt.figure(figsize=(14, 8), facecolor=facecolor)
gs = fig.add_gridspec(2, 1)

gs.update(wspace=0.2, hspace=0.7)

ax0 = fig.add_subplot(gs[0, 0])
ax1 = fig.add_subplot(gs[1, 0])

ax0.set_facecolor(facecolor)
for s in [""top"",""right"", ""left"", ""bottom""]:
 ax0.spines[s].set_visible(False)

ax1.set_facecolor(facecolor)
for s in [""top"",""right"",""left"", ""bottom""]:
 ax1.spines[s].set_visible(False)
 
sns.violinplot(x=""HomePlanet"", y=""RoomService"", data=train_df, hue='Destination', ax=ax0, color=colors[3])
sns.violinplot(x=""AgeBins"", y=""RoomService"", data=train_df, hue='VIP', split=True, ax=ax1, color=colors[1])

ax0.legend(ncol=3, facecolor=facecolor, edgecolor=facecolor, loc='upper center')
ax0.text(-0.6, 15000, 'RoomService Distributions', fontweight='bold', fontsize=24, color=colors[3]);
ax0.text(-0.6, 13000, 'by Destination & HomePlanet', fontweight='bold', fontsize=24, color=colors[3]);

ax1.legend(ncol=2, facecolor=facecolor, edgecolor=facecolor, loc='upper center');
ax1.text(-0.7, 14000, 'RoomService Distributions', fontweight='bold', fontsize=24, color=colors[1]);
ax1.text(-0.7, 12000, 'by VIP & Age', fontweight='bold', fontsize=24, color=colors[1]);
",advanced-analysis-to-infinity-and-beyond.ipynb
back to top 4.9 FoodCourt FoodCourt preferred residents Europe. VIP passengers is also preffered this option. Passengers whose years between30 50 preffered FooCourt more then other. ,"facecolor = '#f6f5f5'
fig = plt.figure(figsize=(14, 8), facecolor=facecolor)
gs = fig.add_gridspec(2, 1)

gs.update(wspace=0.2, hspace=0.7)

ax0 = fig.add_subplot(gs[0, 0])
ax1 = fig.add_subplot(gs[1, 0])

ax0.set_facecolor(facecolor)
for s in [""top"",""right"", ""left"", ""bottom""]:
 ax0.spines[s].set_visible(False)

ax1.set_facecolor(facecolor)
for s in [""top"",""right"",""left"", ""bottom""]:
 ax1.spines[s].set_visible(False)
 
sns.violinplot(x=""HomePlanet"", y=""FoodCourt"", data=train_df, hue='Destination', ax=ax0, color=colors[3])
sns.violinplot(x=""AgeBins"", y=""FoodCourt"", data=train_df, hue='VIP', split=True, ax=ax1, color=colors[1])

ax0.legend(ncol=3, facecolor=facecolor, edgecolor=facecolor, loc='upper center')
ax0.text(-0.6, 50000, 'FoodCourt Distributions', fontweight='bold', fontsize=24, color=colors[3]);
ax0.text(-0.6, 44000, 'by Destination & HomePlanet', fontweight='bold', fontsize=24, color=colors[3]);

ax1.legend(ncol=2, facecolor=facecolor, edgecolor=facecolor, loc='upper center');
ax1.text(-0.6, 54000, 'FoodCourt Distributions', fontweight='bold', fontsize=24, color=colors[1]);
ax1.text(-0.6, 46000, 'by VIP & Age', fontweight='bold', fontsize=24, color=colors[1]);
",advanced-analysis-to-infinity-and-beyond.ipynb
back to top 4.10 ShoppingMall ShoppingMall preferred residents Europe and whose destination to Trappist and Cancri. Passengers whose years between10 50 preffered ShoppingMall more then other. ,"facecolor = '#f6f5f5'
fig = plt.figure(figsize=(14, 8), facecolor=facecolor)
gs = fig.add_gridspec(2, 1)

gs.update(wspace=0.2, hspace=0.7)

ax0 = fig.add_subplot(gs[0, 0])
ax1 = fig.add_subplot(gs[1, 0])

ax0.set_facecolor(facecolor)
for s in [""top"",""right"", ""left"", ""bottom""]:
 ax0.spines[s].set_visible(False)

ax1.set_facecolor(facecolor)
for s in [""top"",""right"",""left"", ""bottom""]:
 ax1.spines[s].set_visible(False)
 
sns.violinplot(x=""HomePlanet"", y=""ShoppingMall"", data=train_df, hue='Destination', ax=ax0, color=colors[3])
sns.violinplot(x=""AgeBins"", y=""ShoppingMall"", data=train_df, hue='VIP', split=True, ax=ax1, color=colors[1])

ax0.legend(ncol=3, facecolor=facecolor, edgecolor=facecolor, loc='upper center')
ax0.text(-0.6, 18000, 'ShoppingMall Distributions', fontweight='bold', fontsize=24, color=colors[3]);
ax0.text(-0.6, 15000, 'by Destination & HomePlanet', fontweight='bold', fontsize=24, color=colors[3]);

ax1.legend(ncol=2, facecolor=facecolor, edgecolor=facecolor, loc='upper center');
ax1.text(-0.6, 18000, 'ShoppingMall Distributions', fontweight='bold', fontsize=24, color=colors[1]);
ax1.text(-0.6, 15000, 'by VIP & Age', fontweight='bold', fontsize=24, color=colors[1]);
",advanced-analysis-to-infinity-and-beyond.ipynb
back to top 4.11 Spa SPA preferred residents Europe. SPA preferred also whose older and age more than 30. ,"facecolor = '#f6f5f5'
fig = plt.figure(figsize=(14, 8), facecolor=facecolor)
gs = fig.add_gridspec(2, 1)

gs.update(wspace=0.2, hspace=0.7)

ax0 = fig.add_subplot(gs[0, 0])
ax1 = fig.add_subplot(gs[1, 0])

ax0.set_facecolor(facecolor)
for s in [""top"",""right"", ""left"", ""bottom""]:
 ax0.spines[s].set_visible(False)

ax1.set_facecolor(facecolor)
for s in [""top"",""right"",""left"", ""bottom""]:
 ax1.spines[s].set_visible(False)
 
sns.violinplot(x=""HomePlanet"", y=""Spa"", data=train_df, hue='Destination', ax=ax0, color=colors[3])
sns.violinplot(x=""AgeBins"", y=""Spa"", data=train_df, hue='VIP', split=True, ax=ax1, color=colors[1])

ax0.legend(ncol=3, facecolor=facecolor, edgecolor=facecolor, loc='upper center')
ax0.text(-0.6, 33000, 'Spa Distributions', fontweight='bold', fontsize=24, color=colors[3]);
ax0.text(-0.6, 28000, 'by Destination & HomePlanet', fontweight='bold', fontsize=24, color=colors[3]);

ax1.legend(ncol=2, facecolor=facecolor, edgecolor=facecolor, loc='upper center');
ax1.text(-0.6, 33000, 'Spa Distributions', fontweight='bold', fontsize=24, color=colors[1]);
ax1.text(-0.6, 28000, 'by VIP & Age', fontweight='bold', fontsize=24, color=colors[1]);
",advanced-analysis-to-infinity-and-beyond.ipynb
back to top 4.12 VRDeck VRDeck preferred also whose young and without VIP. VRDeck preferred VIP passengers whose age more then 30. Target audience is VIP passengers 45 years old. ,"facecolor = '#f6f5f5'
fig = plt.figure(figsize=(14, 8), facecolor=facecolor)
gs = fig.add_gridspec(2, 1)

gs.update(wspace=0.2, hspace=0.7)

ax0 = fig.add_subplot(gs[0, 0])
ax1 = fig.add_subplot(gs[1, 0])

ax0.set_facecolor(facecolor)
for s in [""top"",""right"", ""left"", ""bottom""]:
 ax0.spines[s].set_visible(False)

ax1.set_facecolor(facecolor)
for s in [""top"",""right"",""left"", ""bottom""]:
 ax1.spines[s].set_visible(False)
 
sns.violinplot(x=""HomePlanet"", y=""VRDeck"", data=train_df, hue='Destination', ax=ax0, color=colors[3])
sns.violinplot(x=""AgeBins"", y=""VRDeck"", data=train_df, hue='VIP', split=True, ax=ax1, color=colors[1])

ax0.legend(ncol=3, facecolor=facecolor, edgecolor=facecolor, loc='upper center')
ax0.text(-0.6, 31000, 'VRDeck Distributions', fontweight='bold', fontsize=24, color=colors[3]);
ax0.text(-0.6, 26000, 'by Destination & HomePlanet', fontweight='bold', fontsize=24, color=colors[3]);

ax1.legend(ncol=2, facecolor=facecolor, edgecolor=facecolor, loc='upper center');
ax1.text(-0.7, 31000, 'VRDeck Distributions', fontweight='bold', fontsize=24, color=colors[1]);
ax1.text(-0.7, 26000, 'by VIP & Age', fontweight='bold', fontsize=24, color=colors[1]);
",advanced-analysis-to-infinity-and-beyond.ipynb
You have already seen the code to load the soccer football data:,import numpy as np ,advanced-uses-of-shap-values.ipynb
Convert from string Yes No to binary,"y =(data['Man of the Match']== ""Yes"") ",advanced-uses-of-shap-values.ipynb
package used to calculate Shap values,import shap ,advanced-uses-of-shap-values.ipynb
Create object that can calculate shap values,explainer = shap.TreeExplainer(my_model) ,advanced-uses-of-shap-values.ipynb
"Calculate shap values for all of val X rather than a single row, to have more data for plot.",shap_values = explainer.shap_values(val_X) ,advanced-uses-of-shap-values.ipynb
Make plot. Index of 1 is explained in text below.,"shap.summary_plot(shap_values[1], val_X) ",advanced-uses-of-shap-values.ipynb
package used to calculate Shap values,import shap ,advanced-uses-of-shap-values.ipynb
Create object that can calculate shap values,explainer = shap.TreeExplainer(my_model) ,advanced-uses-of-shap-values.ipynb
calculate shap values. This is what we will plot.,shap_values = explainer.shap_values(X) ,advanced-uses-of-shap-values.ipynb
make plot.,"shap.dependence_plot('Ball Possession %' , shap_values[1], X , interaction_index = ""Goal Scored"") ",advanced-uses-of-shap-values.ipynb
Exploring Datasets,"import numpy as np 
import pandas as pd 
import os
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
import warnings
import json
import random
import sys",all-imputation-techniques-with-pros-and-cons.ipynb
 Basic Exploration ,"print(""Data Shape: There are {:,.0f} rows and {:,.0f} columns.\nMissing values = {}, Duplicates = {}.\n"".
 format(df.shape[0], df.shape[1],df.isna().sum().sum(), df.duplicated().sum()))",all-imputation-techniques-with-pros-and-cons.ipynb
shape of DF,df.shape ,all-imputation-techniques-with-pros-and-cons.ipynb
We are using a for loop for all the columns present in dataset with average null values greater than 0,na_variables =[var for var in df.columns if df[var]. isnull (). mean ()> 0] ,all-imputation-techniques-with-pros-and-cons.ipynb
Implementing the CCA techniques to remove Missing Data,data_cca = df.dropna(axis = 0) ,all-imputation-techniques-with-pros-and-cons.ipynb
Verifying the final shape of the remaining dataset,data_cca.shape ,all-imputation-techniques-with-pros-and-cons.ipynb
When to Use: When data is not MAR Missing At Random . Suitable for All. Code: ,"train_df=df.copy()
na_variables =[var for var in train_df.columns if train_df[var].isnull().mean() > 0 ]",all-imputation-techniques-with-pros-and-cons.ipynb
"Using Arbitary Imputation technique, we will Impute missing Gender with Missing You can use any other value also ",arb_impute = train_df['F_1_0']. fillna(random.choice(train_df['F_1_0']. unique ())) ,all-imputation-techniques-with-pros-and-cons.ipynb
When to Use: Data is Missing at Random MAR Missing data is not more than 5 6 of the dataset. Code: ,train_df['F_1_0'].groupby(train_df['F_1_0']).count(),all-imputation-techniques-with-pros-and-cons.ipynb
Using Frequent Category Imputer,frq_impute = train_df['F_1_0']. fillna('-0.665735') ,all-imputation-techniques-with-pros-and-cons.ipynb
"This is done using statistical values like mean, median. However, none of these guarantees unbiased data, especially if there are many missing values.Mean is most useful when the original data is not skewed, while the median is more robust, not sensitive to outliers, and thus used when data is skewed.In a normally distributed data, one can get all the values that are within 2 standard deviations from the mean. Next, fill in the missing values by generating random numbers between mean 2 std mean 2 std ","xdf=df.copy()

average=xdf.F_1_0.mean()
std=xdf.F_1_0.std()
count_nan_age=xdf.F_1_0.isnull().sum()

rand = np.random.randint(average - 2*std, average + 2*std, size = count_nan_age)

print(""Before"",xdf.F_1_0.isnull().sum())

xdf[""F_1_0""][np.isnan(xdf[""F_1_0""])] = rand
print(""After"",xdf.F_1_0.isnull().sum())",all-imputation-techniques-with-pros-and-cons.ipynb
seeing co relation,"sns.set(rc = { 'figure.figsize' :(8 , 8)}) ",all-imputation-techniques-with-pros-and-cons.ipynb
Interesting corelation! ,"print(""F_4_11"",xdf.F_4_11.isnull().sum())
print(""F_4_8"",xdf.F_4_8.isnull().sum())",all-imputation-techniques-with-pros-and-cons.ipynb
loading modules,from sklearn.experimental import enable_iterative_imputer ,all-imputation-techniques-with-pros-and-cons.ipynb
"The KNNImputer class provides imputation for filling in missing values using the k Nearest Neighbors approach. By default, a euclidean distance metric that supports missing values, nan euclidean distances, is used to find the nearest neighbors. Each missing feature is imputed using values from n neighbors nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor. If a sample has more than one feature missing, then the neighbors for that sample can be different depending on the particular feature being imputed. When the number of available neighbors is less than n neighbors and there are no defined distances to the training set, the training set average for that feature is used during imputation. If there is at least one neighbor with a defined distance, the weighted or unweighted average of the remaining neighbors will be used during imputation. If a feature is always missing in training, it is removed during transform. ","from sklearn.impute import KNNImputer
nan = np.nan
X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]
imputer = KNNImputer(n_neighbors=2, weights=""uniform"")
imputed_x=imputer.fit_transform(X)
print(X,imputed_x)",all-imputation-techniques-with-pros-and-cons.ipynb
"The MissingIndicator transformer is useful to transform a dataset into corresponding binary matrix indicating the presence of missing values in the dataset. This transformation is useful in conjunction with imputation. When using imputation, preserving the information about which values had been missing can be informative. Note that both the SimpleImputer and IterativeImputer have the boolean parameter add indicator False by default which when set to True provides a convenient way of stacking the output of the MissingIndicator transformer with the output of the imputer.NaN is usually used as the placeholder for missing values. However, it enforces the data type to be float. The parameter missing values allows to specify other placeholder such as integer. In the following example, we will use 1 as missing values:","from sklearn.impute import MissingIndicator
X = np.array([[-1, -1, 1, 3],
 [4, -1, 0, -1],
 [8, -1, 1, 0]])
indicator = MissingIndicator(missing_values=-1)
mask_missing_values_only = indicator.fit_transform(X)
mask_missing_values_only",all-imputation-techniques-with-pros-and-cons.ipynb
output submission.py,"with open(""../input/welltrained/submission.py"" , ""r"")as f , open('submission.py' , 'w')as out : ",alphazero-baseline-connectx.ipynb
Ignore warnings,import warnings ,an-interactive-data-science-tutorial.ipynb
Handle table like data and matrices,import numpy as np ,an-interactive-data-science-tutorial.ipynb
Modelling Algorithms,from sklearn.tree import DecisionTreeClassifier ,an-interactive-data-science-tutorial.ipynb
Modelling Helpers,"from sklearn.preprocessing import Imputer , Normalizer , scale ",an-interactive-data-science-tutorial.ipynb
Visualisation,import matplotlib as mpl ,an-interactive-data-science-tutorial.ipynb
Configure visualisations,% matplotlib inline ,an-interactive-data-science-tutorial.ipynb
2.2 Setup helper Functions There is no need to understand this code. Just run it to simplify the code later in the tutorial.Simply run the cell below by selecting it and pressing the play button.,"def plot_histograms(df , variables , n_rows , n_cols): ",an-interactive-data-science-tutorial.ipynb
 var name var name Distribution ," ax.set_title('Skew: ' + str(round(float(df[var_name]. skew ()) ,))) ",an-interactive-data-science-tutorial.ipynb
Improves appearance a bit., fig.tight_layout () ,an-interactive-data-science-tutorial.ipynb
get titanic test csv files as a DataFrame,"train = pd.read_csv(""../input/train.csv"") ",an-interactive-data-science-tutorial.ipynb
"Run the code to see the variables, then read the variable description below to understand them.",titanic.head () ,an-interactive-data-science-tutorial.ipynb
"2.4.1 Next have a look at some key information about the variables An numeric variable is one with values of integers or real numbers while a categorical variable is a variable that can take on one of a limited, and usually fixed, number of possible values, such as blood type.Notice especially what type of variable each is, how many observations there are and some of the variable values.An interesting observation could for example be the minimum age 0.42, do you know why this is?Select the cell below and run it by pressing the play button.",titanic.describe(),an-interactive-data-science-tutorial.ipynb
2.4.2 A heat map of correlation may give us a understanding of which variables are important Select the cell below and run it by pressing the play button.,plot_correlation_map( titanic ),an-interactive-data-science-tutorial.ipynb
Plot distributions of Age of passangers who survived or did not survive,"plot_distribution(titanic , var = 'Age' , target = 'Survived' , row = 'Sex') ",an-interactive-data-science-tutorial.ipynb
Plot survival rate by Embarked,"plot_categories(titanic , cat = 'Embarked' , target = 'Survived') ",an-interactive-data-science-tutorial.ipynb
Transform Sex into binary values 0 and 1,"sex = pd.Series(np.where(full.Sex == 'male' , 1 , 0), name = 'Sex') ",an-interactive-data-science-tutorial.ipynb
Create a new variable for every unique value of Embarked,"embarked = pd.get_dummies(full.Embarked , prefix = 'Embarked') ",an-interactive-data-science-tutorial.ipynb
Create a new variable for every unique value of Embarked,"pclass = pd.get_dummies(full.Pclass , prefix = 'Pclass') ",an-interactive-data-science-tutorial.ipynb
Create dataset,imputed = pd.DataFrame () ,an-interactive-data-science-tutorial.ipynb
Fill missing values of Age with the average of Age mean ,imputed['Age']= full.Age.fillna(full.Age.mean ()) ,an-interactive-data-science-tutorial.ipynb
Fill missing values of Fare with the average of Fare mean ,imputed['Fare']= full.Fare.fillna(full.Fare.mean ()) ,an-interactive-data-science-tutorial.ipynb
3.3.1 Extract titles from passenger names Titles reflect social status and may predict survival probabilitySelect the cell below and run it by pressing the play button.,title = pd.DataFrame () ,an-interactive-data-science-tutorial.ipynb
we extract the title from each name,"title['Title']= full['Name']. map(lambda name : name.split(',')[ 1]. split('.')[ 0]. strip ()) ",an-interactive-data-science-tutorial.ipynb
3.3.2 Extract Cabin category information from the Cabin numberSelect the cell below and run it by pressing the play button.,cabin = pd.DataFrame () ,an-interactive-data-science-tutorial.ipynb
replacing missing cabins with U for Uknown ,cabin['Cabin']= full.Cabin.fillna('U') ,an-interactive-data-science-tutorial.ipynb
mapping each Cabin value with the cabin letter,cabin['Cabin']= cabin['Cabin']. map(lambda c : c[0]) ,an-interactive-data-science-tutorial.ipynb
dummy encoding ...,"cabin = pd.get_dummies(cabin['Cabin'], prefix = 'Cabin') ",an-interactive-data-science-tutorial.ipynb
"a function that extracts each prefix of the ticket, returns XXX if no prefix i.e the ticket is a digit ",def cleanTicket(ticket): ,an-interactive-data-science-tutorial.ipynb
Extracting dummy variables from tickets:,ticket['Ticket']= full['Ticket']. map(cleanTicket) ,an-interactive-data-science-tutorial.ipynb
3.3.4 Create family size and category for family size The two variables Parch and SibSp are used to create the famiy size variableSelect the cell below and run it by pressing the play button.,family = pd.DataFrame () ,an-interactive-data-science-tutorial.ipynb
introducing a new feature : the size of families including the passenger ,family['FamilySize']= full['Parch']+ full['SibSp']+ 1 ,an-interactive-data-science-tutorial.ipynb
introducing other features based on the family size,family['Family_Single']= family['FamilySize']. map(lambda s : 1 if s == 1 else 0) ,an-interactive-data-science-tutorial.ipynb
"imputed , embarked , pclass , sex , family , cabin , ticket","full_X = pd.concat ([imputed , embarked , cabin , sex], axis = 1) ",an-interactive-data-science-tutorial.ipynb
"Create all datasets that are necessary to train, validate and test models",train_valid_X = full_X[0 : 891] ,an-interactive-data-science-tutorial.ipynb
3.4.3 Feature importance Selecting the optimal features in the model is important. We will now try to evaluate what the most important variables are for the model to make the prediction.Select the cell below and run it by pressing the play button.,"plot_variable_importance(train_X, train_y)",an-interactive-data-science-tutorial.ipynb
4.1.1 Random Forests Model Try a random forest model by running the cell below. ,model = RandomForestClassifier(n_estimators=100),an-interactive-data-science-tutorial.ipynb
4.1.2 Support Vector Machines Try a Support Vector Machines model by running the cell below. ,model = SVC(),an-interactive-data-science-tutorial.ipynb
4.1.3 Gradient Boosting Classifier Try a Gradient Boosting Classifier model by running the cell below. ,model = GradientBoostingClassifier(),an-interactive-data-science-tutorial.ipynb
4.1.4 K nearest neighbors Try a k nearest neighbors model by running the cell below. ,model = KNeighborsClassifier(n_neighbors = 3),an-interactive-data-science-tutorial.ipynb
4.1.5 Gaussian Naive Bayes Try a Gaussian Naive Bayes model by running the cell below. ,model = GaussianNB(),an-interactive-data-science-tutorial.ipynb
4.1.6 Logistic Regression Try a Logistic Regression model by running the cell below. ,model = LogisticRegression(),an-interactive-data-science-tutorial.ipynb
4.2 Train the selected model When you have selected a dataset with the features you want and a model you would like to try it is now time to train the model. After all our preparation model training is simply done with the one line below.Select the cell below and run it by pressing the play button.,"model.fit( train_X , train_y )",an-interactive-data-science-tutorial.ipynb
Score the model,"print(model.score(train_X , train_y), model.score(valid_X , valid_y)) ",an-interactive-data-science-tutorial.ipynb
5.2.1 Automagic It s also possible to automatically select the optimal number of features and visualize this. This is uncommented and can be tried in the competition part of the tutorial.Select the cell below and run it by pressing the play button.,"rfecv = RFECV(estimator = model , step = 1 , cv = StratifiedKFold(train_y , 2), scoring = 'accuracy') ",an-interactive-data-science-tutorial.ipynb
 DeploymentDeployment in this context means publishing the resulting prediction from the model to the Kaggle leaderboard. To do this do the following: select the cell below and run it by pressing the play button. Press the Publish button in top right corner. Select Output on the notebook menubar Select the result dataset and press Submit to Competition button ,"test_Y = model.predict( test_X )
passenger_id = full[891:].PassengerId
test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )
test.shape
test.head()
test.to_csv( 'titanic_pred.csv' , index = False )",an-interactive-data-science-tutorial.ipynb
A refresh of this dataset would be Awesome!Lets give it some new life with spaCy,import spacy ,and-the-winner-is-spacy-render.ipynb
spacy.prefer gpu ,nlp = spacy.load('en_core_web_sm') ,and-the-winner-is-spacy-render.ipynb
How does spaCy workThe following SVG graphic from their site explains a lot more that I could ,"from IPython.display import Image
Image(url='https://spacy.io/assets/img/architecture.svg')",and-the-winner-is-spacy-render.ipynb
test style ent with different options,"displacy.render(list(doc_sents)[ : 1], style = 'dep' , page = False , jupyter = True , options = { 'compact' : True }) ",and-the-winner-is-spacy-render.ipynb
Tagging Text Made Simple,"tokens = pd.DataFrame(
 [[token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop, [child for child in token.children]] for token in doc],
 columns=['text', 'lemma', 'pos', 'tag', 'dep', 'shape', 'is_alpha', 'is_stop', 'child'])
tokens.head()",and-the-winner-is-spacy-render.ipynb
Named EntitiesThis would have been really helpfull in the two competitions below: ,"ent = pd.DataFrame(
 [[ent.text, ent.start_char, ent.end_char, ent.label_] for ent in doc.ents],
 columns=['text', 'start_char', 'end_char', 'label'])
ent.head()",and-the-winner-is-spacy-render.ipynb
Lets review the Word Token Stop Words feature There seems to be a little overlap on the Stop Words without preprocessing to lower,"tokens[((tokens['is_stop']==False) & (tokens['is_alpha']==True))]['text'].str.lower().value_counts()[:20].reset_index().plot(kind='bar', x='index', y='text')",and-the-winner-is-spacy-render.ipynb
"A solid new Natural Language Packag with many more features available for training and further segmentation, could have used this on the Home Depot Competition : Enjoy the help guide: Also enjoy the Blog Posting: ","import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
np.random.seed(14)

p = '../input/street-view-getting-started-with-julia/'
st_chars = pd.read_csv(p + '/trainLabels.csv')
st_chars['path'] = st_chars['ID'].map(lambda x: p + 'train/train/' + str(x) + '.Bmp')

s = 'Happy Kaggling'
fig=plt.figure(figsize=(6, 4))
for i in range(len(s)):
 fig.add_subplot(2, 8, i+1)
 if s[i]==' ':
 img = Image.new('RGB', (10,10), (255, 255, 255))
 else:
 p = np.random.choice(st_chars[st_chars['Class']==s[i]]['path'].values)
 img = Image.open(p)
 plt.imshow(img); plt.axis('off')
plt.show()",and-the-winner-is-spacy-render.ipynb
You can use translations to augment the training data and get extra power from your model. Variations of this method were used in the Jigsaw Multilingual Toxic Comment Classification and other competitions.I ll demo the method on the excellent notebook which scores 0.774. I ll translate non English text to English and English text to another language.,!pip install git+https://github.com/ssut/py-googletrans.git,augmenting-data-with-translations.ipynb
Here s the training data with a quick check of language distribution. ,"train = pd.read_csv('../input/contradictory-my-dear-watson/train.csv', index_col=['id'])
display(train, train.lang_abv.value_counts())",augmenting-data-with-translations.ipynb
"Googletrans is fairly fast. Even so, I highly recommend you use multiprocessing of some sort to speed translation.","def translate(words , dest): ",augmenting-data-with-translations.ipynb
"Now we build a model, tune it, and predict as with the original. In addition to using the multilingual model, you can translate everything to a single language and use a model for that language. ","from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
import transformers
from transformers import TFAutoModel, AutoTokenizer",augmenting-data-with-translations.ipynb
"Detect hardware, return appropriate distribution strategy",try : ,augmenting-data-with-translations.ipynb
set: this is always the case on Kaggle., tpu = tf.distribute.cluster_resolver.TPUClusterResolver () ,augmenting-data-with-translations.ipynb
Default distribution strategy in Tensorflow. Works on CPU and single GPU., strategy = tf.distribute.get_strategy () ,augmenting-data-with-translations.ipynb
Our batch size will depend on number of replicas,batch_size = 16 * strategy.num_replicas_in_sync ,augmenting-data-with-translations.ipynb
First load the tokenizer,tokenizer = AutoTokenizer.from_pretrained(model_name) ,augmenting-data-with-translations.ipynb
Convert the text so that we can feed it to batch encode plus,"train_text = train[[ 'premise' , 'hypothesis']].values.tolist () ",augmenting-data-with-translations.ipynb
"Now, we use the tokenizer we loaded to encode the text","auto = tf.data.experimental.AUTOTUNE

train_dataset = (
 tf.data.Dataset
 .from_tensor_slices((x_train, y_train))
 .repeat()
 .shuffle(2048)
 .batch(batch_size)
 .prefetch(auto)
)

valid_dataset = (
 tf.data.Dataset
 .from_tensor_slices((x_valid, y_valid))
 .batch(batch_size)
 .cache()
 .prefetch(auto)
)

test_dataset = (
 tf.data.Dataset
 .from_tensor_slices(x_test)
 .batch(batch_size)
)",augmenting-data-with-translations.ipynb
First load the transformer layer, transformer_encoder = TFAutoModel.from_pretrained(model_name) ,augmenting-data-with-translations.ipynb
This will be the input tokens," input_ids = Input(shape =(max_len ,), dtype = tf.int32 , name = ""input_ids"") ",augmenting-data-with-translations.ipynb
"Now, we encode the text using the transformers we just loaded", sequence_output = transformer_encoder(input_ids)[ 0] ,augmenting-data-with-translations.ipynb
"Only extract the token used for classification, which is "," cls_token = sequence_output[: , 0 , :] ",augmenting-data-with-translations.ipynb
"Finally, pass it through a 3 way softmax, since there s 3 possible labels"," out = Dense(3 , activation = 'softmax')( cls_token) ",augmenting-data-with-translations.ipynb
It s time to build and compile the model," model = Model(inputs = input_ids , outputs = out) ",augmenting-data-with-translations.ipynb
linear algebra,import numpy as np ,automl-getting-started-notebook.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,automl-getting-started-notebook.ipynb
workaround to fix gapic v1 error,from google.api_core.gapic_v1.client_info import ClientInfo ,automl-getting-started-notebook.ipynb
Set your own values for these. bucket name should be the project id lcm .,PROJECT_ID = 'cloudml-demo' ,automl-getting-started-notebook.ipynb
Region must be us central1,region = 'us-central1' ,automl-getting-started-notebook.ipynb
adding ClientInfo here to get the gapic v1 call in place,client = automl.AutoMlClient(client_info = ClientInfo ()) ,automl-getting-started-notebook.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,automl-getting-started-notebook.ipynb
Any results you write to the current directory are saved as output.,"nlp_train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')
nlp_test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')
def callback(operation_future):
 result = operation_future.result()",automl-getting-started-notebook.ipynb
Data spelunking How often does fire come up in this dataset?,"nlp_train_df.loc[nlp_train_df['text'].str.contains('fire', na=False, case=False)]",automl-getting-started-notebook.ipynb
Does the presence of the word fire help determine whether the tweets here are real or false?,"nlp_train_df.loc[nlp_train_df['text'].str.contains('fire', na=False, case=False)].target.value_counts()",automl-getting-started-notebook.ipynb
GCS upload download utilities These functions make upload and download of files from the kernel to Google Cloud Storage easier. This is needed for AutoML,"def upload_blob(bucket_name, source_file_name, destination_blob_name):
 """"""Uploads a file to the bucket. https://cloud.google.com/storage/docs/ """"""
 bucket = storage_client.get_bucket(bucket_name)
 blob = bucket.blob(destination_blob_name)
 blob.upload_from_filename(source_file_name)
 print('File {} uploaded to {}'.format(
 source_file_name,
 'gs://' + bucket_name + '/' + destination_blob_name))
 
def download_to_kaggle(bucket_name,destination_directory,file_name,prefix=None):
 """"""Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook""""""
 os.makedirs(destination_directory, exist_ok = True)
 full_file_path = os.path.join(destination_directory, file_name)
 blobs = storage_client.list_blobs(bucket_name,prefix=prefix)
 for blob in blobs:
 blob.download_to_filename(full_file_path)",automl-getting-started-notebook.ipynb
"Select the text body and the target value, for sending to AutoML NL","nlp_train_df[[ 'text' , 'target']].to_csv('train.csv' , index = False , header = False) ",automl-getting-started-notebook.ipynb
Create our class instance,"amw = AutoMLWrapper(client=client, 
 project_id=PROJECT_ID, 
 bucket_name=bucket_name, 
 region='us-central1', 
 dataset_display_name=dataset_display_name, 
 model_display_name=model_display_name)
 ",automl-getting-started-notebook.ipynb
"Create or retreive dataset Check to see if this dataset already exists. If not, create it","print(f'Getting dataset ready at {datetime.fromtimestamp(time.time()).strftime(""%Y-%m-%d, %H:%M:%S UTC"")}')
if not amw.get_dataset_by_display_name(dataset_display_name):
 print('dataset not found')
 amw.create_dataset()
 amw.import_gcs_data(training_gcs_path)

amw.dataset
print(f'Dataset ready at {datetime.fromtimestamp(time.time()).strftime(""%Y-%m-%d, %H:%M:%S UTC"")}')
",automl-getting-started-notebook.ipynb
Kick off the training for the model And retrieve the training info after completion. Start model deployment.,"print(f'Getting model trained at {datetime.fromtimestamp(time.time()).strftime(""%Y-%m-%d, %H:%M:%S UTC"")}')

if not amw.get_model_by_display_name():
 print(f'Training model at {datetime.fromtimestamp(time.time()).strftime(""%Y-%m-%d, %H:%M:%S UTC"")}')
 amw.train_model()

print(f'Model trained. Ensuring model is deployed at {datetime.fromtimestamp(time.time()).strftime(""%Y-%m-%d, %H:%M:%S UTC"")}')
amw.deploy_model()
amw.model
print(f'Model trained and deployed at {datetime.fromtimestamp(time.time()).strftime(""%Y-%m-%d, %H:%M:%S UTC"")}')
",automl-getting-started-notebook.ipynb
"Prediction Note that prediction will not run until deployment finishes, which takes a bit of time. However, once you have your model deployed, this notebook won t re train the model, thanks to the various safeguards put in place. Instead, it will take the existing trained model and make predictions and generate the submission file.",nlp_test_df.head(),automl-getting-started-notebook.ipynb
Create client for prediction service.,prediction_client = automl.PredictionServiceClient () ,automl-getting-started-notebook.ipynb
optional Undeploy model Undeploy the model to stop charges,amw.undeploy_model(),automl-getting-started-notebook.ipynb
Create submission output,predictions_df.head(),automl-getting-started-notebook.ipynb
nlp test df id ,"submission_df = submission_df.rename(columns={'class':'target'})
submission_df.head()",automl-getting-started-notebook.ipynb
Submit predictions to the competition!,"submission_df.to_csv(""submission.csv"", index=False, header=True)",automl-getting-started-notebook.ipynb
Import,"import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
import re
import os
import html
import matplotlib.pyplot as plt
import seaborn as sb

from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from gensim.models import Word2Vec, Phrases
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Bidirectional
from keras.layers.embeddings import Embedding
from keras.preprocessing.sequence import pad_sequences",bag-of-words-meets-bags-of-popcorn-randomforest.ipynb
EDA,"train=pd.read_csv(""../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip"",'\t')
test=pd.read_csv(""../input/word2vec-nlp-tutorial/testData.tsv.zip"",""\t"")
att=pd.read_csv(""../input/imdb-review-dataset/imdb_master.csv"")",bag-of-words-meets-bags-of-popcorn-randomforest.ipynb
Clean Data,df.review = html.unescape(df.review) ,bag-of-words-meets-bags-of-popcorn-randomforest.ipynb
removing html symbols,"df.review = df.review.str.replace('http\S+|www.\S+' , '' , case = False). str.replace(r""\&\#[0-9]+\;"" , """" , regex = True) \ ",bag-of-words-meets-bags-of-popcorn-randomforest.ipynb
Tokenization,"corpora = df[""review""].values
tokenized = [word_tokenize(corpus) for corpus in corpora]

print(tokenized[2222])",bag-of-words-meets-bags-of-popcorn-randomforest.ipynb
Word Embedding for Feature Engineering,"%%time
from gensim.models import word2vec
np.set_printoptions(suppress=True)

feature_size = 256
context_size = 5
min_word = 1

word_vec= word2vec.Word2Vec(tokenized, vector_size=feature_size, \
 window=context_size, min_count=min_word, \
 epochs=50, seed=42)",bag-of-words-meets-bags-of-popcorn-randomforest.ipynb
Final Dataframe,"word_vec_unpack = [(word, idx) for word, idx in \
 word_vec.wv.key_to_index.items()]

tokens, indexes = zip(*word_vec_unpack)

word_vec_df = pd.DataFrame(word_vec.wv.vectors[indexes, :], index=tokens)

word_vec_df.head()",bag-of-words-meets-bags-of-popcorn-randomforest.ipynb
Model Predict,"r.fit(text,y)
pred=r.predict(test)
pred = pred.astype(int)
pred",bag-of-words-meets-bags-of-popcorn-randomforest.ipynb
linear algebra,import numpy as np ,bag-of-words-meets-bags-of-popcorn.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,bag-of-words-meets-bags-of-popcorn.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,bag-of-words-meets-bags-of-popcorn.ipynb
data processing and data level operation,import pandas as pd ,bag-of-words-meets-bags-of-popcorn.ipynb
Linear Algebra,import numpy as np ,bag-of-words-meets-bags-of-popcorn.ipynb
Visualization libraries,import matplotlib.pyplot as plt ,bag-of-words-meets-bags-of-popcorn.ipynb
Current working directory,"print('current workind directory ==== ' , os.getcwd ()) ",bag-of-words-meets-bags-of-popcorn.ipynb
Loading data,"train = pd.read_csv('/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip' , delimiter = '\t') ",bag-of-words-meets-bags-of-popcorn.ipynb
Creating a new col,train['length']= train['review']. apply(len) ,bag-of-words-meets-bags-of-popcorn.ipynb
Histogram of count of letters,train['length']. plot.hist(bins = 100) ,bag-of-words-meets-bags-of-popcorn.ipynb
Creating a function for cleaning of data,def clean_text(raw_text): ,bag-of-words-meets-bags-of-popcorn.ipynb
1. remove HTML tags, raw_text = BeautifulSoup(raw_text). get_text () ,bag-of-words-meets-bags-of-popcorn.ipynb
2. removing all non letters from text," letters_only = re.sub(""[^a-zA-Z]"" , "" "" , raw_text) ",bag-of-words-meets-bags-of-popcorn.ipynb
"3. Convert to lower case, split into individual words", words = letters_only.lower (). split () ,bag-of-words-meets-bags-of-popcorn.ipynb
4. Create variable which contain set of stopwords," stops = set(stopwords.words(""english"")) ",bag-of-words-meets-bags-of-popcorn.ipynb
5. Remove stop word returning, return[w for w in words if not w in stops] ,bag-of-words-meets-bags-of-popcorn.ipynb
Cleaning review and also adding a new col as its len count of words,train['clean_review']= train['review']. apply(clean_text) ,bag-of-words-meets-bags-of-popcorn.ipynb
Checking the smallest review,print(train[train['length_clean_review']== 4][ 'review']. iloc[0]) ,bag-of-words-meets-bags-of-popcorn.ipynb
Vectorization,from sklearn.feature_extraction.text import CountVectorizer,bag-of-words-meets-bags-of-popcorn.ipynb
bow bag of word,bow_transform = CountVectorizer(analyzer = clean_text). fit(train['review']) ,bag-of-words-meets-bags-of-popcorn.ipynb
Print total number of vocab words,print(len(bow_transform.vocabulary_)) ,bag-of-words-meets-bags-of-popcorn.ipynb
Creating bag of words for our review variable,review_bow = bow_transform.transform(train['review']) ,bag-of-words-meets-bags-of-popcorn.ipynb
TF IDF,"from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer().fit(review_bow)
tfidf1 = tfidf_transformer.transform(bow1)
print(tfidf1)",bag-of-words-meets-bags-of-popcorn.ipynb
Modeling Part,"from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(train['review'], train['sentiment'], test_size=0.22, random_state=101)

len(X_train), len(X_test), len(X_train) + len(X_test)",bag-of-words-meets-bags-of-popcorn.ipynb
Result Function,from sklearn.metrics import classification_report ,bag-of-words-meets-bags-of-popcorn.ipynb
Predicting Stats Function,"def pred(predicted , compare): ",bag-of-words-meets-bags-of-popcorn.ipynb
check accuracy of model, print('Classification paradox :------->>') ,bag-of-words-meets-bags-of-popcorn.ipynb
"Training Model, Logistic Regression",from sklearn.linear_model import LogisticRegression ,bag-of-words-meets-bags-of-popcorn.ipynb
Test Set Result,predictions = pipeline.predict(X_test) ,bag-of-words-meets-bags-of-popcorn.ipynb
Saving Output,from sklearn.linear_model import LogisticRegression ,bag-of-words-meets-bags-of-popcorn.ipynb
linear algebra,import numpy as np ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
for removing HTML tags,from bs4 import BeautifulSoup ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
removing punctuation and numbers,import re ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
removing stop words,import nltk ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
word vectors word2vec ,import gensim ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
speeding up training,import cython ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
"Data Cleaning Here, I ll do the data preprocessing removing HTML tags, removing punctuation, removing unnecessary newlines spaces and lower casing, removing stop words, stemming, and lemmatization .",from nltk.corpus import stopwords ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
inspired by tutorial,"def review_to_wordlist(review , remove_stop_words = False): ",bag-of-words-word2vec-for-sentiment-analysis.ipynb
"Creating Bag of Words Representation Here, I will create the bag of words representation of the train and test data with CountVectorizer.",from sklearn.feature_extraction.text import CountVectorizer ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
"Using RandomForestClassifier on Bag of Words Finally, for the Bag of Words data, I will train a RandomForest model and use it to predict the outcomes on the test data.","from sklearn.ensemble import RandomForestClassifier

model_bow = RandomForestClassifier(n_estimators=100)
model_bow.fit(train_data_features, train_data[""sentiment""])",bag-of-words-word2vec-for-sentiment-analysis.ipynb
Final predictions with the bag of words and XGBoost model,predictions_bow = model_bow.predict(test_data_features) ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
Load punkt tokenizer,tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
Function to split review into sentences,"def review_to_sentences(review , tokenizer , remove_stopwords = False): ",bag-of-words-word2vec-for-sentiment-analysis.ipynb
Using tokenizer to split paragraph into sentences, sentences1 = tokenizer.tokenize(review.strip ()) ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
go over each sentence, sentences = [] ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
call review to wordlist," sentences.append(review_to_wordlist(sentence , remove_stopwords)) ",bag-of-words-word2vec-for-sentiment-analysis.ipynb
"Next, I ll apply the sentence splitter to the reviews to get the sentences to train the word2vec model.",sentences = [] ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
adding train sentences together in a list for training the word2vec model not removing stop words for better model training ,"for i in range(0 , train_data[""review""]. size): ",bag-of-words-word2vec-for-sentiment-analysis.ipynb
adding unlabeled train sentences in a list for training the word2vec model,"for i in range(0 , unlabeled_train_data[""review""]. size): ",bag-of-words-word2vec-for-sentiment-analysis.ipynb
Taking a look at a sentence in sentences,print(sentences[0]) ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
Import the built in logging module and configure it so that Word2Vec creates nice output messages from tutorial ,import logging ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
should be is,"print(model_w2v.wv.doesnt_match(""where when what why who is"".split ())) ",bag-of-words-word2vec-for-sentiment-analysis.ipynb
should be servant,"print(model_w2v.wv.doesnt_match(""king queen servant"".split ())) ",bag-of-words-word2vec-for-sentiment-analysis.ipynb
"mostly seem correct, either move as in walking or move as in pushing though furiou doesn t really match ","print(model_w2v.wv.most_similar(""move"")) ",bag-of-words-word2vec-for-sentiment-analysis.ipynb
"Using Average Word Vectors If we average the individual vectors in a text, the final vector can usually give good results when training though the sentence order is lost . Therefore, we will be using average word vectors to convert our reviews.","def review_to_vector(review , model , num_features): ",bag-of-words-word2vec-for-sentiment-analysis.ipynb
initializing an empty np array," feature_vector = np.zeros(( num_features ,), dtype = ""float32"") ",bag-of-words-word2vec-for-sentiment-analysis.ipynb
set of words in model s vocabulary, set_of_words = set(model.wv.index_to_key) ,bag-of-words-word2vec-for-sentiment-analysis.ipynb
"Using RandomForestClassifier on Word Vectors Finally, for the Word Vector data, I will train a RandomForest model and use it to predict the outcomes on the test data.","model_vec = RandomForestClassifier(n_estimators = 100)
model_vec.fit(train_data_vectors, train_data[""sentiment""])",bag-of-words-word2vec-for-sentiment-analysis.ipynb
Importing Libraries,"import numpy as np
import pandas as pd",bag-of-words.ipynb
Importing Dataset,"dataset = pd.read_csv('../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip',delimiter=""\t"")",bag-of-words.ipynb
Data Cleaning,"import re
from bs4 import BeautifulSoup
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer",bag-of-words.ipynb
replaced punctuations with spaces," reviews = re.sub('[^a-zA-Z]' , ' ' , letters_only) ",bag-of-words.ipynb
Splitting Data into train set and test set,"from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=1250)
x = cv.fit_transform(corpus).toarray()
y = dataset.iloc[:,1].values

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)",bag-of-words.ipynb
Training the Logistic Model on Training set,"from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression()
classifier.fit(x_train,y_train)",bag-of-words.ipynb
Predicting the test result,"from sklearn.metrics import confusion_matrix,accuracy_score, classification_report

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))",bag-of-words.ipynb
ROC Curve,"import matplotlib.pyplot as plt
%matplotlib inline",bag-of-words.ipynb
create ROC curve,"plt.plot(fpr , tpr) ",bag-of-words.ipynb
Submit the prediction,"testset = pd.read_csv('../input/word2vec-nlp-tutorial/testData.tsv.zip',delimiter=""\t"")",bag-of-words.ipynb
replaced punctuations with spaces," reviews = re.sub('[^a-zA-Z]' , ' ' , letters_only) ",bag-of-words.ipynb
linear algebra,import numpy as np ,baseline-model-using-nn-for-movie-review.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,baseline-model-using-nn-for-movie-review.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,baseline-model-using-nn-for-movie-review.ipynb
Any results you write to the current directory are saved as output.,"import zipfile

files=['/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip',
 '/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip',
 '/kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip']

for file in files :
 zip = zipfile.ZipFile(file,'r')
 zip.extractall()
 zip.close()",baseline-model-using-nn-for-movie-review.ipynb
 EDA of review texts Character distriubtion of each review Word distriubtion of each review Word cloud of each word Distribution by Sentiment class Ratio with special characters ,train.head(),baseline-model-using-nn-for-movie-review.ipynb
Distribution of words in one review is similar both in train and test set The mean words count is 233 and std is 173 words The character count seems to show similar distribution with word count ,from wordcloud import WordCloud ,baseline-model-using-nn-for-movie-review.ipynb
join function can help merge all words into one string. means space can be a sep between words.,"cloud = WordCloud(width = 800 , height = 600). generate("" "".join(train['review'])) ",baseline-model-using-nn-for-movie-review.ipynb
"br is the most frequent one. But br is a sort of HTML tag, Thus it should be removed. movie or film is the theme which all reviews share. Thus I suppose idf inverse document frequency shoul be close to zero ","fig, axe = plt.subplots(1,3, figsize=(23,5))
sns.countplot(train['sentiment'], ax=axe[0])
sns.boxenplot(x=train['sentiment'], y=train['length'], data=train, ax=axe[1])
sns.boxenplot(x=train['sentiment'], y=train['word_n'], data=train, ax=axe[2])",baseline-model-using-nn-for-movie-review.ipynb
"The distribution of sentiment is half and half between zero and one The review length distribution by sentiment is similar but if somebody feels harshly dissatisfied, the reveiw tends to be wordy more outliers ","print('the review with question mark is {}'.format(np.mean(train['review'].apply(lambda x : '?' in x))))
print('the review with fullstop mark is {}'.format(np.mean(train['review'].apply(lambda x : '.' in x))))
print('the ratio of the first capital letter is {}'.format(np.mean(train['review'].apply(lambda x : x[0].isupper()))))
print('the ratio with the capital letter is {}'.format(np.mean(train['review'].apply(lambda x : max(y.isupper() for y in x)))))
print('the ratio with the number is {}'.format(np.mean(train['review'].apply(lambda x : max(y.isdigit() for y in x)))))",baseline-model-using-nn-for-movie-review.ipynb
" Preprocessing Remove HTML tags such as br using BeautifulSoup Only english character will remain using regular expression By NLTK, stopwords will be eliminated ","import re
import json
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer",baseline-model-using-nn-for-movie-review.ipynb
review column : 2," review = train.iloc[i , 2] ",baseline-model-using-nn-for-movie-review.ipynb
review column : 1," review = test.iloc[i , 1] ",baseline-model-using-nn-for-movie-review.ipynb
"After preprocessing, the distribution by sentiment in train data is not so different from previous state except total counts ","from keras.preprocessing.text import Tokenizer
tk = Tokenizer()
tk.fit_on_texts(list(train['review'])+list(test['review']))
text_seq_tr=tk.texts_to_sequences(train['review'])
text_seq_te=tk.texts_to_sequences(test['review'])
word_ind=tk.word_index",baseline-model-using-nn-for-movie-review.ipynb
"Usiung keras, tokenization and mapping to numbers are done When fitting, use all data from train and text data set, which prevents model from errors ","print('Total word count is :',len(word_ind))",baseline-model-using-nn-for-movie-review.ipynb
"max length is set, if length more than max length, zero value will replace that place ","from sklearn.model_selection import train_test_split
x_train, x_valid, y_train, y_valid = train_test_split(pad_train, train['sentiment'], random_state=77, test_size=0.07, stratify=train['sentiment'])",baseline-model-using-nn-for-movie-review.ipynb
"use validation set, when we make a model. test size is set in between 5 to 10 , to use more data ",len(tk.word_index),baseline-model-using-nn-for-movie-review.ipynb
 Modeling sequential model using adam optimizer set early stopping and model checkpoint patient option ,"from keras import Sequential
from keras.layers import Dense, Embedding, Flatten

model=Sequential()
model.add(Embedding(101247,65, input_length=400))
model.add(Flatten())
model.add(Dense(2,activation='softmax'))
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'] )",baseline-model-using-nn-for-movie-review.ipynb
Use 0.5 as thereshold to specify one or zero ,"sub=sub[['id','sentiment']]",baseline-model-using-nn-for-movie-review.ipynb
Using Pandas to Get Familiar With Your DataThe first step in any machine learning project is familiarize yourself with the data. You ll use the Pandas library for this. Pandas is the primary tool data scientists use for exploring and manipulating data. Most people abbreviate pandas in their code as pd. We do this with the command,import pandas as pd,basic-data-exploration.ipynb
save filepath to variable for easier access,melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv' ,basic-data-exploration.ipynb
read the data and store data in DataFrame titled melbourne data,melbourne_data = pd.read_csv(melbourne_file_path) ,basic-data-exploration.ipynb
print a summary of the data in Melbourne data,melbourne_data.describe () ,basic-data-exploration.ipynb
Importing required Libraries.,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from nltk.corpus import stopwords
from nltk.util import ngrams
from sklearn.feature_extraction.text import CountVectorizer
from collections import defaultdict
from collections import Counter
plt.style.use('ggplot')
stop=set(stopwords.words('english'))
import re
from nltk.tokenize import word_tokenize
import gensim
import string
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tqdm import tqdm
from keras.models import Sequential
from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D
from keras.initializers import Constant
from sklearn.model_selection import train_test_split
from keras.optimizers import Adam

",basic-eda-cleaning-and-glove.ipynb
Loading the data and getting basic idea,"tweet= pd.read_csv('../input/nlp-getting-started/train.csv')
test=pd.read_csv('../input/nlp-getting-started/test.csv')
tweet.head(3)",basic-eda-cleaning-and-glove.ipynb
"Before we begin with anything else,let s check the class distribution.There are only two classes 0 and 1.","x=tweet.target.value_counts()
sns.barplot(x.index,x)
plt.gca().set_ylabel('samples')",basic-eda-cleaning-and-glove.ipynb
Number of characters in tweets,"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
tweet_len=tweet[tweet['target']==1]['text'].str.len()
ax1.hist(tweet_len,color='red')
ax1.set_title('disaster tweets')
tweet_len=tweet[tweet['target']==0]['text'].str.len()
ax2.hist(tweet_len,color='green')
ax2.set_title('Not disaster tweets')
fig.suptitle('Characters in tweets')
plt.show()
",basic-eda-cleaning-and-glove.ipynb
Number of words in a tweet,"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
tweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))
ax1.hist(tweet_len,color='red')
ax1.set_title('disaster tweets')
tweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))
ax2.hist(tweet_len,color='green')
ax2.set_title('Not disaster tweets')
fig.suptitle('Words in a tweet')
plt.show()
",basic-eda-cleaning-and-glove.ipynb
Average word length in a tweet,"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
word=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')
ax1.set_title('disaster')
word=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')
ax2.set_title('Not disaster')
fig.suptitle('Average word length in each tweet')",basic-eda-cleaning-and-glove.ipynb
First we will analyze tweets with class 0.,"corpus=create_corpus(0)

dic=defaultdict(int)
for word in corpus:
 if word in stop:
 dic[word]+=1
 
top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] 
",basic-eda-cleaning-and-glove.ipynb
"Now,we will analyze tweets with class 1.","corpus=create_corpus(1)

dic=defaultdict(int)
for word in corpus:
 if word in stop:
 dic[word]+=1

top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] 
 


x,y=zip(*top)
plt.bar(x,y)",basic-eda-cleaning-and-glove.ipynb
First let s check tweets indicating real disaster.,"plt.figure(figsize=(10,5))
corpus=create_corpus(1)

dic=defaultdict(int)
import string
special = string.punctuation
for i in (corpus):
 if i in special:
 dic[i]+=1
 
x,y=zip(*dic.items())
plt.bar(x,y)",basic-eda-cleaning-and-glove.ipynb
"Now,we will move on to class 0.","plt.figure(figsize=(10,5))
corpus=create_corpus(0)

dic=defaultdict(int)
import string
special = string.punctuation
for i in (corpus):
 if i in special:
 dic[i]+=1
 
x,y=zip(*dic.items())
plt.bar(x,y,color='green')",basic-eda-cleaning-and-glove.ipynb
Common words ?,"
counter=Counter(corpus)
most=counter.most_common()
x=[]
y=[]
for word,count in most[:40]:
 if (word not in stop) :
 x.append(word)
 y.append(count)",basic-eda-cleaning-and-glove.ipynb
we will do a bigram n 2 analysis over the tweets.Let s check the most common bigrams in tweets.,"def get_top_tweet_bigrams(corpus, n=None):
 vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)
 bag_of_words = vec.transform(corpus)
 sum_words = bag_of_words.sum(axis=0) 
 words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
 words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
 return words_freq[:n]",basic-eda-cleaning-and-glove.ipynb
"Data Cleaning As we know,twitter tweets always have to be cleaned before we go onto modelling.So we will do some basic cleaning such as spelling correction,removing punctuations,removing html tags and emojis etc.So let s start.","df=pd.concat([tweet,test])
df.shape",basic-eda-cleaning-and-glove.ipynb
Removing urls,"example=""New competition launched :https://www.kaggle.com/c/nlp-getting-started""",basic-eda-cleaning-and-glove.ipynb
Removing HTML tags,"example = """"""<div>
<h1>Real or Fake</h1>
<p>Kaggle </p>
<a href=""https://www.kaggle.com/c/nlp-getting-started"">getting started</a>
</div>""""""",basic-eda-cleaning-and-glove.ipynb
Reference : ,def remove_emoji(text): ,basic-eda-cleaning-and-glove.ipynb
Removing punctuations,"def remove_punct(text):
 table=str.maketrans('','',string.punctuation)
 return text.translate(table)

example=""I am a #king""
print(remove_punct(example))",basic-eda-cleaning-and-glove.ipynb
Even if I m not good at spelling I can correct it with python : I will use pyspellcheker to do that.,!pip install pyspellchecker,basic-eda-cleaning-and-glove.ipynb
"Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here.","
def create_corpus(df):
 corpus=[]
 for tweet in tqdm(df['text']):
 words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]
 corpus.append(words)
 return corpus
 
 ",basic-eda-cleaning-and-glove.ipynb
Baseline Model,"model=Sequential()

embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),
 input_length=MAX_LEN,trainable=False)

model.add(embedding)
model.add(SpatialDropout1D(0.2))
model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))


optimzer=Adam(learning_rate=1e-5)

model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])

",basic-eda-cleaning-and-glove.ipynb
Making our submission,sample_sub=pd.read_csv('../input/nlp-getting-started/sample_submission.csv'),basic-eda-cleaning-and-glove.ipynb
Load dataset,"Xdata , Ydata = load_dataset () ",basic-fully-connected-nn.ipynb
Preview dataset samples,"show_image(Xtrain[0], Ytrain[0]) ",basic-fully-connected-nn.ipynb
Train model,"model.fit(Xtrain , Ytrain , epochs = 500) ",basic-fully-connected-nn.ipynb
Load test data,def load_testset (): ,basic-fully-connected-nn.ipynb
Preview results on test data,def show_results(image_index): ,basic-fully-connected-nn.ipynb
linear algebra,import numpy as np ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import seaborn as sns ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
Natural Language Tool Kit,import nltk ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
 Load Data ,"train = pd.read_csv(""/kaggle/input/nlp-getting-started/train.csv"")
test = pd.read_csv(""/kaggle/input/nlp-getting-started/test.csv"")
submission = pd.read_csv(""/kaggle/input/nlp-getting-started/sample_submission.csv"")",basic-nlp-with-tensorflow-and-wordcloud.ipynb
"id a unique identifier for each tweet text the text of the tweet location the location the tweet was sent from may be blank keyword a particular keyword from the tweet may be blank target in train.csv only, this denotes whether a tweet is about a real disaster 1 or not 0 ",test.head(),basic-nlp-with-tensorflow-and-wordcloud.ipynb
 Visualization of data ,"missing = train.isnull().sum() 
missing[missing>0].sort_values(ascending=False).iplot(kind='bar',title='Null values present in train Dataset', color=['red'])
",basic-nlp-with-tensorflow-and-wordcloud.ipynb
Total of 221 unique keywords,train.keyword.nunique () ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
Check the top 15 locations,"train.location.value_counts ()[ : 20]. iplot(kind = 'bar' , title = 'Top 20 location in tweet' , color = 'blue') ",basic-nlp-with-tensorflow-and-wordcloud.ipynb
remove htps to the world Cloud,STOPWORDS.add('https') ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
typecaste each val to string, val = str(val) ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
split the value, tokens = val.split () ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
Converts each token into lowercase, for i in range(len(tokens)) : ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
How many http words has this text?,train.loc[train['text']. str.contains('http' )]. target.value_counts () ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
Remove all text that start with html,train['text']= train['text']. apply(lambda x : remove_html(x)) ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
lets check if this clean works,train.loc[train['text']. str.contains('http' )]. target.value_counts () ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
Remove all text that start with html in test,test['text']= test['text']. apply(lambda x : remove_html(x)) ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
"Now remove stopwords, pass to lower add delimiter and more",def clean_text(text): ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
split to array default delimiter is , text = text.split () ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
Apply clean text,train['text']= train['text']. apply(lambda x : clean_text(x)) ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
Apply clean text,test['text']= test['text']. apply(lambda x : clean_text(x)) ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
How many unique words have this text,def counter_word(text): ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
The maximum number of words to be used. most frequent ,vocab_size = len(counter) ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
Max number of words in each complaint.,max_length = 20 ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
oov took its set for words out our word index,"oov_tok = ""<XXX>"" ",basic-nlp-with-tensorflow-and-wordcloud.ipynb
"this is base in 80 of the data, an only text and targert at this moment",training_sentences = train.text[0 : training_size] ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
Lets see the first 10 elements,"print(""THe first word Index are: "") ",basic-nlp-with-tensorflow-and-wordcloud.ipynb
If you want to see completed word index,"training_sequences = tokenizer.texts_to_sequences(training_sentences)
training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)",basic-nlp-with-tensorflow-and-wordcloud.ipynb
check Inverse for see how it works,"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
",basic-nlp-with-tensorflow-and-wordcloud.ipynb
Lets see the first 10 elements,"print(""THe first reverse word Index are: "") ",basic-nlp-with-tensorflow-and-wordcloud.ipynb
If you want to see completed reverse word index,"def decode(text):
 return ' '.join([reverse_word_index.get(i, '?') for i in text])",basic-nlp-with-tensorflow-and-wordcloud.ipynb
this can be usefull for check predictions,decode(training_sequences[1]) ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
Model Definition with LSTM,"model.summary()
",basic-nlp-with-tensorflow-and-wordcloud.ipynb
predict clases because is classification problem with the split test,predictions = model.predict_classes(testing_padded) ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
Showing Confusion Matrix,"def plot_cm(y_true , y_pred , title , figsize =(5 , 4)) : ",basic-nlp-with-tensorflow-and-wordcloud.ipynb
Showing Confusion Matrix,"plot_cm(testing_labels , predictions , 'Confution matrix of Tweets' , figsize =(7 , 7)) ",basic-nlp-with-tensorflow-and-wordcloud.ipynb
Now working with test dataset,"
testing_sequences2 = tokenizer.texts_to_sequences(test.text)
testing_padded2 = pad_sequences(testing_sequences2, maxlen=max_length, padding=padding_type, truncating=trunc_type)",basic-nlp-with-tensorflow-and-wordcloud.ipynb
sample of submission,submission.head () ,basic-nlp-with-tensorflow-and-wordcloud.ipynb
Please switch on the TPU before running these lines.,! curl https : // raw.githubusercontent.com / pytorch / xla / master / contrib / scripts / env - setup.py - o pytorch - xla - env - setup.py ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Imports required to use TPUs with Pytorch.,import torch_xla ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
set a seed value,torch.manual_seed(555) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Bert Vocabulary,from transformers import BertTokenizer ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Instantiate the Bert tokenizer,"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased' , do_lower_case = True) ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
How many words tokens does Bert have in it s vocab?,len(tokenizer.vocab) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
This is how to see which tokens are associated with a particular word.,bert_vocab = tokenizer.vocab ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
"Given a token, this is how to see what word is associated with that token.",bert_keys = [] ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
"This value could be set as 256, 512 etc.",MAX_LEN = 10 ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
These have already been converted to torch tensors.,input_ids = encoded_dict['input_ids'][ 0] ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
For two input sentences,MAX_LEN = 15 ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
These are torch tensors.,print(input_ids) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
XLM RoBERTa Vocabulary,"from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification

MODEL_TYPE = 'xlm-roberta-base'

tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Check the vocab size,tokenizer.vocab_size ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
What are the special tokens,tokenizer.special_tokens_map ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
"This value could be set as 256, 512 etc.",MAX_LEN = 10 ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
These have already been converted to torch tensors.,input_ids = encoded_dict['input_ids'][ 0] ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
For two input sentences,MAX_LEN = 15 ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
These are torch tensors.,print(input_ids) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
input ids from above,input_ids = encoded_dict['input_ids'][ 0] ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
"This value could be set as 256, 512 etc.",MAX_LEN = 15 ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
"This value could be set as 256, 512 etc.",MAX_LEN = 15 ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Here you can see the overlap.,print(encoded_dict['input_ids']) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
2.1. Load the Data,os.listdir('../input/contradictory-my-dear-watson'),basics-of-bert-and-xlm-roberta-pytorch.ipynb
Load the training data.,path = '../input/contradictory-my-dear-watson/train.csv' ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Load the test data.,path = '../input/contradictory-my-dear-watson/test.csv' ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
2.2. Create 5 Folds,"from sklearn.model_selection import KFold , StratifiedKFold ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
shuffle,df = shuffle(df_train) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
initialize kfold,"kf = StratifiedKFold(n_splits = 5 , shuffle = True , random_state = 1024) ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
for stratification,y = df['label'] ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Put the folds into a list. This is a list of tuples.,"fold_list = list(kf.split(df , y)) ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
map the train and val index values to dataframe rows, df_train = df[df.index.isin(fold[0])] ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Display one train fold,df_train = train_df_list[0] ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Display one val fold,df_val = val_df_list[0] ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
3.1. Train a Bert Model,MODEL_TYPE = 'bert-base-multilingual-uncased' ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
"Therefore, will will only train on 3 folds.",NUM_FOLDS_TO_TRAIN = 3 ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
For TPU,device = xm.xla_device () ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Instantiate the tokenizer,from transformers import BertTokenizer ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Load the BERT tokenizer.,print('Loading BERT tokenizer...') ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Create the Dataloader,class CompDataset(Dataset): ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
get the sentence from the dataframe," sentence1 = self.df_data.loc[index , 'premise'] ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Test the dataloader,"df_train = df_train.reset_index(drop=True)
df_val = df_val.reset_index(drop=True)",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Get one train batch,"padded_token_list , att_mask , token_type_ids , target = next(iter(train_dataloader)) ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Get one val batch,"padded_token_list , att_mask , token_type_ids , target = next(iter(val_dataloader)) ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Get one test batch,"padded_token_list , att_mask , token_type_ids = next(iter(test_dataloader)) ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Get one train batch,"outputs = model(b_input_ids, 
 token_type_ids=b_token_type_ids, 
 attention_mask=b_input_mask,
 labels=b_labels)",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Inspect the model s output,outputs,basics-of-bert-and-xlm-roberta-pytorch.ipynb
"The output is a tuple: loss, preds ",len(outputs) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
This is the loss.,outputs[0] ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
These are the predictions.,outputs[1] ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
This is the accuracy without any fine tuning.,"val_acc = accuracy_score(y_true , y_pred) ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
The loss and preds are Torch tensors,print(type(outputs[0])) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
The same applies to the other fold models.,% % time ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Set a seed value.,seed_val = 1024 ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
the number of folds that the model is being trained on.,fold_val_acc_list = [] ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
append an empty list, fold_val_acc_list.append([]) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
For each epoch...,"for epoch in range(0 , NUM_EPOCHS): ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Get the number of folds, num_folds = len(train_df_list) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
We will use this list to calculate the cv at the end of the epoch., epoch_acc_scores_list = [] ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
For each fold...," for fold_index in range(0 , NUM_FOLDS_TO_TRAIN): ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
........................., if epoch == 0 : ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Check that the models have been saved,! ls ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
The same applies to the other fold models.,fold_val_acc_list ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Create the dataloader,test_data = TestDataset(df_test) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
 ,print('\nTest Set...') ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Load the fold model, path_model = 'model_' + str(fold_index)+ '.bin' ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Send the model to the device., model.to(device) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Put the model in evaluation mode., model.eval () ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
This step saves memory and speeds up validation., torch.set_grad_enabled(False) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Reset the total loss for this epoch., total_val_loss = 0 ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Sum the predictions of all fold models,"for i , item in enumerate(model_preds_list): ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Sum the matrices, preds = item + preds ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Average the predictions,avg_preds = preds /(len(model_preds_list)) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
The row order in the test set and the sample submission is the same.,path = '../input/contradictory-my-dear-watson/sample_submission.csv' ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Assign the preds to the prediction column,df_sample['prediction']= test_preds ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Create a submission csv file,"df_sample.to_csv('submission.csv' , index = False) ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Check that the fold models have been saved.,! ls ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Check the distribution of the predicted classes.,df_sample['prediction']. value_counts () ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
3.2. Train an XLM RoBERTa Model,"MODEL_TYPE = 'xlm-roberta-base'


L_RATE = 1e-5
MAX_LEN = 256

NUM_EPOCHS = 3
BATCH_SIZE = 32
NUM_CORES = os.cpu_count()

NUM_CORES",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Tell PyTorch to use the TPU.,device = xm.xla_device () ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Load the dataHere we will only use fold 0 for training.,"df_train = train_df_list[0]

df_train.head()",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Instantiate the Tokenizer,"from transformers import XLMRobertaTokenizer , XLMRobertaForSequenceClassification ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
xlm roberta large,print('Loading XLMRoberta tokenizer...') ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Create the Dataloader,"df_train = df_train.reset_index(drop=True)
df_val = df_val.reset_index(drop=True)",basics-of-bert-and-xlm-roberta-pytorch.ipynb
get the sentence from the dataframe," sentence1 = self.df_data.loc[index , 'premise'] ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Test the dataloader,"train_data = CompDataset(df_train)
val_data = CompDataset(df_val)
test_data = TestDataset(df_test)

train_dataloader = torch.utils.data.DataLoader(train_data,
 batch_size=BATCH_SIZE,
 shuffle=True,
 num_workers=NUM_CORES)

val_dataloader = torch.utils.data.DataLoader(val_data,
 batch_size=BATCH_SIZE,
 shuffle=True,
 num_workers=NUM_CORES)

test_dataloader = torch.utils.data.DataLoader(test_data,
 batch_size=BATCH_SIZE,
 shuffle=False,
 num_workers=NUM_CORES)



print(len(train_dataloader))
print(len(val_dataloader))
print(len(test_dataloader))",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Get one train batch,"padded_token_list , att_mask , target = next(iter(train_dataloader)) ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Get one val batch,"padded_token_list , att_mask , target = next(iter(val_dataloader)) ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Get one test batch,"padded_token_list , att_mask = next(iter(test_dataloader)) ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Define the Model,from transformers import XLMRobertaForSequenceClassification ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Pass a batch of train samples to the model.,batch = next(iter(train_dataloader)) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Send the data to the device,b_input_ids = batch[0]. to(device) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Inspect the model s output,outputs,basics-of-bert-and-xlm-roberta-pytorch.ipynb
"The output is a tuple: loss, preds ",len(outputs) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
This is the loss.,outputs[0] ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
These are the predictions.,outputs[1] ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
This is the accuracy without fine tuning.,"val_acc = accuracy_score(y_true , y_pred) ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
The loss and preds are Torch tensors,print(type(outputs[0])) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Create the dataloaders.,train_data = CompDataset(df_train) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Set the seed.,seed_val = 101 ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Store the average loss after each epoch so we can plot them.,loss_values = [] ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
For each epoch...,"for epoch in range(0 , NUM_EPOCHS): ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
 , print('Training...') ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
put the model into train mode, model.train () ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
This turns gradient calculations on and off., torch.set_grad_enabled(True) ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Reset the total loss for this epoch., total_train_loss = 0 ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Make a prediction on the test set,"for j , batch in enumerate(test_dataloader): ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Take the argmax. This returns the column index of the max value in each row.,"preds = np.argmax(stacked_preds , axis = 1) ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
The row order in the test set and the sample submission is the same.,path = '../input/contradictory-my-dear-watson/sample_submission.csv' ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Assign the preds to the prediction column,df_sample['prediction']= preds ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
"Therefore, it won t be possible to submit this csv file for leaderboard scoring.","df_sample.to_csv('xlmroberta_submission.csv' , index = False) ",basics-of-bert-and-xlm-roberta-pytorch.ipynb
Check that the model has been saved.,! ls ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Check the distribution of the predicted classes.,df_sample['prediction']. value_counts () ,basics-of-bert-and-xlm-roberta-pytorch.ipynb
Keeps track of columns that have missing values filled in., NAlist = [] ,binary-classification-autoviz-top-feature.ipynb
Exclude strings, if props[col]. dtype != object : ,binary-classification-autoviz-top-feature.ipynb
Print current column type," print(""******************************"") ",binary-classification-autoviz-top-feature.ipynb
"make variables for Int, max and min", IsInt = False ,binary-classification-autoviz-top-feature.ipynb
"Integer does not support NA, therefore, NA needs to be filled", if not np.isfinite(props[col]).all (): ,binary-classification-autoviz-top-feature.ipynb
test if column can be converted to an integer, asint = props[col]. fillna(0). astype(np.int64) ,binary-classification-autoviz-top-feature.ipynb
Make Integer unsigned Integer datatypes, if IsInt : ,binary-classification-autoviz-top-feature.ipynb
Make float datatypes 32 bit, else : ,binary-classification-autoviz-top-feature.ipynb
Print new column type," print(""dtype after: "" , props[col]. dtype) ",binary-classification-autoviz-top-feature.ipynb
Print final result," print(""___MEMORY USAGE AFTER COMPLETION:___"") ",binary-classification-autoviz-top-feature.ipynb
"IntroductionSo far in this course, we ve learned about how neural networks can solve regression problems. Now we re going to apply neural networks to another common machine learning problem: classification. Most everything we ve learned up until now still applies. The main difference is in the loss function we use and in what kind of outputs we want the final layer to produce.Binary ClassificationClassification into one of two classes is a common machine learning problem. You might want to predict whether or not a customer is likely to make a purchase, whether or not a credit card transaction was fraudulent, whether deep space signals show evidence of a new planet, or a medical test evidence of a disease. These are all binary classification problems.In your raw data, the classes might be represented by strings like Yes and No , or Dog and Cat . Before using this data we ll assign a class label: one class will be 0 and the other will be 1. Assigning numeric labels puts the data in a form a neural network can use.Accuracy and Cross EntropyAccuracy is one of the many metrics in use for measuring success on a classification problem. Accuracy is the ratio of correct predictions to total predictions: accuracy number correct total. A model that always predicted correctly would have an accuracy score of 1.0. All else being equal, accuracy is a reasonable metric to use whenever the classes in the dataset occur with about the same frequency.The problem with accuracy and most other classification metrics is that it can t be used as a loss function. SGD needs a loss function that changes smoothly, but accuracy, being a ratio of counts, changes in jumps . So, we have to choose a substitute to act as the loss function. This substitute is the cross entropy function.Now, recall that the loss function defines the objective of the network during training. With regression, our goal was to minimize the distance between the expected outcome and the predicted outcome. We chose MAE to measure this distance.For classification, what we want instead is a distance between probabilities, and this is what cross entropy provides. Cross entropy is a sort of measure for the distance from one probability distribution to another. Cross entropy penalizes incorrect probability predictions. The idea is that we want our network to predict the correct class with probability 1.0. The further away the predicted probability is from 1.0, the greater will be the cross entropy loss.The technical reasons we use cross entropy are a bit subtle, but the main thing to take away from this section is just this: use cross entropy for a classification loss other metrics you might care about like accuracy will tend to improve along with it.Making Probabilities with the Sigmoid FunctionThe cross entropy and accuracy functions both require probabilities as inputs, meaning, numbers from 0 to 1. To covert the real valued outputs produced by a dense layer into probabilities, we attach a new kind of activation function, the sigmoid activation. The sigmoid function maps real numbers into the interval . To get the final class prediction, we define a threshold probability. Typically this will be 0.5, so that rounding will give us the correct class: below 0.5 means the class with label 0 and 0.5 or above means the class with label 1. A 0.5 threshold is what Keras uses by default with its accuracy metric.Example Binary ClassificationNow let s try it out!The Ionosphere dataset contains features obtained from radar signals focused on the ionosphere layer of the Earth s atmosphere. The task is to determine whether the signal shows the presence of some object, or just empty air.",import pandas as pd ,binary-classification.ipynb
drop the empty feature in column 2,"df_train.dropna(axis = 1 , inplace = True) ",binary-classification.ipynb
"We ll define our model just like we did for the regression tasks, with one exception. In the final layer include a sigmoid activation so that the model will produce class probabilities.","from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
 layers.Dense(4, activation='relu', input_shape=[33]),
 layers.Dense(4, activation='relu'), 
 layers.Dense(1, activation='sigmoid'),
])",binary-classification.ipynb
"Add the cross entropy loss and accuracy metric to the model with its compile method. For two class problems, be sure to use binary versions. Problems with more classes will be slightly different. The Adam optimizer works great for classification too, so we ll stick with it.","model.compile(
 optimizer='adam',
 loss='binary_crossentropy',
 metrics=['binary_accuracy'],
)",binary-classification.ipynb
"We ll take a look at the learning curves as always, and also inspect the best values for the loss and accuracy we got on the validation set. Remember that early stopping will restore the weights to those that got these values. ",history_df = pd.DataFrame(history.history) ,binary-classification.ipynb
Start the plot at epoch 5,"history_df.loc[5 : ,['loss' , 'val_loss']].plot () ",binary-classification.ipynb
"OverviewThe new model CapsuleNet proposed by Sara Sabour and Geoffry Hinton claims to deliver state of the art results on MNIST. The kernel aims to create and train the model using the Kaggle Dataset and then make a submission to see where it actually ends up. Given the constraint of using a Kaggle Kernel means it can t be trained as long as we would like or with GPU s but IMHO if a model can t be reasonably well trained in an hour on a 28x28 dataset, that model probably won t be too useful in the immediate future.Implementation Details Keras implementation of CapsNet in Hinton s paper Dynamic Routing Between Capsules. Code adapted from Author: Xifeng Guo, E mail: guoxifeng1990 163.com, Github: The current version maybe only works for TensorFlow backend. Actually it will be straightforward to re write to TF code. Adopting to other backends should be easy, but I have not tested this. Result: Validation accuracy 99.5 after 20 epochs. Still under fitting. About 110 seconds per epoch on a single GTX1070 GPU card","import numpy as np
import os
import pandas as pd
from keras.preprocessing.image import ImageDataGenerator
from keras import callbacks
from keras.utils.vis_utils import plot_model",capsulenet-on-mnist.ipynb
Capsule Layers Here is the implementation of the necessary layers for the CapsuleNet. These are not optimized yet and can be made significantly more performant. ,import keras.backend as K ,capsulenet-on-mnist.ipynb
"Build the Model Here we use the layers to build up the model. The model is a bit different from a standard model, it is meaning it attempts to predict the class from the image, and then at the same time, using the same capsule reconstruct the image from the class. The approach appears very cGAN like where the task of reconstructing better helps the model understand the image data better.","from keras import layers , models ",capsulenet-on-mnist.ipynb
Load MNIST Data Here we load and reformat the Kaggle contest data,"from sklearn.model_selection import train_test_split
data_train = pd.read_csv('../input/train.csv')
X_full = data_train.iloc[:,1:]
y_full = data_train.iloc[:,:1]
x_train, x_test, y_train, y_test = train_test_split(X_full, y_full, test_size = 0.3)",capsulenet-on-mnist.ipynb
Show the results on the hold out,"test(model=model, data=(x_test[:100], y_test[:100]))",capsulenet-on-mnist.ipynb
Apply Model to the Competition Data Here we apply the model to the compitition data,data_test = pd.read_csv('../input/test.csv') ,capsulenet-on-mnist.ipynb
"In this tutorial, you will learn what a categorical variable is, along with three approaches for handling this type of data.IntroductionA categorical variable takes only a limited number of values. Consider a survey that asks how often you eat breakfast and provides four options: Never , Rarely , Most days , or Every day . In this case, the data is categorical, because responses fall into a fixed set of categories. If people responded to a survey about which what brand of car they owned, the responses would fall into categories like Honda , Toyota , and Ford . In this case, the data is also categorical. You will get an error if you try to plug these variables into most machine learning models in Python without preprocessing them first. In this tutorial, we ll compare three approaches that you can use to prepare your categorical data.Three Approaches1 Drop Categorical VariablesThe easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.2 Ordinal EncodingOrdinal encoding assigns each unique value to a different integer.This approach assumes an ordering of the categories: Never 0 Rarely 1 Most days 2 Every day 3 .This assumption makes sense in this example, because there is an indisputable ranking to the categories. Not all categorical variables have a clear ordering in the values, but we refer to those that do as ordinal variables. For tree based models like decision trees and random forests , you can expect ordinal encoding to work well with ordinal variables. 3 One Hot EncodingOne hot encoding creates new columns indicating the presence or absence of each possible value in the original data. To understand this, we ll work through an example.In the original dataset, Color is a categorical variable with three categories: Red , Yellow , and Green . The corresponding one hot encoding contains one column for each possible value, and one row for each row in the original dataset. Wherever the original value was Red , we put a 1 in the Red column if the original value was Yellow , we put a 1 in the Yellow column, and so on. In contrast to ordinal encoding, one hot encoding does not assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data e.g., Red is neither more nor less than Yellow . We refer to categorical variables without an intrinsic ranking as nominal variables.One hot encoding generally does not perform well if the categorical variable takes on a large number of values i.e., you generally won t use it for variables taking more than 15 different values . ExampleAs in the previous tutorial, we will work with the Melbourne Housing dataset. We won t focus on the data loading step. Instead, you can imagine you are at a point where you already have the training and validation data in X train, X valid, y train, and y valid. ",import pandas as pd ,categorical-variables.ipynb
Read the data,data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv') ,categorical-variables.ipynb
Separate target from predictors,y = data.Price ,categorical-variables.ipynb
We take a peek at the training data with the head method below. ,X_train.head(),categorical-variables.ipynb
Get list of categorical variables,s =(X_train.dtypes == 'object') ,categorical-variables.ipynb
"Define Function to Measure Quality of Each ApproachWe define a function score dataset to compare the three different approaches to dealing with categorical variables. This function reports the mean absolute error MAE from a random forest model. In general, we want the MAE to be as low as possible!",from sklearn.ensemble import RandomForestRegressor ,categorical-variables.ipynb
Function for comparing different approaches,"def score_dataset(X_train , X_valid , y_train , y_valid): ",categorical-variables.ipynb
Score from Approach 1 Drop Categorical Variables We drop the object columns with the select dtypes method. ,"drop_X_train = X_train.select_dtypes(exclude=['object'])
drop_X_valid = X_valid.select_dtypes(exclude=['object'])

print(""MAE from Approach 1 (Drop categorical variables):"")
print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))",categorical-variables.ipynb
Score from Approach 2 Ordinal Encoding Scikit learn has a OrdinalEncoder class that can be used to get ordinal encodings. We loop over the categorical variables and apply the ordinal encoder separately to each column.,from sklearn.preprocessing import OrdinalEncoder ,categorical-variables.ipynb
Make copy to avoid changing original data,label_X_train = X_train.copy () ,categorical-variables.ipynb
Apply ordinal encoder to each column with categorical data,ordinal_encoder = OrdinalEncoder () ,categorical-variables.ipynb
"In the code cell above, for each column, we randomly assign each unique value to a different integer. This is a common approach that is simpler than providing custom labels however, we can expect an additional boost in performance if we provide better informed labels for all ordinal variables.Score from Approach 3 One Hot Encoding We use the OneHotEncoder class from scikit learn to get one hot encodings. There are a number of parameters that can be used to customize its behavior. We set handle unknown ignore to avoid errors when the validation data contains classes that aren t represented in the training data, and setting sparse False ensures that the encoded columns are returned as a numpy array instead of a sparse matrix .To use the encoder, we supply only the categorical columns that we want to be one hot encoded. For instance, to encode the training data, we supply X train object cols . object cols in the code cell below is a list of the column names with categorical data, and so X train object cols contains all of the categorical data in the training set. ",from sklearn.preprocessing import OneHotEncoder ,categorical-variables.ipynb
Apply one hot encoder to each column with categorical data,"OH_encoder = OneHotEncoder(handle_unknown = 'ignore' , sparse = False) ",categorical-variables.ipynb
One hot encoding removed index put it back,OH_cols_train.index = X_train.index ,categorical-variables.ipynb
Remove categorical columns will replace with one hot encoding ,"num_X_train = X_train.drop(object_cols , axis = 1) ",categorical-variables.ipynb
Add one hot encoded columns to numerical features,"OH_X_train = pd.concat ([num_X_train , OH_cols_train], axis = 1) ",categorical-variables.ipynb
ConnectX environment was defined in v0.1.6,! pip install 'kaggle-environments>=0.1.6' ,cell-swarm.ipynb
Create ConnectX Environment,"from kaggle_environments import evaluate, make, utils

env = make(""connectx"", debug=True)
env.render()",cell-swarm.ipynb
"Create an AgentTo create the submission, an agent function should be fully encapsulated no external dependencies . When your agent is being evaluated against others, it will not have access to the Kaggle docker image. Only the following can be imported: Python Standard Library Modules, gym, numpy, scipy, pytorch 1.3.1, cpu only , and more may be added later.","def cell_swarm(obs , conf): ",cell-swarm.ipynb
if cell is inside swarm s borders, if y >= 0 and y < conf.rows and x >= 0 and x < conf.columns : ,cell-swarm.ipynb
Test your Agent,env.reset () ,cell-swarm.ipynb
Play as the first agent against negamax agent.,"env.run ([cell_swarm , cell_swarm]) ",cell-swarm.ipynb
"env.run cell swarm, negamax ","env.render(mode = ""ipython"" , width = 500 , height = 450) ",cell-swarm.ipynb
Play as first position against negamax agent.,"trainer = env.train ([None , ""negamax""]) ",cell-swarm.ipynb
"env.render mode ipython , width 100, height 90, header False, controls False ",env.render () ,cell-swarm.ipynb
Evaluate your Agent,def mean_reward(rewards): ,cell-swarm.ipynb
 None represents which agent you ll manually play as first or second player .,"env.play ([cell_swarm , None], width = 500 , height = 450) ",cell-swarm.ipynb
Write Submission File,"import inspect
import os

def write_agent_to_file(function, file):
 with open(file, ""a"" if os.path.exists(file) else ""w"") as f:
 f.write(inspect.getsource(function))
 print(function, ""written to"", file)

write_agent_to_file(cell_swarm, ""submission.py"")",cell-swarm.ipynb
Note: Stdout replacement is a temporary workaround.,import sys ,cell-swarm.ipynb
linear algebra,import numpy as np ,character-recognition-cnn.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,character-recognition-cnn.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,character-recognition-cnn.ipynb
Any results you write to the current directory are saved as output.,tf.enable_eager_execution(),character-recognition-cnn.ipynb
Remove grey images,"train_data.drop([283, 2289, 3135], inplace=True)
train_data.reset_index(inplace=True)",character-recognition-cnn.ipynb
Data pipeline,"def transform_img(img , label = None): ",character-recognition-cnn.ipynb
img cnt tf.keras.applications.resnet50.preprocess input img cnt , img_cnt /= 255 ,character-recognition-cnn.ipynb
img cnt img cnt std mean," return img_cnt , label ",character-recognition-cnn.ipynb
print lbs.numpy .shape , plt.imshow(imgs[3]) ,character-recognition-cnn.ipynb
Tf Model,"Activation = 'elu'
Input = tf.keras.layers.Input
Conv2D = functools.partial(
 tf.keras.layers.Conv2D,
 activation=Activation,
 padding='same'
 )
Dense = functools.partial(
 tf.keras.layers.Dense
 )
Dropout = tf.keras.layers.Dropout
Avgpool = tf.keras.layers.AveragePooling2D
MaxPool2D = tf.keras.layers.MaxPool2D
BatchNorm = tf.keras.layers.BatchNormalization
Flatten = tf.keras.layers.Flatten",character-recognition-cnn.ipynb
"avg 2 Avgpool 2, 2 conv 4 ", batch_norm_4 = BatchNorm ()( conv_4) ,character-recognition-cnn.ipynb
Training,"logdir = os.path.join(""/tmp/logs"", datetime.datetime.now().strftime(""%Y%m%d-%H%M%S""))
callbacks = [
 tf.keras.callbacks.ModelCheckpoint(filepath='./weights.hdf5', verbose=1, save_best_only=True),
 tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),
 tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
]",character-recognition-cnn.ipynb
Inferance,"test_imgs = []
for dirname, _, filenames in os.walk(TEST_IMGS_BASE_PATH):
 for filename in filenames:
 test_imgs.append(os.path.join(dirname, filename))
print(test_imgs[:5])
test_imgs = np.array(test_imgs)",character-recognition-cnn.ipynb
linear algebra,import numpy as np ,classification-comparing-different-algorithms.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,classification-comparing-different-algorithms.ipynb
 Basic Exploration ,df.head(),classification-comparing-different-algorithms.ipynb
axes is 2d array 3x3 ,"fig , axes = plt.subplots(nrows = 6 , ncols = 4) ",classification-comparing-different-algorithms.ipynb
Convert axes to 1d array of length 9,axes = axes.flatten () ,classification-comparing-different-algorithms.ipynb
Code Example,"from sklearn.naive_bayes import MultinomialNB
mnb = MultinomialNB().fit(x_train, y_train)
print(""score on test: "" + str(mnb.score(x_test, y_test)))
print(""score on train: ""+ str(mnb.score(x_train, y_train)))",classification-comparing-different-algorithms.ipynb
Code Example,"from sklearn.linear_model import LogisticRegression
lr=LogisticRegression(max_iter=1000)
lr.fit(x_train, y_train)
print(""score on test: "" + str(lr.score(x_test, y_test)))
print(""score on train: ""+ str(lr.score(x_train, y_train)))",classification-comparing-different-algorithms.ipynb
Code Examples :,"from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(algorithm = 'brute', n_jobs=-1)
knn.fit(x_train, y_train)
print(""train shape: "" + str(x_train.shape))
print(""score on test: "" + str(knn.score(x_test, y_test)))
print(""score on train: ""+ str(knn.score(x_train, y_train)))",classification-comparing-different-algorithms.ipynb
Code Example,"from sklearn.svm import LinearSVC
svm=LinearSVC(C=0.0001)
svm.fit(x_train, y_train)
print(""score on test: "" + str(svm.score(x_test, y_test)))
print(""score on train: ""+ str(svm.score(x_train, y_train)))",classification-comparing-different-algorithms.ipynb
Code Example:,"from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()
clf.fit(x_train, y_train)
print(""score on test: "" + str(clf.score(x_test, y_test)))
print(""score on train: "" + str(clf.score(x_train, y_train)))",classification-comparing-different-algorithms.ipynb
Code Example:,from sklearn.ensemble import BaggingClassifier ,classification-comparing-different-algorithms.ipynb
n estimators: number of decision trees,"bg = BaggingClassifier(DecisionTreeClassifier (), max_samples = 0.5 , max_features = 1.0 , n_estimators = 10) ",classification-comparing-different-algorithms.ipynb
Code Example,"from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
adb = AdaBoostClassifier(DecisionTreeClassifier(min_samples_split=10,max_depth=4),n_estimators=10,learning_rate=0.6)
adb.fit(x_train, y_train)
print(""score on test: "" + str(adb.score(x_test, y_test)))
print(""score on train: ""+ str(adb.score(x_train, y_train)))",classification-comparing-different-algorithms.ipynb
Code Example,from sklearn.ensemble import RandomForestClassifier ,classification-comparing-different-algorithms.ipynb
n estimators number of decision trees,"rf = RandomForestClassifier(n_estimators = 30 , max_depth = 9) ",classification-comparing-different-algorithms.ipynb
Code Example,from sklearn.ensemble import VotingClassifier ,classification-comparing-different-algorithms.ipynb
4 support vector machine svm,"evc = VotingClassifier(estimators =[( 'mnb' , mnb),('lr' , lr),('rf' , rf),('svm' , svm )], voting = 'hard') ",classification-comparing-different-algorithms.ipynb
linear algebra,import numpy as np ,classification-using-machine-and-deep-learning.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,classification-using-machine-and-deep-learning.ipynb
ignore warnings,import warnings ,classification-using-machine-and-deep-learning.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,classification-using-machine-and-deep-learning.ipynb
Loading the datasets,"train = pd.read_csv(""/kaggle/input/just-the-basics-the-after-party/train.csv"" , header = None) ",classification-using-machine-and-deep-learning.ipynb
Displaying the train set,train.head () ,classification-using-machine-and-deep-learning.ipynb
Displaying the test set,test.head () ,classification-using-machine-and-deep-learning.ipynb
Displaying the labels for the train set,labels.head () ,classification-using-machine-and-deep-learning.ipynb
Checking the shapes,"print(f""Train features: {train.shape}\nTrain labels: {labels.shape}\nTest features: {test.shape}"") ",classification-using-machine-and-deep-learning.ipynb
Train set overview,train.info () ,classification-using-machine-and-deep-learning.ipynb
Test set overview,test.info () ,classification-using-machine-and-deep-learning.ipynb
Labels overview,labels.info () ,classification-using-machine-and-deep-learning.ipynb
Handling the missing data,"print(""Before:"") ",classification-using-machine-and-deep-learning.ipynb
Splitting the training set into training and validation sets,split_size = int(len(train)* 0.75) ,classification-using-machine-and-deep-learning.ipynb
Scaling the training and validation sets,from sklearn.preprocessing import StandardScaler ,classification-using-machine-and-deep-learning.ipynb
Building machine learning models and making predictions on the validation set,"from sklearn.linear_model import LogisticRegression , SGDClassifier ",classification-using-machine-and-deep-learning.ipynb
Building the Neural Network and making predictions on the validation set,from tensorflow.keras.models import Sequential ,classification-using-machine-and-deep-learning.ipynb
Predictions,pd.DataFrame(predictions_dict) ,classification-using-machine-and-deep-learning.ipynb
Accuracy Scores,"pd.DataFrame({ 'Accuracy % ' :[round(score * 100 , 2)for score in scores_dict.values()]} , index = scores_dict.keys ()) ",classification-using-machine-and-deep-learning.ipynb
Make predictions using the Random Forest Classifier on the test set,train_scaled = scaler.fit_transform(train_cleaned) ,classification-using-machine-and-deep-learning.ipynb
Submitting the resutls,output = pd.DataFrame({ 'Predictions' : final_predictions }) ,classification-using-machine-and-deep-learning.ipynb
"It s important to remember that this Cluster feature is categorical. Here, it s shown with a label encoding that is, as a sequence of integers as a typical clustering algorithm would produce depending on your model, a one hot encoding may be more appropriate.The motivating idea for adding cluster labels is that the clusters will break up complicated relationships across features into simpler chunks. Our model can then just learn the simpler chunks one by one instead having to learn the complicated whole all at once. It s a divide and conquer strategy. Clustering the YearBuilt feature helps this linear model learn its relationship to SalePrice. The figure shows how clustering can improve a simple linear model. The curved relationship between the YearBuilt and SalePrice is too complicated for this kind of model it underfits. On smaller chunks however the relationship is almost linear, and that the model can learn easily.k Means ClusteringThere are a great many clustering algorithms. They differ primarily in how they measure similarity or proximity and in what kinds of features they work with. The algorithm we ll use, k means, is intuitive and easy to apply in a feature engineering context. Depending on your application another algorithm might be more appropriate.K means clustering measures similarity using ordinary straight line distance Euclidean distance, in other words . It creates clusters by placing a number of points, called centroids, inside the feature space. Each point in the dataset is assigned to the cluster of whichever centroid it s closest to. The k in k means is how many centroids that is, clusters it creates. You define the k yourself.You could imagine each centroid capturing points through a sequence of radiating circles. When sets of circles from competing centroids overlap they form a line. The result is what s called a Voronoi tessallation. The tessallation shows you to what clusters future data will be assigned the tessallation is essentially what k means learns from its training data.The clustering on the Ames dataset above is a k means clustering. Here is the same figure with the tessallation and centroids shown. K means clustering creates a Voronoi tessallation of the feature space. Let s review how the k means algorithm learns the clusters and what that means for feature engineering. We ll focus on three parameters from scikit learn s implementation: n clusters, max iter, and n init.It s a simple two step process. The algorithm starts by randomly initializing some predefined number n clusters of centroids. It then iterates over these two operations: 1. assign points to the nearest cluster centroid 2. move each centroid to minimize the distance to its pointsIt iterates over these two steps until the centroids aren t moving anymore, or until some maximum number of iterations has passed max iter .It often happens that the initial random position of the centroids ends in a poor clustering. For this reason the algorithm repeats a number of times n init and returns the clustering that has the least total distance between each point and its centroid, the optimal clustering.The animation below shows the algorithm in action. It illustrates the dependence of the result on the initial centroids and the importance of iterating until convergence. The K means clustering algorithm on Airbnb rentals in NYC. You may need to increase the max iter for a large number of clusters or n init for a complex dataset. Ordinarily though the only parameter you ll need to choose yourself is n clusters k, that is . The best partitioning for a set of features depends on the model you re using and what you re trying to predict, so it s best to tune it like any hyperparameter through cross validation, say .Example California HousingAs spatial features, California Housing s Latitude and Longitude make natural candidates for k means clustering. In this example we ll cluster these with MedInc median income to create economic segments in different regions of California.","
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.cluster import KMeans

plt.style.use(""seaborn-whitegrid"")
plt.rc(""figure"", autolayout=True)
plt.rc(
 ""axes"",
 labelweight=""bold"",
 labelsize=""large"",
 titleweight=""bold"",
 titlesize=14,
 titlepad=10,
)

df = pd.read_csv(""../input/fe-course-data/housing.csv"")
X = df.loc[:, [""MedInc"", ""Latitude"", ""Longitude""]]
X.head()",clustering-with-k-means.ipynb
Create cluster feature,kmeans = KMeans(n_clusters = 6) ,clustering-with-k-means.ipynb
"Now let s look at a couple plots to see how effective this was. First, a scatter plot that shows the geographic distribution of the clusters. It seems like the algorithm has created separate segments for higher income areas on the coasts.","sns.relplot(
 x=""Longitude"", y=""Latitude"", hue=""Cluster"", data=X, height=6,
);",clustering-with-k-means.ipynb
"The target in this dataset is MedHouseVal median house value . These box plots show the distribution of the target within each cluster. If the clustering is informative, these distributions should, for the most part, separate across MedHouseVal, which is indeed what we see.","X[""MedHouseVal""] = df[""MedHouseVal""]
sns.catplot(x=""MedHouseVal"", y=""Cluster"", data=X, kind=""boxen"", height=6);",clustering-with-k-means.ipynb
linear algebra,import numpy as np ,compare-performance-metrics-dt-knn-svc-rf-mlp.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,compare-performance-metrics-dt-knn-svc-rf-mlp.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,compare-performance-metrics-dt-knn-svc-rf-mlp.ipynb
import stuff,import pandas as pd ,compare-performance-metrics-dt-knn-svc-rf-mlp.ipynb
explore data,train_data = pd.read_csv('../input/data-science-london-scikit-learn/train.csv') ,compare-performance-metrics-dt-knn-svc-rf-mlp.ipynb
prepare data,X = train_data ,compare-performance-metrics-dt-knn-svc-rf-mlp.ipynb
This function was originally written in the following notebook ,"def evaluate(model , y_train_pred , y_val_pred): ",compare-performance-metrics-dt-knn-svc-rf-mlp.ipynb
initialize models,dt_model = DecisionTreeRegressor(random_state = 1) ,compare-performance-metrics-dt-knn-svc-rf-mlp.ipynb
fit models,"dt_model.fit(train_X , train_y) ",compare-performance-metrics-dt-knn-svc-rf-mlp.ipynb
predict,dt_model_val_pred = dt_model.predict(val_X) ,compare-performance-metrics-dt-knn-svc-rf-mlp.ipynb
evaluate,"evaluate(dt_model , dt_model_train_pred , dt_model_val_pred) ",compare-performance-metrics-dt-knn-svc-rf-mlp.ipynb
invite people for the Kaggle party,import pandas as pd ,comprehensive-data-exploration-with-python.ipynb
bring in the six packs,df_train = pd.read_csv('../input/train.csv') ,comprehensive-data-exploration-with-python.ipynb
check the decoration,df_train.columns ,comprehensive-data-exploration-with-python.ipynb
descriptive statistics summary,df_train['SalePrice']. describe () ,comprehensive-data-exploration-with-python.ipynb
histogram,sns.distplot(df_train['SalePrice']) ; ,comprehensive-data-exploration-with-python.ipynb
skewness and kurtosis,"print(""Skewness: %f"" % df_train['SalePrice']. skew ()) ",comprehensive-data-exploration-with-python.ipynb
scatter plot grlivarea saleprice,var = 'GrLivArea' ,comprehensive-data-exploration-with-python.ipynb
scatter plot totalbsmtsf saleprice,var = 'TotalBsmtSF' ,comprehensive-data-exploration-with-python.ipynb
box plot overallqual saleprice,var = 'OverallQual' ,comprehensive-data-exploration-with-python.ipynb
"Like all the pretty girls, SalePrice enjoys OverallQual . Note to self: consider whether McDonald s is suitable for the first date.","var = 'YearBuilt'
data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)
f, ax = plt.subplots(figsize=(16, 8))
fig = sns.boxplot(x=var, y=""SalePrice"", data=data)
fig.axis(ymin=0, ymax=800000);
plt.xticks(rotation=90);",comprehensive-data-exploration-with-python.ipynb
correlation matrix,corrmat = df_train.corr () ,comprehensive-data-exploration-with-python.ipynb
number of variables for heatmap,k = 10 ,comprehensive-data-exploration-with-python.ipynb
scatterplot,sns.set () ,comprehensive-data-exploration-with-python.ipynb
missing data,total = df_train.isnull (). sum (). sort_values(ascending = False) ,comprehensive-data-exploration-with-python.ipynb
dealing with missing data,"df_train = df_train.drop(( missing_data[missing_data['Total']> 1]).index , 1) ",comprehensive-data-exploration-with-python.ipynb
just checking that there s no missing data missing...,df_train.isnull (). sum (). max () ,comprehensive-data-exploration-with-python.ipynb
standardizing data,"saleprice_scaled = StandardScaler (). fit_transform(df_train['SalePrice'][ : , np.newaxis]) ; ",comprehensive-data-exploration-with-python.ipynb
bivariate analysis saleprice grlivarea,var = 'GrLivArea' ,comprehensive-data-exploration-with-python.ipynb
deleting points,"df_train.sort_values(by = 'GrLivArea' , ascending = False)[ : 2] ",comprehensive-data-exploration-with-python.ipynb
bivariate analysis saleprice grlivarea,var = 'TotalBsmtSF' ,comprehensive-data-exploration-with-python.ipynb
histogram and normal probability plot,"sns.distplot(df_train['SalePrice'], fit = norm); ",comprehensive-data-exploration-with-python.ipynb
applying log transformation,df_train['SalePrice']= np.log(df_train['SalePrice']) ,comprehensive-data-exploration-with-python.ipynb
transformed histogram and normal probability plot,"sns.distplot(df_train['SalePrice'], fit = norm); ",comprehensive-data-exploration-with-python.ipynb
histogram and normal probability plot,"sns.distplot(df_train['GrLivArea'], fit = norm); ",comprehensive-data-exploration-with-python.ipynb
data transformation,df_train['GrLivArea']= np.log(df_train['GrLivArea']) ,comprehensive-data-exploration-with-python.ipynb
transformed histogram and normal probability plot,"sns.distplot(df_train['GrLivArea'], fit = norm); ",comprehensive-data-exploration-with-python.ipynb
histogram and normal probability plot,"sns.distplot(df_train['TotalBsmtSF'], fit = norm); ",comprehensive-data-exploration-with-python.ipynb
"if area 0 it gets 1, for area 0 it gets 0","df_train['HasBsmt']= pd.Series(len(df_train['TotalBsmtSF']) , index = df_train.index) ",comprehensive-data-exploration-with-python.ipynb
transform data,"df_train.loc[df_train['HasBsmt']== 1 , 'TotalBsmtSF']= np.log(df_train['TotalBsmtSF']) ",comprehensive-data-exploration-with-python.ipynb
histogram and normal probability plot,"sns.distplot(df_train[df_train['TotalBsmtSF']> 0][ 'TotalBsmtSF'], fit = norm); ",comprehensive-data-exploration-with-python.ipynb
scatter plot,"plt.scatter(df_train['GrLivArea'], df_train['SalePrice']) ; ",comprehensive-data-exploration-with-python.ipynb
scatter plot,"plt.scatter(df_train[df_train['TotalBsmtSF']> 0][ 'TotalBsmtSF'], df_train[df_train['TotalBsmtSF']> 0][ 'SalePrice']) ; ",comprehensive-data-exploration-with-python.ipynb
convert categorical variable into dummy,df_train = pd.get_dummies(df_train) ,comprehensive-data-exploration-with-python.ipynb
"Welcome Welcome to a fun adventure with TPUs and flower classification . In this fun notebook we will go step by step and create a deep learning model to perform flower classification on 104 different species! If you find this notebook interesting, please upvote it, it means a lot to me and it keeps me motivated to improve it as shown in my long list of items below This notebook builds heavily on Ryan s awesome notebook as part of the Deep Learning Kaggle course on Computer Vision at: explore 25 combinations of transfer learning and hyperparameter tuning and compare their performance val accuracy, where did the val loss and accuracy perform best, f1 score as well as precision and recall. Look out for my personal notes, labeled as Note ... , as we take this journey together...My Personal Plan of Action: Understand how TPUs work and how to use them Explore transfer learning with 10 models pretrained on either imagenet or noisy student and evaluate their performance Explore training large CNN models from scratch and evaluate their performance Explore 10 hyperparameter tuning methods and evaluate their performance Explore 25 combinations of models and tuning methods above and evaluate their performance Ensemble models with loaded weights and evaluate their performance Build a great looking vizualization that captures and highlights model tuning performance Be generous with comments, either as markdown or in code so anyone can follow along Respond to each comment on this notebook and learn from each other Meet and interact with kagglers online Reach out and thank other Kagglers for their amazing notebooks, share feedback and learn from each other Promote and share our cool online community Deep Learning Adventures Have fun while building cool models Climb up the leaderboard with new explorations Plot model performance report in 3D Start and maintain a discussion thread around this notebook Visualize different data augmentation methods Visualize incorrect predictions Explore Test Time Augmentation TTA Find and use more training data, increasing the variety our models are exposed to from 12K training images to 68K Convert a few models from .h5 to TensorFlow Lite using TensorFlow Lite converter Deploy a few models models on mobile and IoT devices Explore other dimensions 224x224 images Explore models from TF Hub or Model Garden Explore Mixed precision Explore ELI5 and model explainability Read a paper or two on different data augmentation methods for computer vision Earn 5 votes bronze medal for this notebook Earn 20 votes silver medal for this notebook Earn 50 votes gold medal for this notebook your help is needed here ","from IPython.display import Image
Image(filename=""../input/images/Petals to the Metal 31.png"", width=1200, height=1000)",computer-vision-petals-to-the-metal.ipynb
Weights for 12K images,"Image(filename = ""../input/images/Petals to the Metal 25.png"" , width = 1200 , height = 1000) ",computer-vision-petals-to-the-metal.ipynb
Weights for 70K images,"Image(filename = ""../input/images/Petals to the Metal 26.png"" , width = 1200 , height = 1000) ",computer-vision-petals-to-the-metal.ipynb
"If you would like to learn more as well as join a larger data science community , feel free to join us at: All our sessions are recorded and available on YouTube at: ","Image(filename=""../input/images/Deep Learning Adventures.png"", width=1200, height=1000) ",computer-vision-petals-to-the-metal.ipynb
Step 1: ImportsWe begin by importing several Python packages.,"!pip install seaborn --upgrade
import seaborn as sns

import matplotlib.pyplot as plt
from matplotlib import cm
import math, re, os
import pandas as pd
import numpy as np
import random
import plotly.express as px

import tensorflow as tf
print(""Tensorflow version "" + tf.__version__)",computer-vision-petals-to-the-metal.ipynb
"Detect TPU, return appropriate distribution strategy",try : ,computer-vision-petals-to-the-metal.ipynb
See Note 1.1 above , tpu = tf.distribute.cluster_resolver.TPUClusterResolver () ,computer-vision-petals-to-the-metal.ipynb
See Note 1.2 above , tf.config.experimental_connect_to_cluster(tpu) ,computer-vision-petals-to-the-metal.ipynb
See Note 1.2 above , tf.tpu.experimental.initialize_tpu_system(tpu) ,computer-vision-petals-to-the-metal.ipynb
See Note 1.3 above , strategy = tf.distribute.experimental.TPUStrategy(tpu) ,computer-vision-petals-to-the-metal.ipynb
"We ll use the distribution strategy when we create our neural network model. Then, TensorFlow will distribute the training among the eight TPU cores by creating eight different replicas of the model, one for each core.Step 3: Loading the Competition DataGet GCS PathWhen used with TPUs, datasets need to be stored in a Google Cloud Storage bucket. You can use data from any public GCS bucket by giving its path just like you would data from kaggle input . The following will retrieve the GCS path for this competition s dataset.",from kaggle_datasets import KaggleDatasets ,computer-vision-petals-to-the-metal.ipynb
what do gcs paths look like?,print(GCS_DS_PATH) ,computer-vision-petals-to-the-metal.ipynb
See Note 2.1 above ,"IMAGE_SIZE =[512 , 512] ",computer-vision-petals-to-the-metal.ipynb
See Note 2.2 above ,AUTO = tf.data.experimental.AUTOTUNE ,computer-vision-petals-to-the-metal.ipynb
"Note 9 Use additional data, tuning6, private dataset Inspired by Dmitry s notebook here and Araik s notebook here See also external data and how to use them and Kirill s tf flower photo tfrec dataset",GCS_DS_PATH_EXT = KaggleDatasets (). get_gcs_path('tf-flower-photo-tfrec') ,computer-vision-petals-to-the-metal.ipynb
tuning4,SEED = 2020 ,computer-vision-petals-to-the-metal.ipynb
Data pipeline code is executed on the CPU part of the TPU while the TPU itself is computing gradients.," flag = random.randint(1 , 3) ",computer-vision-petals-to-the-metal.ipynb
"Perform data augmentation, tuning7 Inspired by Xuanzhi Huang and Rahul Paul s notebook here",import tensorflow_addons as tfa ,computer-vision-petals-to-the-metal.ipynb
Randomly make some changes to the images and return the new images and labels,"def data_augment_v3(image , label): ",computer-vision-petals-to-the-metal.ipynb
Set seed for data augmentation, seed = 100 ,computer-vision-petals-to-the-metal.ipynb
Randomly resize and then crop images," image = tf.image.resize(image ,[720 , 720]) ",computer-vision-petals-to-the-metal.ipynb
Randomly reset brightness of images," image = tf.image.random_brightness(image , 0.6 , seed = seed) ",computer-vision-petals-to-the-metal.ipynb
Randomly reset saturation of images," image = tf.image.random_saturation(image , 3 , 5 , seed = seed) ",computer-vision-petals-to-the-metal.ipynb
Randomly reset contrast of images," image = tf.image.random_contrast(image , 0.3 , 0.5 , seed = seed) ",computer-vision-petals-to-the-metal.ipynb
Blur images," image = tfa.image.mean_filter2d(image , filter_shape = 10) ",computer-vision-petals-to-the-metal.ipynb
Randomly flip images," image = tf.image.random_flip_left_right(image , seed = seed) ",computer-vision-petals-to-the-metal.ipynb
"image tfa.image.transform image, 1.0, 1.0, 250, 0.0, 1.0, 0.0, 0.0, 0.0 "," return image , label ",computer-vision-petals-to-the-metal.ipynb
"Create Data PipelinesIn this final step we ll use the tf.data API to define an efficient data pipeline for each of the training, validation, and test splits.","def data_augment(image , label): ",computer-vision-petals-to-the-metal.ipynb
Data pipeline code is executed on the CPU part of the TPU while the TPU itself is computing gradients., image = tf.image.random_flip_left_right(image) ,computer-vision-petals-to-the-metal.ipynb
"image tf.image.random saturation image, 0, 2 "," return image , label ",computer-vision-petals-to-the-metal.ipynb
tuning4," dataset = dataset.map(data_augment , num_parallel_calls = AUTO) ",computer-vision-petals-to-the-metal.ipynb
the training dataset must repeat for several epochs, dataset = dataset.repeat () ,computer-vision-petals-to-the-metal.ipynb
prefetch next batch while training autotune prefetch buffer size , dataset = dataset.prefetch(AUTO) ,computer-vision-petals-to-the-metal.ipynb
"the number of data items is written in the name of the .tfrec files, i.e. flowers00 230.tfrec 230 data items"," n =[int(re.compile(r""-([0-9]*)\.""). search(filename). group(1)) for filename in filenames] ",computer-vision-petals-to-the-metal.ipynb
"Note 3 1. To go fast on a TPU, increase the batch size. The rule of thumb is to use batches of 128 elements per core ex: batch size of 128 8 1024 for a TPU with 8 cores . At this size, the 128x128 hardware matrix multipliers of the TPU see hardware section below are most likely to be kept busy. You start seeing interesting speedups from a batch size of 8 per core though. In the sample above, the batch size is scaled with the core count through this line of code: BATCH SIZE 16 tpu strategy.num replicas in sync Source: ",strategy.num_replicas_in_sync,computer-vision-petals-to-the-metal.ipynb
See Note 3.1 above ,BATCH_SIZE = 16 * strategy.num_replicas_in_sync ,computer-vision-petals-to-the-metal.ipynb
"These datasets are tf.data.Dataset objects. You can think about a dataset in TensorFlow as a stream of data records. The training and validation sets are streams of image, label pairs.","np.set_printoptions(threshold = 15 , linewidth = 80) ",computer-vision-petals-to-the-metal.ipynb
See Note 3.1 above ," print(image.numpy (). shape , label.numpy (). shape) ",computer-vision-petals-to-the-metal.ipynb
"The test set is a stream of image, idnum pairs idnum here is the unique identifier given to the image that we ll use later when we make our submission as a csv file.","print(""Test data shapes:"") ",computer-vision-petals-to-the-metal.ipynb
See Note 3.1 above ," print(image.numpy (). shape , idnum.numpy (). shape) ",computer-vision-petals-to-the-metal.ipynb
U unicode string,"print(""Test data IDs:"" , idnum.numpy (). astype('U')) ",computer-vision-petals-to-the-metal.ipynb
Step 4: Explore DataLet s take a moment to look at some of the images in the dataset.,from matplotlib import pyplot as plt ,computer-vision-petals-to-the-metal.ipynb
"binary string in this case,these are image ID strings", if numpy_labels.dtype == object : ,computer-vision-petals-to-the-metal.ipynb
"If no labels, only image IDs, return None for labels this is the case for test data "," return numpy_images , numpy_labels ",computer-vision-petals-to-the-metal.ipynb
You can display a single batch of images from a dataset with another of our helper functions. The next cell will turn the dataset into an iterator of batches of 20 images.,ds_iter = iter(ds_train.unbatch().batch(20)),computer-vision-petals-to-the-metal.ipynb
Use the Python next function to pop out the next batch in the stream and display it with the helper function.,"one_batch = next(ds_iter)
display_batch_of_images(one_batch)",computer-vision-petals-to-the-metal.ipynb
"tuning7, show a sample of data augmented",row = 3 ,computer-vision-petals-to-the-metal.ipynb
Map the images to the data augmentation function for image processing,augmented_element = one_element.repeat (). map(data_augment). batch(row * col) ,computer-vision-petals-to-the-metal.ipynb
Map the images to the data augmentation function for image processing,augmented_element = one_element.repeat (). map(data_augment_v2). batch(row * col) ,computer-vision-petals-to-the-metal.ipynb
Map the images to the data augmentation function for image processing,augmented_element = one_element.repeat (). map(data_augment_v3). batch(row * col) ,computer-vision-petals-to-the-metal.ipynb
"Step 5: Define ModelNow we re ready to create a neural network for classifying images! We ll use what s known as transfer learning. With transfer learning, you reuse part of a pretrained model to get a head start on a new dataset.For this tutorial, we ll to use a model called VGG16 pretrained on ImageNet . Later, you might want to experiment with other models included with Keras. Xception wouldn t be a bad choice. The distribution strategy we created earlier contains a context manager, strategy.scope. This context manager tells TensorFlow how to divide the work of training among the eight TPU cores. When using TensorFlow with a TPU, it s important to define your model in a strategy.scope context.","[*IMAGE_SIZE, 3]",computer-vision-petals-to-the-metal.ipynb
Note 4 Let s transfer learn from different neural network architectures from tf.keras.applications and keep track of their performance Source: Note that TF 2.4 has many more models available for transfer learning Source: ,"', '.join(tf.keras.applications.__dir__())",computer-vision-petals-to-the-metal.ipynb
 Petals to the Metal 70K images trainable True DenseNet201.h5 ,"checkpoint_filepath = ""Petals_to_the_Metal-70K_images-trainable_True-MobileNetV2.h5"" ",computer-vision-petals-to-the-metal.ipynb
This callback will stop the training when there is no improvement in the validation loss for three consecutive epochs.,"early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss' , patience = 3) ",computer-vision-petals-to-the-metal.ipynb
Note 10 Track learning rate during training NotFoundError: Container worker does not exist. Could not find resource: worker AnonymousVar8064 Encountered when executing an operation using EagerExecutor. This error cancels all future operations and poisons their output tensors.,"NotFoundError = """"""
class LRTensorBoard(TensorBoard):
 def __init__(self, log_dir, **kwargs): # add other arguments to __init__ if you need
 super().__init__(log_dir=log_dir, **kwargs)

 def on_epoch_end(self, epoch, logs=None):
 logs = logs or {}
 logs.update({'lr': K.eval(self.model.optimizer.lr)})
 super().on_epoch_end(epoch, logs)

lr_tracking = LRTensorBoard(log_dir=""./lr_tracking"")
""""""",computer-vision-petals-to-the-metal.ipynb
Writing your own callbacks Not needed,class LearningRateTracking(tf.keras.callbacks.Callback): ,computer-vision-petals-to-the-metal.ipynb
tuning9,use_efficientnet = False ,computer-vision-petals-to-the-metal.ipynb
Calculate weight for each class tuning11 Inspired by Flower Classification DenseNet 201,weight_per_class = True ,computer-vision-petals-to-the-metal.ipynb
?, TARGET_NUM_PER_CLASS = 122 ,computer-vision-petals-to-the-metal.ipynb
barplot color based on value," bplot = sns.barplot(x = data.index , y = 'class_weight' , data = data , palette = cm.Blues(data['class_weight']* 0.15)) ; ",computer-vision-petals-to-the-metal.ipynb
Skip to Model Ensemble Note 11 or look at previous versions of this notebook for transfer learning or end to end training,using_ensemble_models = False,computer-vision-petals-to-the-metal.ipynb
"The sparse categorical versions of the loss and metrics are appropriate for a classification task with more than two labels, like this one.",if not using_ensemble_models : ,computer-vision-petals-to-the-metal.ipynb
if not using ensemble models:,"tf.keras.utils.plot_model(model , show_shapes = True) ",computer-vision-petals-to-the-metal.ipynb
Step 6: TrainingLearning Rate ScheduleWe ll train this network with a special learning rate schedule.,if not using_ensemble_models : ,computer-vision-petals-to-the-metal.ipynb
Define training epochs, EPOCHS = 30 ,computer-vision-petals-to-the-metal.ipynb
See Note 3.1 above , BATCH_SIZE = 16 * strategy.num_replicas_in_sync ,computer-vision-petals-to-the-metal.ipynb
"Fit ModelAnd now we re ready to train the model. After defining a few parameters, we re good to go!",if not using_ensemble_models : ,computer-vision-petals-to-the-metal.ipynb
Model performance,"if not using_ensemble_models:
 display_training_curves_v2( 
 history.history['loss'],
 history.history['val_loss'],
 history.history['lr'],
 'loss',
 211,
 )

 display_training_curves_v2(
 history.history['sparse_categorical_accuracy'],
 history.history['val_sparse_categorical_accuracy'],
 history.history['lr'],
 'accuracy',
 212,
 )",computer-vision-petals-to-the-metal.ipynb
Note 5 continued The model weights that are considered the best are loaded into the model.,checkpoint_filepath,computer-vision-petals-to-the-metal.ipynb
Note 12 Convert a few models from .h5 to TensorFlow Lite using TensorFlow Lite converter Deploy a few models models on mobile and IoT devices,model.summary(),computer-vision-petals-to-the-metal.ipynb
Convert the model,converter = tf.lite.TFLiteConverter.from_keras_model(model) ,computer-vision-petals-to-the-metal.ipynb
Save the model,"with open(tflite_model_name , 'wb')as f : ",computer-vision-petals-to-the-metal.ipynb
Note 11 Use an ensemble of top 2 performers EfficientNetB7 and DenseNet201 See my model dataset as well as Dmitry s notebook and Rosa s original notebook,"def get_pretrained_model(model_name , image_dataset_weights , trainable = True): ",computer-vision-petals-to-the-metal.ipynb
Ensemble both models,"from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix",computer-vision-petals-to-the-metal.ipynb
"since we are splitting the dataset and iterating separately on images and labels, order matters.", cmdataset = get_validation_dataset(ordered = True) ,computer-vision-petals-to-the-metal.ipynb
get everything as one batch, cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))). numpy () ,computer-vision-petals-to-the-metal.ipynb
best alpha 0.35,"if using_ensemble_models:
 print(best_alpha, max(scores))",computer-vision-petals-to-the-metal.ipynb
Run predictions on the test dataset,if using_ensemble_models : ,computer-vision-petals-to-the-metal.ipynb
best alpha 0.35, print('Computing predictions...') ,computer-vision-petals-to-the-metal.ipynb
Get image ids from test set and convert to unicode," test_ids_ds = test_ds.map(lambda image , idnum : idnum). unbatch () ",computer-vision-petals-to-the-metal.ipynb
"Step 7: Evaluate PredictionsBefore making your final predictions on the test set, it s a good idea to evaluate your model s predictions on the validation set. This can help you diagnose problems in training or suggest ways your model could be improved. We ll look at two common ways of validation: plotting the confusion matrix and visual validation.","def display_confusion_matrix(cmat , score , precision , recall): ",computer-vision-petals-to-the-metal.ipynb
set up the subplots on the first call, if subplot % 10 == 1 : ,computer-vision-petals-to-the-metal.ipynb
"ax.set ylim 0.28,1.05 ", ax.set_xlabel('epoch') ,computer-vision-petals-to-the-metal.ipynb
Confusion MatrixA confusion matrix shows the actual class of an image tabulated against its predicted class. It is one of the best tools you have for evaluating the performance of a classifier.The following cell does some processing on the validation data and then creates the matrix with the confusion matrix function included in scikit learn.,cmdataset = get_validation_dataset(ordered = True) ,computer-vision-petals-to-the-metal.ipynb
You might be familiar with metrics like F1 score or precision and recall. This cell will compute these metrics and display them with a plot of the confusion matrix. These metrics are defined in the Scikit learn module sklearn.metrics we ve imported them in the helper script for you. ,"score = f1_score(
 cm_correct_labels,
 cm_predictions,
 labels=labels,
 average='macro',
)

precision = precision_score(
 cm_correct_labels,
 cm_predictions,
 labels=labels,
 average='macro',
)

recall = recall_score(
 cm_correct_labels,
 cm_predictions,
 labels=labels,
 average='macro',
)

display_confusion_matrix(cmat, score, precision, recall)",computer-vision-petals-to-the-metal.ipynb
Create model performance report ,"model_performance_report = pd.DataFrame(columns=['model-family', 'model', 'epochs', 'arg min loss', 'arg max accuracy', 
 'min loss', 'max accuracy', 'f1', 'precision', 'recall'])

model_performance_report.loc[len(model_performance_report)]={ 'model-family': 'VGG',
 'model':'VGG16', 
 'epochs':12, 
 'arg min loss':11, 
 'arg max accuracy':11,
 'min loss':3.47,
 'max accuracy':0.23,
 'f1':0.123,
 'precision':0.146,
 'recall':0.226}

model_performance_report.loc[len(model_performance_report)]={ 'model-family': 'DenseNet',
 'model':'DenseNet201', 
 'epochs':12, 
 'arg min loss':11, 
 'arg max accuracy':10,
 'min loss':1.31,
 'max accuracy':0.74,
 'f1':0.643,
 'precision':0.761,
 'recall':0.599}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'InceptionResNet',
 'model':'InceptionResNetV2', 
 'epochs':12, 
 'arg min loss':11, 
 'arg max accuracy':11,
 'min loss':1.57,
 'max accuracy':0.66,
 'f1':0.513,
 'precision':0.640,
 'recall':0.480}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'Inception', 
 'model':'InceptionV3', 
 'epochs':12, 
 'arg min loss':11, 
 'arg max accuracy':11,
 'min loss':1.48,
 'max accuracy':0.69,
 'f1':0.581,
 'precision':0.728,
 'recall':0.538}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'MobileNet', 
 'model':'MobileNet', 
 'epochs':12, 
 'arg min loss':11, 
 'arg max accuracy':10,
 'min loss':1.11,
 'max accuracy':0.76,
 'f1':0.717,
 'precision':0.798,
 'recall':0.679}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'MobileNet',
 'model':'MobileNetV2', 
 'epochs':12, 
 'arg min loss':11, 
 'arg max accuracy':11,
 'min loss':1.26,
 'max accuracy':0.72,
 'f1':0.650,
 'precision':0.763,
 'recall':0.606}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'NASNetMobile',
 'model':'NASNetMobile', 
 'epochs':12, 
 'arg min loss':11, 
 'arg max accuracy':11,
 'min loss':2.69,
 'max accuracy':0.38,
 'f1':0.224,
 'precision':0.401,
 'recall':0.203}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'ResNet',
 'model':'ResNet50', 
 'epochs':12, 
 'arg min loss':11, 
 'arg max accuracy':11,
 'min loss':3.85,
 'max accuracy':0.12,
 'f1':0.017,
 'precision':0.035,
 'recall':0.025}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'ResNet',
 'model':'R101V2', 
 'epochs':12, 
 'arg min loss':11, 
 'arg max accuracy':9,
 'min loss':0.87,
 'max accuracy':0.83,
 'f1':0.775,
 'precision':0.842,
 'recall':0.741}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'VGG',
 'model':'VGG19', 
 'epochs':12, 
 'arg min loss':11, 
 'arg max accuracy':11,
 'min loss':3.58,
 'max accuracy':0.21,
 'f1':0.031,
 'precision':0.036,
 'recall':0.048}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'Xception',
 'model':'Xception', 
 'epochs':12, 
 'arg min loss':11, 
 'arg max accuracy':11,
 'min loss':1.43,
 'max accuracy':0.71,
 'f1':0.575,
 'precision':0.712,
 'recall':0.536}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'ResNet',
 'model':'R2 30e', 
 'epochs':30, 
 'arg min loss':29, 
 'arg max accuracy':28,
 'min loss':0.83,
 'max accuracy':0.83,
 'f1':0.788,
 'precision':0.863,
 'recall':0.753}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'ResNet',
 'model':'R101V2 1,2,3+OF', 
 'epochs':30, 
 'arg min loss':26, 
 'arg max accuracy':27,
 'min loss':0.52,
 'max accuracy':0.88,
 'f1':0.864,
 'precision':0.916,
 'recall':0.842}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'DenseNet',
 'model':'D 1,2', 
 'epochs':30, 
 'arg min loss':29, 
 'arg max accuracy':29,
 'min loss':0.92,
 'max accuracy':0.81,
 'f1':0.767,
 'precision':0.833,
 'recall':0.732}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'DenseNet',
 'model':'D201 1,2,4', 
 'epochs':30, 
 'arg min loss':29, 
 'arg max accuracy':27,
 'min loss':0.92,
 'max accuracy':0.82,
 'f1':0.772,
 'precision':0.846,
 'recall':0.734}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'ResNet',
 'model':'R101V2 1,2,4', 
 'epochs':30, 
 'arg min loss':29, 
 'arg max accuracy':28,
 'min loss':0.66,
 'max accuracy':0.85,
 'f1':0.829,
 'precision':0.870,
 'recall':0.802}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'ResNet',
 'model':'R101V2 1,2,4,5', 
 'epochs':30, 
 'arg min loss':29, 
 'arg max accuracy':23,
 'min loss':0.66,
 'max accuracy':0.86,
 'f1':0.829,
 'precision':0.883,
 'recall':0.802}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'DenseNet',
 'model':'D 1,8', 
 'epochs':30, 
 'arg min loss':26, 
 'arg max accuracy':28,
 'min loss':0.23,
 'max accuracy':0.95,
 'f1':0.945,
 'precision':0.950,
 'recall':0.946}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'ResNet',
 'model':'R101V2 1,8', 
 'epochs':30, 
 'arg min loss':10, 
 'arg max accuracy':16,
 'min loss':0.36,
 'max accuracy':0.92,
 'f1':0.909,
 'precision':0.913,
 'recall':0.911}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'ResNet',
 'model':'D 1,2,8', 
 'epochs':30, 
 'arg min loss':10, 
 'arg max accuracy':11,
 'min loss':0.21,
 'max accuracy':0.95,
 'f1':0.953,
 'precision':0.960,
 'recall':0.950}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'EfficientNet',
 'model':'EB7 1,2,9,10', 
 'epochs':30, 
 'arg min loss':29, 
 'arg max accuracy':27,
 'min loss':0.73,
 'max accuracy':0.84,
 'f1':0.779,
 'precision':0.839,
 'recall':0.755}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'EfficientNet',
 'model':'EB7 +11', 
 'epochs':30, 
 'arg min loss':29, 
 'arg max accuracy':28,
 'min loss':1.0,
 'max accuracy':0.81,
 'f1':0.775,
 'precision':0.769,
 'recall':0.821}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'EfficientNet',
 'model':'EB7 1,2,8,9,10,11', 
 'epochs':30, 
 'arg min loss':15, 
 'arg max accuracy':18,
 'min loss':0.25,
 'max accuracy':0.96,
 'f1':0.955,
 'precision':0.950,
 'recall':0.964}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'ResNet',
 'model':'D 1,2,8,11', 
 'epochs':30, 
 'arg min loss':24, 
 'arg max accuracy':23,
 'min loss':0.22,
 'max accuracy':0.95,
 'f1':0.956,
 'precision':0.957,
 'recall':0.958}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'Ensemble',
 'model':'Ensemble EB7+D201', 
 'epochs':30, 
 'arg min loss':24, 
 'arg max accuracy':23,
 'min loss':0.22,
 'max accuracy':0.95,
 'f1':0.962,
 'precision':0.960,
 'recall':0.966}

extra_columns = ['total params', 'trainable params', 'non-trainable params','training time per epoch (sec)']
model_performance_report[extra_columns] = pd.DataFrame([[np.nan, np.nan, np.nan, np.nan]], index=model_performance_report.index)

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'DenseNet',
 'model':'D 1,2,6',
 'total params':18_521_768,
 'trainable params':199_784,
 'non-trainable params':18_321_984,
 'training time per epoch (sec)':114,
 'epochs':30, 
 'arg min loss':29, 
 'arg max accuracy':29,
 'min loss':0.71,
 'max accuracy':0.85,
 'f1':0.826,
 'precision':0.791,
 'recall':0.890}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'DenseNet',
 'model':'D 1,2,6,12',
 'total params':18_521_768,
 'trainable params':199_784,
 'non-trainable params':18_321_984,
 'training time per epoch (sec)':114,
 'epochs':30, 
 'arg min loss':29, 
 'arg max accuracy':29,
 'min loss':0.71,
 'max accuracy':0.85,
 'f1':0.826,
 'precision':0.791,
 'recall':0.890}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'DenseNet',
 'model':'D 1,2,6,8',
 'total params':18_521_768,
 'trainable params':18_292_712,
 'non-trainable params':229_056,
 'training time per epoch (sec)':274,
 'epochs':30, 
 'arg min loss':26, 
 'arg max accuracy':28,
 'min loss':0.22,
 'max accuracy':0.96,
 'f1':0.948,
 'precision':0.942,
 'recall':0.957}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'DenseNet',
 'model':'D 1,2,6,8,12',
 'total params':18_521_768,
 'trainable params':18_292_712,
 'non-trainable params':229_056,
 'training time per epoch (sec)':274,
 'epochs':30, 
 'arg min loss':26, 
 'arg max accuracy':28,
 'min loss':0.22,
 'max accuracy':0.96,
 'f1':0.948,
 'precision':0.942,
 'recall':0.957}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'EfficientNet',
 'model':'EB7 1,2,6,8,9,10,11', 
 'total params':64_364_024,
 'trainable params':64_053_304,
 'non-trainable params':310_720,
 'training time per epoch (sec)':511, 
 'epochs':30, 
 'arg min loss':20, 
 'arg max accuracy':28,
 'min loss':0.24,
 'max accuracy':0.96,
 'f1':0.956,
 'precision':0.949,
 'recall':0.967}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'Ensemble',
 'model':'Ensemble 6,12 EB7+D201', 
 'total params':82_885_792,
 'trainable params':82_346_016,
 'non-trainable params':539_776,
 'training time per epoch (sec)':785, 
 'epochs':30, 
 'arg min loss':20, 
 'arg max accuracy':28,
 'min loss':0.24,
 'max accuracy':0.96,
 'f1':0.962,
 'precision':0.956,
 'recall':0.971}

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'MobileNet',
 'model':'MobileNetV2 1,2,6', 
 'total params':2_391_208,
 'trainable params':133_224,
 'non-trainable params':2_257_984,
 'training time per epoch (sec)':79, 
 'epochs':30, 
 'arg min loss':29, 
 'arg max accuracy':26,
 'min loss':0.83,
 'max accuracy':0.8,
 'f1':0.781,
 'precision':0.752,
 'recall':0.850} 

model_performance_report.loc[len(model_performance_report)]={ 'model-family':'MobileNet',
 'model':'MobileNetV2 1,2,6,8', 
 'total params':2_391_208,
 'trainable params':2_357_096,
 'non-trainable params':34_112,
 'training time per epoch (sec)':102, 
 'epochs':30, 
 'arg min loss':24, 
 'arg max accuracy':27,
 'min loss':0.27,
 'max accuracy':0.95,
 'f1':0.936,
 'precision':0.929,
 'recall':0.951}",computer-vision-petals-to-the-metal.ipynb
Plot miles per gallon against horsepower with other semantics,"with sns.axes_style(""whitegrid"" , { 'grid.linestyle' : '--' }): ",computer-vision-petals-to-the-metal.ipynb
Plot miles per gallon against horsepower with other semantics,"with sns.axes_style(""whitegrid"" , { 'grid.linestyle' : '--' }): ",computer-vision-petals-to-the-metal.ipynb
Plot model performance report in 3D ,model_performance_report.head(3),computer-vision-petals-to-the-metal.ipynb
Filter only for 12 epoch models,model_performance_report_filtered = model_performance_report.query('epochs == 12'). copy () ,computer-vision-petals-to-the-metal.ipynb
Filter only for 30 epoch models,model_performance_report_filtered = model_performance_report.query('epochs == 30'). copy () ,computer-vision-petals-to-the-metal.ipynb
Filter only for 30 epoch models,model_performance_report_filtered = model_performance_report.query('epochs == 30'). copy () ,computer-vision-petals-to-the-metal.ipynb
Filter only for 30 epoch models,model_performance_report_filtered = model_performance_report.query('epochs == 30'). copy () ,computer-vision-petals-to-the-metal.ipynb
"Visual ValidationIt can also be helpful to look at some examples from the validation set and see what class your model predicted. This can help reveal patterns in the kinds of images your model has trouble with.This cell will set up the validation set to display 20 images at a time you can change this to display more or fewer, if you like.","dataset = get_validation_dataset()
dataset = dataset.unbatch().batch(20)
batch = iter(dataset)",computer-vision-petals-to-the-metal.ipynb
And here is a set of flowers with their predicted species. Run the cell again to see another set.,"images, labels = next(batch)",computer-vision-petals-to-the-metal.ipynb
Mismatches on validation data Inspired by Rosa s notebook here,"mismatches = sum(cm_predictions!=cm_correct_labels)
print('Number of mismatches on validation data: {} out of {} or ({:.2%})'.format(mismatches, NUM_VALIDATION_IMAGES, mismatches/NUM_VALIDATION_IMAGES))",computer-vision-petals-to-the-metal.ipynb
"since we are splitting the dataset and iterating separately on images and labels, order matters.",cmdataset = get_validation_dataset(ordered = True) ,computer-vision-petals-to-the-metal.ipynb
get everything as one batch,cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))). numpy () ,computer-vision-petals-to-the-metal.ipynb
mismatches labels.append cm correct labels image index ,"dataset = get_validation_dataset()
dataset = dataset.unbatch().batch(20)
batch = iter(dataset)
images, labels = next(batch)",computer-vision-petals-to-the-metal.ipynb
Let s take a look again at some training images,"one_batch = next(ds_iter)
display_batch_of_images(one_batch)",computer-vision-petals-to-the-metal.ipynb
tuning12,using_tta = False ,computer-vision-petals-to-the-metal.ipynb
tuning4," dataset = dataset.map(data_augment , num_parallel_calls = AUTO) ",computer-vision-petals-to-the-metal.ipynb
"dataset dataset.map data augment v3, num parallel calls AUTO tuning4 0.44 performance : ", dataset = dataset.batch(BATCH_SIZE) ,computer-vision-petals-to-the-metal.ipynb
"since we are splitting the dataset and iterating separately on images and ids, order matters.", test_ds = get_test_dataset(ordered = True) ,computer-vision-petals-to-the-metal.ipynb
We ll generate a file submission.csv. This file is what you ll submit to get your score on the leaderboard.,"print('using_ensemble_models:' , using_ensemble_models) ",computer-vision-petals-to-the-metal.ipynb
Get image ids from test set and convert to unicode,"test_ids_ds = test_ds.map(lambda image , idnum : idnum). unbatch () ",computer-vision-petals-to-the-metal.ipynb
1. Enable Internet in the Kernel Settings side pane ,import sys ,connectx-baseline.ipynb
ConnectX environment was defined in v0.1.6,! pip install 'kaggle-environments>=0.1.6' ,connectx-baseline.ipynb
Create ConnectX Environment,"from kaggle_environments import evaluate, make, utils

env = make(""connectx"", debug=True)
env.render()",connectx-baseline.ipynb
"Create an AgentTo create the submission, an agent function should be fully encapsulated no external dependencies . When your agent is being evaluated against others, it will not have access to the Kaggle docker image. Only the following can be imported: Python Standard Library Modules, gym, numpy, scipy, pytorch 1.3.1, cpu only , and more may be added later.","def my_agent(obs , conf): ",connectx-baseline.ipynb
set board x y as mark, board[x][ y]= mark ,connectx-baseline.ipynb
if some points in axis already found axis blocked," blocked =[False , False , False , False] ",connectx-baseline.ipynb
i is amount of marks required to add points," for i in range(conf.inarow , 2 , - 1): ",connectx-baseline.ipynb
points, p = 0 ,connectx-baseline.ipynb
lowest cell, lc = 0 ,connectx-baseline.ipynb
 in air points, ap = 0 ,connectx-baseline.ipynb
"axis S N, only if one mark required for victory", if i == conf.inarow and blocked[0]is False : ,connectx-baseline.ipynb
The numbers just change the size of the screen,env.reset () ,connectx-baseline.ipynb
Play as the first agent against default random agent.,"env.run ([my_agent , ""random""]) ",connectx-baseline.ipynb
Play as first position against negamax agent.,"trainer = env.train ([None , ""negamax""]) ",connectx-baseline.ipynb
"env.render mode ipython , width 100, height 90, header False, controls False ",env.render () ,connectx-baseline.ipynb
Evaluate your Agent,def mean_reward(rewards): ,connectx-baseline.ipynb
Run multiple episodes to estimate its performance.,"print(""My Agent vs Random Agent:"" , mean_reward(evaluate(""connectx"" ,[my_agent , ""random""], num_episodes = 10))) ",connectx-baseline.ipynb
"env.play my agent, None , width 500, height 450 ","env.play ([None , my_agent], width = 500 , height = 450) ",connectx-baseline.ipynb
Write Submission File,"def write_agent_to_file(function, file):
 with open(file, ""a"" if os.path.exists(file) else ""w"") as f:
 f.write(inspect.getsource(function))
 print(function, ""written to"", file)

write_agent_to_file(my_agent, ""submission.py"")",connectx-baseline.ipynb
Validate Submission Play your submission against itself. This is the first episode the competition will run to weed out erroneous agents.Why validate? This roughly verifies that your submission is fully encapsulated and can be run remotely.,"out = sys.stdout
submission = utils.read_file(""/kaggle/working/submission.py"")
agent = utils.get_last_callable(submission)
sys.stdout = out

env = make(""connectx"", debug=True)
env.run([agent, agent])
print(""Success!"" if env.state[0].status == env.state[1].status == ""DONE"" else ""Failed..."")",connectx-baseline.ipynb
ConnectX environment was defined in v0.1.6,! pip install 'kaggle-environments>=0.1.6' ,connectx-getting-started.ipynb
Create ConnectX Environment,"from kaggle_environments import evaluate, make, utils

env = make(""connectx"", debug=True)
env.render()",connectx-getting-started.ipynb
This agent random chooses a non empty column.,"def my_agent(observation , configuration): ",connectx-getting-started.ipynb
Test your Agent,env.reset () ,connectx-getting-started.ipynb
Play as the first agent against default random agent.,"env.run ([my_agent , ""random""]) ",connectx-getting-started.ipynb
Play as first position against random agent.,"trainer = env.train ([None , ""random""]) ",connectx-getting-started.ipynb
"env.render mode ipython , width 100, height 90, header False, controls False ",env.render () ,connectx-getting-started.ipynb
Evaluate your Agent,def mean_reward(rewards): ,connectx-getting-started.ipynb
Run multiple episodes to estimate its performance.,"print(""My Agent vs Random Agent:"" , mean_reward(evaluate(""connectx"" ,[my_agent , ""random""], num_episodes = 10))) ",connectx-getting-started.ipynb
 None represents which agent you ll manually play as first or second player .,"env.play ([None , ""negamax""], width = 500 , height = 450) ",connectx-getting-started.ipynb
Write Submission File,"import inspect
import os

def write_agent_to_file(function, file):
 with open(file, ""a"" if os.path.exists(file) else ""w"") as f:
 f.write(inspect.getsource(function))
 print(function, ""written to"", file)

write_agent_to_file(my_agent, ""submission.py"")",connectx-getting-started.ipynb
Note: Stdout replacement is a temporary workaround.,import sys ,connectx-getting-started.ipynb
NOTE: qq requires using apt get not apt,! apt - get install time dos2unix - y - qq ,connectx-mcts-bitboard-bitsquares-heuristic.ipynb
!git checkout e275d53b40ce499ee670b0c1dd00bef17235affe,! cd / ai - games / ; git log - n1 ,connectx-mcts-bitboard-bitsquares-heuristic.ipynb
CodebaseThis can also be viewed on github: ,!cd /ai-games/games/connectx; /ai-games/kaggle_compile.py agents/MontyCarlo/MontyCarloBitsquares.py | dos2unix | sed 's/^# *@njit/@numba.njit/g' > /MontyCarloBitsquares.compiled.py,connectx-mcts-bitboard-bitsquares-heuristic.ipynb
Disable tests for other agents,! perl - p - i - e 's/^[# ]*(.*(?<!MontyCarloBitsquares)\(\).*)$/# $1/' / ai - games / games / connectx / tests / fixtures / agents.py ,connectx-mcts-bitboard-bitsquares-heuristic.ipynb
Copy datafile from previous notebook run comment out prefer github datafiles,! cp - f / kaggle / input / * / data / * base64.py / ai - games / games / connectx / data / ,connectx-mcts-bitboard-bitsquares-heuristic.ipynb
CWD still relative to data directory,! cd / ai - games / games / connectx ; time - p python3./ training_montycarlo.py | grep 'save\|load' ; ,connectx-mcts-bitboard-bitsquares-heuristic.ipynb
BUGFIX: windows lineendings when generated inside kaggle notebook,! dos2unix / ai - games / games / connectx / data / * base64.py 2 > / dev / null ,connectx-mcts-bitboard-bitsquares-heuristic.ipynb
"NOTE: This doesn t happen when running kaggle compile.py on localhost, only when running from inside an notebook",! perl - p - i - e 's/^(import numba|bitboard_type)/#$&/' / kaggle / working / submission.py ,connectx-mcts-bitboard-bitsquares-heuristic.ipynb
"Test we have no compilation errorsBUG: Notebook is currently experiencing out of memory errors on commit when training is included, so removing the runtime element of this notebook",%run submission.py,connectx-mcts-bitboard-bitsquares-heuristic.ipynb
Export data files to home directory to be downloadble from kaggle notebook output,"!cp -rf /ai-games/games/connectx/data /kaggle/working/
!rm -f /kaggle/working/data/__init__.py",connectx-mcts-bitboard-bitsquares-heuristic.ipynb
 Note: The first episode in the competition will run this to weed out erroneous agents.,"env = make(""connectx"" , debug = True) ",connectx-mcts-bitboard-bitsquares-heuristic.ipynb
Versus Negamax,"env = make(""connectx"", debug=True)
env.run([""negamax"", ""/kaggle/working/submission.py""])
print(""\nEXCELLENT SUBMISSION!"" if env.toJSON()[""statuses""] == [""DONE"", ""DONE""] else ""MAYBE BAD SUBMISSION?"")
env.render(mode=""ipython"", width=500, height=450)",connectx-mcts-bitboard-bitsquares-heuristic.ipynb
ConnectX environment was defined in v0.1.6,! pip install 'kaggle-environments>=0.1.6' ,connectx-rule-based.ipynb
Create ConnectX Environment,"from kaggle_environments import evaluate, make, utils

env = make(""connectx"", debug=True)
env.render()",connectx-rule-based.ipynb
"Create an AgentTo create the submission, an agent function should be fully encapsulated no external dependencies . When your agent is being evaluated against others, it will not have access to the Kaggle docker image. Only the following can be imported: Python Standard Library Modules, gym, numpy, scipy, pytorch 1.3.1, cpu only , and more may be added later.","def my_agent(obs , conf): ",connectx-rule-based.ipynb
set board x y as mark, board[x][ y]= mark ,connectx-rule-based.ipynb
if some points in axis already found axis blocked," blocked =[False , False , False , False] ",connectx-rule-based.ipynb
i is amount of marks required to add points," for i in range(conf.inarow , 2 , - 1): ",connectx-rule-based.ipynb
points, p = 0 ,connectx-rule-based.ipynb
lowest cell, lc = 0 ,connectx-rule-based.ipynb
 in air points, ap = 0 ,connectx-rule-based.ipynb
"axis S N, only if one mark required for victory", if i == conf.inarow and blocked[0]is False : ,connectx-rule-based.ipynb
Test your Agent,env.reset () ,connectx-rule-based.ipynb
Play as the first agent against negamax agent.,"env.run ([my_agent , my_agent]) ",connectx-rule-based.ipynb
"env.run my agent, negamax ","env.render(mode = ""ipython"" , width = 500 , height = 450) ",connectx-rule-based.ipynb
Play as first position against negamax agent.,"trainer = env.train ([None , ""negamax""]) ",connectx-rule-based.ipynb
"env.render mode ipython , width 100, height 90, header False, controls False ",env.render () ,connectx-rule-based.ipynb
Evaluate your Agent,def mean_reward(rewards): ,connectx-rule-based.ipynb
Run multiple episodes to estimate its performance.,"print(""My Agent vs Random Agent:"" , mean_reward(evaluate(""connectx"" ,[my_agent , ""random""], num_episodes = 10))) ",connectx-rule-based.ipynb
 None represents which agent you ll manually play as first or second player .,"env.play ([my_agent , None], width = 500 , height = 450) ",connectx-rule-based.ipynb
Write Submission File,"import inspect
import os

def write_agent_to_file(function, file):
 with open(file, ""a"" if os.path.exists(file) else ""w"") as f:
 f.write(inspect.getsource(function))
 print(function, ""written to"", file)

write_agent_to_file(my_agent, ""submission.py"")",connectx-rule-based.ipynb
Note: Stdout replacement is a temporary workaround.,import sys ,connectx-rule-based.ipynb
Install libraries Back to Table of Contents,!pip install 'kaggle-environments==0.1.6' > /dev/null 2>&1,connectx-with-deep-q-learning-pytorch.ipynb
Import libraries Back to Table of Contents,"import numpy as np
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
from kaggle_environments import evaluate, make",connectx-with-deep-q-learning-pytorch.ipynb
"Define useful classes NOTE: All classes here were copied from my previous kernel and switched from using TF2.0 to PyTorch. If you prefer TF2.0, let check ConnectX with Deep Q Learning kernel. Back to Table of Contents",class ConnectX(gym.Env): ,connectx-with-deep-q-learning-pytorch.ipynb
Define required gym fields examples :, config = self.env.configuration ,connectx-with-deep-q-learning-pytorch.ipynb
Define helper functions Back to Table of Contents,"def play_game(env , TrainNet , TargetNet , epsilon , copy_step): ",connectx-with-deep-q-learning-pytorch.ipynb
Using epsilon greedy to get an action," action = TrainNet.get_action(observations , epsilon) ",connectx-with-deep-q-learning-pytorch.ipynb
Caching the information of current state, prev_observations = observations ,connectx-with-deep-q-learning-pytorch.ipynb
Take action," observations , reward , done , _ = env.step(action) ",connectx-with-deep-q-learning-pytorch.ipynb
Apply new rules, if done : ,connectx-with-deep-q-learning-pytorch.ipynb
Won, if reward == 1 : ,connectx-with-deep-q-learning-pytorch.ipynb
Lost, elif reward == 0 : ,connectx-with-deep-q-learning-pytorch.ipynb
Draw, else : ,connectx-with-deep-q-learning-pytorch.ipynb
as Magolor s magolor idea, reward = 0.5 ,connectx-with-deep-q-learning-pytorch.ipynb
Adding experience into buffer," exp = { 's' : prev_observations , 'a' : action , 'r' : reward , 's2' : observations , 'done' : done } ",connectx-with-deep-q-learning-pytorch.ipynb
Train the training model by using experiences in buffer and the target model, TrainNet.train(TargetNet) ,connectx-with-deep-q-learning-pytorch.ipynb
Update the weights of the target model when reaching enough copy step , TargetNet.copy_weights(TrainNet) ,connectx-with-deep-q-learning-pytorch.ipynb
Create ConnectX environment Back to Table of Contents,env = ConnectX(),connectx-with-deep-q-learning-pytorch.ipynb
Configure hyper parameters Back to Table of Contents,"gamma = 0.99
copy_step = 25
hidden_units = [128, 128, 128, 128, 128]
max_experiences = 10000
min_experiences = 100
batch_size = 32
lr = 1e-2
epsilon = 0.5
decay = 0.9999
min_epsilon = 0.1
episodes = 20000

precision = 7",connectx-with-deep-q-learning-pytorch.ipynb
Train the agent Back to Table of Contents,num_states = env.observation_space.n + 1 ,connectx-with-deep-q-learning-pytorch.ipynb
Last 100 steps,all_avg_rewards = np.empty(episodes) ,connectx-with-deep-q-learning-pytorch.ipynb
Initialize models,"TrainNet = DQN(num_states , num_actions , hidden_units , gamma , max_experiences , min_experiences , batch_size , lr) ",connectx-with-deep-q-learning-pytorch.ipynb
plt.show ,"plt.plot(all_avg_rewards)
plt.xlabel('Episode')
plt.ylabel('Avg rewards (100)')
plt.show()",connectx-with-deep-q-learning-pytorch.ipynb
Save weights Back to Table of Contents,TrainNet.save_weights('./weights.pth'),connectx-with-deep-q-learning-pytorch.ipynb
Create an agent Back to Table of Contents,fc_layers = [] ,connectx-with-deep-q-learning-pytorch.ipynb
Get all hidden layers weights,for i in range(len(hidden_units)) : ,connectx-with-deep-q-learning-pytorch.ipynb
Evaluate the agent Back to Table of Contents,from submission import my_agent,connectx-with-deep-q-learning-pytorch.ipynb
Run multiple episodes to estimate agent s performance.,"print(""My Agent vs. Random Agent:"" , mean_reward(evaluate(""connectx"" ,[my_agent , ""random""], num_episodes = 10))) ",connectx-with-deep-q-learning-pytorch.ipynb
Install libraries Back to Table of Contents,!pip install 'kaggle-environments==0.1.6' > /dev/null 2>&1,connectx-with-deep-q-learning.ipynb
Import libraries Back to Table of Contents,"import numpy as np
import gym
import tensorflow as tf
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
from kaggle_environments import evaluate, make",connectx-with-deep-q-learning.ipynb
Define useful classes NOTE: I use the neural network in Siwei Xu s tutorial with some proper modifications to adapt to the problem in ConnectX competition and be able to save trained and load pre trained models.Back to Table of Contents,class ConnectX(gym.Env): ,connectx-with-deep-q-learning.ipynb
Define required gym fields examples :, config = self.env.configuration ,connectx-with-deep-q-learning.ipynb
Define helper functions Back to Table of Contents,"def play_game(env , TrainNet , TargetNet , epsilon , copy_step): ",connectx-with-deep-q-learning.ipynb
Using epsilon greedy to get an action," action = TrainNet.get_action(observations , epsilon) ",connectx-with-deep-q-learning.ipynb
Caching the information of current state, prev_observations = observations ,connectx-with-deep-q-learning.ipynb
Take action," observations , reward , done , _ = env.step(action) ",connectx-with-deep-q-learning.ipynb
Apply new rules, if done : ,connectx-with-deep-q-learning.ipynb
Won, if reward == 1 : ,connectx-with-deep-q-learning.ipynb
Lost, elif reward == 0 : ,connectx-with-deep-q-learning.ipynb
Draw, else : ,connectx-with-deep-q-learning.ipynb
Try to prevent the agent from taking a long move, reward = - 0.05 ,connectx-with-deep-q-learning.ipynb
Adding experience into buffer," exp = { 's' : prev_observations , 'a' : action , 'r' : reward , 's2' : observations , 'done' : done } ",connectx-with-deep-q-learning.ipynb
Train the training model by using experiences in buffer and the target model, TrainNet.train(TargetNet) ,connectx-with-deep-q-learning.ipynb
Update the weights of the target model when reaching enough copy step , TargetNet.copy_weights(TrainNet) ,connectx-with-deep-q-learning.ipynb
Create ConnectX environment Back to Table of Contents,env = ConnectX(),connectx-with-deep-q-learning.ipynb
Configure hyper parameters Back to Table of Contents,gamma = 0.99 ,connectx-with-deep-q-learning.ipynb
Train the agent Back to Table of Contents,num_states = env.observation_space.n + 1 ,connectx-with-deep-q-learning.ipynb
Last 100 steps,all_avg_rewards = np.empty(episodes) ,connectx-with-deep-q-learning.ipynb
Initialize models,"TrainNet = DQN(num_states , num_actions , hidden_units , gamma , max_experiences , min_experiences , batch_size , lr) ",connectx-with-deep-q-learning.ipynb
plt.show ,"plt.plot(all_avg_rewards)
plt.xlabel('Episode')
plt.ylabel('Avg rewards (100)')
plt.show()",connectx-with-deep-q-learning.ipynb
Save weights Back to Table of Contents,TrainNet.save_weights('./weights.h5'),connectx-with-deep-q-learning.ipynb
Create an agent Back to Table of Contents,fc_layers = [] ,connectx-with-deep-q-learning.ipynb
Get all hidden layers weights,for i in range(len(hidden_units)) : ,connectx-with-deep-q-learning.ipynb
Evaluate the agent Back to Table of Contents,from submission import my_agent,connectx-with-deep-q-learning.ipynb
Run multiple episodes to estimate agent s performance.,"print(""My Agent vs. Random Agent:"" , mean_reward(evaluate(""connectx"" ,[my_agent , ""random""], num_episodes = 10))) ",connectx-with-deep-q-learning.ipynb
Install libraries Back to Table of Contents,!pip install 'kaggle-environments==0.1.6' > /dev/null 2>&1,connectx-with-q-learning.ipynb
Import libraries Back to Table of Contents,"import numpy as np
import gym
import random
import matplotlib.pyplot as plt
from random import choice
from tqdm.notebook import tqdm
from kaggle_environments import evaluate, make",connectx-with-q-learning.ipynb
"Define useful classes NOTE: It s not easy to generate a Q Table with all possible states and even if I can do so, the huge number of states will cost much of memory. So, I use the approach that dynamically adding newly discovered states into an object of QTable class created below.Back to Table of Contents",class ConnectX(gym.Env): ,connectx-with-q-learning.ipynb
Define required gym fields examples :, config = self.env.configuration ,connectx-with-q-learning.ipynb
Get a copy, board = state.board[:] ,connectx-with-q-learning.ipynb
Create ConnectX environment Back to Table of Contents,env = ConnectX(),connectx-with-q-learning.ipynb
Configure hyper parameters Back to Table of Contents,"alpha = 0.1
gamma = 0.6
epsilon = 0.99
min_epsilon = 0.1

episodes = 10000

alpha_decay_step = 1000
alpha_decay_rate = 0.9
epsilon_decay_rate = 0.9999",connectx-with-q-learning.ipynb
Train the agent Back to Table of Contents,q_table = QTable(env.action_space) ,connectx-with-q-learning.ipynb
Last 100 steps,all_avg_rewards = [] ,connectx-with-q-learning.ipynb
Apply new rules, if done : ,connectx-with-q-learning.ipynb
Won, if reward == 1 : ,connectx-with-q-learning.ipynb
Lost, elif reward == 0 : ,connectx-with-q-learning.ipynb
Draw, else : ,connectx-with-q-learning.ipynb
Try to prevent the agent from taking a long move, reward = - 0.05 ,connectx-with-q-learning.ipynb
Update Q value, new_value =(1 - alpha)* old_value + alpha *(reward + gamma * next_max) ,connectx-with-q-learning.ipynb
plt.show ,"plt.plot(all_avg_rewards)
plt.xlabel('Episode')
plt.ylabel('Avg rewards (100)')
plt.show()",connectx-with-q-learning.ipynb
Create an Agent Back to Table of Contents,"tmp_dict_q_table = q_table.table.copy()
dict_q_table = dict()

for k in tmp_dict_q_table:
 if np.count_nonzero(tmp_dict_q_table[k]) > 0:
 dict_q_table[k] = int(np.argmax(tmp_dict_q_table[k]))",connectx-with-q-learning.ipynb
Evaluate the agent Back to Table of Contents,from submission import my_agent,connectx-with-q-learning.ipynb
Run multiple episodes to estimate agent s performance.,"print(""My Agent vs Random Agent:"" , mean_reward(evaluate(""connectx"" ,[my_agent , ""random""], num_episodes = 10))) ",connectx-with-q-learning.ipynb
Import Important packages,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
from kaggle_datasets import KaggleDatasets
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input,Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
import transformers
from transformers import TFAutoModel, AutoTokenizer
from sklearn.model_selection import StratifiedKFold,KFold
plt.style.use('fivethirtyeight')
import warnings
warnings.filterwarnings('ignore')
import os",contradiction-xlm-kfold-starter.ipynb
Getting Basic Idea,"df_train=pd.read_csv(os.path.join(path,""train.csv""))
df_test=pd.read_csv(os.path.join(path,""test.csv""))",contradiction-xlm-kfold-starter.ipynb
Language distribution,langs = df_train.language.unique () ,contradiction-xlm-kfold-starter.ipynb
Class distribution,langs = df_train.label.unique () ,contradiction-xlm-kfold-starter.ipynb
"Detect hardware, return appropriate distribution strategy",try : ,contradiction-xlm-kfold-starter.ipynb
set: this is always the case on Kaggle., tpu = tf.distribute.cluster_resolver.TPUClusterResolver () ,contradiction-xlm-kfold-starter.ipynb
Default distribution strategy in Tensorflow. Works on CPU and single GPU., strategy = tf.distribute.get_strategy () ,contradiction-xlm-kfold-starter.ipynb
Our batch size will depend on number of replic,BATCH_SIZE = 16 * strategy.num_replicas_in_sync ,contradiction-xlm-kfold-starter.ipynb
Fast Encoder,"def quick_encode(df,maxlen=100):
 
 values = df[['premise','hypothesis']].values.tolist()
 tokens=tokenizer.batch_encode_plus(values,max_length=maxlen,pad_to_max_length=True)
 
 return np.array(tokens['input_ids'])

x_train = quick_encode(df_train)
x_test = quick_encode(df_test)
y_train = df_train.label.values
 ",contradiction-xlm-kfold-starter.ipynb
Dataset ,"

def create_dist_dataset(X, y,val,batch_size= BATCH_SIZE):
 
 
 dataset = tf.data.Dataset.from_tensor_slices((X,y)).shuffle(len(X))
 
 if not val:
 dataset = dataset.repeat().batch(batch_size).prefetch(AUTO)
 else:
 dataset = dataset.batch(batch_size).prefetch(AUTO)

 
 
 return dataset



test_dataset = (
 tf.data.Dataset
 .from_tensor_slices((x_test))
 .batch(BATCH_SIZE)
)
",contradiction-xlm-kfold-starter.ipynb
Model,"def build_model(transformer , max_len): ",contradiction-xlm-kfold-starter.ipynb
It s time to build and compile the model," model = Model(inputs = input_ids , outputs = out) ",contradiction-xlm-kfold-starter.ipynb
LR Scheduler source :,"def build_lrfn(lr_start=0.00001, lr_max=0.00003, 
 lr_min=0.000001, lr_rampup_epochs=3, 
 lr_sustain_epochs=0, lr_exp_decay=.6):
 lr_max = lr_max * strategy.num_replicas_in_sync

 def lrfn(epoch):
 if epoch < lr_rampup_epochs:
 lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start
 elif epoch < lr_rampup_epochs + lr_sustain_epochs:
 lr = lr_max
 else:
 lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min
 return lr
 
 return lrfn",contradiction-xlm-kfold-starter.ipynb
Kfold CV,"skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)
val_score=[]
history=[]


for fold,(train_ind,valid_ind) in enumerate(skf.split(x_train,y_train)):
 
 if fold < 4:
 
 print(""fold"",fold+1)
 
 
 tf.tpu.experimental.initialize_tpu_system(tpu)
 
 train_data = create_dist_dataset(x_train[train_ind],y_train[train_ind],val=False)
 valid_data = create_dist_dataset(x_train[valid_ind],y_train[valid_ind],val=True)
 
 Checkpoint=tf.keras.callbacks.ModelCheckpoint(f""roberta_base.h5"", monitor='val_loss', verbose=0, save_best_only=True,
 save_weights_only=True, mode='min')
 
 with strategy.scope():
 transformer_layer = TFAutoModel.from_pretrained(MODEL)
 model = build_model(transformer_layer, max_len=MAX_LEN)
 
 

 n_steps = len(train_ind)//BATCH_SIZE
 print(""training model {} "".format(fold+1))

 train_history = model.fit(
 train_data,
 steps_per_epoch=n_steps,
 validation_data=valid_data,
 epochs=EPOCHS,callbacks=[Checkpoint],verbose=1)
 
 print(""Loading model..."")
 model.load_weights(f""roberta_base.h5"")
 
 

 print(""fold {} validation accuracy {}"".format(fold+1,np.mean(train_history.history['val_accuracy'])))
 print(""fold {} validation loss {}"".format(fold+1,np.mean(train_history.history['val_loss'])))
 
 val_score.append(train_history.history['val_accuracy'])
 history.append(train_history)

 val_score.append(np.mean(train_history.history['val_accuracy']))
 
 print('predict on test....')
 preds=model.predict(test_dataset,verbose=1)
 
 pred_test+=preds/4
 

 
print(""Mean Validation accuracy : "",np.mean(val_score))",contradiction-xlm-kfold-starter.ipynb
Evaluation,"
plt.figure(figsize=(15,10))

for i,hist in enumerate(history):

 plt.subplot(2,2,i+1)
 plt.plot(np.arange(EPOCHS),hist.history['accuracy'],label='train accu')
 plt.plot(np.arange(EPOCHS),hist.history['val_accuracy'],label='validation acc')
 plt.gca().title.set_text(f'Fold {i+1} accuracy curve')
 plt.legend()


 ",contradiction-xlm-kfold-starter.ipynb
Submission,"submission = pd.read_csv(os.path.join(path,'sample_submission.csv'))
submission['prediction'] = np.argmax(pred_test,axis=1)
submission.head()",contradiction-xlm-kfold-starter.ipynb
linear algebra,import numpy as np ,contradictory-my-dear-watson-everything-you-need.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,contradictory-my-dear-watson-everything-you-need.ipynb
Data Visualisation,from plotly.offline import iplot ,contradictory-my-dear-watson-everything-you-need.ipynb
Data Preprocessing,import re ,contradictory-my-dear-watson-everything-you-need.ipynb
Visializing similarity of words,from sklearn.manifold import TSNE ,contradictory-my-dear-watson-everything-you-need.ipynb
Models,import xgboost as xgb ,contradictory-my-dear-watson-everything-you-need.ipynb
Simple Data Exploration,"train_df = pd.read_csv(""/kaggle/input/contradictory-my-dear-watson/train.csv"")
test_df = pd.read_csv(""/kaggle/input/contradictory-my-dear-watson/test.csv"")
print(""Number of rows and columns in train data : "",train_df.shape)
print(""Number of rows and columns in test data : "",test_df.shape)",contradictory-my-dear-watson-everything-you-need.ipynb
Target Variable Exploration,"Accuracy=pd.DataFrame()
Accuracy['Type']=train_df.label.value_counts().index
Accuracy['Count']=train_df.label.value_counts().values
Accuracy['Type']=Accuracy['Type'].replace(0,'Entailment')
Accuracy['Type']=Accuracy['Type'].replace(1,'Neutral')
Accuracy['Type']=Accuracy['Type'].replace(2,'Contradiction')
Accuracy",contradictory-my-dear-watson-everything-you-need.ipynb
Languages in Train and Test data,"Languages=pd.DataFrame()
Languages['Type']=train_df.language.value_counts().index
Languages['Count']=train_df.language.value_counts().values",contradictory-my-dear-watson-everything-you-need.ipynb
"ObservationFrom the above graph, we can see that English is dominating language in the given dataset.","Languages_test=pd.DataFrame()
Languages_test['Type']=test_df.language.value_counts().index
Languages_test['Count']=test_df.language.value_counts().values
a = sum(Languages_test.Count)
Languages_test.Count = Languages_test.Count.div(a).mul(100).round(2)",contradictory-my-dear-watson-everything-you-need.ipynb
"While dealing the text data, feature engineering can be done in two parts. They are Meta features features that are extracted from the text like number of words, number of stop words, number of punctuations etc Text based features features directly based on the text words like frequency, svd, word2vec etc. Meta Features:We will start with creating meta featues and see how good are they at predicting the spooky authors. The feature list is as follows: Number of words in the text Number of unique words in the text Number of characters in the text Number of stopwords Number of punctuations Number of upper case words Number of title case words Average length of the words We ll try to analyse the Meta features between Premesis and Hypothesis. If possible we ll try to include them in models, in later part of this notebook",import string,contradictory-my-dear-watson-everything-you-need.ipynb
Number of words in the text,"Meta_features[""premise_num_words""]= train_df[""premise""]. apply(lambda x : len(str(x). split ())) ",contradictory-my-dear-watson-everything-you-need.ipynb
Number of characters in the text,"Meta_features[""premise_num_chars""]= train_df[""premise""]. apply(lambda x : len(str(x))) ",contradictory-my-dear-watson-everything-you-need.ipynb
Number of punctuations in the text,"Meta_features[""premise_num_punctuations""]= train_df[""premise""]. apply(lambda x : len ([c for c in str(x)if c in string.punctuation])) ",contradictory-my-dear-watson-everything-you-need.ipynb
Average length of the words in the text,"Meta_features[""premise_mean_word_len""]= train_df[""premise""]. apply(lambda x : np.mean ([len(w)for w in str(x). split()])) ",contradictory-my-dear-watson-everything-you-need.ipynb
"ObservationThe distribution of words across the classes are almost the same for contradiction and Neutral, whereas, it is little bit less in Entailment ","fig = go.Figure()
for category in categories:
 fig.add_trace(go.Violin(x=Meta_features['label'][Meta_features['label'] == category],
 y=Meta_features['premise_num_punctuations'][Meta_features['label'] == category],
 name=Name[category],
 box_visible=True,
 meanline_visible=True))
 

fig.update_layout( title={
 'text': ""Number of Punctuations in Premise per category"",
 'y':0.9,
 'x':0.5,
 'xanchor': 'center',
 'yanchor': 'top'})

fig.show()",contradictory-my-dear-watson-everything-you-need.ipynb
"ObservationThe distribution of punctuations across the classes are almost the same for contradiction and entailment, whereas, it is little bit less in Neutral",fig = go.Figure () ,contradictory-my-dear-watson-everything-you-need.ipynb
Overlay both histograms,fig = go.Figure () ,contradictory-my-dear-watson-everything-you-need.ipynb
Overlay both histograms,fig = go.Figure () ,contradictory-my-dear-watson-everything-you-need.ipynb
train df.hypothesis train df.lang abv! en train df.hypothesis train df.lang abv! en .apply lambda x: Translation x ,"train_df = pd.read_csv(""../input/contradictory-my-watson-translated/train_translated.csv"")
test_df = pd.read_csv(""../input/contradictory-my-watson-translated/test_translated.csv"")",contradictory-my-dear-watson-everything-you-need.ipynb
"t SNE on word vectors t Distributed Stochastic Neighbor Embedding is a non linear dimensionality reduction algorithm used for exploring high dimensional data. It works by taking a group of high dimensional vocabulary word feature vectors, then compresses them down to 2 dimensional x,y coordinate pairs. The idea is to keep similar words close together on the plane, while maximizing the distance between dissimilar words. ","def tsne_plot(model):
 ""Creates and TSNE model and plots it""
 labels = []
 tokens = []

 for word in model.wv.vocab:
 tokens.append(model[word])
 labels.append(word)
 
 tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)
 new_values = tsne_model.fit_transform(tokens)

 x = []
 y = []
 for value in new_values:
 x.append(value[0])
 y.append(value[1])
 
 plt.figure(figsize=(16, 16)) 
 for i in range(len(x)):
 plt.scatter(x[i],y[i])
 plt.annotate(labels[i],
 xy=(x[i], y[i]),
 xytext=(5, 2),
 textcoords='offset points',
 ha='right',
 va='bottom')
 plt.show()",contradictory-my-dear-watson-everything-you-need.ipynb
A more selective model,"model = word2vec.Word2Vec(corpus , size = 100 , window = 20 , min_count = 150 , workers = 4) ",contradictory-my-dear-watson-everything-you-need.ipynb
Number of words in the text,"train_df[""premise_num_words""]= train_df[""premise""]. apply(lambda x : len(str(x). split ())) ",contradictory-my-dear-watson-everything-you-need.ipynb
Number of characters in the text,"train_df[""premise_num_chars""]= train_df[""premise""]. apply(lambda x : len(str(x))) ",contradictory-my-dear-watson-everything-you-need.ipynb
Number of punctuations in the text,"train_df[""premise_num_punctuations""]= train_df[""premise""]. apply(lambda x : len ([c for c in str(x)if c in string.punctuation])) ",contradictory-my-dear-watson-everything-you-need.ipynb
Average length of the words in the text,"train_df[""premise_mean_word_len""]= train_df[""premise""]. apply(lambda x : np.mean ([len(w)for w in str(x). split()])) ",contradictory-my-dear-watson-everything-you-need.ipynb
Language Transformation,lb_make = LabelEncoder () ,contradictory-my-dear-watson-everything-you-need.ipynb
lang abv Transformation,lb_make = LabelEncoder () ,contradictory-my-dear-watson-everything-you-need.ipynb
removing short word, if len(i)>= 3 : ,contradictory-my-dear-watson-everything-you-need.ipynb
premise,"tfidf_vec = TfidfVectorizer(analyzer = 'word' , max_features = 1000) ",contradictory-my-dear-watson-everything-you-need.ipynb
premise,"tfidf_vec = TfidfVectorizer(analyzer = 'word' , max_features = 1000) ",contradictory-my-dear-watson-everything-you-need.ipynb
"Detect hardware, return appropriate distribution strategy",try : ,contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
set: this is always the case on Kaggle., tpu = tf.distribute.cluster_resolver.TPUClusterResolver () ,contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
Default distribution strategy in Tensorflow. Works on CPU and single GPU., strategy = tf.distribute.get_strategy () ,contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
Define variablesMake sure to keep those variables in mind as you navigate this notebook! They are all placed below so you can easily change and rerun this notebook.Don t worry about the model right now. We will come back to it later.,model_name = 'jplu/tf-xlm-roberta-large' ,contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
Our batch size will depend on number of replicas,batch_size = 16 * strategy.num_replicas_in_sync ,contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
Load datasetsJust regular CSV files. Nothing scary here!,"train = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')
test = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')
submission = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/sample_submission.csv')",contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
First load the tokenizer,tokenizer = AutoTokenizer.from_pretrained(model_name) ,contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
Convert the text so that we can feed it to batch encode plus,"train_text = train[[ 'premise' , 'hypothesis']].values.tolist () ",contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
Train and validation split happens here:,"x_train, x_valid, y_train, y_valid = train_test_split(
 train_encoded['input_ids'], train.label.values, 
 test_size=0.2, random_state=2020
)

x_test = test_encoded['input_ids']",contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
"Convert to tf.data.Datasettf.data.Dataset is one of many different ways to define the input to our models. Here, it is a good choice since it is easily compatible with TPUs. Read more about it in this article.","auto = tf.data.experimental.AUTOTUNE

train_dataset = (
 tf.data.Dataset
 .from_tensor_slices((x_train, y_train))
 .repeat()
 .shuffle(2048)
 .batch(batch_size)
 .prefetch(auto)
)

valid_dataset = (
 tf.data.Dataset
 .from_tensor_slices((x_valid, y_valid))
 .batch(batch_size)
 .cache()
 .prefetch(auto)
)

test_dataset = (
 tf.data.Dataset
 .from_tensor_slices(x_test)
 .batch(batch_size)
)",contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
"Train the modelIt s time to teach our lovely XLM Roberta how to infer natural language. Notice here we are using strategy.scope . We need to load transformer encoder inside this scope in order to tell Tensorflow that we want our model on the TPUs. Otherwise, it will try to load it in your CPU machine!XLM Roberta is one of the best models out there for multilingual classification tasks. Essentially, it is a model that was trained on inherently multilingual text, and used methods that helped it become larger, train longer and on more data! Highly recommend you to read this blog post by the authors, as well as the Huggingface docs on the subject.",with strategy.scope (): ,contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
First load the transformer layer, transformer_encoder = TFAutoModel.from_pretrained(model_name) ,contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
This will be the input tokens," input_ids = Input(shape =(max_len ,), dtype = tf.int32 , name = ""input_ids"") ",contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
"Now, we encode the text using the transformers we just loaded", sequence_output = transformer_encoder(input_ids)[ 0] ,contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
"Only extract the token used for classification, which is "," cls_token = sequence_output[: , 0 , :] ",contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
"Finally, pass it through a 3 way softmax, since there s 3 possible laels"," out = Dense(3 , activation = 'softmax')( cls_token) ",contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
It s time to build and compile the model," model = Model(inputs = input_ids , outputs = out) ",contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
Unhide below to see the exactly training accuracy and loss after each epoch:,"n_steps = len(x_train) // batch_size

train_history = model.fit(
 train_dataset,
 steps_per_epoch=n_steps,
 validation_data=valid_dataset,
 epochs=n_epochs
)",contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
Predict on test set and submit,"test_preds = model.predict(test_dataset, verbose=1)
submission['prediction'] = test_preds.argmax(axis=1)",contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
"Visualize Training HistoryWith Plotly Express, this can be done in one function call:",hist = train_history.history,contradictory-watson-concise-keras-xlm-r-on-tpu.ipynb
Format kernel, kernel = np.array(kernel) ,convolution-and-relu.ipynb
Plot kernel, cmap = plt.get_cmap('Blues_r') ,convolution-and-relu.ipynb
"Optionally, add value labels", if label : ,convolution-and-relu.ipynb
"IntroductionIn the last lesson, we saw that a convolutional classifier has two parts: a convolutional base and a head of dense layers. We learned that the job of the base is to extract visual features from an image, which the head would then use to classify the image.Over the next few lessons, we re going to learn about the two most important types of layers that you ll usually find in the base of a convolutional image classifier. These are the convolutional layer with ReLU activation, and the maximum pooling layer. In Lesson 5, you ll learn how to design your own convnet by composing these layers into blocks that perform the feature extraction.This lesson is about the convolutional layer with its ReLU activation function.Feature ExtractionBefore we get into the details of convolution, let s discuss the purpose of these layers in the network. We re going to see how these three operations convolution, ReLU, and maximum pooling are used to implement the feature extraction process.The feature extraction performed by the base consists of three basic operations: 1. Filter an image for a particular feature convolution 2. Detect that feature within the filtered image ReLU 3. Condense the image to enhance the features maximum pooling The next figure illustrates this process. You can see how these three operations are able to isolate some particular characteristic of the original image in this case, horizontal lines . The three steps of feature extraction. Typically, the network will perform several extractions in parallel on a single image. In modern convnets, it s not uncommon for the final layer in the base to be producing over 1000 unique visual features.Filter with ConvolutionA convolutional layer carries out the filtering step. You might define a convolutional layer in a Keras model something like this:",from tensorflow import keras ,convolution-and-relu.ipynb
"You could think about the activation function as scoring pixel values according to some measure of importance. The ReLU activation says that negative values are not important and so sets them to 0. Everything unimportant is equally unimportant. Here is ReLU applied the feature maps above. Notice how it succeeds at isolating the features. Like other activation functions, the ReLU function is nonlinear. Essentially this means that the total effect of all the layers in the network becomes different than what you would get by just adding the effects together which would be the same as what you could achieve with only a single layer. The nonlinearity ensures features will combine in interesting ways as they move deeper into the network. We ll explore this feature compounding more in Lesson 5. Example Apply Convolution and ReLUWe ll do the extraction ourselves in this example to understand better what convolutional networks are doing behind the scenes .Here is the image we ll use for this example:","
import tensorflow as tf
import matplotlib.pyplot as plt
plt.rc('figure', autolayout=True)
plt.rc('axes', labelweight='bold', labelsize='large',
 titleweight='bold', titlesize=18, titlepad=10)
plt.rc('image', cmap='magma')

image_path = '../input/computer-vision-resources/car_feature.jpg'
image = tf.io.read_file(image_path)
image = tf.io.decode_jpeg(image)

plt.figure(figsize=(6, 6))
plt.imshow(tf.squeeze(image), cmap='gray')
plt.axis('off')
plt.show();",convolution-and-relu.ipynb
"For the filtering step, we ll define a kernel and then apply it with the convolution. The kernel in this case is an edge detection kernel. You can define it with tf.constant just like you d define an array in Numpy with np.array. This creates a tensor of the sort TensorFlow uses.","import tensorflow as tf

kernel = tf.constant([
 [-1, -1, -1],
 [-1, 8, -1],
 [-1, -1, -1],
])

plt.figure(figsize=(3, 3))
show_kernel(kernel)",convolution-and-relu.ipynb
Reformat for batch compatibility.,"image = tf.image.convert_image_dtype(image , dtype = tf.float32) ",convolution-and-relu.ipynb
"Next is the detection step with the ReLU function. This function is much simpler than the convolution, as it doesn t have any parameters to set.","image_detect = tf.nn.relu(image_filter)

plt.figure(figsize=(6, 6))
plt.imshow(tf.squeeze(image_detect))
plt.axis('off')
plt.show();",convolution-and-relu.ipynb
linear algebra,import numpy as np ,convolutional-neural-network-cnn-tutorial.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,convolutional-neural-network-cnn-tutorial.ipynb
import warnings,import warnings ,convolutional-neural-network-cnn-tutorial.ipynb
filter warnings,warnings.filterwarnings('ignore') ,convolutional-neural-network-cnn-tutorial.ipynb
"For example, running this by clicking run or pressing Shift Enter will list the files in the input directory",import os ,convolutional-neural-network-cnn-tutorial.ipynb
read train,"train = pd.read_csv(""../input/train.csv"") ",convolutional-neural-network-cnn-tutorial.ipynb
read test,"test = pd.read_csv(""../input/test.csv"") ",convolutional-neural-network-cnn-tutorial.ipynb
put labels into y train variable,"Y_train = train[""label""] ",convolutional-neural-network-cnn-tutorial.ipynb
Drop label column,"X_train = train.drop(labels =[""label""], axis = 1) ",convolutional-neural-network-cnn-tutorial.ipynb
visualize number of digits classes,"plt.figure(figsize =(15 , 7)) ",convolutional-neural-network-cnn-tutorial.ipynb
plot some samples,img = X_train.iloc[0]. as_matrix () ,convolutional-neural-network-cnn-tutorial.ipynb
plot some samples,img = X_train.iloc[3]. as_matrix () ,convolutional-neural-network-cnn-tutorial.ipynb
Normalize the data,X_train = X_train / 255.0 ,convolutional-neural-network-cnn-tutorial.ipynb
Reshape,"X_train = X_train.values.reshape(- 1 , 28 , 28 , 1) ",convolutional-neural-network-cnn-tutorial.ipynb
convert to one hot encoding,from keras.utils.np_utils import to_categorical ,convolutional-neural-network-cnn-tutorial.ipynb
Split the train and the validation set for the fitting,from sklearn.model_selection import train_test_split ,convolutional-neural-network-cnn-tutorial.ipynb
Some examples,"plt.imshow(X_train[2][ : , : , 0], cmap = 'gray') ",convolutional-neural-network-cnn-tutorial.ipynb
Create Model conv max pool dropout conv max pool dropout fully connected 2 layer Dropout: Dropout is a technique where randomly selected neurons are ignored during training ,from sklearn.metrics import confusion_matrix ,convolutional-neural-network-cnn-tutorial.ipynb
convert to one hot encoding,from keras.utils.np_utils import to_categorical ,convolutional-neural-network-cnn-tutorial.ipynb
Define the optimizer,"optimizer = Adam(lr = 0.001 , beta_1 = 0.9 , beta_2 = 0.999) ",convolutional-neural-network-cnn-tutorial.ipynb
Compile the model,"model.compile(optimizer = optimizer , loss = ""categorical_crossentropy"" , metrics =[""accuracy""]) ",convolutional-neural-network-cnn-tutorial.ipynb
for better result increase the epochs,epochs = 10 ,convolutional-neural-network-cnn-tutorial.ipynb
Plot the loss and accuracy curves for training and validation,"plt.plot(history.history['val_loss'], color = 'b' , label = ""validation loss"") ",convolutional-neural-network-cnn-tutorial.ipynb
confusion matrix,import seaborn as sns ,convolutional-neural-network-cnn-tutorial.ipynb
Predict the values from the validation dataset,Y_pred = model.predict(X_val) ,convolutional-neural-network-cnn-tutorial.ipynb
Convert predictions classes to one hot vectors,"Y_pred_classes = np.argmax(Y_pred , axis = 1) ",convolutional-neural-network-cnn-tutorial.ipynb
Convert validation observations to one hot vectors,"Y_true = np.argmax(Y_val , axis = 1) ",convolutional-neural-network-cnn-tutorial.ipynb
compute the confusion matrix,"confusion_mtx = confusion_matrix(Y_true , Y_pred_classes) ",convolutional-neural-network-cnn-tutorial.ipynb
plot the confusion matrix,"f , ax = plt.subplots(figsize =(8 , 8)) ",convolutional-neural-network-cnn-tutorial.ipynb
Create the game environment,"from kaggle_environments import make , evaluate ",create-a-connectx-agent.ipynb
Set debug True to see the errors if your agent refuses to run,"env = make(""connectx"" , debug = True) ",create-a-connectx-agent.ipynb
"Create an agentTo create the submission, the agent function should be fully encapsulated. In other words, it should have no external dependencies: all of the imports and helper functions need to be included.","def my_agent(obs , config): ",create-a-connectx-agent.ipynb
Imports and helper functions, import numpy as np ,create-a-connectx-agent.ipynb
Gets board at next step if agent drops piece in selected column," def drop_piece(grid , col , piece , config): ",create-a-connectx-agent.ipynb
Returns True if dropping piece in column results in game win," def check_winning_move(obs , config , col , piece): ",create-a-connectx-agent.ipynb
Convert the board to a 2D grid," grid = np.asarray(obs.board). reshape(config.rows , config.columns) ",create-a-connectx-agent.ipynb
horizontal, for row in range(config.rows): ,create-a-connectx-agent.ipynb
vertical, for row in range(config.rows -(config.inarow - 1)) : ,create-a-connectx-agent.ipynb
positive diagonal, for row in range(config.rows -(config.inarow - 1)) : ,create-a-connectx-agent.ipynb
negative diagonal," for row in range(config.inarow - 1 , config.rows): ",create-a-connectx-agent.ipynb
Agent makes selection, valid_moves =[col for col in range(config.columns)if obs.board[col]== 0] ,create-a-connectx-agent.ipynb
Create a submission fileThe next code cell writes your agent to a Python file that can be submitted to the competition.,"import inspect
import os

def write_agent_to_file(function, file):
 with open(file, ""a"" if os.path.exists(file) else ""w"") as f:
 f.write(inspect.getsource(function))
 print(function, ""written to"", file)

write_agent_to_file(my_agent, ""submission.py"")",create-a-connectx-agent.ipynb
"Validate your submission fileThe code cell below has the agent in your submission file play one game round against itself.If it returns Success! , then you have correctly defined your agent.","import sys
from kaggle_environments import utils

out = sys.stdout
submission = utils.read_file(""/kaggle/working/submission.py"")
agent = utils.get_last_callable(submission)
sys.stdout = out

env = make(""connectx"", debug=True)
env.run([agent, agent])
print(""Success!"" if env.state[0].status == env.state[1].status == ""DONE"" else ""Failed..."")",create-a-connectx-agent.ipynb
Step 1: ImportsWe begin by importing several Python packages.,"import math, re, os
import numpy as np
import tensorflow as tf

print(""Tensorflow version "" + tf.__version__)",create-your-first-submission.ipynb
"Detect TPU, return appropriate distribution strategy",try : ,create-your-first-submission.ipynb
"We ll use the distribution strategy when we create our neural network model. Then, TensorFlow will distribute the training among the eight TPU cores by creating eight different replicas of the model, one for each core.Step 3: Loading the Competition DataGet GCS PathWhen used with TPUs, datasets need to be stored in a Google Cloud Storage bucket. You can use data from any public GCS bucket by giving its path just like you would data from kaggle input . The following will retrieve the GCS path for this competition s dataset.",from kaggle_datasets import KaggleDatasets ,create-your-first-submission.ipynb
what do gcs paths look like?,print(GCS_DS_PATH) ,create-your-first-submission.ipynb
"You can use data from any public dataset here on Kaggle in just the same way. If you d like to use data from one of your private datasets, see here.Load DataWhen used with TPUs, datasets are often serialized into TFRecords. This is a format convenient for distributing data to each of the TPUs cores. We ve hidden the cell that reads the TFRecords for our dataset since the process is a bit long. You could come back to it later for some guidance on using your own datasets with TPUs.","IMAGE_SIZE =[512 , 512] ",create-your-first-submission.ipynb
"Create Data PipelinesIn this final step we ll use the tf.data API to define an efficient data pipeline for each of the training, validation, and test splits.","def data_augment(image , label): ",create-your-first-submission.ipynb
part of the TPU while the TPU itself is computing gradients., image = tf.image.random_flip_left_right(image) ,create-your-first-submission.ipynb
"image tf.image.random saturation image, 0, 2 "," return image , label ",create-your-first-submission.ipynb
the training dataset must repeat for several epochs, dataset = dataset.repeat () ,create-your-first-submission.ipynb
prefetch next batch while training autotune prefetch buffer size , dataset = dataset.prefetch(AUTO) ,create-your-first-submission.ipynb
"files, i.e. flowers00 230.tfrec 230 data items"," n =[int(re.compile(r""-([0-9]*)\.""). search(filename). group(1)) for filename in filenames] ",create-your-first-submission.ipynb
Define the batch size. This will be 16 with TPU off and 128 16 8 with TPU on,BATCH_SIZE = 16 * strategy.num_replicas_in_sync ,create-your-first-submission.ipynb
"These datasets are tf.data.Dataset objects. You can think about a dataset in TensorFlow as a stream of data records. The training and validation sets are streams of image, label pairs.","np.set_printoptions(threshold=15, linewidth=80)

print(""Training data shapes:"")
for image, label in ds_train.take(3):
 print(image.numpy().shape, label.numpy().shape)
print(""Training data label examples:"", label.numpy())",create-your-first-submission.ipynb
"The test set is a stream of image, idnum pairs idnum here is the unique identifier given to the image that we ll use later when we make our submission as a csv file.","print(""Test data shapes:"") ",create-your-first-submission.ipynb
U unicode string,"print(""Test data IDs:"" , idnum.numpy (). astype('U')) ",create-your-first-submission.ipynb
Step 4: Explore DataLet s take a moment to look at some of the images in the dataset.,from matplotlib import pyplot as plt ,create-your-first-submission.ipynb
"binary string in this case,", if numpy_labels.dtype == object : ,create-your-first-submission.ipynb
these are image ID strings, numpy_labels =[None for _ in enumerate(numpy_images )] ,create-your-first-submission.ipynb
the case for test data ," return numpy_images , numpy_labels ",create-your-first-submission.ipynb
You can display a single batch of images from a dataset with another of our helper functions. The next cell will turn the dataset into an iterator of batches of 20 images.,ds_iter = iter(ds_train.unbatch().batch(20)),create-your-first-submission.ipynb
Use the Python next function to pop out the next batch in the stream and display it with the helper function.,"one_batch = next(ds_iter)
display_batch_of_images(one_batch)",create-your-first-submission.ipynb
"Step 5: Define ModelNow we re ready to create a neural network for classifying images! We ll use what s known as transfer learning. With transfer learning, you reuse part of a pretrained model to get a head start on a new dataset.For this tutorial, we ll to use a model called VGG16 pretrained on ImageNet . Later, you might want to experiment with other models included with Keras. Xception wouldn t be a bad choice. The distribution strategy we created earlier contains a context manager, strategy.scope. This context manager tells TensorFlow how to divide the work of training among the eight TPU cores. When using TensorFlow with a TPU, it s important to define your model in a strategy.scope context.",EPOCHS = 12 ,create-your-first-submission.ipynb
"The sparse categorical versions of the loss and metrics are appropriate for a classification task with more than two labels, like this one.","model.compile(
 optimizer='adam',
 loss = 'sparse_categorical_crossentropy',
 metrics=['sparse_categorical_accuracy'],
)

model.summary()",create-your-first-submission.ipynb
Define training epochs,EPOCHS = 12 ,create-your-first-submission.ipynb
"This next cell shows how the loss and metrics progressed during training. Thankfully, it converges!","display_training_curves(
 history.history['loss'],
 history.history['val_loss'],
 'loss',
 211,
)
display_training_curves(
 history.history['sparse_categorical_accuracy'],
 history.history['val_sparse_categorical_accuracy'],
 'accuracy',
 212,
)",create-your-first-submission.ipynb
"Step 7: Evaluate PredictionsBefore making your final predictions on the test set, it s a good idea to evaluate your model s predictions on the validation set. This can help you diagnose problems in training or suggest ways your model could be improved. We ll look at two common ways of validation: plotting the confusion matrix and visual validation.",import matplotlib.pyplot as plt ,create-your-first-submission.ipynb
set up the subplots on the first call, if subplot % 10 == 1 : ,create-your-first-submission.ipynb
"ax.set ylim 0.28,1.05 ", ax.set_xlabel('epoch') ,create-your-first-submission.ipynb
Confusion MatrixA confusion matrix shows the actual class of an image tabulated against its predicted class. It is one of the best tools you have for evaluating the performance of a classifier.The following cell does some processing on the validation data and then creates the matrix with the confusion matrix function included in scikit learn.,cmdataset = get_validation_dataset(ordered = True) ,create-your-first-submission.ipynb
You might be familiar with metrics like F1 score or precision and recall. This cell will compute these metrics and display them with a plot of the confusion matrix. These metrics are defined in the Scikit learn module sklearn.metrics we ve imported them in the helper script for you. ,"score = f1_score(
 cm_correct_labels,
 cm_predictions,
 labels=labels,
 average='macro',
)
precision = precision_score(
 cm_correct_labels,
 cm_predictions,
 labels=labels,
 average='macro',
)
recall = recall_score(
 cm_correct_labels,
 cm_predictions,
 labels=labels,
 average='macro',
)
display_confusion_matrix(cmat, score, precision, recall)",create-your-first-submission.ipynb
"Visual ValidationIt can also be helpful to look at some examples from the validation set and see what class your model predicted. This can help reveal patterns in the kinds of images your model has trouble with.This cell will set up the validation set to display 20 images at a time you can change this to display more or fewer, if you like.","dataset = get_validation_dataset()
dataset = dataset.unbatch().batch(20)
batch = iter(dataset)",create-your-first-submission.ipynb
And here is a set of flowers with their predicted species. Run the cell again to see another set.,"images, labels = next(batch)
probabilities = model.predict(images)
predictions = np.argmax(probabilities, axis=-1)
display_batch_of_images((images, labels), predictions)",create-your-first-submission.ipynb
"Step 8: Make Test PredictionsOnce you re satisfied with everything, you re ready to make predictions on the test set.","test_ds = get_test_dataset(ordered=True)

print('Computing predictions...')
test_images_ds = test_ds.map(lambda image, idnum: image)
probabilities = model.predict(test_images_ds)
predictions = np.argmax(probabilities, axis=-1)
print(predictions)",create-your-first-submission.ipynb
We ll generate a file submission.csv. This file is what you ll submit to get your score on the leaderboard.,print('Generating submission.csv file...') ,create-your-first-submission.ipynb
Get image ids from test set and convert to unicode,"test_ids_ds = test_ds.map(lambda image , idnum : idnum). unbatch () ",create-your-first-submission.ipynb
"IntroductionOnce you ve identified a set of features with some potential, it s time to start developing them. In this lesson, you ll learn a number of common transformations you can do entirely in Pandas. If you re feeling rusty, we ve got a great course on Pandas.We ll use four datasets in this lesson having a range of feature types: US Traffic Accidents, 1985 Automobiles, Concrete Formulations, and Customer Lifetime Value. The following hidden cell loads them up.","
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

plt.style.use(""seaborn-whitegrid"")
plt.rc(""figure"", autolayout=True)
plt.rc(
 ""axes"",
 labelweight=""bold"",
 labelsize=""large"",
 titleweight=""bold"",
 titlesize=14,
 titlepad=10,
)

accidents = pd.read_csv(""../input/fe-course-data/accidents.csv"")
autos = pd.read_csv(""../input/fe-course-data/autos.csv"")
concrete = pd.read_csv(""../input/fe-course-data/concrete.csv"")
customer = pd.read_csv(""../input/fe-course-data/customer.csv"")",creating-features.ipynb
"Tips on Discovering New Features Understand the features. Refer to your dataset s data documentation, if available. Research the problem domain to acquire domain knowledge. If your problem is predicting house prices, do some research on real estate for instance. Wikipedia can be a good starting point, but books and journal articles will often have the best information. Study previous work. Solution write ups from past Kaggle competitions are a great resource. Use data visualization. Visualization can reveal pathologies in the distribution of a feature or complicated relationships that could be simplified. Be sure to visualize your dataset as you work through the feature engineering process. Mathematical TransformsRelationships among numerical features are often expressed through mathematical formulas, which you ll frequently come across as part of your domain research. In Pandas, you can apply arithmetic operations to columns just as if they were ordinary numbers.In the Automobile dataset are features describing a car s engine. Research yields a variety of formulas for creating potentially useful new features. The stroke ratio , for instance, is a measure of how efficient an engine is versus how performant:","autos[""stroke_ratio""] = autos.stroke / autos.bore

autos[[""stroke"", ""bore"", ""stroke_ratio""]].head()",creating-features.ipynb
"The more complicated a combination is, the more difficult it will be for a model to learn, like this formula for an engine s displacement , a measure of its power:","autos[""displacement""] = (
 np.pi * ((0.5 * autos.bore) ** 2) * autos.stroke * autos.num_of_cylinders
)",creating-features.ipynb
"If the feature has 0.0 values, use np.log1p log 1 x instead of np.log","accidents[""LogWindSpeed""]= accidents.WindSpeed.apply(np.log1p) ",creating-features.ipynb
Plot a comparison,"fig , axs = plt.subplots(1 , 2 , figsize =(8 , 4)) ",creating-features.ipynb
"Check out our lesson on normalization in Data Cleaning where you ll also learn about the Box Cox transformation, a very general kind of normalizer.CountsFeatures describing the presence or absence of something often come in sets, the set of risk factors for a disease, say. You can aggregate such features by creating a count.These features will be binary 1 for Present, 0 for Absent or boolean True or False . In Python, booleans can be added up just as if they were integers.In Traffic Accidents are several features indicating whether some roadway object was near the accident. This will create a count of the total number of roadway features nearby using the sum method:","roadway_features = [""Amenity"", ""Bump"", ""Crossing"", ""GiveWay"",
 ""Junction"", ""NoExit"", ""Railway"", ""Roundabout"", ""Station"", ""Stop"",
 ""TrafficCalming"", ""TrafficSignal""]
accidents[""RoadwayFeatures""] = accidents[roadway_features].sum(axis=1)

accidents[roadway_features + [""RoadwayFeatures""]].head(10)",creating-features.ipynb
"You could also use a dataframe s built in methods to create boolean values. In the Concrete dataset are the amounts of components in a concrete formulation. Many formulations lack one or more components that is, the component has a value of 0 . This will count how many components are in a formulation with the dataframe s built in greater than gt method:","components =[""Cement"", ""BlastFurnaceSlag"", ""FlyAsh"", ""Water"",
 ""Superplasticizer"", ""CoarseAggregate"", ""FineAggregate""]
concrete[""Components""] = concrete[components].gt(0).sum(axis=1)

concrete[components + [""Components""]].head(10)",creating-features.ipynb
You could also join simple features into a composed feature if you had reason to believe there was some interaction in the combination:,"autos[""make_and_style""] = autos[""make""] + ""_"" + autos[""body_style""]
autos[[""make"", ""body_style"", ""make_and_style""]].head()",creating-features.ipynb
"The mean function is a built in dataframe method, which means we can pass it as a string to transform. Other handy methods include max, min, median, var, std, and count. Here s how you could calculate the frequency with which each state occurs in the dataset:","customer[""StateFreq""] = (
 customer.groupby(""State"")
 [""State""]
 .transform(""count"")
 / customer.State.count()
)

customer[[""State"", ""StateFreq""]].head(10)",creating-features.ipynb
Create splits,df_train = customer.sample(frac = 0.5) ,creating-features.ipynb
"Create the average claim amount by coverage type, on the training set","df_train[""AverageClaim""]= df_train.groupby(""Coverage"")[ ""ClaimAmount""]. transform(""mean"") ",creating-features.ipynb
ExampleWe ll work with the same data as in the previous tutorial. We load the input data in X and the output data in y.,import pandas as pd ,cross-validation.ipynb
Read the data,data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv') ,cross-validation.ipynb
Select subset of predictors,"cols_to_use =['Rooms' , 'Distance' , 'Landsize' , 'BuildingArea' , 'YearBuilt'] ",cross-validation.ipynb
Select target,y = data.Price ,cross-validation.ipynb
"Then, we define a pipeline that uses an imputer to fill in missing values and a random forest model to make predictions. While it s possible to do cross validation without pipelines, it is quite difficult! Using a pipeline will make the code remarkably straightforward.","from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),
 ('model', RandomForestRegressor(n_estimators=50,
 random_state=0))
 ])",cross-validation.ipynb
We obtain the cross validation scores with the cross val score function from scikit learn. We set the number of folds with the cv parameter.,from sklearn.model_selection import cross_val_score ,cross-validation.ipynb
"The scoring parameter chooses a measure of model quality to report: in this case, we chose negative mean absolute error MAE . The docs for scikit learn show a list of options. It is a little surprising that we specify negative MAE. Scikit learn has a convention where all metrics are defined so a high number is better. Using negatives here allows them to be consistent with that convention, though negative MAE is almost unheard of elsewhere. We typically want a single measure of model quality to compare alternative models. So we take the average across experiments.","print(""Average MAE score (across experiments):"")
print(scores.mean())",cross-validation.ipynb
Imports,"import os , warnings ",custom-convnets.ipynb
Reproducability,def set_seed(seed = 31415): ,custom-convnets.ipynb
Set Matplotlib defaults,"plt.rc('figure' , autolayout = True) ",custom-convnets.ipynb
Step 2 Define ModelHere is a diagram of the model we ll use: Now we ll define the model. See how our model consists of three blocks of Conv2D and MaxPool2D layers the base followed by a head of Dense layers. We can translate this diagram more or less directly into a Keras Sequential model just by filling in the appropriate parameters.,from tensorflow import keras ,custom-convnets.ipynb
"Notice in this definition is how the number of filters doubled block by block: 64, 128, 256. This is a common pattern. Since the MaxPool2D layer is reducing the size of the feature maps, we can afford to increase the quantity we create.Step 3 TrainWe can train this model just like the model from Lesson 1: compile it with an optimizer along with a loss and metric appropriate for binary classification.","model.compile(
 optimizer=tf.keras.optimizers.Adam(epsilon=0.01),
 loss='binary_crossentropy',
 metrics=['binary_accuracy']
)

history = model.fit(
 ds_train,
 validation_data=ds_valid,
 epochs=40,
 verbose=0,
)
",custom-convnets.ipynb
Libraries and Configurations,"import random, re, math
import numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
import tensorflow as tf, tensorflow.keras.backend as K
from kaggle_datasets import KaggleDatasets
print('Tensorflow version ' + tf.__version__)
from sklearn.model_selection import KFold",cutmix-and-mixup-on-gpu-tpu.ipynb
"Detect hardware, return appropriate distribution strategy",try : ,cutmix-and-mixup-on-gpu-tpu.ipynb
TPU detection. No parameters necessary if TPU NAME environment variable is set. On Kaggle this is always the case., tpu = tf.distribute.cluster_resolver.TPUClusterResolver () ,cutmix-and-mixup-on-gpu-tpu.ipynb
default distribution strategy in Tensorflow. Works on CPU and single GPU., strategy = tf.distribute.get_strategy () ,cutmix-and-mixup-on-gpu-tpu.ipynb
Configuration,"IMAGE_SIZE =[512 , 512] ",cutmix-and-mixup-on-gpu-tpu.ipynb
Mixed Precision and or XLA Mixed Precision and XLA are not being used in this notebook but you can experiment using them. Change the following booleans to enable mixed precision and or XLA on GPU TPU. By default TPU already uses some mixed precision but we can add more and it already uses XLA . These allow the GPU TPU memory to handle larger batch sizes and can speed up the training process. The Nvidia V100 GPU has special Tensor Cores which get utilized when mixed precision is enabled. Unfortunately Kaggle s Nvidia P100 GPU does not have Tensor Cores to receive speed up.,"MIXED_PRECISION = False
XLA_ACCELERATE = False

if MIXED_PRECISION:
 from tensorflow.keras.mixed_precision import experimental as mixed_precision
 if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')
 else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')
 mixed_precision.set_policy(policy)
 print('Mixed precision enabled')

if XLA_ACCELERATE:
 tf.config.optimizer.set_jit(True)
 print('Accelerated Linear Algebra enabled')",cutmix-and-mixup-on-gpu-tpu.ipynb
Data access,GCS_DS_PATH = KaggleDatasets (). get_gcs_path('tpu-getting-started') ,cutmix-and-mixup-on-gpu-tpu.ipynb
Starting with a high LR would break the pre trained weights.,LR_START = 0.00001 ,cutmix-and-mixup-on-gpu-tpu.ipynb
Dataset Functions From starter kernel 1 ,def decode_image(image_data): ,cutmix-and-mixup-on-gpu-tpu.ipynb
"convert image to floats in 0, 1 range"," image = tf.cast(image , tf.float32)/ 255.0 ",cutmix-and-mixup-on-gpu-tpu.ipynb
explicit size needed for TPU," image = tf.reshape(image ,[* IMAGE_SIZE , 3]) ",cutmix-and-mixup-on-gpu-tpu.ipynb
"CutMix Augmentation The following code does cutmix using the GPU TPU. Change the variables SWITCH, CUTMIX PROB and MIXUP PROB in function transform to control the amount of augmentation during training. CutMix will occur SWITCH CUTMIX PROB often and MixUp will occur 1 SWITCH MIXUP PROB often during training.","def onehot(image,label):
 CLASSES = 104
 return image,tf.one_hot(label,CLASSES)",cutmix-and-mixup-on-gpu-tpu.ipynb
output a batch of images with cutmix applied, DIM = IMAGE_SIZE[0] ,cutmix-and-mixup-on-gpu-tpu.ipynb
DO CUTMIX WITH PROBABILITY DEFINED ABOVE," P = tf.cast(tf.random.uniform([], 0 , 1)<= PROBABILITY , tf.int32) ",cutmix-and-mixup-on-gpu-tpu.ipynb
CHOOSE RANDOM IMAGE TO CUTMIX WITH," k = tf.cast(tf.random.uniform([], 0 , AUG_BATCH), tf.int32) ",cutmix-and-mixup-on-gpu-tpu.ipynb
CHOOSE RANDOM LOCATION," x = tf.cast(tf.random.uniform([], 0 , DIM), tf.int32) ",cutmix-and-mixup-on-gpu-tpu.ipynb
this is beta dist with alpha 1.0," b = tf.random.uniform([], 0 , 1) ",cutmix-and-mixup-on-gpu-tpu.ipynb
MAKE CUTMIX IMAGE," one = image[j , ya : yb , 0 : xa , :] ",cutmix-and-mixup-on-gpu-tpu.ipynb
MAKE CUTMIX LABEL," a = tf.cast(WIDTH * WIDTH / DIM / DIM , tf.float32) ",cutmix-and-mixup-on-gpu-tpu.ipynb
RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR maybe use Python typing instead? ," image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH , DIM , DIM , 3)) ",cutmix-and-mixup-on-gpu-tpu.ipynb
Display CutMix Augmentation,"row = 6; col = 4;
row = min(row,AUG_BATCH//col)
all_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()
augmented_element = all_elements.repeat().batch(AUG_BATCH).map(cutmix)

for (img,label) in augmented_element:
 plt.figure(figsize=(15,int(15*row/col)))
 for j in range(row*col):
 plt.subplot(row,col,j+1)
 plt.axis('off')
 plt.imshow(img[j,])
 plt.show()
 break",cutmix-and-mixup-on-gpu-tpu.ipynb
"MixUp Augmentation The following code does mixup using the GPU TPU. Change the variables SWITCH, CUTMIX PROB and MIXUP PROB in function transform to control the amount of augmentation during training. CutMix will occur SWITCH CUTMIX PROB often and MixUp will occur 1 SWITCH MIXUP PROB often during training.","def mixup(image , label , PROBABILITY = 1.0): ",cutmix-and-mixup-on-gpu-tpu.ipynb
output a batch of images with mixup applied, DIM = IMAGE_SIZE[0] ,cutmix-and-mixup-on-gpu-tpu.ipynb
DO MIXUP WITH PROBABILITY DEFINED ABOVE," P = tf.cast(tf.random.uniform([], 0 , 1)<= PROBABILITY , tf.float32) ",cutmix-and-mixup-on-gpu-tpu.ipynb
CHOOSE RANDOM," k = tf.cast(tf.random.uniform([], 0 , AUG_BATCH), tf.int32) ",cutmix-and-mixup-on-gpu-tpu.ipynb
this is beta dist with alpha 1.0," a = tf.random.uniform([], 0 , 1)* P ",cutmix-and-mixup-on-gpu-tpu.ipynb
MAKE MIXUP IMAGE," img1 = image[j ,] ",cutmix-and-mixup-on-gpu-tpu.ipynb
MAKE CUTMIX LABEL, if len(label.shape)== 1 : ,cutmix-and-mixup-on-gpu-tpu.ipynb
RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR maybe use Python typing instead? ," image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH , DIM , DIM , 3)) ",cutmix-and-mixup-on-gpu-tpu.ipynb
THIS FUNCTION APPLIES BOTH CUTMIX AND MIXUP, DIM = IMAGE_SIZE[0] ,cutmix-and-mixup-on-gpu-tpu.ipynb
FOR SWITCH PERCENT OF TIME WE DO CUTMIX AND 1 SWITCH WE DO MIXUP," image2 , label2 = cutmix(image , label , CUTMIX_PROB) ",cutmix-and-mixup-on-gpu-tpu.ipynb
RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR maybe use Python typing instead? ," image4 = tf.reshape(tf.stack(imgs),(AUG_BATCH , DIM , DIM , 3)) ",cutmix-and-mixup-on-gpu-tpu.ipynb
Display MixUp Augmentation,"row = 6; col = 4;
row = min(row,AUG_BATCH//col)
all_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()
augmented_element = all_elements.repeat().batch(AUG_BATCH).map(mixup)

for (img,label) in augmented_element:
 plt.figure(figsize=(15,int(15*row/col)))
 for j in range(row*col):
 plt.subplot(row,col,j+1)
 plt.axis('off')
 plt.imshow(img[j,])
 plt.show()
 break",cutmix-and-mixup-on-gpu-tpu.ipynb
"Display 33 33 33 CutMix MixUp None Change the variables SWITCH, CUTMIX PROB and MIXUP PROB in function transform to control the amount of augmentation during training. CutMix will occur SWITCH CUTMIX PROB often and MixUp will occur 1 SWITCH MIXUP PROB often. Currently SWITCH 0.5, CUTMIX PROB 0.666 and MIXUP PROB 0.666 resulting in 33 cutmix, 33 mixup, and 33 none during training.","row = 6; col = 4;
row = min(row,AUG_BATCH//col)
all_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()
augmented_element = all_elements.repeat().batch(AUG_BATCH).map(transform)

for (img,label) in augmented_element:
 plt.figure(figsize=(15,int(15*row/col)))
 for j in range(row*col):
 plt.subplot(row,col,j+1)
 plt.axis('off')
 plt.imshow(img[j,])
 plt.show()
 break",cutmix-and-mixup-on-gpu-tpu.ipynb
"Build, Train, Infer Model This is the 5 Fold workflow copied from Ragnar s notebook here 1 . Now we add cutmix and or mixup augmentation to the training images on the fly! Since cutmix and or mixup returns one hot encoded labels, we change the loss and metric to categorical crossentropy instead of sparse categorical crossentropy.",from tensorflow.keras.applications import DenseNet201 ,cutmix-and-mixup-on-gpu-tpu.ipynb
Confusion Matrix and Validation Score Try forking and modifying this notebook to maximize validation score below. Tune the data augmentation and or train for more epochs to increase accuracy. Good luck! Code below is from starter kernel 1 .,"def display_confusion_matrix(cmat, score, precision, recall):
 plt.figure(figsize=(15,15))
 ax = plt.gca()
 ax.matshow(cmat, cmap='Reds')
 ax.set_xticks(range(len(CLASSES)))
 ax.set_xticklabels(CLASSES, fontdict={'fontsize': 7})
 plt.setp(ax.get_xticklabels(), rotation=45, ha=""left"", rotation_mode=""anchor"")
 ax.set_yticks(range(len(CLASSES)))
 ax.set_yticklabels(CLASSES, fontdict={'fontsize': 7})
 plt.setp(ax.get_yticklabels(), rotation=45, ha=""right"", rotation_mode=""anchor"")
 titlestring = """"
 if score is not None:
 titlestring += 'f1 = {:.3f} '.format(score)
 if precision is not None:
 titlestring += '\nprecision = {:.3f} '.format(precision)
 if recall is not None:
 titlestring += '\nrecall = {:.3f} '.format(recall)
 if len(titlestring) > 0:
 ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})
 plt.show()",cutmix-and-mixup-on-gpu-tpu.ipynb
get everything as one batch, all_labels.append(next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))). numpy ()) ,cutmix-and-mixup-on-gpu-tpu.ipynb
Dataset,class ImageTransform : ,cyclegan-pytorch-lightning.ipynb
Data Module,class MonetDataModule(pl.LightningDataModule): ,cyclegan-pytorch-lightning.ipynb
Sanity Check,data_dir = '../input/gan-getting-started' ,cyclegan-pytorch-lightning.ipynb
Model,class Upsample(nn.Module): ,cyclegan-pytorch-lightning.ipynb
Sanity Check,net = CycleGAN_Unet_Generator () ,cyclegan-pytorch-lightning.ipynb
Sanity Check,net = CycleGAN_Discriminator () ,cyclegan-pytorch-lightning.ipynb
CycleGAN Lightning Module ,class CycleGAN_LightningSystem(pl.LightningModule): ,cyclegan-pytorch-lightning.ipynb
Train Generator, if optimizer_idx == 0 or optimizer_idx == 1 : ,cyclegan-pytorch-lightning.ipynb
MSELoss," val_base = self.generator_loss(self.D_base(self.G_stylebase(style_img)) , valid) ",cyclegan-pytorch-lightning.ipynb
Reconstruction," reconstr_base = self.mae(self.G_stylebase(self.G_basestyle(base_img)) , base_img) ",cyclegan-pytorch-lightning.ipynb
Identity," id_base = self.mae(self.G_stylebase(base_img), base_img) ",cyclegan-pytorch-lightning.ipynb
Loss Weight, G_loss = val_loss + self.reconstr_w * reconstr_loss + self.id_w * id_loss ,cyclegan-pytorch-lightning.ipynb
Train Discriminator, elif optimizer_idx == 2 or optimizer_idx == 3 : ,cyclegan-pytorch-lightning.ipynb
MSELoss," D_base_gen_loss = self.discriminator_loss(self.D_base(self.G_stylebase(style_img)) , fake) ",cyclegan-pytorch-lightning.ipynb
Loss Weight, D_loss =(D_gen_loss + D_base_valid_loss + D_style_valid_loss)/ 3 ,cyclegan-pytorch-lightning.ipynb
Count up, self.cnt_train_step += 1 ,cyclegan-pytorch-lightning.ipynb
Display Model Output, target_img_paths = glob.glob('../input/gan-getting-started/photo_jpg/*.jpg')[ : 4] ,cyclegan-pytorch-lightning.ipynb
Reverse Normalization, gen_img = gen_img * 0.5 + 0.5 ,cyclegan-pytorch-lightning.ipynb
Visualize," fig = plt.figure(figsize =(18 , 8)) ",cyclegan-pytorch-lightning.ipynb
Config ,data_dir = '../input/gan-getting-started' ,cyclegan-pytorch-lightning.ipynb
Reverse Normalization, gen_img = gen_img * 0.5 + 0.5 ,cyclegan-pytorch-lightning.ipynb
Make Zipfile," shutil.make_archive(""/kaggle/working/images"" , 'zip' , ""/kaggle/images"") ",cyclegan-pytorch-lightning.ipynb
Delete Origin file, shutil.rmtree('../images') ,cyclegan-pytorch-lightning.ipynb
Loss Plot,"fig , axes = plt.subplots(ncols = 1 , nrows = 2 , figsize =(18 , 12), facecolor = 'w') ",cyclegan-pytorch-lightning.ipynb
Seed,"def set_seed(seed):
 random.seed(seed)
 os.environ['PYTHONHASHSEED'] = str(seed)
 np.random.seed(seed)
 torch.manual_seed(seed)
 torch.cuda.manual_seed(seed)
 torch.backends.cudnn.deterministic = True",cyclegan-pytorch.ipynb
Dataset and Dataloader,"class ImageDataset(Dataset):
 def __init__(self, monet_dir, photo_dir, size=(256, 256), normalize=True):
 super().__init__()
 self.monet_dir = monet_dir
 self.photo_dir = photo_dir
 self.monet_idx = dict()
 self.photo_idx = dict()
 if normalize:
 self.transform = transforms.Compose([
 transforms.Resize(size),
 transforms.ToTensor(),
 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) 
 ])
 else:
 self.transform = transforms.Compose([
 transforms.Resize(size),
 transforms.ToTensor() 
 ])
 for i, fl in enumerate(os.listdir(self.monet_dir)):
 self.monet_idx[i] = fl
 for i, fl in enumerate(os.listdir(self.photo_dir)):
 self.photo_idx[i] = fl

 def __getitem__(self, idx):
 rand_idx = int(np.random.uniform(0, len(self.monet_idx.keys())))
 photo_path = os.path.join(self.photo_dir, self.photo_idx[rand_idx])
 monet_path = os.path.join(self.monet_dir, self.monet_idx[idx])
 photo_img = Image.open(photo_path)
 photo_img = self.transform(photo_img)
 monet_img = Image.open(monet_path)
 monet_img = self.transform(monet_img)
 return photo_img, monet_img

 def __len__(self):
 return min(len(self.monet_idx.keys()), len(self.photo_idx.keys()))",cyclegan-pytorch.ipynb
Save and Load,"def load_checkpoint(ckpt_path, map_location=None):
 ckpt = torch.load(ckpt_path, map_location=map_location)
 print(' [*] Loading checkpoint from %s succeed!' % ckpt_path)
 return ckpt",cyclegan-pytorch.ipynb
Model,"def Upsample(in_ch, out_ch, use_dropout=True, dropout_ratio=0.5):
 if use_dropout:
 return nn.Sequential(
 nn.ConvTranspose2d(in_ch, out_ch, 3, stride=2, padding=1, output_padding=1),
 nn.InstanceNorm2d(out_ch),
 nn.Dropout(dropout_ratio),
 nn.GELU()
 )
 else:
 return nn.Sequential(
 nn.ConvTranspose2d(in_ch, out_ch, 3, stride=2, padding=1, output_padding=1),
 nn.InstanceNorm2d(out_ch),
 nn.GELU()
 )",cyclegan-pytorch.ipynb
Some additional classes and functions,"def update_req_grad(models, requires_grad=True):
 for model in models:
 for param in model.parameters():
 param.requires_grad = requires_grad",cyclegan-pytorch.ipynb
from iterations to iterations.,class sample_fake(object): ,cyclegan-pytorch.ipynb
GAN Class,class CycleGAN(object): ,cyclegan-pytorch.ipynb
Save before train,"save_checkpoint(save_dict, 'init.ckpt')",cyclegan-pytorch.ipynb
Run Generator over all images,"class PhotoDataset(Dataset):
 def __init__(self, photo_dir, size=(256, 256), normalize=True):
 super().__init__()
 self.photo_dir = photo_dir
 self.photo_idx = dict()
 if normalize:
 self.transform = transforms.Compose([
 transforms.Resize(size),
 transforms.ToTensor(),
 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) 
 ])
 else:
 self.transform = transforms.Compose([
 transforms.Resize(size),
 transforms.ToTensor() 
 ])
 for i, fl in enumerate(os.listdir(self.photo_dir)):
 self.photo_idx[i] = fl

 def __getitem__(self, idx):
 photo_path = os.path.join(self.photo_dir, self.photo_idx[idx])
 photo_img = Image.open(photo_path)
 photo_img = self.transform(photo_img)
 return photo_img

 def __len__(self):
 return len(self.photo_idx.keys())",cyclegan-pytorch.ipynb
"I m Something of a Painter Myself Introduction to CycleGAN Monet paintings This notebook is based on the competition baseline, I just did some refactoring to create helper functions and make everything easier to experiment with, besides that the main contribution is adding the possibility to use data augmentations, as we have very little data here, this will probably help.CycleGAN references: Git repository with many cool informations. ArXiv paper Understanding and Implementing CycleGAN in TensorFlow What is CycleGAN?From the authors: We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G: X Y, such that the distribution of images from G X is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under constrained, we couple it with an inverse mapping F: Y X and introduce a cycle consistency loss to push F G X X and vice versa . In essence it maps and image to a given domaind, if you are turning horses into zebra the image will be the horse and the domain is the zebras, in our case the photos are the image and the domain are the Monet paintings.Turning horses into zebras and zebras into horses Turning photos into Monet paintings our task But it doesn t always works as expected CycleGAN architectureLooking at the code below may be hard to get what is happening, this image will help the understandingFirst, we get the regular generator discriminator thing, where the generator tries to generate images that seem to be drawn to the given domain in the example will be creating zebra images , but it would be possible that the generator generates only the same zebra image or zebra images that do not look like the imputed horse image, this is why the model has a second generator, this second generator uses the first generated image and tries to recreate the original imputed horse image, this way the first generator has to generate zebra images that look like the imputed horse image.In the end, you will get 4 sub models: A generator that can generate zebras images A generator that can generate horses images A discriminator that can identify real zebras images A discriminator that can identify real horses images Let s get to the code","import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa
from kaggle_datasets import KaggleDatasets
from tensorflow.keras.callbacks import History
import matplotlib.pyplot as plt
import numpy as np
import re
import os
import math
import random
import cv2

try:
 tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
 print('Device:', tpu.master())
 tf.config.experimental_connect_to_cluster(tpu)
 tf.tpu.experimental.initialize_tpu_system(tpu)
 strategy = tf.distribute.experimental.TPUStrategy(tpu)
except:
 strategy = tf.distribute.get_strategy()",cyclegan.ipynb
View one image,import imageio ,cyclegan.ipynb
View one image,import imageio ,cyclegan.ipynb
Image preprocessing,"rand_monet = r""../input/gan-getting-started/monet_jpg/0260d15306.jpg""
rand_photo = r""../input/gan-getting-started/photo_jpg/000ded5c41.jpg""",cyclegan.ipynb
loop over the image channels," for(chan , color)in zip(chans , colors): ",cyclegan.ipynb
channel," hist = cv2.calcHist ([chan],[0], None ,[256],[0 , 256]) ",cyclegan.ipynb
plot the histogram," plt.plot(hist , color = color) ",cyclegan.ipynb
"We ll be using a UNET architecture for our CycleGAN. To build our generator, let s first define our downsample and upsample methods.The downsample, as the name suggests, reduces the 2D dimensions, the width and height, of the image by the stride. The stride is the length of the step the filter takes. Since the stride is 2, the filter is applied to every other pixel, hence reducing the weight and height by 2.We ll be using an instance normalization instead of batch normalization. As the instance normalization is not standard in the TensorFlow API, we ll use the layer from TensorFlow Add ons.","OUTPUT_CHANNELS = 3

def downsample(filters, size, apply_instancenorm=True):
 initializer = tf.random_normal_initializer(0., 0.02)
 gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)

 result = keras.Sequential()
 result.add(layers.Conv2D(filters, size, strides=2, padding='same',kernel_initializer=initializer, use_bias=False))

 if apply_instancenorm:
 result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))

 result.add(layers.LeakyReLU())

 return result",cyclegan.ipynb
Upsample does the opposite of downsample and increases the dimensions of the of the image. Conv2DTranspose does basically the opposite of a Conv2D layer.,"def upsample(filters, size, apply_dropout=False):
 initializer = tf.random_normal_initializer(0., 0.02)
 gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)

 result = keras.Sequential()
 result.add(layers.Conv2DTranspose(filters, size, strides=2,padding='same',kernel_initializer=initializer,use_bias=False))

 result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))

 if apply_dropout:
 result.add(layers.Dropout(0.5))

 result.add(layers.ReLU())

 return result",cyclegan.ipynb
"The generator first downsamples the input image and then upsample while establishing long skip connections. Skip connections are a way to help bypass the vanishing gradient problem by concatenating the output of a layer to multiple layers instead of only one. Here we concatenate the output of the downsample layer to the upsample layer in a symmetrical fashion. 64 512 Encoder, 512 Till Upsample droput 512,4 with drop out is Transformer and 512 64 is decoder",def Generator (): ,cyclegan.ipynb
bs batch size,"tf.keras.utils.plot_model(Generator(), show_shapes=True, dpi=64)",cyclegan.ipynb
"The discriminator takes in the input image and classifies it as real or fake generated . Instead of outputing a single node, the discriminator outputs a smaller 2D image with higher pixel values indicating a real classification and lower values indicating a fake classification. Each layer of discriminator has a instance normal lization so as to normalize the model and last has the choice between real and fake done by sigmoid.",def Discriminator (): ,cyclegan.ipynb
" bs, 128, 128, 64 "," down1 = downsample(64 , 4 , False)( x) ",cyclegan.ipynb
" bs, 64, 64, 128 "," down2 = downsample(128 , 4)( down1) ",cyclegan.ipynb
" bs, 32, 32, 256 "," down3 = downsample(256 , 4)( down2) ",cyclegan.ipynb
" bs, 34, 34, 256 ", zero_pad1 = layers.ZeroPadding2D ()( down3) ,cyclegan.ipynb
" bs, 31, 31, 512 "," conv = layers.Conv2D(512 , 4 , strides = 1 , kernel_initializer = initializer , use_bias = False)( zero_pad1) ",cyclegan.ipynb
" bs, 33, 33, 512 ", zero_pad2 = layers.ZeroPadding2D ()( leaky_relu) ,cyclegan.ipynb
"Training a CycleGAN: In the CycleGAN s case, the architecture is complex, and as a result, we need a structure that allows us to keep accessing the original attributes and methods that we have defined. As a result, we will write out the CycleGAN as a Python class of its own with methods to build the Generator and Discriminator, and run the training.For the training to execute we will need a seperate Generator and discriminator function which we will feed to CycleGAN as methods which in turn needs the upsample and downsample of image.For downsampling we are using the Conv2D as primary layer and LeakyReLU as activation For upsampling we are using the Conv2DTranspose as primary layer and Dropout at 0.3, ReLU as secondary layers",class CycleGan(keras.Model): ,cyclegan.ipynb
real monet y real photo x m gen G p gen F p disc DX m disc DY, with tf.GradientTape(persistent = True)as tape : ,cyclegan.ipynb
G x ," fake_monet = self.m_gen(real_photo , training = True) ",cyclegan.ipynb
F G x ," cycled_photo = self.p_gen(fake_monet , training = True) ",cyclegan.ipynb
F y ," fake_photo = self.p_gen(real_monet , training = True) ",cyclegan.ipynb
G F y ," cycled_monet = self.m_gen(fake_photo , training = True) ",cyclegan.ipynb
G y ," same_monet = self.m_gen(real_monet , training = True) ",cyclegan.ipynb
F x ," same_photo = self.p_gen(real_photo , training = True) ",cyclegan.ipynb
DY y ," disc_real_monet = self.m_disc(real_monet , training = True) ",cyclegan.ipynb
DX x ," disc_real_photo = self.p_disc(real_photo , training = True) ",cyclegan.ipynb
DY G x ," disc_fake_monet = self.m_disc(fake_monet , training = True) ",cyclegan.ipynb
DX F y ," disc_fake_photo = self.p_disc(fake_photo , training = True) ",cyclegan.ipynb
evaluates total generator loss," total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet , same_monet , self.lambda_cycle) ",cyclegan.ipynb
evaluates discriminator loss," monet_disc_loss = self.disc_loss_fn(disc_real_monet , disc_fake_monet) ",cyclegan.ipynb
The discriminator loss function below compares real images to a matrix of 1s and fake images to a matrix of 0s. The perfect discriminator will output all 1s for real images and all 0s for fake images. The discriminator loss outputs the average of the real and generated loss.,with strategy.scope (): ,cyclegan.ipynb
 1 D1," real_loss = tf.keras.losses.BinaryCrossentropy(from_logits = True , reduction = tf.keras.losses.Reduction.NONE)( tf.ones_like(real), real) ",cyclegan.ipynb
 0 D0," generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits = True , reduction = tf.keras.losses.Reduction.NONE)( tf.zeros_like(generated), generated) ",cyclegan.ipynb
"The generator wants to fool the discriminator into thinking the generated image is real. The perfect generator will have the discriminator output only 1s. Thus, it compares the generated image to a matrix of 1s to find the loss.","with strategy.scope():
 def generator_loss(generated):
 return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)",cyclegan.ipynb
"We want our original photo and the twice transformed photo to be similar to one another. Thus, we can calculate the cycle consistency loss be finding the average of their difference.","with strategy.scope():
 def calc_cycle_loss(real_image, cycled_image, LAMBDA):
 loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))
 return LAMBDA * loss1",cyclegan.ipynb
"The identity loss compares the image with its generator i.e. photo with photo generator . If given a photo as input, we want it to generate the same image as the image was originally a photo. The identity loss compares the input with the output of the generator.","with strategy.scope():
 def identity_loss(real_image, same_image, LAMBDA):
 loss = tf.reduce_mean(tf.abs(real_image - same_image))
 return LAMBDA * 0.5 * loss",cyclegan.ipynb
Got the idea from ,"def plot_acc_and_loss(history, load=False):
 monet_g = []
 photo_g = []
 monet_d = []
 photo_d = []
 if load==True:
 for i in range(np.array(history[""monet_gen_loss""]).shape[0]):
 monet_g.append(np.array(history[""monet_gen_loss""][i]).squeeze().mean())
 photo_g.append(np.array(history[""photo_gen_loss""][i]).squeeze().mean())
 monet_d.append(np.array(history[""monet_disc_loss""][i]).squeeze().mean())
 photo_d.append(np.array(history[""photo_disc_loss""][i]).squeeze().mean())
 else:
 for i in range(np.array(history.history[""monet_gen_loss""]).shape[0]):
 monet_g.append(np.array(history.history[""monet_gen_loss""][i]).squeeze().mean())
 photo_g.append(np.array(history.history[""photo_gen_loss""][i]).squeeze().mean())
 monet_d.append(np.array(history.history[""monet_disc_loss""][i]).squeeze().mean())
 photo_d.append(np.array(history.history[""photo_disc_loss""][i]).squeeze().mean())
 
 fig, axs = plt.subplots(1, 2, figsize=(15, 5))
 axs[0].plot(monet_g,label=""Monet"")
 axs[0].plot(photo_g,label=""Photo"")
 axs[0].set_title(""generator loss"")
 axs[0].legend()

 axs[1].plot(monet_d, label=""Monet"")
 axs[1].plot(photo_d,label=""Photo"")
 axs[1].set_title(""discriminator loss"")
 axs[1].legend()

 plt.show()",cyclegan.ipynb
Call this if loading outputs,import pickle ,cyclegan.ipynb
Switch load to false if you have trained the model,"plot_acc_and_loss(history , load = True) ",cyclegan.ipynb
Creating a gif from each predictions, anim_file = 'CycleGAN.gif' ,cyclegan.ipynb
Three first frames are the converted picture, writer.append_data(init_pic) ,cyclegan.ipynb
Switch load to false if you have trained the model and put the correct photo number,"def gen_input_img(num_photo=0, load=False):
 fig, ax = plt.subplots(figsize=(5,5))
 
 if load == True:
 img = np.array(PIL.Image.open('../input/gan-getting-started/monet_jpg/000c1e3bff.jpg'))
 plt.imshow(img)
 ax.axis(""off"")
 
 else:
 img = photo[3]*0.5 + 0.5
 plt.imshow(img)
 ax.axis(""off"")
 plt.title('Input photo') 
",cyclegan.ipynb
Switch load to false if you have trained the model and put the correct photo number,"import PIL
from IPython import display
import imageio

import shutil",cyclegan.ipynb
Switch load to false if you have trained the model and put the correct photo number,! pip install git+https://github.com/tensorflow/docs,cyclegan.ipynb
Prediction evolution according to epoch,embed.embed_file(anim_file) ,cyclegan.ipynb
Prediction evolution according to epoch,embed.embed_file(anim_file) ,cyclegan.ipynb
make predition," prediction = generator_model(img , training = False)[ 0]. numpy () ",cyclegan.ipynb
re scale, prediction =(prediction * 127.5 + 127.5). astype(np.uint8) ,cyclegan.ipynb
Create folder to save generated images,os.makedirs('./images1') ,cyclegan.ipynb
converting to zip,"shutil.make_archive('/kaggle/working/images1/' , 'zip' , './images1') ",cyclegan.ipynb
Making necessary imports,import numpy as np ,data-augmentation-for-facial-keypoint-detection.ipynb
Augmentation Hyperparameter Settings Experiment with various augmentation choices,horizontal_flip = False ,data-augmentation-for-facial-keypoint-detection.ipynb
Whether to include samples with missing keypoint values. Note that the missing values would however be filled using Pandas ffill later.,include_unclean_data = True ,data-augmentation-for-facial-keypoint-detection.ipynb
Index of sample train image used for visualizing various augmentations,sample_image_index = 20 ,data-augmentation-for-facial-keypoint-detection.ipynb
Rotation angle in degrees includes both clockwise anti clockwise rotations ,rotation_angles =[12] ,data-augmentation-for-facial-keypoint-detection.ipynb
Horizontal vertical shift amount in pixels includes shift from all 4 corners ,pixel_shifts =[12] ,data-augmentation-for-facial-keypoint-detection.ipynb
Extracting files to working directory,"print(""Contents of input/facial-keypoints-detection directory: "")
!ls ../input/facial-keypoints-detection/

print(""\nExtracting .zip dataset files to working directory ..."")
!unzip -u ../input/facial-keypoints-detection/test.zip
!unzip -u ../input/facial-keypoints-detection/training.zip

print(""\nCurrent working directory:"")
!pwd
print(""\nContents of working directory:"")
!ls",data-augmentation-for-facial-keypoint-detection.ipynb
Reading inputs to a Pandas DataFrame,"%%time

train_file = 'training.csv'
test_file = 'test.csv'
idlookup_file = '../input/facial-keypoints-detection/IdLookupTable.csv'
train_data = pd.read_csv(train_file)
test_data = pd.read_csv(test_file)
idlookup_data = pd.read_csv(idlookup_file)",data-augmentation-for-facial-keypoint-detection.ipynb
Function to plot facial keypoints with images,"def plot_sample(image, keypoint, axis, title):
 image = image.reshape(96,96)
 axis.imshow(image, cmap='gray')
 axis.scatter(keypoint[0::2], keypoint[1::2], marker='x', s=20)
 plt.title(title)",data-augmentation-for-facial-keypoint-detection.ipynb
Exploring Data,train_data.head().T,data-augmentation-for-facial-keypoint-detection.ipynb
Check for any images with missing pixel values,"print(""Length of train data: {}"".format(len(train_data)))
print(""Number of Images with missing pixel values: {}"".format(len(train_data) - int(train_data.Image.apply(lambda x: len(x.split())).value_counts().values)))",data-augmentation-for-facial-keypoint-detection.ipynb
Find columns having Null values and their counts,train_data.isnull().sum(),data-augmentation-for-facial-keypoint-detection.ipynb
We can observe that approx. 68 of data is missing for several keypoints,% % time ,data-augmentation-for-facial-keypoint-detection.ipynb
Separate data into clean unclean subsets,"%%time

def load_images(image_data):
 images = []
 for idx, sample in image_data.iterrows():
 image = np.array(sample['Image'].split(' '), dtype=int)
 image = np.reshape(image, (96,96,1))
 images.append(image)
 images = np.array(images)/255.
 return images

def load_keypoints(keypoint_data):
 keypoint_data = keypoint_data.drop('Image',axis = 1)
 keypoint_features = []
 for idx, sample_keypoints in keypoint_data.iterrows():
 keypoint_features.append(sample_keypoints)
 keypoint_features = np.array(keypoint_features, dtype = 'float')
 return keypoint_features

clean_train_images = load_images(clean_train_data)
print(""Shape of clean_train_images: {}"".format(np.shape(clean_train_images)))
clean_train_keypoints = load_keypoints(clean_train_data)
print(""Shape of clean_train_keypoints: {}"".format(np.shape(clean_train_keypoints)))
test_images = load_images(test_data)
print(""Shape of test_images: {}"".format(np.shape(test_images)))

train_images = clean_train_images
train_keypoints = clean_train_keypoints
fig, axis = plt.subplots()
plot_sample(clean_train_images[sample_image_index], clean_train_keypoints[sample_image_index], axis, ""Sample image & keypoints"")

if include_unclean_data:
 unclean_train_images = load_images(unclean_train_data)
 print(""Shape of unclean_train_images: {}"".format(np.shape(unclean_train_images)))
 unclean_train_keypoints = load_keypoints(unclean_train_data)
 print(""Shape of unclean_train_keypoints: {}\n"".format(np.shape(unclean_train_keypoints)))
 train_images = np.concatenate((train_images, unclean_train_images))
 train_keypoints = np.concatenate((train_keypoints, unclean_train_keypoints))",data-augmentation-for-facial-keypoint-detection.ipynb
Performing Horizontal Flipping for Data Augmentation,"def left_right_flip(images , keypoints): ",data-augmentation-for-facial-keypoint-detection.ipynb
Flip column wise axis 2 ," flipped_images = np.flip(images , axis = 2) ",data-augmentation-for-facial-keypoint-detection.ipynb
Subtract only X co ordinates of keypoints from 96 for horizontal flipping," flipped_keypoints.append ([96. - coor if idx % 2 == 0 else coor for idx , coor in enumerate(sample_keypoints )]) ",data-augmentation-for-facial-keypoint-detection.ipynb
Performing Rotation Augmentation,"def rotate_augmentation(images , keypoints): ",data-augmentation-for-facial-keypoint-detection.ipynb
Rotation augmentation for a list of angle values, for angle in rotation_angles : ,data-augmentation-for-facial-keypoint-detection.ipynb
Obtain angle in radians from angle in degrees notice negative sign for change in clockwise vs anti clockwise directions from conventional rotation to cv2 s image rotation , angle_rad = - angle * pi / 180. ,data-augmentation-for-facial-keypoint-detection.ipynb
For train images, for image in images : ,data-augmentation-for-facial-keypoint-detection.ipynb
For train keypoints, for keypoint in keypoints : ,data-augmentation-for-facial-keypoint-detection.ipynb
Subtract the middle value of the image dimension, rotated_keypoint = keypoint - 48. ,data-augmentation-for-facial-keypoint-detection.ipynb
Add the earlier subtracted value, rotated_keypoint += 48. ,data-augmentation-for-facial-keypoint-detection.ipynb
Performing Brightness Alteration for Data Augmentation,"def alter_brightness(images , keypoints): ",data-augmentation-for-facial-keypoint-detection.ipynb
"Increased brightness by a factor of 1.2 clip any values outside the range of 1,1 "," inc_brightness_images = np.clip(images * 1.2 , 0.0 , 1.0) ",data-augmentation-for-facial-keypoint-detection.ipynb
"Decreased brightness by a factor of 0.6 clip any values outside the range of 1,1 "," dec_brightness_images = np.clip(images * 0.6 , 0.0 , 1.0) ",data-augmentation-for-facial-keypoint-detection.ipynb
Performing Horizontal Vertical shift,"def shift_images(images , keypoints): ",data-augmentation-for-facial-keypoint-detection.ipynb
Augmenting over several pixel shift values, for shift in pixel_shifts : ,data-augmentation-for-facial-keypoint-detection.ipynb
Adding Random Noise for Data Augmentation,def add_noise(images): ,data-augmentation-for-facial-keypoint-detection.ipynb
"Adding random normal noise to the input image clip the resulting noisy image between 1,1 "," noisy_image = cv2.add(image , 0.008 * np.random.randn(96 , 96 , 1)) ",data-augmentation-for-facial-keypoint-detection.ipynb
Visualize Train images corresponding Keypoints,"print(""Shape of final train_images: {}"".format(np.shape(train_images)))
print(""Shape of final train_keypoints: {}"".format(np.shape(train_keypoints)))

print(""\n Clean Train Data: "")
fig = plt.figure(figsize=(20,8))
for i in range(10):
 axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])
 plot_sample(clean_train_images[i], clean_train_keypoints[i], axis, """")
plt.show()

if include_unclean_data:
 print(""Unclean Train Data: "")
 fig = plt.figure(figsize=(20,8))
 for i in range(10):
 axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])
 plot_sample(unclean_train_images[i], unclean_train_keypoints[i], axis, """")
 plt.show()

if horizontal_flip:
 print(""Horizontal Flip Augmentation: "")
 fig = plt.figure(figsize=(20,8))
 for i in range(10):
 axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])
 plot_sample(flipped_train_images[i], flipped_train_keypoints[i], axis, """")
 plt.show()

if rotation_augmentation:
 print(""Rotation Augmentation: "")
 fig = plt.figure(figsize=(20,8))
 for i in range(10):
 axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])
 plot_sample(rotated_train_images[i], rotated_train_keypoints[i], axis, """")
 plt.show()
 
if brightness_augmentation:
 print(""Brightness Augmentation: "")
 fig = plt.figure(figsize=(20,8))
 for i in range(10):
 axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])
 plot_sample(altered_brightness_train_images[i], altered_brightness_train_keypoints[i], axis, """")
 plt.show()

if shift_augmentation:
 print(""Shift Augmentation: "")
 fig = plt.figure(figsize=(20,8))
 for i in range(10):
 axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])
 plot_sample(shifted_train_images[i], shifted_train_keypoints[i], axis, """")
 plt.show()
 
if random_noise_augmentation:
 print(""Random Noise Augmentation: "")
 fig = plt.figure(figsize=(20,8))
 for i in range(10):
 axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])
 plot_sample(noisy_train_images[i], clean_train_keypoints[i], axis, """")
 plt.show()",data-augmentation-for-facial-keypoint-detection.ipynb
Building a model,model = Sequential () ,data-augmentation-for-facial-keypoint-detection.ipynb
"Input dimensions: None, 96, 96, 1 ","model.add(Convolution2D(32 ,(3 , 3), padding = 'same' , use_bias = False , input_shape =(96 , 96 , 1))) ",data-augmentation-for-facial-keypoint-detection.ipynb
"Input dimensions: None, 96, 96, 32 ","model.add(Convolution2D(32 ,(3 , 3), padding = 'same' , use_bias = False)) ",data-augmentation-for-facial-keypoint-detection.ipynb
"Input dimensions: None, 48, 48, 32 ","model.add(Convolution2D(64 ,(3 , 3), padding = 'same' , use_bias = False)) ",data-augmentation-for-facial-keypoint-detection.ipynb
"Input dimensions: None, 48, 48, 64 ","model.add(Convolution2D(64 ,(3 , 3), padding = 'same' , use_bias = False)) ",data-augmentation-for-facial-keypoint-detection.ipynb
"Input dimensions: None, 24, 24, 64 ","model.add(Convolution2D(96 ,(3 , 3), padding = 'same' , use_bias = False)) ",data-augmentation-for-facial-keypoint-detection.ipynb
"Input dimensions: None, 24, 24, 96 ","model.add(Convolution2D(96 ,(3 , 3), padding = 'same' , use_bias = False)) ",data-augmentation-for-facial-keypoint-detection.ipynb
"Input dimensions: None, 12, 12, 96 ","model.add(Convolution2D(128 ,(3 , 3), padding = 'same' , use_bias = False)) ",data-augmentation-for-facial-keypoint-detection.ipynb
"Input dimensions: None, 12, 12, 128 ","model.add(Convolution2D(128 ,(3 , 3), padding = 'same' , use_bias = False)) ",data-augmentation-for-facial-keypoint-detection.ipynb
"Input dimensions: None, 6, 6, 128 ","model.add(Convolution2D(256 ,(3 , 3), padding = 'same' , use_bias = False)) ",data-augmentation-for-facial-keypoint-detection.ipynb
"Input dimensions: None, 6, 6, 256 ","model.add(Convolution2D(256 ,(3 , 3), padding = 'same' , use_bias = False)) ",data-augmentation-for-facial-keypoint-detection.ipynb
"Input dimensions: None, 3, 3, 256 ","model.add(Convolution2D(512 ,(3 , 3), padding = 'same' , use_bias = False)) ",data-augmentation-for-facial-keypoint-detection.ipynb
"Input dimensions: None, 3, 3, 512 ","model.add(Convolution2D(512 ,(3 , 3), padding = 'same' , use_bias = False)) ",data-augmentation-for-facial-keypoint-detection.ipynb
"Input dimensions: None, 3, 3, 512 ",model.add(Flatten ()) ,data-augmentation-for-facial-keypoint-detection.ipynb
Training the model,% % time ,data-augmentation-for-facial-keypoint-detection.ipynb
Load a pre trained model if present ,if os.path.exists('../input/data-augmentation-for-facial-keypoint-detection/best_model.hdf5'): ,data-augmentation-for-facial-keypoint-detection.ipynb
Define necessary callbacks,"checkpointer = ModelCheckpoint(filepath = 'best_model.hdf5' , monitor = 'val_mae' , verbose = 1 , save_best_only = True , mode = 'min') ",data-augmentation-for-facial-keypoint-detection.ipynb
Compile the model,"model.compile(optimizer = 'adam' , loss = 'mean_squared_error' , metrics =['mae' , 'acc']) ",data-augmentation-for-facial-keypoint-detection.ipynb
Train the model,"history = model.fit(train_images , train_keypoints , epochs = NUM_EPOCHS , batch_size = BATCH_SIZE , validation_split = 0.05 , callbacks =[checkpointer]) ",data-augmentation-for-facial-keypoint-detection.ipynb
summarize history for mean absolute error,try : ,data-augmentation-for-facial-keypoint-detection.ipynb
summarize history for accuracy, plt.plot(history.history['acc']) ,data-augmentation-for-facial-keypoint-detection.ipynb
summarize history for loss, plt.plot(history.history['loss']) ,data-augmentation-for-facial-keypoint-detection.ipynb
Fit the model on full dataset,% % time ,data-augmentation-for-facial-keypoint-detection.ipynb
Modify ModelCheckpoint callback to save model with best train mae to disk instead of best validation mae ,"checkpointer = ModelCheckpoint(filepath = 'best_model.hdf5' , monitor = 'mae' , verbose = 1 , save_best_only = True , mode = 'min') ",data-augmentation-for-facial-keypoint-detection.ipynb
Predicting on Test Set,"%%time
 
model = load_model('best_model.hdf5')
test_preds = model.predict(test_images)",data-augmentation-for-facial-keypoint-detection.ipynb
Visualizing Test Predictions,"fig = plt.figure(figsize=(20,16))
for i in range(20):
 axis = fig.add_subplot(4, 5, i+1, xticks=[], yticks=[])
 plot_sample(test_images[i], test_preds[i], axis, """")
plt.show()",data-augmentation-for-facial-keypoint-detection.ipynb
Generating Submission File,"feature_names = list(idlookup_data['FeatureName'])
image_ids = list(idlookup_data['ImageId']-1)
row_ids = list(idlookup_data['RowId'])

feature_list = []
for feature in feature_names:
 feature_list.append(feature_names.index(feature))
 
predictions = []
for x,y in zip(image_ids, feature_list):
 predictions.append(test_preds[x][y])
 
row_ids = pd.Series(row_ids, name = 'RowId')
locations = pd.Series(predictions, name = 'Location')
locations = locations.clip(0.0,96.0)
submission_result = pd.concat([row_ids,locations],axis = 1)
submission_result.to_csv('submission.csv',index = False)",data-augmentation-for-facial-keypoint-detection.ipynb
Imports,"import os , warnings ",data-augmentation.ipynb
Reproducability,def set_seed(seed = 31415): ,data-augmentation.ipynb
Set Matplotlib defaults,"plt.rc('figure' , autolayout = True) ",data-augmentation.ipynb
"Step 2 Define ModelTo illustrate the effect of augmentation, we ll just add a couple of simple transformations to the model from Tutorial 1.",from tensorflow import keras ,data-augmentation.ipynb
these are a new feature in TF 2.2,from tensorflow.keras.layers.experimental import preprocessing ,data-augmentation.ipynb
Step 3 Train and EvaluateAnd now we ll start the training!,"model.compile(
 optimizer='adam',
 loss='binary_crossentropy',
 metrics=['binary_accuracy'],
)

history = model.fit(
 ds_train,
 validation_data=ds_valid,
 epochs=30,
 verbose=0,
)",data-augmentation.ipynb
"Train Test ContaminationA different type of leak occurs when you aren t careful to distinguish training data from validation data. Recall that validation is meant to be a measure of how the model does on data that it hasn t considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavior. This is sometimes called train test contamination. For example, imagine you run preprocessing like fitting an imputer for missing values before calling train test split . The end result? Your model may get good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions.After all, you incorporated data from the validation or test data into how you make predictions, so the may do well on that particular data even if it can t generalize to new data. This problem becomes even more subtle and more dangerous when you do more complex feature engineering.If your validation is based on a simple train test split, exclude the validation data from any type of fitting, including the fitting of preprocessing steps. This is easier if you use scikit learn pipelines. When using cross validation, it s even more critical that you do your preprocessing inside the pipeline!ExampleIn this example, you will learn one way to detect and remove target leakage.We will use a dataset about credit card applications and skip the basic data set up code. The end result is that information about each credit card application is stored in a DataFrame X. We ll use it to predict which applications were accepted in a Series y.",import pandas as pd ,data-leakage.ipynb
"Since this is a small dataset, we will use cross validation to ensure accurate measures of model quality.",from sklearn.pipeline import make_pipeline ,data-leakage.ipynb
"Since there is no preprocessing, we don t need a pipeline used anyway as best practice! ",my_pipeline = make_pipeline(RandomForestClassifier(n_estimators = 100)) ,data-leakage.ipynb
"With experience, you ll find that it s very rare to find models that are accurate 98 of the time. It happens, but it s uncommon enough that we should inspect the data more closely for target leakage.Here is a summary of the data, which you can also find under the data tab: card: 1 if credit card application accepted, 0 if not reports: Number of major derogatory reports age: Age n years plus twelfths of a year income: Yearly income divided by 10,000 share: Ratio of monthly credit card expenditure to yearly income expenditure: Average monthly credit card expenditure owner: 1 if owns home, 0 if rents selfempl: 1 if self employed, 0 if not dependents: 1 number of dependents months: Months living at current address majorcards: Number of major credit cards held active: Number of active credit accounts A few variables look suspicious. For example, does expenditure mean expenditure on this card or on cards used before applying?At this point, basic data comparisons can be very helpful:","expenditures_cardholders = X.expenditure[y]
expenditures_noncardholders = X.expenditure[~y]

print('Fraction of those who did not receive a card and had no expenditures: %.2f' \
 %((expenditures_noncardholders == 0).mean()))
print('Fraction of those who received a card and had no expenditures: %.2f' \
 %(( expenditures_cardholders == 0).mean()))",data-leakage.ipynb
Drop leaky predictors from dataset,"potential_leaks =['expenditure' , 'share' , 'active' , 'majorcards'] ",data-leakage.ipynb
linear algebra,import numpy as np ,data-science-london-classification.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,data-science-london-classification.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,data-science-london-classification.ipynb
"You can also write temporary files to kaggle temp , but they won t be saved outside of the current session",from subprocess import check_output ,data-science-london-classification.ipynb
data analysis and wrangling,import pandas as pd ,data-science-london-classification.ipynb
visualization,import seaborn as sns ,data-science-london-classification.ipynb
machine learning,from sklearn.linear_model import LogisticRegression ,data-science-london-classification.ipynb
read csv comma separated value into data,"train = pd.read_csv('../input/data-science-london-scikit-learn/train.csv' , header = None) ",data-science-london-classification.ipynb
look at available plot styles,print(plt.style.available) ,data-science-london-classification.ipynb
B.1 check shape of training and test set,"print('Data Description')
print('The shape of our training set: ',train.shape[0], 'rows ', 'and', train.shape[1] , 'columns' )
print('The shape of our testing set: ',trainLabel.shape[0], 'rows', 'and', trainLabel.shape[1], 'column')
print('The shape of our testing set: ',test.shape[0], 'rows', 'and', test.shape[1], 'columns')

",data-science-london-classification.ipynb
B.2 Analyze by describing data Pandas also helps describe the datasets answering following questions early in our project.Which features are available in the dataset?,print(train.columns.values),data-science-london-classification.ipynb
preview the data from head,train.head(3) ,data-science-london-classification.ipynb
split data train into Numeric and Categorocal,numeric = train.select_dtypes(exclude = 'object') ,data-science-london-classification.ipynb
"Which features are categorical? These values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization. ","print(""\nNumber of categorical features : "",(len(categorical.axes[1])))
print(""\n"", categorical.axes[1])
categorical.head()",data-science-london-classification.ipynb
"Which features are numerical? Which features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization. ","print(""\nNumber of numeric features : "",(len(numeric.axes[1])))
print(""\n"", numeric.axes[1])",data-science-london-classification.ipynb
Note All Features are Numerical Quantitative data ,"train.info()
print('_'*50)
test.info()",data-science-london-classification.ipynb
missing data in Traing examples,total = train.isnull (). sum (). sort_values(ascending = False) ,data-science-london-classification.ipynb
missing data in Traing Label target label examples,total = trainLabel.isnull (). sum (). sort_values(ascending = False) ,data-science-london-classification.ipynb
missing data in Test examples,total = test.isnull (). sum (). sort_values(ascending = False) ,data-science-london-classification.ipynb
"C. Model, predict and solve Grid Search Naive Bayes classifier KNN or k Nearest Neighbors Random Forrest Logistic Regression Support Vector Machines Decision Tree XGBOOST Classifier AdaBoosting Classifier GradientBoostingClassifier HistGradientBoostingClassifier Principal Component Analysis PCA Gaussian Mixture ",import pandas as pd ,data-science-london-classification.ipynb
NAIBE BAYES,from sklearn.naive_bayes import GaussianNB ,data-science-london-classification.ipynb
KNN,from sklearn.neighbors import KNeighborsClassifier ,data-science-london-classification.ipynb
RANDOM FOREST,from sklearn.ensemble import RandomForestClassifier ,data-science-london-classification.ipynb
LOGISTIC REGRESSION,from sklearn.linear_model import LogisticRegression ,data-science-london-classification.ipynb
SVM,from sklearn.svm import SVC ,data-science-london-classification.ipynb
DECISON TREE,from sklearn.tree import DecisionTreeClassifier ,data-science-london-classification.ipynb
XGBOOST,from xgboost import XGBClassifier ,data-science-london-classification.ipynb
AdaBoosting Classifier,from sklearn.ensemble import AdaBoostClassifier ,data-science-london-classification.ipynb
GradientBoosting Classifier,from sklearn.ensemble import GradientBoostingClassifier ,data-science-london-classification.ipynb
HistGradientBoostingClassifier,from sklearn.experimental import enable_hist_gradient_boosting ,data-science-london-classification.ipynb
We further split the training set in to a train and test set to validate our model.,"X_train,X_test,y_train,y_test = train_test_split(train,trainLabel,test_size=0.30, random_state=101)
X_train.shape,X_test.shape,y_train.shape,y_test.shape",data-science-london-classification.ipynb
Importing libraries,from sklearn.neighbors import KNeighborsClassifier ,data-science-london-classification.ipynb
"In theory, it recovers the true number of components only in the asymptotic regime",lowest_bic = np.infty ,data-science-london-classification.ipynb
"spherical, diagonal, tied or full covariance.","cv_types =['spherical' , 'tied' , 'diag' , 'full'] ",data-science-london-classification.ipynb
"The predict proba method will take in new data points and predict the responsibilities for each Gaussian. In other words, the probability that this data point came from each distribution.Now Applying Grid Search Algorithm: To identify the best algorithm and best parameters","X_train,X_test,y_train,y_test = train_test_split(gmm_train,trainLabel,test_size=0.30, random_state=101)
X_train.shape,X_test.shape,y_train.shape,y_test.shape",data-science-london-classification.ipynb
Fitting our model,"stack_list[3]. fit(X_train_1 , trainLabel) ",data-science-london-classification.ipynb
FRAMING OUR SOLUTION,y_submit.columns =['Solution'] ,data-science-london-classification.ipynb
Exporting the data to submit.,"y_submit.to_csv('Submission.csv',index=False)",data-science-london-classification.ipynb
linear algebra,import numpy as np ,data-science-london-scikit-learn-modeling.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,data-science-london-scikit-learn-modeling.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,data-science-london-scikit-learn-modeling.ipynb
"You can also write temporary files to kaggle temp , but they won t be saved outside of the current session","df_train = pd.read_csv(""/kaggle/input/data-science-london-scikit-learn/train.csv"", header = None)
trainLabels = pd.read_csv(""/kaggle/input/data-science-london-scikit-learn/trainLabels.csv"", header = None)
df_test = pd.read_csv(""/kaggle/input/data-science-london-scikit-learn/test.csv"", header = None)",data-science-london-scikit-learn-modeling.ipynb
"kfold model selection.KFold shuffle True,n splits 10,random state 0 "," cv_results = cross_val_score(model , X , y , cv = 10 , scoring = 'roc_auc') ",data-science-london-scikit-learn-modeling.ipynb
linear algebra,import numpy as np ,data-science-london-scikit-learn-practice.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,data-science-london-scikit-learn-practice.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,data-science-london-scikit-learn-practice.ipynb
"You can also write temporary files to kaggle temp , but they won t be saved outside of the current session","from matplotlib import pyplot as plt
import seaborn as sns
import warnings 
warnings.simplefilter(""ignore"")",data-science-london-scikit-learn-practice.ipynb
"best model accuracy from Random forest tree 88 to 90 bootstrap : False, criterion : gini , max features : log2 , n estimators : 50 ","from sklearn.ensemble import RandomForestClassifier

%time
clr = RandomForestClassifier(bootstrap=False, criterion=""gini"", max_features=""log2"", n_estimators=50)
clr.fit(x_train, y_train)",data-science-london-scikit-learn-practice.ipynb
linear algebra,import numpy as np ,data-science-london-scikit-learn.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,data-science-london-scikit-learn.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,data-science-london-scikit-learn.ipynb
Importing libraries.,"import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from imblearn.over_sampling import RandomOverSampler, SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.naive_bayes import GaussianNB
from scipy import stats
from sklearn.metrics import accuracy_score, roc_curve, auc, mean_squared_error, f1_score
import warnings
warnings.filterwarnings('ignore')",data-science-london-scikit-learn.ipynb
Reading the data.,!ls ../input/data-science-london-scikit-learn,data-science-london-scikit-learn.ipynb
Preprocessnig,train_label.head(),data-science-london-scikit-learn.ipynb
Findning duplicate rows.,dupli = train[train.duplicated()] ,data-science-london-scikit-learn.ipynb
Removing all the columns having only 1 unique value.,for i in train.columns : ,data-science-london-scikit-learn.ipynb
Before removing Outliers.,"plt.figure(figsize =(20 , 20)) ",data-science-london-scikit-learn.ipynb
Count plot for target column.,print(train['target']. value_counts ()) ,data-science-london-scikit-learn.ipynb
Applying Standard Scaler.,"x = train.drop('target' , axis = 1) ",data-science-london-scikit-learn.ipynb
Building models.,"def Models(model, name, d, xtrain, ytrain, xtest, ytest):
 print(""Working on {} model"".format(name))
 
 cla = model
 cla.fit(xtrain, ytrain)
 
 predicted = cla.predict(xtrain)
 tr_auc = accuracy_score(predicted, ytrain)*100
 
 predicted = cla.predict(xtest)
 te_auc = accuracy_score(predicted, ytest)*100
 
 F1 = f1_score(predicted, ytest)
 MSE = mean_squared_error(predicted, ytest)
 
 d['Name'].append(name)
 d['Training ACU'].append(tr_auc)
 d['Testing ACU'].append(te_auc)
 d['F1_Score'].append(F1)
 d['MSE'].append(MSE)
 
 print(""**********""*5)
 print()
 return d",data-science-london-scikit-learn.ipynb
Predciting on test data.,"acu_data = pd.DataFrame(data = d)
acu_data",data-science-london-scikit-learn.ipynb
linear algebra,import numpy as np ,data-science-london-scikit.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,data-science-london-scikit.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,data-science-london-scikit.ipynb
import libry,import numpy as np ,data-science-london-scikit.ipynb
reading data,"train = pd.read_csv('../input/data-science-london-scikit-learn/train.csv' , header = None) ",data-science-london-scikit.ipynb
modeal complex,"n_egl = np.arange(4 , 30) ",data-science-london-scikit.ipynb
Plot,"plt.figure(figsize =[13 , 8]) ",data-science-london-scikit.ipynb
pridact,best_knn.predict(X_test) ,data-science-london-scikit.ipynb
ather slution withe data scaling StandardScaler minmax scale Normalizer ,"from sklearn.preprocessing import StandardScaler,minmax_scale,Normalizer
std=StandardScaler()
x_std=std.fit_transform(X)
min_x=minmax_scale(X)
Nor=Normalizer()
Nor_x=Nor.fit_transform(X)",data-science-london-scikit.ipynb
complex model,"n_gl = np.arange(4 , 40) ",data-science-london-scikit.ipynb
plot,"plt.figure(figsize =[12 , 7]) ",data-science-london-scikit.ipynb
fiting,"best_KNN.fit(Nor_x , y) ",data-science-london-scikit.ipynb
prdict,best_KNN.predict(X_test) ,data-science-london-scikit.ipynb
Calculating Confusion Matrix,"CM = confusion_matrix(y_test , best_KNN.predict(X_test)) ",data-science-london-scikit.ipynb
drawing confusion matrix,"sns.heatmap(CM , center = True) ",data-science-london-scikit.ipynb
Calculating classification Report :,"classification_report(y_test , best_KNN.predict(X_test), labels = None , target_names = None , sample_weight = None , digits = 2 , output_dict = False) ",data-science-london-scikit.ipynb
Loading Data,"Data = pd.read_csv('../input/data-science-london-scikit-learn/train.csv',header=None)
Label = pd.read_csv('../input/data-science-london-scikit-learn/trainLabels.csv',header=None,names=['label'])
",data-visualization-preprocessing-gridsearchcv.ipynb
Get Describtion Information About Data,"Data.info()
D=Data.copy(deep=True)",data-visualization-preprocessing-gridsearchcv.ipynb
Get Correlation Between Label Column And Train Data,x = Data.corrwith(Label['label']) ,data-visualization-preprocessing-gridsearchcv.ipynb
", fmt .6g ","sns.heatmap(x , annot = True , cmap = 'Blues') ",data-visualization-preprocessing-gridsearchcv.ipynb
Replace Columns That Has Bad Correlation With Label as they Made Miss Leading by a Two New columns has Correlation one Positive And one Negative,"EC = Data.columns
Drop_col=[]
for i in EC:
 if(((Data[i].corr(Label['label']) < 0.01) and (Data[i].corr(Label['label'])> 0)) or 
 ((Data[i].corr(Label['label']) < 0) and (Data[i].corr(Label['label'])> -0.09))):
 Drop_col.append(i)
 Data.drop(i, axis=1, inplace=True)
 
EC=Data.columns
",data-visualization-preprocessing-gridsearchcv.ipynb
Using PCA To Make 2 New Columns By Combination Of Bad Correlation Columns,"x=D.iloc[:,Drop_col]

model = PCA(n_components= 2)
model.fit(x)

data = model.transform(x)
data = pd.DataFrame(data)
Data = pd.concat([Data, data], axis=1)",data-visualization-preprocessing-gridsearchcv.ipynb
"First Graph Represent That The Maximun number of Label is 1 Second Graph represent that and specific The number of Unique Values in this Column is 2 0 , 1 ","sns.distplot(Label['label'], fit=norm);
fig = plt.figure()
res = stats.probplot(Label['label'], plot=plt)",data-visualization-preprocessing-gridsearchcv.ipynb
Data Visualization Get Columns Normal Graph To Make Sure That Data Has Normal Distribution With Density,"horizontal_concat = pd.concat([Data, Label['label']], axis=1)
horizontal_concat=pd.DataFrame(horizontal_concat).astype('object')
sns.pairplot(horizontal_concat, corner=True)",data-visualization-preprocessing-gridsearchcv.ipynb
Standard Scaler for Data,"sc_X = StandardScaler()
X_train = sc_X.fit_transform(Data)",data-visualization-preprocessing-gridsearchcv.ipynb
Doing All Previous Data Preprocessing On Test Data,"Test_Data = pd.read_csv('../input/data-science-london-scikit-learn/test.csv',header=None)
Test_Data.info()
D=Test_Data.copy(deep=True)
Test_Data = Test_Data.loc[:,EC]
x=D.iloc[:,Drop_col]
model = PCA(n_components= 2)
model.fit(x)
data = model.transform(x)
data = pd.DataFrame(data)
Test_Data = pd.concat([Test_Data, data], axis=1)",data-visualization-preprocessing-gridsearchcv.ipynb
Machine Learning Modeling,"from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix

",data-visualization-preprocessing-gridsearchcv.ipynb
The Best Score is 90 ,"pred.sort(key=lambda y: y[0])
sns.heatmap(pred[-1][1], center = True)
plt.show()
prediction=pred[-1][-1]
prediction=np.array(prediction)
prediction=prediction.reshape(9000,1)",data-visualization-preprocessing-gridsearchcv.ipynb
"IntroThe models you ve built so far have relied on pre trained models. But they aren t the ideal solution for many use cases. In this lesson, you will learn how to build totally new models.Lesson","from IPython.display import YouTubeVideo
YouTubeVideo('YbNE3zhtsoo', width=800, height=450)",deep-learning-from-scratch.ipynb
Sample Code,"import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.python import keras
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout


img_rows, img_cols = 28, 28
num_classes = 10

def data_prep(raw):
 out_y = keras.utils.to_categorical(raw.label, num_classes)

 num_images = raw.shape[0]
 x_as_array = raw.values[:,1:]
 x_shaped_array = x_as_array.reshape(num_images, img_rows, img_cols, 1)
 out_x = x_shaped_array / 255
 return out_x, out_y

train_file = ""../input/digit-recognizer/train.csv""
raw_data = pd.read_csv(train_file)

x, y = data_prep(raw_data)

model = Sequential()
model.add(Conv2D(20, kernel_size=(3, 3),
 activation='relu',
 input_shape=(img_rows, img_cols, 1)))
model.add(Conv2D(20, kernel_size=(3, 3), activation='relu'))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss=keras.losses.categorical_crossentropy,
 optimizer='adam',
 metrics=['accuracy'])
model.fit(x, y,
 batch_size=128,
 epochs=2,
 validation_split = 0.2)",deep-learning-from-scratch.ipynb
linear algebra,import numpy as np ,deep-neural-network-keras-way.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,deep-neural-network-keras-way.ipynb
"For example, running this by clicking run or pressing Shift Enter will list the files in the input directory",from subprocess import check_output ,deep-neural-network-keras-way.ipynb
"create the training test sets, skipping the header row with 1: ","train = pd.read_csv(""../input/train.csv"") ",deep-neural-network-keras-way.ipynb
all pixel values,"X_train =(train.iloc[: , 1 :]. values). astype('float32') ",deep-neural-network-keras-way.ipynb
only labels i.e targets digits,"y_train = train.iloc[: , 0]. values.astype('int32') ",deep-neural-network-keras-way.ipynb
"Convert train datset to num images, img rows, img cols format","X_train = X_train.reshape(X_train.shape[0], 28 , 28) ",deep-neural-network-keras-way.ipynb
expand 1 more dimention as 1 for colour channel gray,"X_train = X_train.reshape(X_train.shape[0], 28 , 28 , 1) ",deep-neural-network-keras-way.ipynb
Feature StandardizationIt is important preprocessing step. It is used to centre the data around zero mean and unit variance.,"mean_px = X_train.mean().astype(np.float32)
std_px = X_train.std().astype(np.float32)

def standardize(x): 
 return (x-mean_px)/std_px",deep-neural-network-keras-way.ipynb
"One Hot encoding of labels.A one hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the nth digit will be represented as a vector which is 1 in the nth dimension. For example, 3 would be 0,0,0,1,0,0,0,0,0,0 .","from keras.utils.np_utils import to_categorical
y_train= to_categorical(y_train)
num_classes = y_train.shape[1]
num_classes",deep-neural-network-keras-way.ipynb
Lets plot 10th label.,"plt.title(y_train[9])
plt.plot(y_train[9])
plt.xticks(range(10));",deep-neural-network-keras-way.ipynb
fix random seed for reproducibility,seed = 43 ,deep-neural-network-keras-way.ipynb
Linear Model,"from keras.models import Sequential
from keras.layers.core import Lambda , Dense, Flatten, Dropout
from keras.callbacks import EarlyStopping
from keras.layers import BatchNormalization, Convolution2D , MaxPooling2D",deep-neural-network-keras-way.ipynb
"Lets create a simple model from Keras Sequential layer. Lambda layer performs simple arithmetic operations like sum, average, exponentiation etc. In 1st layer of the model we have to define input dimensions of our data in rows,columns,colour channel format. In theano colour channel comes first Flatten will transform input into 1D array. Dense is fully connected layer that means all neurons in previous layers will be connected to all neurons in fully connected layer. In the last layer we have to specify output dimensions classes of the model. Here it s 10, since we have to output 10 different digit labels. ","model= Sequential()
model.add(Lambda(standardize,input_shape=(28,28,1)))
model.add(Flatten())
model.add(Dense(10, activation='softmax'))
print(""input shape "",model.input_shape)
print(""output shape "",model.output_shape)",deep-neural-network-keras-way.ipynb
Compile networkBefore making network ready for training we have to make sure to add below things: A loss function: to measure how good the network is An optimizer: to update network as it sees more data and reduce loss value Metrics: to monitor performance of network ,"from keras.optimizers import RMSprop
model.compile(optimizer=RMSprop(lr=0.001),
 loss='categorical_crossentropy',
 metrics=['accuracy'])",deep-neural-network-keras-way.ipynb
Cross Validation,"from sklearn.model_selection import train_test_split
X = X_train
y = y_train
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=42)
batches = gen.flow(X_train, y_train, batch_size=64)
val_batches=gen.flow(X_val, y_val, batch_size=64)",deep-neural-network-keras-way.ipynb
 bo is for blue dot ,"plt.plot(epochs , loss_values , 'bo') ",deep-neural-network-keras-way.ipynb
b is for blue crosses ,"plt.plot(epochs , val_loss_values , 'b+') ",deep-neural-network-keras-way.ipynb
clear figure,plt.clf () ,deep-neural-network-keras-way.ipynb
"Fully Connected ModelNeurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. Adding another Dense Layer to model.","def get_fc_model():
 model = Sequential([
 Lambda(standardize, input_shape=(28,28,1)),
 Flatten(),
 Dense(512, activation='relu'),
 Dense(10, activation='softmax')
 ])
 model.compile(optimizer='Adam', loss='categorical_crossentropy',
 metrics=['accuracy'])
 return model",deep-neural-network-keras-way.ipynb
Convolutional Neural Network CNNs are extremely efficient for images.,"from keras.layers import Convolution2D, MaxPooling2D

def get_cnn_model():
 model = Sequential([
 Lambda(standardize, input_shape=(28,28,1)),
 Convolution2D(32,(3,3), activation='relu'),
 Convolution2D(32,(3,3), activation='relu'),
 MaxPooling2D(),
 Convolution2D(64,(3,3), activation='relu'),
 Convolution2D(64,(3,3), activation='relu'),
 MaxPooling2D(),
 Flatten(),
 Dense(512, activation='relu'),
 Dense(10, activation='softmax')
 ])
 model.compile(Adam(), loss='categorical_crossentropy',
 metrics=['accuracy'])
 return model",deep-neural-network-keras-way.ipynb
"Data Augmentation It is tehnique of showing slighly different or new images to neural network to avoid overfitting. And to achieve better generalization. In case you have very small dataset, you can use different kinds of data augmentation techniques to increase your data size. Neural networks perform better if you provide them more data.Different data aumentation techniques are as follows: 1. Cropping 2. Rotating 3. Scaling 4. Translating 5. Flipping 6. Adding Gaussian noise to input images etc.","gen =ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,
 height_shift_range=0.08, zoom_range=0.08)
batches = gen.flow(X_train, y_train, batch_size=64)
val_batches = gen.flow(X_val, y_val, batch_size=64)",deep-neural-network-keras-way.ipynb
Adding Batch NormalizationBN helps to fine tune hyperparameters more better and train really deep neural networks.,"from keras.layers.normalization import BatchNormalization

def get_bn_model():
 model = Sequential([
 Lambda(standardize, input_shape=(28,28,1)),
 Convolution2D(32,(3,3), activation='relu'),
 BatchNormalization(axis=1),
 Convolution2D(32,(3,3), activation='relu'),
 MaxPooling2D(),
 BatchNormalization(axis=1),
 Convolution2D(64,(3,3), activation='relu'),
 BatchNormalization(axis=1),
 Convolution2D(64,(3,3), activation='relu'),
 MaxPooling2D(),
 Flatten(),
 BatchNormalization(),
 Dense(512, activation='relu'),
 BatchNormalization(),
 Dense(10, activation='softmax')
 ])
 model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
 return model",deep-neural-network-keras-way.ipynb
Submitting Predictions to Kaggle. Make sure you use full train dataset here to train model and predict on test set.,"model.optimizer.lr=0.01
gen = image.ImageDataGenerator()
batches = gen.flow(X, y, batch_size=64)
history=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=3)",deep-neural-network-keras-way.ipynb
"CodeThere are a lot of great implementations of reinforcement learning algorithms online. In this course, we ll use Stable Baselines.Currently, Stable Baselines is not yet compatible with TensorFlow 2.0. So, we begin by downgrading to TensorFlow 1.0.","
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline",deep-reinforcement-learning.ipynb
Check version of tensorflow,import tensorflow as tf ,deep-reinforcement-learning.ipynb
"There s a bit of extra work that we need to do to make the environment compatible with Stable Baselines. For this, we define the ConnectFourGym class below. This class implements ConnectX as an OpenAI Gym environment and uses several methods: reset will be called at the beginning of every game. It returns the starting game board as a 2D numpy array with 6 rows and 7 columns. change reward customizes the rewards that the agent receives. The competition already has its own system for rewards that are used to rank the agents, and this method changes the values to match the rewards system we designed. step is used to play the agent s choice of action supplied as action , along with the opponent s response. It returns: the resulting game board as a numpy array , the agent s reward from the most recent move only: one of 1, 10, 1, or 1 42 , and whether or not the game has ended if the game has ended, done True otherwise, done False . To learn more about how to define environments, check out the documentation here.","from kaggle_environments import make , evaluate ",deep-reinforcement-learning.ipynb
Learn about spaces here: , self.action_space = spaces.Discrete(self.columns) ,deep-reinforcement-learning.ipynb
Create ConnectFour environment,"env = ConnectFourGym(agent2 = ""random"") ",deep-reinforcement-learning.ipynb
"Stable Baselines requires us to work with vectorized environments. For this, we can use the DummyVecEnv class. The Monitor class lets us watch how the agent s performance gradually improves, as it plays more and more games.","
!apt-get update
!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev
!pip install ""stable-baselines[mpi]==2.10.2""",deep-reinforcement-learning.ipynb
Create directory for logging training information,"log_dir = ""ppo/"" ",deep-reinforcement-learning.ipynb
Logging progress,"monitor_env = Monitor(env , log_dir , allow_early_resets = True) ",deep-reinforcement-learning.ipynb
Create a vectorized environment,vec_env = DummyVecEnv ([lambda : monitor_env]) ,deep-reinforcement-learning.ipynb
"The next step is to specify the architecture of the neural network. In this case, we use a convolutional neural network. To learn more about how to specify architectures with Stable Baselines, check out the documentation here.Note that this is the neural network that outputs the probabilities of selecting each column. Since we use the PPO algorithm PPO1 in the code cell below , our network will also output some additional information called the value of the input . This is outside the scope of this course, but you can learn more by reading about actor critic networks .",from stable_baselines import PPO1 ,deep-reinforcement-learning.ipynb
Neural network for predicting action values,"def modified_cnn(scaled_images , ** kwargs): ",deep-reinforcement-learning.ipynb
Train agent,model.learn(total_timesteps = 100000) ,deep-reinforcement-learning.ipynb
Plot cumulative reward,"with open(os.path.join(log_dir , ""monitor.csv""), 'rt')as fh : ",deep-reinforcement-learning.ipynb
"Finally, we specify the trained agent in the format required for the competition.","def agent1(obs , config): ",deep-reinforcement-learning.ipynb
Use the best model to select a column," col , _ = model.predict(np.array(obs['board']).reshape(6 , 7 , 1)) ",deep-reinforcement-learning.ipynb
Check if selected column is valid, is_valid =(obs['board'][ int(col )]== 0) ,deep-reinforcement-learning.ipynb
"If not valid, select random move.", if is_valid : ,deep-reinforcement-learning.ipynb
Create the game environment,"env = make(""connectx"") ",deep-reinforcement-learning.ipynb
Two random agents play one game round,"env.run ([agent1 , ""random""]) ",deep-reinforcement-learning.ipynb
Show the game,"env.render(mode = ""ipython"") ",deep-reinforcement-learning.ipynb
"And, we calculate how it performs on average, against the random agent.","def get_win_percentages(agent1 , agent2 , n_rounds = 100): ",deep-reinforcement-learning.ipynb
Use default Connect Four setup," config = { 'rows' : 6 , 'columns' : 7 , 'inarow' : 4 } ",deep-reinforcement-learning.ipynb
Agent 1 goes first roughly half the time," outcomes = evaluate(""connectx"" ,[agent1 , agent2], config , [], n_rounds // 2) ",deep-reinforcement-learning.ipynb
Agent 2 goes first roughly half the time," outcomes +=[[ b , a]for[a , b]in evaluate(""connectx"" ,[agent2 , agent1], config , [], n_rounds - n_rounds // 2 )] ",deep-reinforcement-learning.ipynb
For the preparations lets first import the necessary libraries and load the files needed for our EDA,import pandas as pd ,detailed-exploratory-data-analysis-with-python.ipynb
Comment this if the data visualisations doesn t work on your side,% matplotlib inline ,detailed-exploratory-data-analysis-with-python.ipynb
df.count does not include NaN values,df2 = df[[ column for column in df if df[column]. count ()/ len(df)>= 0.3]] ,detailed-exploratory-data-analysis-with-python.ipynb
Now lets take a look at how the housing price is distributed,"print(df['SalePrice'].describe())
plt.figure(figsize=(9, 8))
sns.distplot(df['SalePrice'], color='g', bins=100, hist_kws={'alpha': 0.4});",detailed-exploratory-data-analysis-with-python.ipynb
To do so lets first list all the types of our data from our dataset and take only the numerical ones:,list(set(df.dtypes.tolist())),detailed-exploratory-data-analysis-with-python.ipynb
 avoid having the matplotlib verbose informations,"df_num.hist(figsize =(16 , 20), bins = 50 , xlabelsize = 8 , ylabelsize = 8); ",detailed-exploratory-data-analysis-with-python.ipynb
 1 because the latest row is SalePrice,df_num_corr = df_num.corr ()[ 'SalePrice'][ : - 1] ,detailed-exploratory-data-analysis-with-python.ipynb
"Perfect, we now have a list of strongly correlated values but this list is incomplete as we know that correlation is affected by outliers. So we could proceed as follow: Plot the numerical features and see which ones have very few or explainable outliers Remove the outliers from these features and see which one can have a good correlation without their outliers Btw, correlation by itself does not always explain the relationship between data so ploting them could even lead us to new insights and in the same manner, check that our correlated values have a linear relationship to the SalePrice. For example, relationships such as curvilinear relationship cannot be guessed just by looking at the correlation value so lets take the features we excluded from our correlation table and plot them to see if they show some kind of pattern.","for i in range(0, len(df_num.columns), 5):
 sns.pairplot(data=df_num,
 x_vars=df_num.columns[i:i+5],
 y_vars=['SalePrice'])",detailed-exploratory-data-analysis-with-python.ipynb
So now lets remove these 0 values and repeat the process of finding correlated values: ,import operator ,detailed-exploratory-data-analysis-with-python.ipynb
 1 because the last column is SalePrice,"for i in range(0 , len(df_num.columns)- 1): ",detailed-exploratory-data-analysis-with-python.ipynb
Very interesting! We found another strongly correlated value by cleaning up the data a bit. Now our golden features list var looks like this:,"golden_features_list = [key for key, value in all_correlations if abs(value) >= 0.5]
print(""There is {} strongly correlated values with SalePrice:\n{}"".format(len(golden_features_list), golden_features_list))",detailed-exploratory-data-analysis-with-python.ipynb
We already examined SalePrice correlations,"corr = df_num.drop('SalePrice' , axis = 1). corr () ",detailed-exploratory-data-analysis-with-python.ipynb
Some of the features of our dataset are categorical. To separate the categorical from quantitative features lets refer ourselves to the data description.txt file. According to this file we end up with the folowing columns:,"quantitative_features_list = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF',
 '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',
 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 
 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'SalePrice']
df_quantitative_values = df[quantitative_features_list]
df_quantitative_values.head()",detailed-exploratory-data-analysis-with-python.ipynb
"Still, we have a lot of features to analyse here so let s take the strongly correlated quantitative features from this dataset and analyse them one by one","features_to_analyse = [x for x in quantitative_features_list if x in golden_features_list]
features_to_analyse.append('SalePrice')
features_to_analyse",detailed-exploratory-data-analysis-with-python.ipynb
Let s look at their distribution.,"fig, ax = plt.subplots(round(len(features_to_analyse) / 3), 3, figsize = (18, 12))

for i, ax in enumerate(fig.axes):
 if i < len(features_to_analyse) - 1:
 sns.regplot(x=features_to_analyse[i],y='SalePrice', data=df[features_to_analyse], ax=ax)",detailed-exploratory-data-analysis-with-python.ipynb
quantitative features list : 1 as the last column is SalePrice and we want to keep it,categorical_features =[a for a in quantitative_features_list[: - 1]+ df.columns.tolist ()if(a not in quantitative_features_list[: - 1]) or(a not in df.columns.tolist ())] ,detailed-exploratory-data-analysis-with-python.ipynb
And don t forget the non numerical features,"df_not_num = df_categ.select_dtypes(include = ['O'])
print('There is {} non numerical features including:\n{}'.format(len(df_not_num.columns), df_not_num.columns.tolist()))",detailed-exploratory-data-analysis-with-python.ipynb
Now lets plot some of them,"plt.figure(figsize = (10, 6))
ax = sns.boxplot(x='BsmtExposure', y='SalePrice', data=df_categ)
plt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor=""k"")
plt.xticks(rotation=45)",detailed-exploratory-data-analysis-with-python.ipynb
And finally lets look at their distribution,"fig, axes = plt.subplots(round(len(df_not_num.columns) / 3), 3, figsize=(12, 30))

for i, ax in enumerate(fig.axes):
 if i < len(df_not_num.columns):
 ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)
 sns.countplot(x=df_not_num.columns[i], alpha=0.7, data=df_not_num, ax=ax)

fig.tight_layout()",detailed-exploratory-data-analysis-with-python.ipynb
Import,"import math , re , os , time ",detailed-guide-to-custom-training-with-tpus.ipynb
"from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7",! pip install - q efficientnet ,detailed-guide-to-custom-training-with-tpus.ipynb
"Detect hardware, return appropriate distribution strategy",try : ,detailed-guide-to-custom-training-with-tpus.ipynb
TPU detection. No parameters necessary if TPU NAME environment variable is set. On Kaggle this is always the case., tpu = tf.distribute.cluster_resolver.TPUClusterResolver () ,detailed-guide-to-custom-training-with-tpus.ipynb
default distribution strategy in Tensorflow. Works on CPU and single GPU., strategy = tf.distribute.get_strategy () ,detailed-guide-to-custom-training-with-tpus.ipynb
"Competition data access TPUs read data directly from Google Cloud Storage GCS . This Kaggle utility will copy the dataset to a GCS bucket co located with the TPU. Once done, use !ls kaggle input to list attached datasets. Tips: If you have multiple datasets attached to the notebook, you should pass the name of a specific dataset to the get gcs path function. Here, the name of the dataset is the name of the directory it is mounted in. ",GCS_DS_PATH = KaggleDatasets (). get_gcs_path('tpu-getting-started') ,detailed-guide-to-custom-training-with-tpus.ipynb
you can list the bucket with !gsutil ls GCS DS PATH ,! gsutil ls $ GCS_DS_PATH ,detailed-guide-to-custom-training-with-tpus.ipynb
"For GPU training, please select 224 x 224 px image size.","IMAGE_SIZE =[192 , 192] ",detailed-guide-to-custom-training-with-tpus.ipynb
00 09,"print(f""number of flower classes: {len(CLASSES)}"")",detailed-guide-to-custom-training-with-tpus.ipynb
numpy and matplotlib defaults,"np.set_printoptions(threshold = 15 , linewidth = 80) ",detailed-guide-to-custom-training-with-tpus.ipynb
"binary string in this case, these are image ID strings", if numpy_labels.dtype == object : ,detailed-guide-to-custom-training-with-tpus.ipynb
"If no labels, only image IDs, return None for labels this is the case for test data "," return numpy_images , numpy_labels ",detailed-guide-to-custom-training-with-tpus.ipynb
"Read from TFRecord files raw datasetHere we use tf.data.TFRecordDataset to read some TFRecord files and peek the content.The simplest way is to specify a list of filenames paths of TFRecord files to it. It is a subclass of tf.data.Dataset.The raw dataset contains tf.train.Example messages, and when iterated over it, we get scalar string tensors.","raw_dataset = tf.data.TFRecordDataset(TRAINING_FILENAMES)
raw_dataset",detailed-guide-to-custom-training-with-tpus.ipynb
Check an element in the raw dataset,"serialized_example = next(iter(raw_dataset))

print('A serialized example looks like:\n\n' + str(serialized_example)[:100] + '...' * 5 + str(serialized_example)[-100:] + '\n')
print(type(serialized_example))",detailed-guide-to-custom-training-with-tpus.ipynb
"what if I don t know how to define the feature description for a raw dataset ...If you are the author who created the TFRecord files, you definitely know how to define the feature description to parse the raw dataset.Otherwise, you can use likeexample tf.train.Example example.ParseFromString serialized example.numpy to check the information. You will get something likefeatures feature key: class value int64 list value: 57 feature key: id value bytes list value: 338ab7bac feature key: image value bytes list value: ....... This should give you enough information to define the feature description.","example = tf.train.Example()
example.ParseFromString(serialized_example.numpy())
print(str(example)[:300] + ' ...')",detailed-guide-to-custom-training-with-tpus.ipynb
Here is how we parse a serialized example in this flower classification dataset and obtain an image.,def decode_image(image_data): ,detailed-guide-to-custom-training-with-tpus.ipynb
"Working with tf.data.DatasetWith the above parsing methods defined, we can define how to load the dataset with more options and further apply shuffling, bacthing, etc. In particular: Use num parallel reads in tf.data.TFRecordDataset to read files in parallel. Set tf.data.Options.experimental deterministic False and use it to get a new dataset that ignores the order of elements. Use num parallel calls in tf.data.Dataset.map method to have parallel processing. Use tf.data.Dataset.prefetch to allow later batches to be prepared while the current batch is being processed. The parallel processing and prefetching are particular important when working with TPU. Since TPU can process batches very quickly, the dataset pipeline should be able to provide data for TPU efficiently, otherwise the TPU will be idle.References: 1. Guide tf.data: Build TensorFlow input pipelines 2. Guide Better performance with the tf.data API 3. Doc Dataset","def load_dataset(filenames , labeled = True , ordered = False): ",detailed-guide-to-custom-training-with-tpus.ipynb
Info about a train dataset,"ds = get_training_dataset(batch_size=3, shuffle_buffer_size=1)
ds",detailed-guide-to-custom-training-with-tpus.ipynb
A train batch,"batch = next(iter(ds))
print('The batch is a {} with {} components.'.format(type(batch).__name__, len(batch)))
print('\nThe 1st compoent is a {} with shape {}'.format(type(batch[0]).__name__, batch[0].shape))
print('The 2nd compoent is a {} with shape {}\n'.format(type(batch[1]).__name__, batch[1].shape))

print('The 2nd compoent looks like')
batch[1]",detailed-guide-to-custom-training-with-tpus.ipynb
Info about a test dataset,"ds = get_test_dataset(batch_size=3)
ds",detailed-guide-to-custom-training-with-tpus.ipynb
A test batch,"batch = next(iter(ds))
print('The batch is a {} with {} components.'.format(type(batch).__name__, len(batch)))
print('\nThe 1st compoent is a {} with shape {}'.format(type(batch[0]).__name__, batch[0].shape))
print('The 2nd compoent is a {} with shape {}'.format(type(batch[1]).__name__, batch[1].shape))

print('\nThe 2nd compoent looks like')
batch[1]",detailed-guide-to-custom-training-with-tpus.ipynb
Peek the training data,"train_dataset = get_training_dataset(batch_size = 16 , shuffle_buffer_size = 1 , ordered = True , drop_remainder = False) ",detailed-guide-to-custom-training-with-tpus.ipynb
run this cell again for next set of images,batch = next(train_iter) ,detailed-guide-to-custom-training-with-tpus.ipynb
peek the validation data,valid_dataset = get_validation_dataset(batch_size = 16) ,detailed-guide-to-custom-training-with-tpus.ipynb
run this cell again for next set of images,display_batch_of_images(next(valid_iter)) ,detailed-guide-to-custom-training-with-tpus.ipynb
peek the test data,test_dataset = get_test_dataset(batch_size = 16) ,detailed-guide-to-custom-training-with-tpus.ipynb
run this cell again for next set of images,display_batch_of_images(next(test_iter)) ,detailed-guide-to-custom-training-with-tpus.ipynb
Number of dataset examples,"print('Original Dataset:\n\ntraining images: {}\nvalidation images: {}\ntest images {}'.format(ORIGINAL_NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))",detailed-guide-to-custom-training-with-tpus.ipynb
"We use .autograph.experimental.do not convert to tell tensorflow not to convert the function get label counting, otherwise we get the following warning WARNING: AutoGraph could not transform . at 0x7f0690681830 and will run it as is. This is OK, because this function is not used in our input pipeline, so not converting it to graph won t slow down the pipeline.","@tf.autograph.experimental.do_not_convert
def get_label_counting(labeled_dataset):
 
 c = Counter()
 labels = []
 for batch in labeled_dataset.map(lambda image, label: label, num_parallel_calls=tf.data.experimental.AUTOTUNE):
 labels.append(batch)
 
 labels = tf.concat(labels, axis=0).numpy()
 c.update(labels)

 return labels, c",detailed-guide-to-custom-training-with-tpus.ipynb
Plot label distribution,"def plot_label_dist(labels , dist_1 , dist_2 , dist_label_1 , dist_label_2 , title = ''): ",detailed-guide-to-custom-training-with-tpus.ipynb
the label locations, x = np.arange(len(labels)) ,detailed-guide-to-custom-training-with-tpus.ipynb
the width of the bars, width = 0.4 ,detailed-guide-to-custom-training-with-tpus.ipynb
"Add some text for labels, title and custom x axis tick labels, etc.", ax.set_ylabel('portion in dataset') ,detailed-guide-to-custom-training-with-tpus.ipynb
Step 1: Review the number of occurrence of each classWe use the counter train counter computed in the Label distribution subsection.,"print(""labels in the original training dataset, sorted by occurrence\n"")
print(""pairs of (class id, counting)\n"")
print(train_counter.most_common())",detailed-guide-to-custom-training-with-tpus.ipynb
"Step 2: Determine how many times an example in a class should repeatThe objective of our approach is to have a more balanced dataset, where the number of occurrence of each class is much closer to those of other classes than it is in the original training dataset.From the above counting, we see that the majority class is class 67 which occurs 782 times, while the minority classes are class 44, 34 and 6, which occur 18 times each. For a given number , we will construct a new dataset where each class occurs approximately times. For example, if we specify , the new dataset will have about examples for each class, which is clearly balanced.For the flexbility of our experiments, we allow to be lower, say, 100 or 300. These still give imbalanced dataset, but less imbalanced than the original dataset.Given a such number and a class , for any training example in the original dataset with label , we now determine the number of times the example should repeat in order to obtain a dataset having the property discussed in the prevous paragraphs. At a first attempt, this will be a float number, which is the ideal value. Based on it, the actual number of times the example will repeat is determined in a randomized way to make the number of occurrences of each class roughly . For example, if examples in class should repeat times, then they will repeat times with a probability and times with a probability .","def get_num_to_repeat_for_class(class_id , target_counting): ",detailed-guide-to-custom-training-with-tpus.ipynb
"Let s check the number of times an example should repeatHere, we take .","_, d = get_nums_to_repeat(782)
d = sorted(d.items(), key=lambda x: x[1], reverse=True)

print('pair of (class id, num to repeat)\n')
for x in d:
 print(x)",detailed-guide-to-custom-training-with-tpus.ipynb
some statistics for ,"oversampled_train_dataset, _ = get_oversampled_training_dataset(target_counting=782, batch_size=16, shuffle_buffer_size=1, repeat_dataset=False, ordered=True, oversample=True, augmentation_fn=None)

_, oversampled_train_counter = get_label_counting(oversampled_train_dataset)

print('Oversampled training dataset:\ntraining images: {}\n'.format(sum(oversampled_train_counter.values())))

print(""labels in the oversampled training dataset, sorted by occurrence: pairs of (label_id, label_counting)\n"")
print(oversampled_train_counter.most_common())

print('\n' + 'averaged number of occurrences: ', np.array(list(oversampled_train_counter.values())).mean())",detailed-guide-to-custom-training-with-tpus.ipynb
compare label distributions,"dist_train_oversampled = np.array([oversampled_train_counter[x] for x in labels]) / sum(oversampled_train_counter.values())

half = len(labels) // 2
plot_label_dist(
 labels[:half],
 dist_train[:half],
 dist_train_oversampled[:half],
 'original',
 'oversampled',
 title='Label distribution in train datasets with/without oversampling: Labels 0-{}'.format(half - 1)
)

plot_label_dist(
 labels[half:],
 dist_train[half:],
 dist_train_oversampled[half:],
 'original',
 'oversampled', 
 title='Label distribution in train datasets with/without oversampling: Labels {}-{}'.format(half, len(labels) - 1)
)",detailed-guide-to-custom-training-with-tpus.ipynb
Peek the oversampled training data,train_iter = iter(train_dataset) ,detailed-guide-to-custom-training-with-tpus.ipynb
Create a trainer to make the experminents easierThe next cell has nothing related to tensorflow TPU. It just contains some stuffs that make our experminents and presentation easier. The actual model definition and training process are defined in the subsequent cells.You can check it if you want to know how our training works in a high level.,"class Flower_Trainer:
 
 def __init__(self, batch_size_per_replica=16, prediction_batch_size_per_replica=64, shuffle_buffer_size=1, oversample=False, target_counting=1, grad_acc_steps=1, augmentation_fn=None, probability=1.0, log_interval=1):
 
 self.batch_size_per_replica = batch_size_per_replica
 self.prediction_batch_size_per_replica = prediction_batch_size_per_replica
 
 self.batch_size = batch_size_per_replica * strategy.num_replicas_in_sync
 self.prediction_batch_size = prediction_batch_size_per_replica * strategy.num_replicas_in_sync

 self.grad_acc_steps = grad_acc_steps
 self.update_size = self.batch_size * self.grad_acc_steps
 
 self.shuffle_buffer_size = shuffle_buffer_size
 self.oversample = oversample
 self.target_counting = target_counting
 
 self.augmentation_fn = augmentation_fn
 
 self.train_ds, self.nb_examples_approx = get_oversampled_training_dataset(
 self.target_counting, self.update_size, self.shuffle_buffer_size,
 repeat_dataset=True, ordered=False,
 oversample=self.oversample, augmentation_fn=self.augmentation_fn,
 probability=probability
 )

 self.updates_per_epoch = self.nb_examples_approx // self.update_size 
 
 self.valid_ds = get_validation_dataset(self.prediction_batch_size)
 self.test_ds = get_test_dataset(self.prediction_batch_size)
 
 self.log_interval = log_interval
 
 def train(self, train_name, model_name, epochs, start_lr, max_lr, end_lr, warmup, lr_scaling, optimized_loop=False, verbose=False):
 
 update_steps = epochs * self.updates_per_epoch
 warmup_steps = int(update_steps * warmup)
 
 model, loss_fn, optimizer, gradient_accumulator, metrics = get_model(model_name, update_steps, warmup_steps, start_lr, max_lr, end_lr, lr_scaling, verbose=verbose)
 
 dist_train_1_epoch_optimized, dist_train_1_epoch_normal, dist_predict_step = get_routines(
 model, loss_fn, optimizer, gradient_accumulator, metrics, self.batch_size_per_replica, self.update_size, self.grad_acc_steps, self.updates_per_epoch
 )

 dist_train_1_epoch = dist_train_1_epoch_normal
 if optimized_loop:
 dist_train_1_epoch = dist_train_1_epoch_optimized
 
 train_fn = get_train_fn(dist_train_1_epoch, dist_predict_step, loss_fn, metrics, log_interval=self.log_interval)
 history, valid_labels, valid_preds = train_fn(train_name, epochs, self.train_ds, self.valid_ds, self.test_ds, self.updates_per_epoch)
 
 return history, valid_labels, valid_preds",detailed-guide-to-custom-training-with-tpus.ipynb
"Linear learning rate with warmupWarmup is commonly used in learning rate schedule where we start training a model with a much smaller learning rate and increase it during the first few epochs steps until the initial learning rate is used.Intuitively, it allows a model to adjust itself less before it becomes more familiar with the dataset. For adaptive optimisers like Adam, warmup also allows the optimizers to compute bettere statistics of the gradients.Here we present a very simple way that turns any learnning rate schedule without warmup into a version that uses warmup. This is only for educational purpose, and we will use a constant learning rate later in this notebook.","class WarmupLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):

 def __init__(self, backend_schedule, start_lr, max_lr, end_lr, opt_steps, warmup_steps, lr_scaling):

 self.start_lr = start_lr
 self.max_lr = max_lr
 self.end_lr = end_lr
 self.opt_steps = tf.cast(opt_steps, tf.float32)
 self.warmup_steps = tf.cast(warmup_steps, tf.float32)
 self.lr_scaling = tf.cast(lr_scaling, tf.float32)
 self.backend_lr = backend_schedule

 self.warmup_incremental = (self.max_lr - self.start_lr) / tf.math.reduce_max([self.warmup_steps, 1.0]) * tf.cast(self.warmup_steps > 0.0, tf.float32)

 def __call__(self, step):

 is_warmup = tf.cast(step < self.warmup_steps, tf.float32)
 warmup_lr = self.warmup_incremental * step + self.start_lr
 decay_lr = self.backend_lr(step - self.warmup_steps)
 lr = (1.0 - is_warmup) * decay_lr + is_warmup * warmup_lr

 return lr * self.lr_scaling",detailed-guide-to-custom-training-with-tpus.ipynb
visualize the learning rate scheduleLet s use the above code to turn learning rate schedules to use warmup and visualize them.,"def plot_lr_schedule(lr_schedule, n_steps):

 steps = [i for i in range(n_steps)]
 lrs = [lr_schedule(x) for x in steps]
 plt.plot(steps, lrs)
 print(""Learning rate schedule: {:.3g} to {:.3g} to {:.3g}"".format(lrs[0], max(lrs), lrs[-1])) 
 plt.show()
 
def plot_lr_schedule_pair(lr_schedule_1, lr_schedule_2, n_steps):
 
 steps = [i for i in range(n_steps)]
 
 plt.figure(figsize=(11.0, 6.0 / 2))
 
 lrs = [lr_schedule_1(x) for x in steps]
 plt.subplot(1, 2, 1)
 plt.plot(steps, lrs)
 plt.title('original lr', fontsize=14, color='black', fontdict={'verticalalignment':'center'}, pad=12.0)
 
 lrs = [lr_schedule_2(x) for x in steps]
 plt.subplot(1, 2, 2)
 plt.plot(steps, lrs)
 plt.title('warmup lr', fontsize=14, color='black', fontdict={'verticalalignment':'center'}, pad=12.0)
 
 plt.show()
 
 
opt_steps, start_lr, max_lr, end_lr, lr_scaling = 1000, 1e-7, 1e-5, 1e-6, 1

warmup_steps = 0
backend_lr = tf.keras.optimizers.schedules.ExponentialDecay(
 max_lr, opt_steps - warmup_steps, decay_rate=(end_lr / max_lr),
)
lr_rate1 = WarmupLearningRateSchedule(backend_lr, start_lr, max_lr, end_lr, opt_steps, warmup_steps, lr_scaling)

warmup_steps = 0
backend_lr = tf.keras.optimizers.schedules.PolynomialDecay(
 initial_learning_rate=max_lr, decay_steps=(opt_steps - warmup_steps), end_learning_rate=end_lr, power=1.0
)
lr_rate3 = WarmupLearningRateSchedule(backend_lr, start_lr, max_lr, end_lr, opt_steps, warmup_steps, lr_scaling)",detailed-guide-to-custom-training-with-tpus.ipynb
Exponential Decay,"warmup_steps = 200
backend_lr = tf.keras.optimizers.schedules.ExponentialDecay(
 max_lr, opt_steps - warmup_steps, decay_rate=(end_lr / max_lr),
)
lr_rate2 = WarmupLearningRateSchedule(backend_lr, start_lr, max_lr, end_lr, opt_steps, warmup_steps, lr_scaling)",detailed-guide-to-custom-training-with-tpus.ipynb
Linear Decay,"warmup_steps = 200
backend_lr = tf.keras.optimizers.schedules.PolynomialDecay(
 initial_learning_rate=max_lr, decay_steps=(opt_steps - warmup_steps), end_learning_rate=end_lr, power=1.0
)
lr_rate4 = WarmupLearningRateSchedule(backend_lr, start_lr, max_lr, end_lr, opt_steps, warmup_steps, lr_scaling)",detailed-guide-to-custom-training-with-tpus.ipynb
"Model, metric and optimizerIn order to use TPU, or tensorflow distribute strategy in general, some objects have to be created inside the strategy s scope. Here is the rule of thumb: Anything that creates variables that will be used in a distributed way must be created inside strategy.scope . This includes: model creation optimizer metrics sometimes, checkpoint restore any custom code that creates distributed variables Once a variable is created inside a strategy s scope, it captures the strategy s information, and you can use it outside the strategy s scope. Unless using a high level API like model.fit , define things inside the strategy s scope won t automatically distribute the computation. This will be discussed in Distributed computation. Inside the scope, everything is defined in a way just like without using distribute strategy. There is however a particularity about the loss function, see Loss classes and reduction.In the next cell, we define the learning rate and the loss object inside the scope, but it s not mandatory.References: 1. Doc TPUStrategy scope 2. Tutorial Custom training with TPUs","def get_model(model_name , update_steps , warmup_steps , start_lr , max_lr , end_lr , lr_scaling , verbose = False): ",detailed-guide-to-custom-training-with-tpus.ipynb
"False transfer learning, True fine tuning", pretrained_model.trainable = True ,detailed-guide-to-custom-training-with-tpus.ipynb
"Check a batch in the distributed dataset Previously, we checked a batch in the training dataset, which is a tuple containing 2 tensors. One is a batch of images, and the other one is a batch of labels. Let s check what we get when we distribute our flower training dataset. Let s create a dataset of batch size .","ds = get_training_dataset(batch_size=9, shuffle_buffer_size=1)
dist_ds = strategy.experimental_distribute_dataset(ds)",detailed-guide-to-custom-training-with-tpus.ipynb
"Look a PerReplica object Here is the second component dist batch 1 . It contains tensors, each of them is a batch of labels that will be processed on a different replica. They have different batch dimensions: , and , but their sum is which is the batch size we used to create the dataset.Similarly, dist batch 0 contains tensors which are batch of images.",dist_batch[1],detailed-guide-to-custom-training-with-tpus.ipynb
"Access PerReplica s contentFor a PerReplica object, you can use the property values to access its content. It turns out to be a tuple. The number of its components is the number of replicas strategy.num replicas in sync.","print('The values contained inside dist_batch[0] (which is a `{}` object) are packed in a {} with {} components.\n'.format(type(dist_batch[0]).__name__, type(dist_batch[0].values).__name__, len(dist_batch[0].values)))
print('The 1st component in `dist_batch[0].values` is a {} with shape {}'.format(type(dist_batch[0].values[0]).__name__, dist_batch[0].values[0].shape))
print('The 4th component in `dist_batch[0].values` is a {} with shape {}'.format(type(dist_batch[0].values[4]).__name__, dist_batch[0].values[4].shape))
print('The last component in `dist_batch[0].values` is a {} with shape {}'.format(type(dist_batch[0].values[-1]).__name__, dist_batch[0].values[-1].shape))",detailed-guide-to-custom-training-with-tpus.ipynb
Check strategy.experimental local results,"@tf.function
def dummy_run(images, labels):
 
 images = images + 1
 labels = labels * 0
 
 return images, labels
 
dummy_images, dummy_labels = strategy.run(dummy_run, args=dist_batch)
dummy_labels",detailed-guide-to-custom-training-with-tpus.ipynb
The returned labels is a PerReplica object.,strategy.experimental_local_results(dummy_labels),detailed-guide-to-custom-training-with-tpus.ipynb
We get a tuple of tensors after calling strategy.experimental local results.,"tf.concat(strategy.experimental_local_results(dummy_labels), axis=0)",detailed-guide-to-custom-training-with-tpus.ipynb
"Finally, the implementation of our routines","def get_routines(model , loss_fn , optimizer , gradient_accumulator , metrics , batch_size_per_replica , update_size , grad_acc_steps , updates_per_epoch): ",detailed-guide-to-custom-training-with-tpus.ipynb
Remember that we use the SUM reduction when we define the loss object.," loss = loss_fn(labels , logits)/ update_size ",detailed-guide-to-custom-training-with-tpus.ipynb
shape batch size per replica ," preds = tf.cast(tf.math.argmax(logits , axis = - 1), dtype = tf.int32) ",detailed-guide-to-custom-training-with-tpus.ipynb
update metrics, metrics['train loss']. update_state(loss) ,detailed-guide-to-custom-training-with-tpus.ipynb
"Put the routines togetherWe are at the final step before the real traning! Here we use the above routines to define the highest level of the training, validation and testing processes, including printing some information and saving the results.","def get_train_fn(dist_train_1_epoch , dist_predict_step , loss_fn , metrics , log_interval = 1): ",detailed-guide-to-custom-training-with-tpus.ipynb
PerReplica object, logits = dist_predict_step(images) ,detailed-guide-to-custom-training-with-tpus.ipynb
Tuple of tensors, logits = strategy.experimental_local_results(logits) ,detailed-guide-to-custom-training-with-tpus.ipynb
tf.Tensor," logits = tf.concat(logits , axis = 0) ",detailed-guide-to-custom-training-with-tpus.ipynb
tf.Tensor," logits = tf.concat(all_logits , axis = 0) ",detailed-guide-to-custom-training-with-tpus.ipynb
update metrics," metrics['valid acc']. update_state(labels , logits) ",detailed-guide-to-custom-training-with-tpus.ipynb
get metrics, acc = metrics['valid acc']. result () ,detailed-guide-to-custom-training-with-tpus.ipynb
reset metrics, metrics['valid acc']. reset_states () ,detailed-guide-to-custom-training-with-tpus.ipynb
get metrics, train_loss = metrics['train loss']. result ()/ updates_per_epoch ,detailed-guide-to-custom-training-with-tpus.ipynb
reset metrics, metrics['train loss']. reset_states () ,detailed-guide-to-custom-training-with-tpus.ipynb
"In this notebook, we only run each configuration once. Ideally, each configuration should be run multiple times and the averaged results are used for comparison. Due to the 3 hours TPU time limit on Kaggle, this is not feasible for the model we use in this notebook.However, most of the conclusions in this notebook are stable and won t be different in another run.",model_name = 'EfficientNetB7' ,detailed-guide-to-custom-training-with-tpus.ipynb
model name Xception ,epochs = 30 ,detailed-guide-to-custom-training-with-tpus.ipynb
Train without optimized loop,"trainer = Flower_Trainer(
 batch_size_per_replica=16, prediction_batch_size_per_replica=64, shuffle_buffer_size=None,
 oversample=False, target_counting=1, grad_acc_steps=1, augmentation_fn=None, log_interval=log_interval
)",detailed-guide-to-custom-training-with-tpus.ipynb
print configuration,"def print_config(trainer):

 print('use oversampling: {}'.format(trainer.oversample))
 
 if trainer.oversample:
 print('target counting of each class for oversampling {}: '.format(trainer.target_counting)) 
 
 print('(approximated) nb. of training examples used: {}'.format(trainer.nb_examples_approx))
 
 print('per replica batch size for training: {}'.format(trainer.batch_size_per_replica))
 print('batch size for training: {}'.format(trainer.batch_size)) 
 print('gradient accumulation steps: {}'.format(trainer.grad_acc_steps))
 print('update size: {}'.format(trainer.update_size))
 print('updates per epoch: {}'.format(trainer.updates_per_epoch))
 
 print('per replica batch size for prediction: {}'.format(trainer.prediction_batch_size_per_replica))
 print('batch size for prediction: {}'.format(trainer.prediction_batch_size))

 print('use data augmentation: {}'.format(trainer.augmentation_fn is not None))",detailed-guide-to-custom-training-with-tpus.ipynb
Plot history,"def plot_history(history , desc = ''): ",detailed-guide-to-custom-training-with-tpus.ipynb
plt.tight layout , if desc : ,detailed-guide-to-custom-training-with-tpus.ipynb
extend by the last epoch, if nb_epochs_1 < nb_epochs_2 : ,detailed-guide-to-custom-training-with-tpus.ipynb
plt.tight layout , if desc_1 and desc_2 : ,detailed-guide-to-custom-training-with-tpus.ipynb
"Train with optimized loopNow let s train with optimized loop. The model performance will be the same, but the training time will be reduced.","history_2, valid_labels, valid_preds = trainer.train(train_name='optimized loop', model_name=model_name, epochs=epochs, start_lr=1e-5, max_lr=1e-5, end_lr=1e-5, warmup=0.2, lr_scaling=1, optimized_loop=True, verbose=False)",detailed-guide-to-custom-training-with-tpus.ipynb
Compare training with without optimized loop,"def compare_training_time(history1, history2, title1, title2):

 avg1 = sum([history1[k]['train timing'] for k in history1 if k != 0]) / (len(history1) - 1)
 avg2 = sum([history2[k]['train timing'] for k in history2 if k != 0]) / (len(history2) - 1)

 print('Training time per epoch\n')
 print(' for the 1st epoch')
 print(f' {title1}: {history1[0][""train timing""]}')
 print(f' {title2}: {history2[0][""train timing""]}\n')
 print(' for the remaining epoch')
 print(f' {title1}: {avg1}')
 print(f' {title2}: {avg2}')",detailed-guide-to-custom-training-with-tpus.ipynb
"The above results confirm that the training time is reduced when the optimized loop is used. Except for the first epoch, the trining speed is about 2x faster! For the 1st epoch, since a computation graph is compilled, it always takes more time to finish.From now on, we will perform training only with the optimized loop.","plot_history_pair(history_1, history_2, desc_1='[usual training loop]', desc_2='[optimized training loop]', short_desc_1='usual loop', short_desc_2='optimized loop')",detailed-guide-to-custom-training-with-tpus.ipynb
"Train with gradient accumulation steps 8Now, let s accumulate the gradients 8 times before updating the model parameters.","trainer = Flower_Trainer(
 batch_size_per_replica=16, prediction_batch_size_per_replica=64, shuffle_buffer_size=None,
 oversample=False, target_counting=1, grad_acc_steps=8, augmentation_fn=None, log_interval=log_interval
)",detailed-guide-to-custom-training-with-tpus.ipynb
Compare training with without gradient accumulation,"compare_training_time(history_2, history_3, 'no gradient accumulation', 'gradient accumulation steps 8')",detailed-guide-to-custom-training-with-tpus.ipynb
"The training time is further but slightly reduced. However, depending on the model size and the number of training epochs, it could be a significant amount.","plot_history_pair(history_2, history_3, desc_1='[no gradient accumulation]', desc_2='[gradient accumulation steps 8]', short_desc_1='no grad. accumulation', short_desc_2='grad. accumulation 8')",detailed-guide-to-custom-training-with-tpus.ipynb
Train with gradient accumulation steps 8 learning rate scaling 8,"history_4, valid_labels, valid_preds = trainer.train(train_name='grad. accumulation + lr. x8', model_name=model_name, epochs=epochs, start_lr=1e-5, max_lr=1e-5, end_lr=1e-5, warmup=0.2, lr_scaling=lr_scaling, optimized_loop=True, verbose=False)",detailed-guide-to-custom-training-with-tpus.ipynb
"learning rate scaling: x1 vs. x8 For gradient accumulation rate , let s see the impact of scaling learning rate by .","plot_history_pair(history_3, history_4, desc_1='lr scaling 1', desc_2='lr scaling 8', short_desc_1='grad. accumulation lr. x1', short_desc_2='grad. accumulation lr. x8')",detailed-guide-to-custom-training-with-tpus.ipynb
compare to training without gradient accumulationo again,"plot_history_pair(history_2, history_4, desc_1='[no gradient accumulation]', desc_2='[gradient accumulation steps 8 + lr. x8]', short_desc_1='no. grad. accumulation', short_desc_2='grad. accumulation lr. x8')",detailed-guide-to-custom-training-with-tpus.ipynb
oversampled dataset with .,"trainer = Flower_Trainer(
 batch_size_per_replica=16, prediction_batch_size_per_replica=64, shuffle_buffer_size=None,
 oversample=True, target_counting=100, grad_acc_steps=8, augmentation_fn=None, log_interval=log_interval
)",detailed-guide-to-custom-training-with-tpus.ipynb
no oversampling vs. oversampling,"plot_history_pair(history_4, history_5, desc_1='[no oversampling]', desc_2='[oversampling N=100]', short_desc_1='no oversampling', short_desc_2='oversamp. N=100')",detailed-guide-to-custom-training-with-tpus.ipynb
oversampled dataset with .Let s increase the number of occurrences of each class.,"trainer = Flower_Trainer(
 batch_size_per_replica=16, prediction_batch_size_per_replica=64, shuffle_buffer_size=None,
 oversample=True, target_counting=300, grad_acc_steps=8, augmentation_fn=None, log_interval=log_interval
)",detailed-guide-to-custom-training-with-tpus.ipynb
oversampling vs. oversampling,"plot_history_pair(history_5, history_6, desc_1='[oversampling N=100]', desc_2='[oversampling N=300]', short_desc_1='oversamp. N=100', short_desc_2='oversamp. N=300')",detailed-guide-to-custom-training-with-tpus.ipynb
no oversampling vs. oversampling,"plot_history_pair(history_4, history_6, desc_1='[no oversampling]', desc_2='[oversampling N=300]', short_desc_1='no oversampling', short_desc_2='oversamp. N=300')",detailed-guide-to-custom-training-with-tpus.ipynb
"oversampled dataset with , but with fewer epochsWhen we train a model with oversampling, it is unfair to compare to the training without oversampling by looking at the same epochs, because oversampling has more training examples in each epoch.For oversampling with , we have about examples in one epoch, which is about times the number of original training example which is . Let s reduce the number of epochs by a similar factor when training with oversampling.","epochs_reduced = int(round(epochs / (trainer.nb_examples_approx / ORIGINAL_NUM_TRAINING_IMAGES)))
history_7, valid_labels, valid_preds = trainer.train(train_name='oversampling 300 + epochs {}'.format(epochs_reduced), model_name=model_name, epochs=epochs_reduced, start_lr=1e-5, max_lr=1e-5, end_lr=1e-5, warmup=0.2, lr_scaling=lr_scaling, optimized_loop=True, verbose=False)",detailed-guide-to-custom-training-with-tpus.ipynb
no oversampling vs. oversampling with fewer epochs,"plot_history_pair(history_4, history_7, desc_1=f'[no oversampling + {epochs} epochs]', desc_2=f'[oversampling 300 + {epochs_reduced} epochs]', short_desc_1='no oversampling', short_desc_2=f'oversamp. epoch {epochs_reduced}')",detailed-guide-to-custom-training-with-tpus.ipynb
Play with perspective transformation in JavaScriptThe code here is from the javascript implementation which I found through the discussion in Computing a projective transformation.You can move the 4 corners to play with perspective transformation!,"import IPython
IPython.display.IFrame(""//jsfiddle.net/dFrHS/1/embedded/result,js,html,css"", width=700, height=500)",detailed-guide-to-custom-training-with-tpus.ipynb
Implement perspective transformation in TensorFlow batch ,"def random_4_points_2D_batch(height , width , batch_size , probability = 1.0): ",detailed-guide-to-custom-training-with-tpus.ipynb
Visualize perspective transformation,"train_dataset = get_training_dataset(batch_size=16, shuffle_buffer_size=1, ordered=True)
train_iter = iter(train_dataset)

transformed_train_dataset = train_dataset.map(lambda images, labels: perspective_transform(images, labels, probability=0.8))
transformed_train_iter = iter(transformed_train_dataset)",detailed-guide-to-custom-training-with-tpus.ipynb
run this cell again for next set of images,batch = next(train_iter) ,detailed-guide-to-custom-training-with-tpus.ipynb
Train with data augmentation,"trainer = Flower_Trainer(
 batch_size_per_replica=16, prediction_batch_size_per_replica=64, shuffle_buffer_size=None,
 oversample=True, target_counting=300, grad_acc_steps=8, augmentation_fn=perspective_transform,
 probability=0.35, log_interval=log_interval
)",detailed-guide-to-custom-training-with-tpus.ipynb
without data augmentation vs. with data augmentation,"plot_history_pair(history_6, history_8, desc_1=f'[without data augmentation]', desc_2=f'[with data augmentation]', short_desc_1='no data aug.', short_desc_2=f'data aug.')",detailed-guide-to-custom-training-with-tpus.ipynb
Table of contents What is Differential Evolution? How does it work? Code to built the model Some simple examples Prepare for the Titanic Build the NNs for Titanic Build an individual for DE Differential Evolution for Titanic Understanding the evolution of hyperparameters Hyperparameter evolution for digits and flowers dataset Conclusion ,"import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
%matplotlib inline

import seaborn as sns
sns.set()

from matplotlib.colors import BoundaryNorm
from matplotlib.ticker import FixedLocator, MaxNLocator
import matplotlib.animation as animation
from matplotlib import rc
rc('animation', html='html5')

from tensorflow import keras

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split",differential-evolution.ipynb
Code to built the model ,"class DifferentialEvolution:

 def __init__(self, scaling_rate, crossover_rate, population_size):
 self.scaling_rate = scaling_rate
 self.crossover_rate = crossover_rate
 self.population_size = population_size
 self.ranges = []
 
 self.generation_counter = 0
 
 self.all_generations = []
 self.min_std_criterion = 0.001
 
 def set_range(self, paramkey, range):
 self.ranges.append([paramkey, range])
 
 def set_objective(self, objective):
 self.objective = objective
 
 def init_generation(self):
 self.n_params = len(self.ranges)
 self.generation = np.zeros(shape=(self.population_size, self.n_params))
 for p in range(self.n_params):
 low = self.ranges[p][1][0]
 high = self.ranges[p][1][1]
 self.generation[:, p] = np.random.uniform(low, high, size=self.population_size)
 
 def get_trials(self):
 self.trials = np.zeros(shape=(self.population_size, self.n_params))
 for i in range(self.population_size):
 target = self.generation[i, :]
 mutant = self.mutate(i)
 self.trials[i, :] = self.crossover(mutant, target)
 
 def get_candidates(self, i):
 to_select = list(np.arange(self.population_size))
 to_select.remove(i)
 candidates = np.random.choice(to_select, 3, replace=False)
 return candidates
 
 def mutate(self, i):
 candidates = self.get_candidates(i)
 difference_vector = self.generation[candidates[1]] - self.generation[candidates[2]]
 mutant = self.generation[candidates[0]] + self.scaling_rate * difference_vector
 return mutant
 
 def crossover(self, mutant, target):
 crossover_units = np.random.uniform(0, 1, self.n_params)
 trial = np.copy(target)
 random_parameter = np.random.choice(self.n_params)
 for param in range(self.n_params):
 if crossover_units[param] <= self.crossover_rate or param == random_parameter:
 trial[param] = mutant[param]
 return trial
 
 def select(self, generation_costs, trials_costs):
 idx = np.where(trials_costs < generation_costs)[0]
 for i in idx:
 self.generation[i, :] = self.trials[i, :]

 def compute_cost(self):
 generation_costs = self.objective(self.generation)
 trials_cost = self.objective(self.trials)
 return generation_costs, trials_cost
 
 def evolve(self):
 self.init_generation()
 self.all_generations.append(self.generation)
 self.best_solutions = []
 gen_costs = self.objective(self.generation)
 while ((np.std(gen_costs) > self.min_std_criterion) and (self.generation_counter < 200)):
 self.get_trials()
 gen_costs, trials_costs = self.compute_cost()
 self.select(gen_costs, trials_costs)
 self.all_generations.append(np.copy(self.generation))
 self.generation_counter += 1
 self.best_solutions.append(np.min(gen_costs))
 print(""Stopped at generation {}"".format(self.generation_counter))",differential-evolution.ipynb
Some simple examples Let s get a first feeling about how this algorithm works and abouts its performance by searching for optima in the parameter space of: the rosenbrock function coming soon the griewank function the himmelblau function Just use an example of your choice and watch Differential Evolution during optimisation.,"example = ""griewank""",differential-evolution.ipynb
"Prepare the Titanic Ok, let s start simple by building a feedforward neural network to classify the survival of the titanic. As I don t want to win this competition but rather to show what you can do with Differential Evolution I like to reduce the feature space by only using: Pclass Sex Age Embarked Fare Family size It should be sufficient. ","titanic = pd.read_csv(""../input/titanic/train.csv"")
titanic.head()",differential-evolution.ipynb
We have to be careful with the fare as it is a price per group family and not per person. You can see that every family member given by the lastname and the ticket has the same fare std of 0 :,"titanic.loc[:, ""lastname""] = titanic.Name.str.split("","").apply(lambda l: l[0])
titanic.groupby([""lastname"",""Ticket""]).Fare.std().fillna(0).max()",differential-evolution.ipynb
Now we can compute the price per person which should work better together with features like Pclass. Otherwise we would conclude for big families that a high price corresponds to a higher Pclass.,"price_map = titanic.groupby([""lastname"", ""Ticket""]).Fare.min() / titanic.groupby([""lastname"", ""Ticket""]).size()
family_size_map = titanic.groupby([""lastname"", ""Ticket""]).size()
titanic = titanic.set_index([""lastname"", ""Ticket""], drop=True)
titanic.loc[:, ""price""] = titanic.index.map(price_map)
titanic.loc[:, ""family_size""] = titanic.index.map(family_size_map)
titanic = titanic.reset_index()",differential-evolution.ipynb
"Ok, let s get one hot encoded values for Pclass, Embarked and Sex:","titanic = pd.get_dummies(titanic, columns=[""Pclass"", ""Embarked"", ""Sex""])",differential-evolution.ipynb
And fill missing ages with median:,titanic.Age = titanic.Age.fillna(titanic.Age.median()),differential-evolution.ipynb
Furthermore we need to transform Age and price such that they look more normally distributed. The gradients of neural networks are direclty influenced by the features themselves and can be misled when outliers are present.,"fig, ax = plt.subplots(1,4,figsize=(20,5))
sns.distplot(titanic.Age, ax=ax[0], color=""Orange"")
sns.distplot(titanic.Age.apply(lambda l: np.log(l+20)), ax=ax[1], color=""Orange"")
sns.distplot(titanic.price, ax=ax[2], color=""Purple"")
sns.distplot(titanic.price.apply(lambda l: np.log(l+0.5)), ax=ax[3], color=""Purple"");",differential-evolution.ipynb
"Ok, that s it! : Let s now build a simple neural network and define the hyperparameters we like to tune using Differential Evolution.","num_titanic_features=titanic.drop(""Survived"", axis=1).shape[1]
num_titanic_features",differential-evolution.ipynb
"Ok, we have optimised some test optimisation functions to see how differential evolution solves the task. Now I like to explore how it will solve hyperparameter tuning for a simple feedforward neural network. I m especially interested in: The number of neurons compared to regularisation given by dropout rates or weight regularisation. The number of epochs compared to the learning rate. Let s go! : ","def build_titanic_nn(num_neurons_1, dropout_1,
 num_neurons_2, dropout_2,
 input_shape=(num_titanic_features,)):
 
 model = keras.Sequential()
 model.add(keras.Input(shape=input_shape))
 model.add(keras.layers.Dense(num_neurons_1, activation=""relu""))
 model.add(keras.layers.Dropout(dropout_1))
 model.add(keras.layers.Dense(num_neurons_2, activation=""relu""))
 model.add(keras.layers.Dropout(dropout_2))
 model.add(keras.layers.Dense(1, activation=""sigmoid""))
 return model",differential-evolution.ipynb
"Before we can use Differential Evolution to search for the best neural network hyperparameters, we need a class for individual neural networks that captures the logic for training and evaluating each net and for storing its parameters:","from collections import OrderedDict
import copy

class Individual_Titanic:
 
 def __init__(self, params):
 self.parameters = copy.deepcopy(params)
 self.history = None
 
 def init_current(self, r_state):
 next_state = 0
 for par in self.parameters.keys():
 state = r_state + next_state
 if self.parameters[par][""val_range""].dtype == np.int:
 self.parameters[par][""current""] = np.random.RandomState(state).randint(
 low=self.parameters[par][""val_range""][0],
 high=self.parameters[par][""val_range""][1]) 
 else:
 self.parameters[par][""current""] = np.random.RandomState(state).uniform(
 low=self.parameters[par][""val_range""][0],
 high=self.parameters[par][""val_range""][1])
 next_state += 1
 
 
 def build_model(self):
 model = build_titanic_nn(num_neurons_1=self.parameters[""num_neurons_1""][""current""],
 dropout_1=self.parameters[""dropout_1""][""current""],
 num_neurons_2=self.parameters[""num_neurons_2""][""current""],
 dropout_2=self.parameters[""dropout_2""][""current""])
 return model
 
 
 def train(self, x_train, y_train, x_dev, y_dev, verbose=0):
 model = self.build_model()
 model.compile(loss=keras.losses.BinaryCrossentropy(),
 optimizer=keras.optimizers.Adam(learning_rate=self.parameters[""lr""][""current""]),
 metrics=[""accuracy""])
 self.history = model.fit(x_train, y_train,
 batch_size=32,
 epochs=self.parameters[""epochs""][""current""],
 verbose=verbose,
 workers=4)
 dev_loss, dev_acc = model.evaluate(x_dev, y_dev)
 self.score = dev_acc
 self.loss = dev_loss
 del model
 ",differential-evolution.ipynb
Here is a small example:,"parameters = {
 ""dropout_1"": {""val_range"": np.array([0.1,0.8]).astype(np.float), ""current"": None},
 ""dropout_2"": {""val_range"": np.array([0.1,0.8]).astype(np.float), ""current"": None},
 ""num_neurons_1"": {""val_range"": np.array([10,300]).astype(np.int), ""current"": None},
 ""num_neurons_2"": {""val_range"": np.array([10,300]).astype(np.int), ""current"": None},
 ""lr"": {""val_range"": np.array([0.001,0.5]).astype(np.float), ""current"": None},
 ""epochs"": {""val_range"": np.array([1,20]).astype(np.int), ""current"": None}
}",differential-evolution.ipynb
Differential Evolution for Titanic ,class DE_NeuralNets : ,differential-evolution.ipynb
How does the solution look like?Differential evolution is able to find the global optimum for the data and model given but it is not guaranteed to converge. In our case I expect that all individuals of the updated generation improve their scores during hyperparameter search. While the worst and best accuracy score might have a big difference in the beginning I assume that they become closer during iteration:,"plt.plot(eve.best_solutions, 'o-', label=""best"")
plt.plot(eve.worst_solutions, '+-', label=""worst"")
plt.ylabel(""Best val accuracy in generation i"")
plt.xlabel(""Generation i"")
plt.title(""Evolution of best individuals"")
plt.legend()",differential-evolution.ipynb
Understanding the evolution of hyperparameters One part I find most interesting in using DE for hyperparameter search is to observe the evolution of hyperparameters and their related score or loss values:,"plt.rcParams[""animation.html""]= ""jshtml"" ",differential-evolution.ipynb
Work in progress,"def build_digits_cnn(num_neurons_1 , dropout_1 , input_shape =(28 , 28 , 1)) : ",differential-evolution.ipynb
"model.add keras.layers.Conv2D filters 128, kernel size 3, activation relu ", model.add(keras.layers.Flatten ()) ,differential-evolution.ipynb
We will use the official tokenization script created by the Google team,! wget - - quiet https : // raw.githubusercontent.com / tensorflow / models / master / official / nlp / bert / tokenization.py ,disaster-nlp-keras-bert-using-tfhub.ipynb
Helper Functions,"def bert_encode(texts, tokenizer, max_len=512):
 all_tokens = []
 all_masks = []
 all_segments = []
 
 for text in texts:
 text = tokenizer.tokenize(text)
 
 text = text[:max_len-2]
 input_sequence = [""[CLS]""] + text + [""[SEP]""]
 pad_len = max_len - len(input_sequence)
 
 tokens = tokenizer.convert_tokens_to_ids(input_sequence)
 tokens += [0] * pad_len
 pad_masks = [1] * len(input_sequence) + [0] * pad_len
 segment_ids = [0] * max_len
 
 all_tokens.append(tokens)
 all_masks.append(pad_masks)
 all_segments.append(segment_ids)
 
 return np.array(all_tokens), np.array(all_masks), np.array(all_segments)",disaster-nlp-keras-bert-using-tfhub.ipynb
"Load and Preprocess Load BERT from the Tensorflow Hub Load CSV files containing training data Load tokenizer from the bert layer Encode the text into tokens, masks, and segment flags ","%%time
module_url = ""https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1""
bert_layer = hub.KerasLayer(module_url, trainable=True)",disaster-nlp-keras-bert-using-tfhub.ipynb
"Model: Build, Train, Predict, Submit","model = build_model(bert_layer, max_len=160)
model.summary()",disaster-nlp-keras-bert-using-tfhub.ipynb
Most basic stuff for EDA.,import pandas as pd ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Core packages for text processing.,import string ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Libraries for text preprocessing.,import nltk ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Loading some sklearn packaces for modelling.,"from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Some packages for word clouds and NER.,"from wordcloud import WordCloud , STOPWORDS ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Core packages for general use throughout the notebook.,import random ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
For customizing our plots.,from matplotlib.ticker import MaxNLocator ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Loading pytorch packages.,import torch ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Setting some options for general use.,stop = set(stopwords.words('english')) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Setting seeds for consistent results.,seed_val = 42 ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Loading the train and test data for visualization exploration.,trainv = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv') ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Taking general look at the both datasets.,display(trainv.sample(5)) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Checking observation and feature numbers for train and test data.,print(trainv.shape) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
"Some basic helper functions to clean text by removing urls, emojis, html tags and punctuations.",def remove_URL(text): ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Tokenizing the tweet base texts.,trainv['tokenized']= trainv['text_clean']. apply(word_tokenize) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Applying part of speech tags.,trainv['pos_tags']= trainv['stopwords_removed']. apply(nltk.tag.pos_tag) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Converting part of speeches to wordnet format.,def get_wordnet_pos(tag): ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Applying word lemmatizer.,wnl = WordNetLemmatizer () ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Displaying target distribution.,"fig , axes = plt.subplots(ncols = 2 , nrows = 1 , figsize =(18 , 6), dpi = 100) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Creating a new feature for the visualization.,trainv['Character Count']= trainv['text_clean']. apply(lambda x : len(str(x))) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Creating a customized chart. and giving in figsize and everything.," fig = plt.figure(constrained_layout = True , figsize =(18 , 8)) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Creating a grid of 3 cols and 3 rows.," grid = gridspec.GridSpec(ncols = 3 , nrows = 3 , figure = fig) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Customizing the histogram grid.," ax1 = fig.add_subplot(grid[0 , : 2]) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Set the title., ax1.set_title('Histogram') ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
plot the histogram.,"plot_dist3(trainv[trainv['target'] == 0], 'Character Count',
 'Characters Per ""Non Disaster"" Tweet')",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
"Word CountsOk let s check number of words per tweet now, they both look somewhat normally distributed, again disaster tweets seems to have slightly more words than non disaster ones. We might dig this deeper to get some more info in next part...Back To Table of Contents","def plot_word_number_histogram(textno, textye):
 
 """"""A function for comparing word counts""""""

 fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), sharey=True)
 sns.distplot(textno.str.split().map(lambda x: len(x)), ax=axes[0], color='#e74c3c')
 sns.distplot(textye.str.split().map(lambda x: len(x)), ax=axes[1], color='#e74c3c')
 
 axes[0].set_xlabel('Word Count')
 axes[0].set_ylabel('Frequency')
 axes[0].set_title('Non Disaster Tweets')
 axes[1].set_xlabel('Word Count')
 axes[1].set_title('Disaster Tweets')
 
 fig.suptitle('Words Per Tweet', fontsize=24, va='baseline')
 
 fig.tight_layout()",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
"Word LengthsThis time we re gonna check if word complexity differs from tweet class. It looks like disaster tweets has longer words than non disaster ones in general. It s pretty visible which is good sign, yet again we can only assume at this stage...Back To Table of Contents","def plot_word_len_histogram(textno, textye):
 
 """"""A function for comparing average word length""""""
 
 fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), sharey=True)
 sns.distplot(textno.str.split().apply(lambda x: [len(i) for i in x]).map(
 lambda x: np.mean(x)),
 ax=axes[0], color='#e74c3c')
 sns.distplot(textye.str.split().apply(lambda x: [len(i) for i in x]).map(
 lambda x: np.mean(x)),
 ax=axes[1], color='#e74c3c')
 
 axes[0].set_xlabel('Word Length')
 axes[0].set_ylabel('Frequency')
 axes[0].set_title('Non Disaster Tweets')
 axes[1].set_xlabel('Word Length')
 axes[1].set_title('Disaster Tweets')
 
 fig.suptitle('Mean Word Lengths', fontsize=24, va='baseline')
 fig.tight_layout()
 ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Displaying most common words.,"fig , axes = plt.subplots(1 , 2 , figsize =(18 , 8)) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
"Most Common BigramsLet s have a look for bigrams this time, which they are sequences of adjacent two words. Again it s pretty obvious to seperate two classes if it s disaster related or not. There are some confusing bigrams in non disaster ones like body bag, emergency service etc. which needs deeper research but we ll leave it here since we got what we looking for in general.Back To Table of Contents","ngrams(2, 'Most Common Bigrams')",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
"Most Common TrigramsAlright! Things are much clearer with sequences of 3 words. The confusing body bags were cross body bags Who uses them in these days anyways! which I found it pretty funny when I found the reason of the confusion. Anyways we can see disasters are highly seperable now from non disaster ones, which is great!Back To Table of Contents","ngrams(3, 'Most Common Trigrams')",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
"Determining TopicsWe ll be using a method called Non Negative Matrix Factorization NMF to see if we can get some defined topics out of our TF IDF matrix, with this way TF IDF will decrease impact of the high frequency words, so we might get more specific topics.When we inspect our top ten topics we might need to use little imagination to help us understand them. Well actually they are pretty seperable again, I d say disaster topics are much more clearer to read, we can see the topics directly by looking at them, meanwhile non disaster ones are more personal topics...Back To Table of Contents","def display_topics(text, no_top_words, topic):
 
 """""" A function for determining the topics present in our corpus with nmf """"""
 
 no_top_words = no_top_words
 tfidf_vectorizer = TfidfVectorizer(
 max_df=0.90, min_df=25, max_features=5000, use_idf=True)
 tfidf = tfidf_vectorizer.fit_transform(text)
 tfidf_feature_names = tfidf_vectorizer.get_feature_names()
 doc_term_matrix_tfidf = pd.DataFrame(
 tfidf.toarray(), columns=list(tfidf_feature_names))
 nmf = NMF(n_components=10, random_state=0,
 alpha=.1, init='nndsvd').fit(tfidf)
 print(topic)
 for topic_idx, topic in enumerate(nmf.components_):
 print('Topic %d:' % (topic_idx+1))
 print(' '.join([tfidf_feature_names[i]
 for i in topic.argsort()[:-no_top_words - 1:-1]]))


display_topics(lis[0], 10, 'Non Disaster Topics\n')",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Setting mask for wordcloud.,mask = np.array(Image.open('/kaggle/input/twittermaskn/twittermask.png')) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Loading NER.,nlp = en_core_web_sm.load () ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
If there s a GPU available...,if torch.cuda.is_available (): ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Tell PyTorch to use the GPU., device = torch.device('cuda') ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
If not...,else : ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Loading the data for modelling.,train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv') ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
"Setting target variables, creating combined data and saving index for dividing combined data later.",labels = train['target']. values ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Tokenizing the combined text data using bert tokenizer.,"tokenizer = BertTokenizer.from_pretrained('bert-large-uncased' , do_lower_case = True) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Print the original tweet.,"print(' Original: ' , combined[0]) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Print the tweet split into tokens.,"print('Tokenized: ' , tokenizer.tokenize(combined[0])) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Print the sentence mapped to token ID s.,"print('Token IDs: ' , tokenizer.convert_tokens_to_ids(tokenizer.tokenize(combined[0]))) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
For every sentence...,for text in combined : ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Tokenize the text and add CLS and SEP tokens.," input_ids = tokenizer.encode(text , add_special_tokens = True) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Update the maximum sentence length.," max_len = max(max_len , len(input_ids)) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Making list of sentence lenghts:,token_lens = [] ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Displaying sentence length dist.,"fig , axes = plt.subplots(figsize =(14 , 6)) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Splitting the train test data after tokenizing.,train = combined[: idx] ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
For every sentence..., for text in sentence : ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Tokenizing all of the train test sentences and mapping the tokens to their word IDs.,"input_ids , attention_masks , labels = tokenize_map(train , labels) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Combine the training inputs into a TensorDataset.,"dataset = TensorDataset(input_ids , attention_masks , labels) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Calculate the number of samples to include in each set.,train_size = int(0.8 * len(dataset)) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Divide the dataset by randomly selecting samples.,"train_dataset , val_dataset = random_split(dataset ,[train_size , val_size]) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
"The DataLoader needs to know our batch size for training, so we specify it here. For fine tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.",batch_size = 32 ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
We ll take training samples in random order.,"prediction_data = TensorDataset(test_input_ids, test_attention_masks)
prediction_sampler = SequentialSampler(prediction_data)
prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Get all of the model s parameters as a list of tuples:,params = list(model.named_parameters ()) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
"We chose to run for 3, but we ll see later that this may be over fitting the training data.",epochs = 3 ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Total number of training steps is number of batches x number of epochs Note that this is not the same as the number of training samples .,total_steps = len(train_dataloader)* epochs ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
"Training and EvaluatingTime to train our model! First we set some helper functions to calculate our metrics and time spent on the process. Then it moves like this, directly from the original notebook, it s pretty good at explaining I shouldn t confuse you with my own way of telling I guess: Training: Unpack our data inputs and labels Load data onto the GPU for acceleration, Clear out the gradients calculated in the previous pass, In pytorch the gradients accumulate by default useful for things like RNNs unless you explicitly clear them out, Forward pass feed input data through the network , Backward pass backpropagation , Tell the network to update parameters with optimizer.step , Track variables for monitoring progress. Evalution: Unpack our data inputs and labels, Load data onto the GPU for acceleration, Forward pass feed input data through the network , Compute loss on our validation data and track variables for monitoring progress. Pytorch hides all of the detailed calculations from us, but we ve commented the code to point out which of the above steps are happening on each line. The code below trains according to our data and saves the learning progress on the way so we can summarize at the end and see our results. We can also turn these to dataframe and plot it to see our eavluation better. So we can decide if the model performs well and not overfitting...Back To Table of Contents","def flat_accuracy(preds, labels):
 
 """"""A function for calculating accuracy scores""""""
 
 pred_flat = np.argmax(preds, axis=1).flatten()
 labels_flat = labels.flatten()
 
 return accuracy_score(labels_flat, pred_flat)

def flat_f1(preds, labels):
 
 """"""A function for calculating f1 scores""""""
 
 pred_flat = np.argmax(preds, axis=1).flatten()
 labels_flat = labels.flatten()
 
 return f1_score(labels_flat, pred_flat)",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Round to the nearest second., elapsed_rounded = int(round(( elapsed))) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Format as hh:mm:ss, return str(datetime.timedelta(seconds = elapsed_rounded)) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
"We ll store a number of quantities such as training and validation loss, validation accuracy, f1 score and timings.",training_stats = [] ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Measure the total training time for the whole run.,total_t0 = time.time () ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
For each epoch...,"for epoch_i in range(0 , epochs): ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Perform one full pass over the training set., print('') ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Measure how long the training epoch takes:, t0 = time.time () ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Reset the total loss for this epoch., total_train_loss = 0 ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
source: , model.train () ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
For each batch of training data...," for step , batch in enumerate(train_dataloader): ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Progress update every 50 batches., if step % 50 == 0 and not step == 0 : ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Calculate elapsed time in minutes., elapsed = format_time(time.time ()- t0) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Report progress.," print(' Batch {:>5,} of {:>5,}. Elapsed: {:}.'.format(step , len(train_dataloader), elapsed)) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
 2 : labels, b_input_ids = batch[0]. to(device). to(torch.int64) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Source: , model.zero_grad () ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Display floats with two decimal places.,"pd.set_option('precision' , 2) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Create a DataFrame from our training statistics.,df_stats = pd.DataFrame(data = training_stats) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Use the epoch as the row index.,df_stats = df_stats.set_index('epoch') ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Display the table.,display(df_stats) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Increase the plot size and font size:,"fig , axes = plt.subplots(figsize =(12 , 8)) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Plot the learning curve:,"plt.plot(df_stats['Training Loss'], 'b-o' , label = 'Training') ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Label the plot:,plt.title('Training & Validation Loss') ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Prediction on test set:,"print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids))) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Put model in evaluation mode:,model.eval () ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Tracking variables :,predictions = [] ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Predict:,for batch in prediction_dataloader : ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Add batch to GPU, batch = tuple(t.to(device)for t in batch) ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Unpack the inputs from our dataloader:," b_input_ids , b_input_mask , = batch ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
"Telling the model not to compute or store gradients, saving memory and speeding up prediction:", with torch.no_grad (): ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Getting list of predictions and then choosing the target value with using argmax on probabilities.,flat_predictions =[item for sublist in predictions for item in sublist] ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Creating submission data.,submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv') ,disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Saving submission to .csv file:,"submission.to_csv('submission.csv' , index = False , header = True) ",disaster-tweets-nlp-eda-bert-with-transformers.ipynb
Setup plotting,import matplotlib.pyplot as plt ,dropout-and-batch-normalization.ipynb
Set Matplotlib defaults,"plt.rc('figure' , autolayout = True) ",dropout-and-batch-normalization.ipynb
"When adding dropout, you may need to increase the number of units in your Dense layers.","from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
 layers.Dense(1024, activation='relu', input_shape=[11]),
 layers.Dropout(0.3),
 layers.BatchNormalization(),
 layers.Dense(1024, activation='relu'),
 layers.Dropout(0.3),
 layers.BatchNormalization(),
 layers.Dense(1024, activation='relu'),
 layers.Dropout(0.3),
 layers.BatchNormalization(),
 layers.Dense(1),
])",dropout-and-batch-normalization.ipynb
"IntroAt the end of this lesson, you will understand and know how to use Stride lengths to make your model faster and reduce memory consumption Dropout to combat overfittingBoth of these techniques are especially useful in large models.Lesson","from IPython.display import YouTubeVideo
YouTubeVideo('fwNLf4t7MR8', width=800, height=450)",dropout-and-strides-for-larger-models.ipynb
Sample Code,"import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.python import keras
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout

img_rows, img_cols = 28, 28
num_classes = 10

def data_prep(raw):
 out_y = keras.utils.to_categorical(raw.label, num_classes)

 num_images = raw.shape[0]
 x_as_array = raw.values[:,1:]
 x_shaped_array = x_as_array.reshape(num_images, img_rows, img_cols, 1)
 out_x = x_shaped_array / 255
 return out_x, out_y

train_size = 30000
train_file = ""../input/digit-recognizer/train.csv""
raw_data = pd.read_csv(train_file)

x, y = data_prep(raw_data)

model = Sequential()
model.add(Conv2D(30, kernel_size=(3, 3),
 strides=2,
 activation='relu',
 input_shape=(img_rows, img_cols, 1)))
model.add(Dropout(0.5))
model.add(Conv2D(30, kernel_size=(3, 3), strides=2, activation='relu'))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss=keras.losses.categorical_crossentropy,
 optimizer='adam',
 metrics=['accuracy'])
model.fit(x, y,
 batch_size=128,
 epochs=2,
 validation_split = 0.2)",dropout-and-strides-for-larger-models.ipynb
linear algebra,import numpy as np ,ds-london-explaining-gaussianmixture-preprocessing.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,ds-london-explaining-gaussianmixture-preprocessing.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,ds-london-explaining-gaussianmixture-preprocessing.ipynb
Let s first load and look at the data,"import pandas as pd 
import warnings
warnings.filterwarnings(""ignore"")

X_train = pd.read_csv(""/kaggle/input/data-science-london-scikit-learn/train.csv"", header=None)
X_test = pd.read_csv(""/kaggle/input/data-science-london-scikit-learn/test.csv"", header=None)
y_train = pd.read_csv(""/kaggle/input/data-science-london-scikit-learn/trainLabels.csv"", header=None)",ds-london-explaining-gaussianmixture-preprocessing.ipynb
Imports and scaling,"import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import validation_curve, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.mixture import GaussianMixture
from sklearn.ensemble import GradientBoostingClassifier

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
y_train = y_train.to_numpy().ravel()
",ds-london-explaining-gaussianmixture-preprocessing.ipynb
Fittting,"estimators = {
 GradientBoostingClassifier(random_state=0): {'learning_rate': np.linspace(0.001, 1, 10)},
 DecisionTreeClassifier(random_state=0): {'max_depth': np.arange(1, 11)},
 RandomForestClassifier(random_state=0): {'max_depth': np.arange(1, 11)},
 KNeighborsClassifier(): {'n_neighbors': np.arange(1, 31, 3)},
 SVC(random_state=0): {'gamma': np.linspace(0.001, 0.1, 10)},
 LogisticRegression(): {'C': np.linspace(0.001, 0.1, 10)},
 SGDClassifier(random_state=0): {'alpha': np.linspace(0.00001, 0.1, 10)}
}

fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(14, 24))
axs = axs.ravel()

for i, (estimator, params) in enumerate(estimators.items()):

 print(f'Training {estimator.__class__.__name__}')
 param_name = list(params)[0]
 param_range = params[param_name]

 grid = GridSearchCV(estimator, params, cv=3, scoring=""accuracy"", n_jobs=-1)
 grid.fit(X_train, y_train)
 train_score, validation_score = validation_curve(
 estimator, X_train, y_train, param_name=param_name, param_range=param_range, cv=3, scoring=""accuracy"", n_jobs=-1
 )

 axs[i].set_title(estimator.__class__.__name__)
 axs[i].plot(param_range, validation_score.mean(axis=1), label='Validation_score')
 axs[i].plot(param_range, train_score.mean(axis=1), label='Train_score')
 axs[i].scatter(grid.best_params_[param_name], grid.best_score_, c='r', label='GridSearchCV hyperparameter choice')
 axs[i].set_xlabel(param_name)
 axs[i].legend()",ds-london-explaining-gaussianmixture-preprocessing.ipynb
In fact the most of the features seem normaly distributed,"X_train = pd.read_csv(""/kaggle/input/data-science-london-scikit-learn/train.csv"", header=None)
X_test = pd.read_csv(""/kaggle/input/data-science-london-scikit-learn/test.csv"", header=None)
y_train = pd.read_csv(""/kaggle/input/data-science-london-scikit-learn/trainLabels.csv"", header=None)

X_train.boxplot(figsize=(15, 8))",ds-london-explaining-gaussianmixture-preprocessing.ipynb
Our assumption here is that the data are coming from a collection of gaussian distributions which is likely seeing the shape of the features ! . So let s try to replace the data by their probability of being in one of the Gaussian mixture cluster. The fitting part will determine the number of clusters and their shape.,"param_grid = {
 'n_components': range(1, 11),
 'covariance_type': ['spherical', 'tied', 'diag', 'full']
}

gm_grid = GridSearchCV(GaussianMixture(random_state=0), param_grid=param_grid, cv=5, n_jobs=-1)
gm_grid.fit(pd.concat([X_train, X_test]))

X_train = gm_grid.best_estimator_.predict_proba(X_train)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
y_train = y_train.to_numpy().ravel()",ds-london-explaining-gaussianmixture-preprocessing.ipynb
Let s try to fit the best models found in the first part,"estimators = {
 RandomForestClassifier(random_state=0): {'max_depth': np.arange(1, 11)},
 KNeighborsClassifier(): {'n_neighbors': np.arange(1, 11)},
}

fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))
axs = axs.ravel()

for i, (estimator, params) in enumerate(estimators.items()):

 print(f'Training {estimator.__class__.__name__}')
 param_name = list(params)[0]
 param_range = params[param_name]

 grid = GridSearchCV(estimator, params, cv=3, scoring=""accuracy"", n_jobs=-1)
 grid.fit(X_train, y_train)
 train_score, validation_score = validation_curve(
 estimator, X_train, y_train, param_name=param_name, param_range=param_range, cv=3, scoring=""accuracy"", n_jobs=-1
 )

 axs[i].set_title(estimator.__class__.__name__)
 axs[i].plot(param_range, validation_score.mean(axis=1), label='Validation_score')
 axs[i].plot(param_range, train_score.mean(axis=1), label='Train_score')
 axs[i].scatter(grid.best_params_[param_name], grid.best_score_, c='r', label='GridSearchCV hyperparameter choice')
 axs[i].set_xlabel(param_name)
 axs[i].legend()",ds-london-explaining-gaussianmixture-preprocessing.ipynb
Trainning of the best model,"X_test = gm_grid.best_estimator_.predict_proba(X_test)
X_test = scaler.transform(X_test)

forest = RandomForestClassifier(random_state=0, max_depth=7)

forest.fit(X_train, y_train)
submission = forest.predict(X_test)

submission = pd.DataFrame(submission, index=range(1, 9001)).reset_index()
submission.columns= ['Id', 'Solution']

submission.to_csv(""submission.csv"", index=False)",ds-london-explaining-gaussianmixture-preprocessing.ipynb
"Introduction: This tutorial is for beginners learning the concept of Scikit Learn library which is a high level framework designed for supervised and unsupervised machine learning algorithms, built on top of NumPy and SciPy libraries, each responsible for lower level data science tasks.This tutorial seeks inspiration from sequence of steps are as follows: Check for missing values in the dataset Pre process data by splitting into Train Test sets Models Classification Feature Scaling by standardizing and normalizing your data Learn its effect by improved accuracy Reduce the dimension of your data using PCA Learn its effect by improved accuracy Applying Gaussian Mixture and Grid Search Learn its effect by improved accuracy Fit our best model ","import numpy as np 
import pandas as pd
from sklearn.metrics import accuracy_score 
from sklearn.model_selection import cross_val_score

import os
print(os.listdir(""../input""))",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
Check for missing data list them in train and test set,datasetHasNan = False ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
now list items,if datasetHasNan == True : ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
PRE PROCESSING Train Test SplitSplit data into train 70 and val 30 ,"from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(train_data,train_labels, test_size = 0.30, random_state = 101)
x_train.shape,x_test.shape,y_train.shape,y_test.shape",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
NAIVE BAYES,from sklearn.naive_bayes import GaussianNB ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
KNN,from sklearn.neighbors import KNeighborsClassifier ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
RANDOM FOREST,from sklearn.ensemble import RandomForestClassifier ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
LOGISTIC REGRESSION,from sklearn.linear_model import LogisticRegression ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
SVM,from sklearn.svm import SVC ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
DECISON TREE,from sklearn.tree import DecisionTreeClassifier ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
XGBOOST,from xgboost import XGBClassifier ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"Feature ScalingTwo approaches are shown below: 1. The StandardScaler assumes your data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1. The normalizer scales each value by dividing each value by its magnitude in n dimensional space for n number of features. ","from sklearn.preprocessing import StandardScaler, Normalizer

std = StandardScaler()
std_train_data = std.fit_transform(train_data)

norm = Normalizer()
norm_train_data = norm.fit_transform(train_data)",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
NAIVE BAYES,from sklearn.naive_bayes import GaussianNB ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"print Naive Bayes ,accuracy score y test, nb predicted ","print('Naive Bayes' , cross_val_score(nb_model , norm_train_data , train_labels.values.ravel (), cv = 10). mean ()) ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
KNN,from sklearn.neighbors import KNeighborsClassifier ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"print KNN ,accuracy score y test, knn predicted ","print('KNN' , cross_val_score(knn_model , norm_train_data , train_labels.values.ravel (), cv = 10). mean ()) ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
RANDOM FOREST,from sklearn.ensemble import RandomForestClassifier ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"print Random Forest ,accuracy score y test,rfc predicted ","print('Random Forest' , cross_val_score(rfc_model , norm_train_data , train_labels.values.ravel (), cv = 10). mean ()) ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
SVM,from sklearn.svm import SVC ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"print SVM ,accuracy score y test, svc predicted ","print('SVM' , cross_val_score(svc_model , norm_train_data , train_labels.values.ravel (), cv = 10). mean ()) ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
DECISION TREE,from sklearn.tree import DecisionTreeClassifier ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"print Decision Tree ,accuracy score y test, dtree predicted ","print('Decision Tree' , cross_val_score(dtree_model , norm_train_data , train_labels.values.ravel (), cv = 10). mean ()) ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
XGBOOST,from xgboost import XGBClassifier ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"print XGBoost ,accuracy score y test, xgb predicted ","print('XGBoost' , cross_val_score(xgb , norm_train_data , train_labels.values.ravel (), cv = 10). mean ()) ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"Principal Component AnalysisPCA helps us to identify patterns in data based on the correlation between features. Used to reduce number of variables in your data by extracting important one from a large pool. Thus, it reduces the dimension of your data with the aim of retaining as much information as possible.Here we will use a straightforward PCA, asking it to preserve 85 of the variance in the projected data.","from sklearn.decomposition import PCA

pca = PCA(0.85, whiten=True)
pca_train_data = pca.fit_transform(train_data)
print(pca_train_data.shape,'\n')

explained_variance = pca.explained_variance_ratio_ 
print(explained_variance)",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
NAIVE BAYES,from sklearn.naive_bayes import GaussianNB ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
KNN,from sklearn.neighbors import KNeighborsClassifier ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"print KNN ,accuracy score y test, knn predicted ","print('KNN' , cross_val_score(knn_model , pca_train_data , train_labels.values.ravel (), cv = 10). mean ()) ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
RANDOM FOREST,from sklearn.ensemble import RandomForestClassifier ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"print Random Forest ,accuracy score y test,rfc predicted ","print('Random Forest' , cross_val_score(rfc_model , pca_train_data , train_labels.values.ravel (), cv = 10). mean ()) ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
SVM,from sklearn.svm import SVC ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"print SVM ,accuracy score y test, svc predicted ","print('SVM' , cross_val_score(svc_model , pca_train_data , train_labels.values.ravel (), cv = 10). mean ()) ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
DECISION TREE,from sklearn.tree import DecisionTreeClassifier ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"print Decision Tree ,accuracy score y test, dtree predicted ","print('Decision Tree' , cross_val_score(dtree_model , pca_train_data , train_labels.values.ravel (), cv = 10). mean ()) ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
XGBOOST,from xgboost import XGBClassifier ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"print XGBoost ,accuracy score y test, xgb predicted ","print('XGBoost' , cross_val_score(xgb , pca_train_data , train_labels.values.ravel (), cv = 10). mean ()) ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
Importing libraries,from sklearn.neighbors import KNeighborsClassifier ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"In theory, it recovers the true number of components only in the asymptotic regime",lowest_bic = np.infty ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"spherical, diagonal, tied or full covariance.","cv_types =['spherical' , 'tied' , 'diag' , 'full'] ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
 Random Forest Classifier ,rfc = RandomForestClassifier(random_state = 99) ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
The first step you need to perform is to create a dictionary of all the parameters and their corresponding set of values that you want to test for best performance.,"n_estimators =[10 , 50 , 100 , 200 , 400] ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
create an instance of the GridSearchCV class,"grid_search_rfc = GridSearchCV(estimator = rfc , param_grid = param_grid , cv = 10 , scoring = 'accuracy' , n_jobs = - 1). fit(gmm_train , train_labels.values.ravel ()) ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
Using 10 folds CV whereas a value of 1 for n jobs parameter means that use all available computing power.,rfc_best = grid_search_rfc.best_estimator_ ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
 KNN ,knn = KNeighborsClassifier () ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
"the decision on which class to assign will be done randomly when weights is set to uniform. By choosing an odd number, there are no ties.","n_neighbors =[3 , 5 , 6 , 7 , 8 , 9 , 10] ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
create an instance of the GridSearchCV class,"grid_search_knn = GridSearchCV(estimator = knn , param_grid = param_grid , cv = 10 , n_jobs = - 1 , scoring = 'accuracy'). fit(gmm_train , train_labels.values.ravel ()) ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
 SVM ,svc = SVC () ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
Fitting our model,"rfc_best.fit(gmm_train , train_labels.values.ravel ()) ",ds-tutorial-pca-gussian-mixture-grid-search.ipynb
FRAMING OUR SOLUTION,rfc_best_pred.columns =['Solution'] ,ds-tutorial-pca-gussian-mixture-grid-search.ipynb
linear algebra,import numpy as np ,easy-keras-facial-keypoint-detection.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,easy-keras-facial-keypoint-detection.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,easy-keras-facial-keypoint-detection.ipynb
Any results you write to the current directory are saved as output.,"from sklearn.model_selection import train_test_split 
from matplotlib import pyplot as plt
%matplotlib inline ",easy-keras-facial-keypoint-detection.ipynb
"training.fillna method ffill ,inplace True ","training.shape, type(training)",easy-keras-facial-keypoint-detection.ipynb
Model,"from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten, AvgPool2D, BatchNormalization, Dropout, Activation, MaxPooling2D
from keras.optimizers import Adam
from keras import regularizers
from keras.layers.advanced_activations import LeakyReLU
from keras.models import Sequential, Model
from keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D",easy-keras-facial-keypoint-detection.ipynb
model.add BatchNormalization ,model.add(LeakyReLU(alpha = 0.1)) ,easy-keras-facial-keypoint-detection.ipynb
"back to top 2. What is econometricsEconometrics is called the art of developing forecasts of socio economic phenomena. It s also about forecasting, about data, but it is some different to machine learning and data science. A first difference is econometrics its about interpretable models, sort of insight and inference is a goal, but machine learning may dont care about this. Thats why we can talk about why CEO prefer people with econometrics knowledge, that can understand what s going with the research object and give interpretable or inference. Im tried to draw what is place econometrics did have in data science. ","from IPython.display import Image
Image(""../input/pictures/econ.png"")",econometrics-is-all-you-need.ipynb
"How we propose a forecasting problem using other sciences such as classical statistics, probability theory and mathematical economics? In statistics, we are limited to methods such as descriptive statistics, histograms, means, variance which one cant give us appropriate results and inference. fact: Usual the most data science courses end education after statistics.","import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from matplotlib.animation import FuncAnimation
from matplotlib import animation
from IPython import display
from cycler import cycler
import seaborn as sns",econometrics-is-all-you-need.ipynb
"back to top 3.1. Regression The first part of this manual we will talk about regressions. Regressions the simplest linear models, but also it is stronger tool in hand of data scientits. At first time you can be afraid of lot of formuls, but all materials is easy to understand if you will try look into logic of forluma. Let s look at simple regression formula to understand what is it and how its working. y target variable, features vector, model parameters or weight vector. Also bias or constant. Now we can find weight vector , with methods of optimization later we will told about this .","Image(""../input/pictures/scat.png"")",econometrics-is-all-you-need.ipynb
 matplotlib widget,plt.rcParams['figure.dpi']= 150 ,econometrics-is-all-you-need.ipynb
plt.show ,"for s in[""top"" , ""right""]: ",econometrics-is-all-you-need.ipynb
 matplotlib widget,"fig , ax = plt.subplots(1 , figsize =(5 , 3), facecolor = '#f6f5f5') ",econometrics-is-all-you-need.ipynb
plt.show ,"ax.set_title('Animation plot of changing slope of x and y' , fontdict = { 'fontsize' : 10 , 'fontweight' : 'bold' }) ",econometrics-is-all-you-need.ipynb
"back to top 3.2. Ordinary Least Squares The most popular method optimization linear regression OLS. First of all it should be said that optimization regression split of two methods: Precise Analytical Method and Approximate Numerical Method. And in this topic we will talk more about first type. But now let s figure it out what is OLS and why you need to know this. As we know that regression finction look like and you can check out that im told about w 0 is constant, but why? Often we can ignore , because we can achieve the same result if make new feature equal 1. Then new created feature will be have weight equal .We want that our regression model as better as possible approximate our dependent.","Image(""../input/pictures/scat2.png"")",econometrics-is-all-you-need.ipynb
"3.2.1. Precise Analytical Method To find the minimum point, we need equate gradient of vectors to zero in this point and get this formula of optimal weight vector.Let s prove this formula. At first we need to find gradient of loss function. For this we will write squared of modulus in inner scalar product. Let s apply the product differential formula and take advantage of the symmetry of the inner scalar product.","Image(""../input/pictures/formula.jpg"")",econometrics-is-all-you-need.ipynb
"back to top 3.3. Anova A new technique called the analysis of variance is used to determine how the different experimental factors affect the average response. As we know regression average line of features. So ANOVA for regression is way to estimate importance of weight of regression and model in general. sum of squares for regression measures the amount of variation explained by using the regression equation. sum of squares for error measures the residual variation in the data that is not explained by the independent variables.So that The degrees of freedom for these sums of squares are found using the following argu ment. There are n 1 total degrees of freedom. Estimating the regression line requires estimating k unknown coefficients the constant which estimates is a function of y and the other estimates. Hence, there are k regression degrees of freedom, leaving n 1 k degrees of freedom for error. The mean squares are calculated as MS SS dfAlso you can made ANOVA table. ","Image(""../input/pictures/anova1.png"")",econometrics-is-all-you-need.ipynb
"back to top 3.4.5. Residual plotBefore using the regression model for its main purpose estimation and prediction of y you should look at computer generated residual plots to make sure that all the regression assumptions are valid. The normal probability plot and the plot of residuals versus fit are shown for the real estate data. There appear to be three observations that donot fit the general pattern. You can see them as outliers in both graphs. These three observa tions should probably be investigated however, they do not provide strong evidence thatthe assumptions are violated.","Image(""../input/pictures/anova2.png"")",econometrics-is-all-you-need.ipynb
Part1: Exploratory Data Analysis EDA ,"import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('fivethirtyeight')
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline",eda-to-prediction-dietanic.ipynb
checking for total null values,data.isnull (). sum () ,eda-to-prediction-dietanic.ipynb
How many Survived??,"f,ax=plt.subplots(1,2,figsize=(18,8))
data['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)
ax[0].set_title('Survived')
ax[0].set_ylabel('')
sns.countplot('Survived',data=data,ax=ax[1])
ax[1].set_title('Survived')
plt.show()",eda-to-prediction-dietanic.ipynb
Sex Categorical Feature,"data.groupby(['Sex','Survived'])['Survived'].count()",eda-to-prediction-dietanic.ipynb
Pclass Ordinal Feature,"pd.crosstab(data.Pclass,data.Survived,margins=True).style.background_gradient(cmap='summer_r')",eda-to-prediction-dietanic.ipynb
"People say Money Can t Buy Everything. But we can clearly see that Passenegers Of Pclass 1 were given a very high priority while rescue. Even though the the number of Passengers in Pclass 3 were a lot higher, still the number of survival from them is very low, somewhere around 25 .For Pclass 1 survived is around 63 while for Pclass2 is around 48 . So money and status matters. Such a materialistic world.Lets Dive in little bit more and check for other interesting observations. Lets check survival rate with Sex and Pclass Together.","pd.crosstab([data.Sex,data.Survived],data.Pclass,margins=True).style.background_gradient(cmap='summer_r')",eda-to-prediction-dietanic.ipynb
Age Continous Feature,"print('Oldest Passenger was of:',data['Age'].max(),'Years')
print('Youngest Passenger was of:',data['Age'].min(),'Years')
print('Average Age on the ship:',data['Age'].mean(),'Years')",eda-to-prediction-dietanic.ipynb
"As we had seen earlier, the Age feature has 177 null values. To replace these NaN values, we can assign them the mean age of the dataset.But the problem is, there were many people with many different ages. We just cant assign a 4 year kid with the mean age that is 29 years. Is there any way to find out what age band does the passenger lie??Bingo!!!!, we can check the Name feature. Looking upon the feature, we can see that the names have a salutation like Mr or Mrs. Thus we can assign the mean values of Mr and Mrs to the respective groups. What s In A Name?? Feature :p",data['Initial']= 0 ,eda-to-prediction-dietanic.ipynb
lets extract the Salutations, data['Initial']= data.Name.str.extract('([A-Za-z]+)\.') ,eda-to-prediction-dietanic.ipynb
Checking the Initials with the Sex,"pd.crosstab(data.Initial , data.Sex). T.style.background_gradient(cmap = 'summer_r') ",eda-to-prediction-dietanic.ipynb
Okay so there are some misspelled Initials like Mlle or Mme that stand for Miss. I will replace them with Miss and same thing for other values.,"data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)",eda-to-prediction-dietanic.ipynb
lets check the average age by Initials,data.groupby('Initial')[ 'Age']. mean () ,eda-to-prediction-dietanic.ipynb
Assigning the NaN Values with the Ceil values of the mean ages,"data.loc[( data.Age.isnull ()) &(data.Initial == 'Mr'), 'Age']= 33 ",eda-to-prediction-dietanic.ipynb
So no null values left finally,data.Age.isnull (). any () ,eda-to-prediction-dietanic.ipynb
Observations: 1 The Toddlers age 5 were saved in large numbers The Women and Child First Policy .2 The oldest Passenger was saved 80 years .3 Maximum number of deaths were in the age group of 30 40.,"sns.factorplot('Pclass','Survived',col='Initial',data=data)
plt.show()",eda-to-prediction-dietanic.ipynb
Embarked Categorical Value,"pd.crosstab([data.Embarked,data.Pclass],[data.Sex,data.Survived],margins=True).style.background_gradient(cmap='summer_r')",eda-to-prediction-dietanic.ipynb
Chances for Survival by Port Of Embarkation,"sns.factorplot('Embarked','Survived',data=data)
fig=plt.gcf()
fig.set_size_inches(5,3)
plt.show()",eda-to-prediction-dietanic.ipynb
The chances for survival for Port C is highest around 0.55 while it is lowest for S.,"f,ax=plt.subplots(2,2,figsize=(20,15))
sns.countplot('Embarked',data=data,ax=ax[0,0])
ax[0,0].set_title('No. Of Passengers Boarded')
sns.countplot('Embarked',hue='Sex',data=data,ax=ax[0,1])
ax[0,1].set_title('Male-Female Split for Embarked')
sns.countplot('Embarked',hue='Survived',data=data,ax=ax[1,0])
ax[1,0].set_title('Embarked vs Survived')
sns.countplot('Embarked',hue='Pclass',data=data,ax=ax[1,1])
ax[1,1].set_title('Embarked vs Pclass')
plt.subplots_adjust(wspace=0.2,hspace=0.5)
plt.show()",eda-to-prediction-dietanic.ipynb
"Observations: 1 Maximum passenegers boarded from S. Majority of them being from Pclass3.2 The Passengers from C look to be lucky as a good proportion of them survived. The reason for this maybe the rescue of all the Pclass1 and Pclass2 Passengers.3 The Embark S looks to the port from where majority of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass3 around 81 didn t survive. 4 Port Q had almost 95 of the passengers were from Pclass3.","sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=data)
plt.show()",eda-to-prediction-dietanic.ipynb
"Filling Embarked NaNAs we saw that maximum passengers boarded from Port S, we replace NaN with S.","data['Embarked'].fillna('S',inplace=True)",eda-to-prediction-dietanic.ipynb
Finally No NaN values,data.Embarked.isnull (). any () ,eda-to-prediction-dietanic.ipynb
"SibSip Discrete Feature This feature represents whether a person is alone or with his family members.Sibling brother, sister, stepbrother, stepsisterSpouse husband, wife ","pd.crosstab([data.SibSp],data.Survived).style.background_gradient(cmap='summer_r')",eda-to-prediction-dietanic.ipynb
Parch,"pd.crosstab(data.Parch,data.Pclass).style.background_gradient(cmap='summer_r')",eda-to-prediction-dietanic.ipynb
The crosstab again shows that larger families were in Pclass3.,"f,ax=plt.subplots(1,2,figsize=(20,8))
sns.barplot('Parch','Survived',data=data,ax=ax[0])
ax[0].set_title('Parch vs Survived')
sns.factorplot('Parch','Survived',data=data,ax=ax[1])
ax[1].set_title('Parch vs Survived')
plt.close(2)
plt.show()",eda-to-prediction-dietanic.ipynb
Fare Continous Feature,"print('Highest Fare was:',data['Fare'].max())
print('Lowest Fare was:',data['Fare'].min())
print('Average Fare was:',data['Fare'].mean())",eda-to-prediction-dietanic.ipynb
The lowest fare is 0.0. Wow!! a free luxorious ride. ,"f,ax=plt.subplots(1,3,figsize=(20,8))
sns.distplot(data[data['Pclass']==1].Fare,ax=ax[0])
ax[0].set_title('Fares in Pclass 1')
sns.distplot(data[data['Pclass']==2].Fare,ax=ax[1])
ax[1].set_title('Fares in Pclass 2')
sns.distplot(data[data['Pclass']==3].Fare,ax=ax[2])
ax[2].set_title('Fares in Pclass 3')
plt.show()",eda-to-prediction-dietanic.ipynb
data.corr correlation matrix,"sns.heatmap(data.corr (), annot = True , cmap = 'RdYlGn' , linewidths = 0.2) ",eda-to-prediction-dietanic.ipynb
"Age bandProblem With Age Feature: As I have mentioned earlier that Age is a continous feature, there is a problem with Continous Variables in Machine Learning Models.Eg:If I say to group or arrange Sports Person by Sex, We can easily segregate them by Male and Female.Now if I say to group them by their Age, then how would you do it? If there are 30 Persons, there may be 30 age values. Now this is problematic.We need to convert these continous values into categorical values by either Binning or Normalisation. I will be using binning i.e group a range of ages into a single bin or assign them a single value.Okay so the maximum age of a passenger was 80. So lets divide the range from 0 80 into 5 bins. So 80 5 16. So bins of size 16.","data['Age_band']=0
data.loc[data['Age']<=16,'Age_band']=0
data.loc[(data['Age']>16)&(data['Age']<=32),'Age_band']=1
data.loc[(data['Age']>32)&(data['Age']<=48),'Age_band']=2
data.loc[(data['Age']>48)&(data['Age']<=64),'Age_band']=3
data.loc[data['Age']>64,'Age_band']=4
data.head(2)",eda-to-prediction-dietanic.ipynb
checking the number of passenegers in each band,data['Age_band']. value_counts (). to_frame (). style.background_gradient(cmap = 'summer') ,eda-to-prediction-dietanic.ipynb
"True that..the survival rate decreases as the age increases irrespective of the Pclass.Family Size and Alone At this point, we can create a new feature called Family size and Alone and analyse it. This feature is the summation of Parch and SibSp. It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers. Alone will denote whether a passenger is alone or not.",data['Family_Size']= 0 ,eda-to-prediction-dietanic.ipynb
family size,data['Family_Size']= data['Parch']+ data['SibSp'] ,eda-to-prediction-dietanic.ipynb
Alone,"data.loc[data.Family_Size == 0 , 'Alone']= 1 ",eda-to-prediction-dietanic.ipynb
"Family Size 0 means that the passeneger is alone. Clearly, if you are alone or family size 0,then chances for survival is very low. For family size 4,the chances decrease too. This also looks to be an important feature for the model. Lets examine this further.","sns.factorplot('Alone','Survived',data=data,hue='Sex',col='Pclass')
plt.show()",eda-to-prediction-dietanic.ipynb
"It is visible that being alone is harmful irrespective of Sex or Pclass except for Pclass3 where the chances of females who are alone is high than those with family.Fare RangeSince fare is also a continous feature, we need to convert it into ordinal value. For this we will use pandas.qcut.So what qcut does is it splits or arranges the values according the number of bins we have passed. So if we pass for 5 bins, it will arrange the values equally spaced into 5 seperate bins or value ranges.","data['Fare_Range']=pd.qcut(data['Fare'],4)
data.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')",eda-to-prediction-dietanic.ipynb
"As discussed above, we can clearly see that as the fare range increases, the chances of survival increases.Now we cannot pass the Fare Range values as it is. We should convert it into singleton values same as we did in Age Band","data['Fare_cat']=0
data.loc[data['Fare']<=7.91,'Fare_cat']=0
data.loc[(data['Fare']>7.91)&(data['Fare']<=14.454),'Fare_cat']=1
data.loc[(data['Fare']>14.454)&(data['Fare']<=31),'Fare_cat']=2
data.loc[(data['Fare']>31)&(data['Fare']<=513),'Fare_cat']=3",eda-to-prediction-dietanic.ipynb
"Clearly, as the Fare cat increases, the survival chances increases. This feature may become an important feature during modeling along with the Sex.Converting String Values into NumericSince we cannot pass strings to a machine learning model, we need to convert features loke Sex, Embarked, etc into numeric values.","data['Sex'].replace(['male','female'],[0,1],inplace=True)
data['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)
data['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)",eda-to-prediction-dietanic.ipynb
"Dropping UnNeeded FeaturesName We don t need name feature as it cannot be converted into any categorical value.Age We have the Age band feature, so no need of this.Ticket It is any random string that cannot be categorised.Fare We have the Fare cat feature, so unneededCabin A lot of NaN values and also many passengers have multiple cabins. So this is a useless feature.Fare Range We have the fare cat feature.PassengerId Cannot be categorised.","data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)
sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})
fig=plt.gcf()
fig.set_size_inches(18,15)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.show()",eda-to-prediction-dietanic.ipynb
logistic regression,from sklearn.linear_model import LogisticRegression ,eda-to-prediction-dietanic.ipynb
support vector Machine,from sklearn import svm ,eda-to-prediction-dietanic.ipynb
Random Forest,from sklearn.ensemble import RandomForestClassifier ,eda-to-prediction-dietanic.ipynb
KNN,from sklearn.neighbors import KNeighborsClassifier ,eda-to-prediction-dietanic.ipynb
Naive bayes,from sklearn.naive_bayes import GaussianNB ,eda-to-prediction-dietanic.ipynb
Decision Tree,from sklearn.tree import DecisionTreeClassifier ,eda-to-prediction-dietanic.ipynb
training and testing data split,from sklearn.model_selection import train_test_split ,eda-to-prediction-dietanic.ipynb
accuracy measure,from sklearn import metrics ,eda-to-prediction-dietanic.ipynb
for confusion matrix,from sklearn.metrics import confusion_matrix ,eda-to-prediction-dietanic.ipynb
Radial Support Vector Machines rbf SVM ,"model=svm.SVC(kernel='rbf',C=1,gamma=0.1)
model.fit(train_X,train_Y)
prediction1=model.predict(test_X)
print('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1,test_Y))",eda-to-prediction-dietanic.ipynb
Linear Support Vector Machine linear SVM ,"model=svm.SVC(kernel='linear',C=0.1,gamma=0.1)
model.fit(train_X,train_Y)
prediction2=model.predict(test_X)
print('Accuracy for linear SVM is',metrics.accuracy_score(prediction2,test_Y))",eda-to-prediction-dietanic.ipynb
Logistic Regression,"model = LogisticRegression()
model.fit(train_X,train_Y)
prediction3=model.predict(test_X)
print('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction3,test_Y))",eda-to-prediction-dietanic.ipynb
Decision Tree,"model=DecisionTreeClassifier()
model.fit(train_X,train_Y)
prediction4=model.predict(test_X)
print('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction4,test_Y))
",eda-to-prediction-dietanic.ipynb
K Nearest Neighbours KNN ,"model=KNeighborsClassifier() 
model.fit(train_X,train_Y)
prediction5=model.predict(test_X)
print('The accuracy of the KNN is',metrics.accuracy_score(prediction5,test_Y))",eda-to-prediction-dietanic.ipynb
Now the accuracy for the KNN model changes as we change the values for n neighbours attribute. The default value is 5. Lets check the accuracies over various values of n neighbours.,"a_index=list(range(1,11))
a=pd.Series()
x=[0,1,2,3,4,5,6,7,8,9,10]
for i in list(range(1,11)):
 model=KNeighborsClassifier(n_neighbors=i) 
 model.fit(train_X,train_Y)
 prediction=model.predict(test_X)
 a=a.append(pd.Series(metrics.accuracy_score(prediction,test_Y)))
plt.plot(a_index, a)
plt.xticks(x)
fig=plt.gcf()
fig.set_size_inches(12,6)
plt.show()
print('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())",eda-to-prediction-dietanic.ipynb
Gaussian Naive Bayes,"model=GaussianNB()
model.fit(train_X,train_Y)
prediction6=model.predict(test_X)
print('The accuracy of the KNN is',metrics.accuracy_score(prediction6,test_Y))",eda-to-prediction-dietanic.ipynb
Random Forests,"model=RandomForestClassifier(n_estimators=100)
model.fit(train_X,train_Y)
prediction7=model.predict(test_X)
print('The accuracy of the KNN is',metrics.accuracy_score(prediction7,test_Y))",eda-to-prediction-dietanic.ipynb
for K fold cross validation,from sklearn.model_selection import KFold ,eda-to-prediction-dietanic.ipynb
score evaluation,from sklearn.model_selection import cross_val_score ,eda-to-prediction-dietanic.ipynb
prediction,from sklearn.model_selection import cross_val_predict ,eda-to-prediction-dietanic.ipynb
"k 10, split the data into 10 equal parts","kfold = KFold(n_splits = 10 , random_state = 22) ",eda-to-prediction-dietanic.ipynb
"The classification accuracy can be sometimes misleading due to imbalance. We can get a summarized result with the help of confusion matrix, which shows where did the model go wrong, or which class did the model predict wrong.Confusion MatrixIt gives the number of correct and incorrect classifications made by the classifier.","f,ax=plt.subplots(3,3,figsize=(12,10))
y_pred = cross_val_predict(svm.SVC(kernel='rbf'),X,Y,cv=10)
sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')
ax[0,0].set_title('Matrix for rbf-SVM')
y_pred = cross_val_predict(svm.SVC(kernel='linear'),X,Y,cv=10)
sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')
ax[0,1].set_title('Matrix for Linear-SVM')
y_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)
sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')
ax[0,2].set_title('Matrix for KNN')
y_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)
sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')
ax[1,0].set_title('Matrix for Random-Forests')
y_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)
sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')
ax[1,1].set_title('Matrix for Logistic Regression')
y_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)
sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')
ax[1,2].set_title('Matrix for Decision Tree')
y_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)
sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')
ax[2,0].set_title('Matrix for Naive Bayes')
plt.subplots_adjust(hspace=0.2,wspace=0.2)
plt.show()",eda-to-prediction-dietanic.ipynb
"Hyper Parameters TuningThe machine learning models are like a Black Box. There are some default parameter values for this Black Box, which we can tune or change to get a better model. Like the C and gamma in the SVM model and similarly different parameters for different classifiers, are called the hyper parameters, which we can tune to change the learning rate of the algorithm and get a better model. This is known as Hyper Parameter Tuning.We will tune the hyper parameters for the 2 best classifiers i.e the SVM and RandomForests.SVM","from sklearn.model_selection import GridSearchCV
C=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]
gamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]
kernel=['rbf','linear']
hyper={'kernel':kernel,'C':C,'gamma':gamma}
gd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)
gd.fit(X,Y)
print(gd.best_score_)
print(gd.best_estimator_)",eda-to-prediction-dietanic.ipynb
Random Forests,"n_estimators=range(100,1000,100)
hyper={'n_estimators':n_estimators}
gd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)
gd.fit(X,Y)
print(gd.best_score_)
print(gd.best_estimator_)",eda-to-prediction-dietanic.ipynb
Voting ClassifierIt is the simplest way of combining predictions from many different simple machine learning models. It gives an average prediction result based on the prediction of all the submodels. The submodels or the basemodels are all of diiferent types.,"from sklearn.ensemble import VotingClassifier
ensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),
 ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),
 ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),
 ('LR',LogisticRegression(C=0.05)),
 ('DT',DecisionTreeClassifier(random_state=0)),
 ('NB',GaussianNB()),
 ('svm',svm.SVC(kernel='linear',probability=True))
 ], 
 voting='soft').fit(train_X,train_Y)
print('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))
cross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = ""accuracy"")
print('The cross validated score is',cross.mean())",eda-to-prediction-dietanic.ipynb
"BaggingBagging is a general ensemble method. It works by applying similar classifiers on small partitions of the dataset and then taking the average of all the predictions. Due to the averaging,there is reduction in variance. Unlike Voting Classifier, Bagging makes use of similar classifiers.Bagged KNNBagging works best with models with high variance. An example for this can be Decision Tree or Random Forests. We can use KNN with small value of n neighbours, as small value of n neighbours.","from sklearn.ensemble import BaggingClassifier
model=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)
model.fit(train_X,train_Y)
prediction=model.predict(test_X)
print('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction,test_Y))
result=cross_val_score(model,X,Y,cv=10,scoring='accuracy')
print('The cross validated score for bagged KNN is:',result.mean())",eda-to-prediction-dietanic.ipynb
Bagged DecisionTree,"model=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)
model.fit(train_X,train_Y)
prediction=model.predict(test_X)
print('The accuracy for bagged Decision Tree is:',metrics.accuracy_score(prediction,test_Y))
result=cross_val_score(model,X,Y,cv=10,scoring='accuracy')
print('The cross validated score for bagged Decision Tree is:',result.mean())",eda-to-prediction-dietanic.ipynb
AdaBoost Adaptive Boosting The weak learner or estimator in this case is a Decsion Tree. But we can change the dafault base estimator to any algorithm of our choice.,"from sklearn.ensemble import AdaBoostClassifier
ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)
result=cross_val_score(ada,X,Y,cv=10,scoring='accuracy')
print('The cross validated score for AdaBoost is:',result.mean())",eda-to-prediction-dietanic.ipynb
Stochastic Gradient BoostingHere too the weak learner is a Decision Tree.,"from sklearn.ensemble import GradientBoostingClassifier
grad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)
result=cross_val_score(grad,X,Y,cv=10,scoring='accuracy')
print('The cross validated score for Gradient Boosting is:',result.mean())",eda-to-prediction-dietanic.ipynb
XGBoost,"import xgboost as xg
xgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)
result=cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')
print('The cross validated score for XGBoost is:',result.mean())",eda-to-prediction-dietanic.ipynb
We got the highest accuracy for AdaBoost. We will try to increase it with Hyper Parameter TuningHyper Parameter Tuning for AdaBoost,"n_estimators=list(range(100,1100,100))
learn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]
hyper={'n_estimators':n_estimators,'learning_rate':learn_rate}
gd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)
gd.fit(X,Y)
print(gd.best_score_)
print(gd.best_estimator_)",eda-to-prediction-dietanic.ipynb
Confusion Matrix for the Best Model,"ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.05)
result=cross_val_predict(ada,X,Y,cv=10)
sns.heatmap(confusion_matrix(Y,result),cmap='winter',annot=True,fmt='2.0f')
plt.show()",eda-to-prediction-dietanic.ipynb
Feature Importance,"f,ax=plt.subplots(2,2,figsize=(15,12))
model=RandomForestClassifier(n_estimators=500,random_state=0)
model.fit(X,Y)
pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])
ax[0,0].set_title('Feature Importance in Random Forests')
model=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)
model.fit(X,Y)
pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')
ax[0,1].set_title('Feature Importance in AdaBoost')
model=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)
model.fit(X,Y)
pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')
ax[1,0].set_title('Feature Importance in Gradient Boosting')
model=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)
model.fit(X,Y)
pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')
ax[1,1].set_title('Feature Importance in XgBoost')
plt.show()",eda-to-prediction-dietanic.ipynb
linear algebra,import numpy as np ,eda-with-plotly-useful-conclusions.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,eda-with-plotly-useful-conclusions.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,eda-with-plotly-useful-conclusions.ipynb
"You can also write temporary files to kaggle temp , but they won t be saved outside of the current session","train = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/train.csv')
test = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/test.csv')
stores = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')
transactions = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv')
oil = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv')
holidays_events = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv')",eda-with-plotly-useful-conclusions.ipynb
TODO: add train insights regarding the datapoints,train.info () ,eda-with-plotly-useful-conclusions.ipynb
add store information,"train = pd.merge(train , stores , how = 'left' , on = 'store_nbr') ",eda-with-plotly-useful-conclusions.ipynb
2.1 KPI Variables ,"fig = go.Figure(data=[go.Table(header=dict(values=['KPI', 'Value']),
 cells=dict(values=[['Number of Stores', 'Number of Different Products', 
 'Window Start Date', 'Window End Date',
 '#Rows in training set', '#Date Points in Train Dataset'], 
 [train['store_nbr'].nunique(), train['family'].nunique(), 
 train['date'].min(), train['date'].max(),
 train.shape[0], train['date'].nunique()]]))
 ])

fig.update_layout({""title"": f'BASIC KPIS of TRAIN DATA'}, height=500, width=500)
fig.show()",eda-with-plotly-useful-conclusions.ipynb
2.2 CHART TIME SERIES AVG SALES ON EACH DAY ,"train_aux = train[['date', 'sales', 'onpromotion']].groupby('date').mean()
train_aux = train_aux.reset_index()
fig = go.Figure(data=go.Scatter(x=train_aux['date'], 
 y=train_aux['sales'],
 marker_color='red', text=""sales""))
fig.update_layout({""title"": f'Avg Sales by date for all stores and products',
 ""xaxis"": {""title"":""Date""},
 ""yaxis"": {""title"":""Avg Unit Sold""},
 ""showlegend"": False})
fig.show()",eda-with-plotly-useful-conclusions.ipynb
2.3 ON PROMOTION VS AVG SALES CHART ,"fig = px.scatter(train_aux[train_aux['onpromotion'] > 0], x=""onpromotion"", y=""sales"", color='sales', 
 color_continuous_scale=""earth"",
 size='sales', log_x=True, size_max=30)

fig.update_layout({""title"": f'Correlation between OnPromotion and Sales (total avg sales and promotion on each day)',
 ""xaxis"": {""title"":""On Promotion""},
 ""yaxis"": {""title"":""Sales""},
 ""showlegend"": False})
fig.show()",eda-with-plotly-useful-conclusions.ipynb
create new features,train['year']= pd.to_datetime(train['date']).dt.year ,eda-with-plotly-useful-conclusions.ipynb
2.5 Sesonality of All the Time Series ,df_sesonality = train.copy () ,eda-with-plotly-useful-conclusions.ipynb
create month and year variables from date colum,df_sesonality['date']= pd.to_datetime(df_sesonality['date']) ,eda-with-plotly-useful-conclusions.ipynb
let s calculate de avg of units sold by Favorita stores everyday,variable = 'sales' ,eda-with-plotly-useful-conclusions.ipynb
plot boxplots for every week of the year,"fig = plt.figure(figsize =(17 , 7)) ",eda-with-plotly-useful-conclusions.ipynb
plot boxplots for every day,"fig = plt.figure(figsize =(17 , 7)) ",eda-with-plotly-useful-conclusions.ipynb
plot boxplots for every day,"fig = plt.figure(figsize =(17 , 7)) ",eda-with-plotly-useful-conclusions.ipynb
2.6 Lags of Sales Variable ,"LAGS = [16, 17, 18, 21, 28, 29, 35, 42, 49, 56]",eda-with-plotly-useful-conclusions.ipynb
check heatmap of lag features.,df_lag_corr = df_lag[[ col for col in list(df_lag.columns)if col.startswith('sales_' )]] ,eda-with-plotly-useful-conclusions.ipynb
"A lag plot is used to help evaluate whether the values in a dataset or time series are random. If the data are random, the lag plot will exhibit no identifiable pattern. If the data are not random, the lag plot will demonstrate a clearly identifiable pattern. The type of pattern can aid the user in identifying the non random structure in the data. Lag plots can also help to identify outliers Lag plots can provide answers to the following questions: Is the data random? Is there serial correlation in the data? What is a suitable model for the data? Are there outliers in the data? If the lag plot of the data set exhibits a linear pattern, it shows that the data are strongly non random and further suggests that an autoregressive AR model might be appropriate. ",category = 'BEVERAGES' ,eda-with-plotly-useful-conclusions.ipynb
df aux df lag,"xlim =(0 , 26000) ",eda-with-plotly-useful-conclusions.ipynb
filter holidays for the training dataset window.,"holidays_events = holidays_events[( holidays_events['date']>= ""2013-01-01"")&(holidays_events['date']<= ""2017-08-15"" )] ",eda-with-plotly-useful-conclusions.ipynb
Let s look at the sales behavior for the whole data,"train_aux = train[[ 'date' , 'sales']].groupby('date'). mean () ",eda-with-plotly-useful-conclusions.ipynb
The earthquake hit the 16th April 2016. One month onwards from that date we can see a slight increase in sales. After the earthquake there were more days off than usual. ,"df_plot = pd.merge(holidays_events, train_aux, on='date', how='inner')
df_plot.loc[df_plot['description'].isin(['Black Friday', 'Cyber Monday']), 'type'] = 'black_friday_cyber_monday'",eda-with-plotly-useful-conclusions.ipynb
3.2 Avg sales on Event Dates,"fig = px.scatter(df_plot , x = ""date"" , y = ""sales"" , size = 'sales' , color = 'type') ",eda-with-plotly-useful-conclusions.ipynb
" OIL DATA Ecuador is highly dependant on oil prices, therefore the prices of some items might be affected by variations in the oil prices. The oil industry is driven by booms and busts. Prices typically rise during periods of global economic strength during which demand outpaces supply. Prices fall when the reverse is true, and supply exceeds demand. Meanwhile, oil supply and demand are driven by a number of key factors: Changes in the value of the U.S. dollar Changes in the policies of the Organization of Petroleum Exporting Countries OPEC Changes in the levels of oil production and inventory The health of the global economy The implementation or collapse of international agreements ",oil.info(),eda-with-plotly-useful-conclusions.ipynb
"price of oil on 2013 01 01 first element of the series is missing, let s fill it with the value of the next day and interpolate the next ones.","oil.loc[oil['date']== '2013-01-01' , 'dcoilwtico']= 93.14 ",eda-with-plotly-useful-conclusions.ipynb
Let s look at the sales behavior for the whole data,"train_aux = train[[ 'date' , 'sales']].groupby('date'). mean () ",eda-with-plotly-useful-conclusions.ipynb
Scatter plot to the see correlation between average unit sold and oil price each day,sales_oil = train.groupby('date'). mean ()[ 'sales'] ,eda-with-plotly-useful-conclusions.ipynb
"we don t have all the oil prices available, we impute them","sales_oil = sales_oil.interpolate(method = 'linear' , limit = 20) ",eda-with-plotly-useful-conclusions.ipynb
4.2 Avg Sales vs Oil Prices Chart ,"fig = px.scatter(sales_oil, x=""dcoilwtico"", y=""sales"", size='sales', color='sales',
 color_continuous_scale=""pinkyl"")

fig.update_layout({""title"": f'Correlation between Oil Prices and Sales (total avg sales and promotion each day)',
 ""xaxis"": {""title"":""Oil Price""},
 ""yaxis"": {""title"":""Sales""},
 ""showlegend"": False})
fig.show()",eda-with-plotly-useful-conclusions.ipynb
4.3 Oil Lags ,oil = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv') ,eda-with-plotly-useful-conclusions.ipynb
oil date pd.to datetime oil date ,"df_oil = pd.merge(train , oil , on = 'date' , how = 'outer') ",eda-with-plotly-useful-conclusions.ipynb
compute lags for oil price,"LAGS_OIL =[1 , 2 , 3 , 4 , 5 , 6 , 7 , 16 , 21 , 28 , 35 , 90 , 180 , 365] ",eda-with-plotly-useful-conclusions.ipynb
compute lags for oil prices,df_lag_oil = df_oil.copy () ,eda-with-plotly-useful-conclusions.ipynb
Ranking of units solds by products at each store.,"df_family = train[[ 'family' , 'sales']].groupby('family'). mean (). sort_values('sales' , ascending = True) ",eda-with-plotly-useful-conclusions.ipynb
5.1 Top Family Products by Avg Sales ,"fig = px.bar(df_family, x='sales', y='family', color='sales', color_continuous_scale=""earth"")
fig.update_layout({""title"": f'AVG SALES FOR EACH FAMILTY PRODUCT',
 ""xaxis"": {""title"":""Avg Unit Sold""},
 ""yaxis"": {""title"":""Category Product""},
 ""showlegend"": True},
 width=1000,
 height=700)

fig.show()",eda-with-plotly-useful-conclusions.ipynb
"Ranking of units solds by store, taking into account all products.","df_store = train[[ 'store_nbr' , 'sales']].groupby('store_nbr'). mean (). sort_values('sales' , ascending = False) ",eda-with-plotly-useful-conclusions.ipynb
df store store nbr store df store store nbr .astype str ,df_store['sales']= df_store['sales'] ,eda-with-plotly-useful-conclusions.ipynb
5.3 Number of stores in each city by type of store,"df = stores.groupby(['city', 'type']).count()[['store_nbr']].reset_index(level=0).reset_index(level=0)[['city', 'type', 'store_nbr']]
map_colors = {'A': '#4d4d00', 'B':'#999900', 'C':'#e6e600', 'D':'#ffff00', 'E':'#ffff99'}
df['colors'] = df['type'].map(map_colors)",eda-with-plotly-useful-conclusions.ipynb
number of stores per city,"df_cities = stores['city']. value_counts (). reset_index (). rename(columns = { ""index"" : ""city"" , ""city"" : ""num_store_per_city"" }) ",eda-with-plotly-useful-conclusions.ipynb
"Ranking of units solds by store, taking into account all products.","df_store = train[[ 'state' , 'sales']].groupby('state'). mean (). sort_values('sales' , ascending = False) ",eda-with-plotly-useful-conclusions.ipynb
df store store nbr store df store store nbr .astype str ,df_store['sales']= df_store['sales'] ,eda-with-plotly-useful-conclusions.ipynb
"Ranking of units solds by store, taking into account all products.","df_store = train[[ 'city' , 'sales']].groupby('city'). sum (). sort_values('sales' , ascending = False) ",eda-with-plotly-useful-conclusions.ipynb
df store store nbr store df store store nbr .astype str ,df_store['sales']= df_store['sales'] ,eda-with-plotly-useful-conclusions.ipynb
6.1 Sales by product category for all the stores ,for family_group in list(train['family']. unique ()) : ,eda-with-plotly-useful-conclusions.ipynb
"If you like the charts, give some love and upvote : ","import plotly.io as pio
list(pio.templates)",eda-with-plotly-useful-conclusions.ipynb
"IntroductionYou have seen how to define a random agent. In this exercise, you ll make a few improvements.To get started, run the code cell below to set up our feedback system.","from learntools.core import binder
binder.bind(globals())
from learntools.game_ai.ex1 import *",exercise-play-the-game.ipynb
"1 A smarter agentWe can improve the performance without devising a complicated strategy, simply by selecting a winning move, if one is available.In this exercise, you will create an agent that: selects the winning move, if it is available. If there is more than one move that lets the agent win the game, the agent can select any of them. Otherwise, it should select a random move.To help you with this goal, we provide some helper functions in the code cell below. ",import numpy as np ,exercise-play-the-game.ipynb
Gets board at next step if agent drops piece in selected column,"def drop_piece(grid , col , piece , config): ",exercise-play-the-game.ipynb
Returns True if dropping piece in column results in game win,"def check_winning_move(obs , config , col , piece): ",exercise-play-the-game.ipynb
Convert the board to a 2D grid," grid = np.asarray(obs.board). reshape(config.rows , config.columns) ",exercise-play-the-game.ipynb
horizontal, for row in range(config.rows): ,exercise-play-the-game.ipynb
vertical, for row in range(config.rows -(config.inarow - 1)) : ,exercise-play-the-game.ipynb
positive diagonal, for row in range(config.rows -(config.inarow - 1)) : ,exercise-play-the-game.ipynb
negative diagonal," for row in range(config.inarow - 1 , config.rows): ",exercise-play-the-game.ipynb
"The check winning move function takes four required arguments: the first two obs and config should be familiar, and: col is any valid move piece is either the agent s mark or the mark of its opponent. The function returns True if dropping the piece in the provided column wins the game for either the agent or its opponent , and otherwise returns False. To check if the agent can win in the next move, you should set piece obs.mark.To complete this exercise, you need to define agent q1 in the code cell below. To do this, you re encouraged to use the check winning move function. The drop piece function defined in the code cell above is called in the check winning move function. Feel free to examine the details, but you won t need a detailed understanding to solve the exercise.",import random ,exercise-play-the-game.ipynb
Your code here: Amend the agent!, for col in valid_moves : ,exercise-play-the-game.ipynb
Check your answer,q_1.check () ,exercise-play-the-game.ipynb
Lines below will give you a hint or solution code,q_1.hint () ,exercise-play-the-game.ipynb
"2 An even smarter agentIn the previous question, you created an agent that selects winning moves. In this problem, you ll amend the code to create an agent that can also block its opponent from winning. In particular, your agent should: Select a winning move, if one is available. Otherwise, it selects a move to block the opponent from winning, if the opponent has a move that it can play in its next turn to win the game. If neither the agent nor the opponent can win in their next moves, the agent selects a random move.To help you with this exercise, you are encouraged to start with the agent from the previous exercise. To check if the opponent has a winning move, you can use the check winning move function, but you ll need to supply a different value for the piece argument. ","def agent_q2(obs , config): ",exercise-play-the-game.ipynb
Check your answer,q_2.check () ,exercise-play-the-game.ipynb
q 2.hint ,q_2.solution () ,exercise-play-the-game.ipynb
Check your answer Run this code cell to receive credit! ,q_3.solution () ,exercise-play-the-game.ipynb
"4 Play against an agentAmend the my agent function below to create your own agent. Feel free to copy an agent that you created above. Note that you ll have to include all of the necessary imports and helper functions. For an example of how this would look with the first agent you created in the exercise, take a look at this notebook.","def my_agent(obs , config): ",exercise-play-the-game.ipynb
Your code here: Amend the agent!, import random ,exercise-play-the-game.ipynb
Run this code cell to get credit for creating an agent,q_4.check () ,exercise-play-the-game.ipynb
"Run the next code cell to play a game round against the agent. To select a move, click on the game screen in the column where you d like to place a disc.After the game finishes, you can re run the code cell to play again!","from kaggle_environments import evaluate, make, utils

env = make(""connectx"", debug=True)
env.play([my_agent, None], width=500, height=450)",exercise-play-the-game.ipynb
"5 Submit to the competitionNow, it s time to make your first submission to the competition! Run the next code cell to write your agent to a submission file.",import inspect ,exercise-play-the-game.ipynb
Check that submission file was created,q_5.check () ,exercise-play-the-game.ipynb
Imports,import numpy as np ,exploring-time-series-plots-beginners-guide.ipynb
5 years,time = np.arange(5 * 365 + 1) ,exploring-time-series-plots-beginners-guide.ipynb
Trend,"def plot_series(time , series , format = ""-"" , start = 0, end = None , label = None , color = None):
 plt.plot(time[start:end] , series[start:end] , format , label = label , color = color)
 plt.xlabel(""Time"")
 plt.ylabel(""Value"")
 if label:
 plt.legend(fontsize = 14)
 plt.grid(True)

def trend(time , slope = 0 ):
 return slope * time",exploring-time-series-plots-beginners-guide.ipynb
Trend Plot 1,"slope = 0.1
series = trend(time , slope)
plt.figure(figsize = (20,6))
plot_series(time , series , color = ""purple"")
plt.title("" Trend Plot - 1"", fontdict = {'fontsize' : 20} )
plt.show()",exploring-time-series-plots-beginners-guide.ipynb
Trend Plot 2,"slope = -1
series = trend(time , slope)
plt.figure(figsize = (20,6))
plot_series(time , series , color = ""purple"")
plt.title("" Trend Plot - 2"", fontdict = {'fontsize' : 20} )
plt.show()",exploring-time-series-plots-beginners-guide.ipynb
Seasonality,"def seasonal_pattern(season_time):
 return np.where(season_time<0.45,
 np.cos(season_time* 2 *np.pi),
 1 / np.exp(3*season_time))

def seasonality(time , period , amplitude = 1 , phase = 0 ):
 season_time = ((time + phase) % period)/ period
 return amplitude *seasonal_pattern(season_time)",exploring-time-series-plots-beginners-guide.ipynb
Seasonality Plot 1,"amplitude = 40
series = seasonality(time , period = 365, amplitude = amplitude , phase = 0 )
plt.figure(figsize=(20,6))
plot_series(time , series, color = ""green"")
plt.title("" Seasonality Plot - 1"", fontdict = {'fontsize' : 20} )
plt.show()",exploring-time-series-plots-beginners-guide.ipynb
Seasonality Plot 2,"amplitude = 100
series = seasonality(time , period = 90, amplitude = amplitude , phase = 25 )
plt.figure(figsize=(20,6))
plot_series(time , series, color = ""green"")
plt.title("" Seasonality Plot - 2"", fontdict = {'fontsize' : 20} )
plt.show()",exploring-time-series-plots-beginners-guide.ipynb
Seasonality Trend Combined plot for seasonality and trend together,"baseline = 10
slope = 0.08
amplitude = 40
series = baseline + trend(time , slope) + seasonality(time , period = 365 , amplitude= amplitude)
plt.figure(figsize=(20,6))
plot_series(time , series, color = ""green"")
plt.title("" Seasonality + Trend Plot "", fontdict = {'fontsize' : 20} )
plt.show()",exploring-time-series-plots-beginners-guide.ipynb
Noise,"def white_noise(time, noise_level = 1 , seed = None):
 random = np.random.RandomState(seed)
 return random.random(len(time)) * noise_level",exploring-time-series-plots-beginners-guide.ipynb
Noise Plot,"noise_level = 10
noise = white_noise(time , noise_level , seed = RANDOM_SEED)
plt.figure(figsize=(20,6))
plot_series(time[:200] , noise[:200], color = ""blue"")
plt.title("" Noise Plot"", fontdict = {'fontsize' : 20} )
plt.show()",exploring-time-series-plots-beginners-guide.ipynb
"Noise Seasonality Trend Combined plot for noise , seasonality and trend together","series = baseline + trend(time , slope) + seasonality(time , period = 365 , amplitude= amplitude)
series += white_noise(time , noise_level = 10 , seed = RANDOM_SEED)
plt.figure(figsize=(20,6))
plot_series(time , series, color = ""blue"")
plt.title("" Noise + Seasonality + Trend Plot"", fontdict = {'fontsize' : 20} )
plt.show()",exploring-time-series-plots-beginners-guide.ipynb
Autocorrelation,"def autocorrelation_1(time , amplitude , seed = None):
 rnd = np.random.RandomState(seed)
 a1 = 0.5
 a2 = -0.1
 rnd_ar = rnd.randn(len(time) + 50)
 rnd_ar[:50] = 100
 for step in range(50, len(time) + 50 ):
 rnd_ar[step] += a1 * rnd_ar[step - 50]
 rnd_ar[step] += a2 * rnd_ar[step - 33]
 return rnd_ar[50:] * amplitude

def autocorrelation_2(time, amplitude, seed=None):
 rnd = np.random.RandomState(seed)
 a1 = 0.8
 ar = rnd.randn(len(time) + 1)
 for step in range(1, len(time) + 1):
 ar[step] += a1 * ar[step - 1]
 return ar[1:] * amplitude",exploring-time-series-plots-beginners-guide.ipynb
Autocorrelation Plot 1,"series = autocorrelation_1(time , amplitude , seed = RANDOM_SEED)
plt.figure(figsize=(20,6))
plot_series(time[:200] , series[:200] , color = ""red"" )
plt.title(""Autocorrelation Plot - 1 "", fontdict = {'fontsize' : 20} )
plt.show()",exploring-time-series-plots-beginners-guide.ipynb
Autocorrelation Plot 2,"series = autocorrelation_2(time , amplitude , seed = RANDOM_SEED)
plt.figure(figsize=(20,6))
plot_series(time[:200] , series[:200] , color = ""red"")
plt.title(""Autocorrelation Plot - 2 "", fontdict = {'fontsize' : 20} )
plt.show()",exploring-time-series-plots-beginners-guide.ipynb
Autocorrelation Trend Combined plot for Autocorrelation and trend together,"amplitude = 10
slope = 2
series = autocorrelation_1(time ,amplitude , seed = RANDOM_SEED) + trend(time , slope)
plt.figure(figsize=(20,6))
plot_series(time[:200], series[:200] , color = ""red"")
plt.title(""Autocorrelation + Trend Plot"", fontdict = {'fontsize' : 20} )
plt.show()",exploring-time-series-plots-beginners-guide.ipynb
"Autocorrelation Seasonality Trend Combined plot for Autocorrelation, seasonality and trend together","amplitude = 10
slope = 2
series = autocorrelation_1(time , amplitude , seed = RANDOM_SEED)+ seasonality(time , period= 50 , amplitude = 150) + trend(time , slope)
plt.figure(figsize=(20,6))
plot_series(time[:300], series[:300] , color = ""red"")
plt.title(""Autocorrelation + Seasonality + Trend Plot"", fontdict = {'fontsize' : 20} )
plt.show()",exploring-time-series-plots-beginners-guide.ipynb
"Autocorrelation Seasonality Noise Trend Combined plot for Autocorrelation, seasonality, noise and trend together","amplitude = 10
slope = 2
series = autocorrelation_1(time , amplitude , seed = RANDOM_SEED)+ seasonality(time , period= 50 , amplitude = 150) + trend(time , slope)
series += white_noise(time , noise_level=100)
plt.figure(figsize=(20,6))
plot_series(time[:300], series[:300] , color = ""red"")
plt.title(""Autocorrelation + Seasonality + Noise + Trend Plot"", fontdict = {'fontsize' : 20} )
plt.show()",exploring-time-series-plots-beginners-guide.ipynb
"Break Point plot at time 200 Combined plot for Autocorrelation, seasonality, noise ,trend and break point together","amplitude1 = 10
amplitude2 = 5
slope1 = 2
slope2 = -2
series1 = autocorrelation_2(time, amplitude1, seed=RANDOM_SEED) + seasonality(time, period=50, amplitude=150) + trend(time, slope1)
series2 = autocorrelation_2(time, amplitude2, seed=RANDOM_SEED) + seasonality(time, period=50, amplitude=2) + trend(time, slope2) + 750+ white_noise(time , 30) 
series1[200:] = series2[200:]
series1 += white_noise(time, noise_level=100 , seed= RANDOM_SEED)
plt.figure(figsize=(20,6))
plot_series(time[:300], series1[:300] , color = ""indigo"")
plt.axvline(x=200 , color = ""black"")
plt.title(""Autocorrelation + Seasonality + Noise + Trend + Break Point Plot"", fontdict = {'fontsize' : 20} )
plt.show()",exploring-time-series-plots-beginners-guide.ipynb
linear algebra,import numpy as np ,facial-key-points-detection-preprocessing-guide.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,facial-key-points-detection-preprocessing-guide.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,facial-key-points-detection-preprocessing-guide.ipynb
"You can also write temporary files to kaggle temp , but they won t be saved outside of the current session","import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf",facial-key-points-detection-preprocessing-guide.ipynb
Check for null values,"print(f'Feature \t\t\t Missing \t Percentage missing\n')
for k,v in train.isna().sum().items():
 print(f'{k !s:30} :{v :8} \t {round(v/len(train),2)}%')",facial-key-points-detection-preprocessing-guide.ipynb
Lets split the data into two parts Good data data that has no missing fields Bad data that has missing feilds we ll later inpute the missing Lets start by working with good data and create a baseline model ,"good_data = train.dropna()
bad_data = train.drop(index=good_data.index)",facial-key-points-detection-preprocessing-guide.ipynb
Dataset information,good_data.info(),facial-key-points-detection-preprocessing-guide.ipynb
IMAGES,good_data['Image']. head () ,facial-key-points-detection-preprocessing-guide.ipynb
Images and targets Lets create an util that accept a csv file and give us back the images and targets,def get_images(data): ,facial-key-points-detection-preprocessing-guide.ipynb
prepare the image," for img in iter(data.loc[: , 'Image']) : ",facial-key-points-detection-preprocessing-guide.ipynb
lets create an util that will display the images,"def display_images(img , feat): ",facial-key-points-detection-preprocessing-guide.ipynb
lets test visualization,"display_images(images[0], targets[0]) ",facial-key-points-detection-preprocessing-guide.ipynb
lets create an util to display the augumentation,"def display_augmentation(img , feat , img_f , feat_f): ",facial-key-points-detection-preprocessing-guide.ipynb
sample image to test augmentation,"image , feature = images[0]. copy (), targets[0]. copy () ",facial-key-points-detection-preprocessing-guide.ipynb
Flipping,flipped_img = np.fliplr(image),facial-key-points-detection-preprocessing-guide.ipynb
lets make an util that give us flipped images and targets,"def flipping_augmentation(images , features): ",facial-key-points-detection-preprocessing-guide.ipynb
lets see one more example,"flip_imgs , flip_feats = flipping_augmentation(images[: 5], targets[: 5]) ",facial-key-points-detection-preprocessing-guide.ipynb
Cropping,"cropped_image = image.copy()

cropped_image[:,:10] = 0
cropped_image[:,86:] = 0
cropped_image[:10,:] = 0
cropped_image[86:,:] = 0


cropped_image.shape",facial-key-points-detection-preprocessing-guide.ipynb
lets make an util that give us cropped images and targets,"def crop_augmentation(images , targets): ",facial-key-points-detection-preprocessing-guide.ipynb
Rotation,"from scipy import ndimage, misc

img_45 = ndimage.rotate(image, 45, reshape=False)
plt.imshow(img_45 , cmap=plt.cm.gray);",facial-key-points-detection-preprocessing-guide.ipynb
shift points in the plane so that the center of rotation is at the origin, points = points - 48 ,facial-key-points-detection-preprocessing-guide.ipynb
rotation matrix, theta = np.radians(angle) ,facial-key-points-detection-preprocessing-guide.ipynb
rotate the points," for i in range(0 , len(points), 2): ",facial-key-points-detection-preprocessing-guide.ipynb
shift again so the origin goes back to the desired center of rotation, points = points + 48 ,facial-key-points-detection-preprocessing-guide.ipynb
rotated feature points,"feat_45 = rotate_points(feature , 45) ",facial-key-points-detection-preprocessing-guide.ipynb
lets see the rotation,"display_augmentation(image , feature , img_45 , feat_45) ",facial-key-points-detection-preprocessing-guide.ipynb
lets make an util that give us rotated images and targets,"def rotate_augmentation(images , features , angle): ",facial-key-points-detection-preprocessing-guide.ipynb
check once more using a different angle,"img_rot , feat_rot = rotate_augmentation(images[: 5], targets[: 5 , :], angle = - 45) ",facial-key-points-detection-preprocessing-guide.ipynb
Brightness,"img = image.copy()
img = np.clip(img*2.5,0,255)
plt.imshow(img, cmap='gray');",facial-key-points-detection-preprocessing-guide.ipynb
lets create an util that adds brightness,"def brightness_augmentation(images , features , factor = 1.5): ",facial-key-points-detection-preprocessing-guide.ipynb
lets see one more example,"img , feat = brightness_augmentation(images[: 5], targets[: 5], factor = 2.0) ",facial-key-points-detection-preprocessing-guide.ipynb
Adding noise,"img = image.copy()
noise = np.random.randint(low=0, high=255, size=img.shape)
factor = 0.25
plt.imshow(img+(noise*factor), cmap='gray');",facial-key-points-detection-preprocessing-guide.ipynb
lets create an utility that adds noise to the image,"def noise_augmentation(images , features , factor): ",facial-key-points-detection-preprocessing-guide.ipynb
lets see one another example,"img , feat = noise_augmentation(images[: 5], targets[: 5], factor = 0.15) ",facial-key-points-detection-preprocessing-guide.ipynb
"images, targets get X y good data ","print('Shape of image data' , images.shape) ",facial-key-points-detection-preprocessing-guide.ipynb
ADDING AUGMENTATION,"def augmentation(img , feat , method): ",facial-key-points-detection-preprocessing-guide.ipynb
flip,"method = flipping_augmentation(images , targets) ",facial-key-points-detection-preprocessing-guide.ipynb
crop,"method = crop_augmentation(images , targets) ",facial-key-points-detection-preprocessing-guide.ipynb
rotate,"for theta in[10 , 15 , - 10 , - 15]: ",facial-key-points-detection-preprocessing-guide.ipynb
brightness,"method = brightness_augmentation(images , targets , factor = 2.0) ",facial-key-points-detection-preprocessing-guide.ipynb
noise,"method = noise_augmentation(images , targets , factor = 0.2) ",facial-key-points-detection-preprocessing-guide.ipynb
just for visual,for k in augmentation_functions.keys (): ,facial-key-points-detection-preprocessing-guide.ipynb
lets check our data one last time before we start building models,"def visualize_data(images , targets): ",facial-key-points-detection-preprocessing-guide.ipynb
"ModelingMobileNetV2 is very similar to the original MobileNet, except that it uses inverted residual blocks with bottlenecking features. It has a drastically lower parameter count than the original MobileNet. MobileNets support any input size greater than 32 x 32, with larger image sizes offering better performance.","pretrained_model = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape =(96 , 96 , 3), include_top = False , weights = 'imagenet') ",facial-key-points-detection-preprocessing-guide.ipynb
"pretrained model tf.keras.applications.resnet v2.ResNet50V2 input shape 96,96,3 ,include top False,weights imagenet ",pretrained_model.trainable = False ,facial-key-points-detection-preprocessing-guide.ipynb
"Each Keras Application expects a specific kind of input preprocessing. For MobileNetV2, call tf.keras.applications.mobilenet v2.preprocess input on your inputs before passing them to the model. mobilenet v2.preprocess input will scale input pixels between 1 and 1.",augmented_images = tf.keras.applications.mobilenet_v2.preprocess_input(augmented_images) ,facial-key-points-detection-preprocessing-guide.ipynb
lets check our data to make sure everything is fine,"visualize_data(augmented_images , augmented_targets) ",facial-key-points-detection-preprocessing-guide.ipynb
lets create a dataset for training and validation,"ds = tf.data.Dataset.from_tensor_slices(( augmented_images , augmented_targets)) ",facial-key-points-detection-preprocessing-guide.ipynb
create a preprocessing layer,class ImageTile(tf.keras.layers.Layer): ,facial-key-points-detection-preprocessing-guide.ipynb
decaying learing rate,def decay_lr(epoch): ,facial-key-points-detection-preprocessing-guide.ipynb
PredictionLets use this model to make prediction,test.head(),facial-key-points-detection-preprocessing-guide.ipynb
Predictions ,test_preds = model.predict(test_images),facial-key-points-detection-preprocessing-guide.ipynb
Submission,"feature_names = list(lookup['FeatureName'])
image_ids = list(lookup['ImageId']-1)
row_ids = list(lookup['RowId'])

feature_list = []
for feature in feature_names:
 feature_list.append(feature_names.index(feature))
 
predictions = []
for x,y in zip(image_ids, feature_list):
 predictions.append(test_preds[x][y])
 
row_ids = pd.Series(row_ids, name = 'RowId')
locations = pd.Series(predictions, name = 'Location')
locations = locations.clip(0.0,96.0)
submission_result = pd.concat([row_ids,locations],axis = 1)
submission_result.to_csv('submission.csv',index = False)",facial-key-points-detection-preprocessing-guide.ipynb
linear algebra,import numpy as np ,facial-key-points-detection-using-residual-network.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,facial-key-points-detection-using-residual-network.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,facial-key-points-detection-using-residual-network.ipynb
Import the necessary packages,import pandas as pd ,facial-key-points-detection-using-residual-network.ipynb
Reading Data,"keyfacial_df = pd.read_csv('../input/facial-keypoints-detection/training.zip', compression='zip', header=0, sep=',', quotechar='""')
test_data = pd.read_csv('../input/facial-keypoints-detection/test.zip', compression='zip', header=0, sep=',', quotechar='""')
IdLookupTable = pd.read_csv('../input/facial-keypoints-detection/IdLookupTable.csv',header=0, sep=',', quotechar='""')
SampleSubmission = pd.read_csv('../input/facial-keypoints-detection/SampleSubmission.csv',header=0, sep=',', quotechar='""')",facial-key-points-detection-using-residual-network.ipynb
Missing values,keyfacial_df.isnull().sum(),facial-key-points-detection-using-residual-network.ipynb
Reshape Image,keyfacial_df['Image'].shape,facial-key-points-detection-using-residual-network.ipynb
"we access their value using .loc command, which get the values for coordinates of the image based on the column it is refering to.","i = np.random.randint(1 , len(keyfacial_df)) ",facial-key-points-detection-using-residual-network.ipynb
Let s view more images in a grid format,"fig = plt.figure(figsize =(20 , 20)) ",facial-key-points-detection-using-residual-network.ipynb
Creating a new copy of the dataframe,import copy ,facial-key-points-detection-using-residual-network.ipynb
horizontal flip,"keyfacial_df_copy['Image']= keyfacial_df_copy['Image']. apply(lambda x : np.flip(x , axis = 1)) ",facial-key-points-detection-using-residual-network.ipynb
"only x cordinate values will change ,So Subtract our x coordinate values from width of the image 96",for i in range(len(columns)) : ,facial-key-points-detection-using-residual-network.ipynb
Original image,"plt.imshow(keyfacial_df['Image'][ 1], cmap = 'gray') ",facial-key-points-detection-using-residual-network.ipynb
horizontal flip image,"plt.imshow(keyfacial_df_copy['Image'][ 1], cmap = 'gray') ",facial-key-points-detection-using-residual-network.ipynb
Concatenating the original dataframe with the augmented dataframe,"augmented_df = np.concatenate(( keyfacial_df , keyfacial_df_copy)) ",facial-key-points-detection-using-residual-network.ipynb
we clip the value between 0 and 255,import random ,facial-key-points-detection-using-residual-network.ipynb
Image with increased brightness,"plt.imshow(keyfacial_df_copy['Image'][ 1], cmap = 'gray') ",facial-key-points-detection-using-residual-network.ipynb
horizontal flip,"keyfacial_df_copy['Image']= keyfacial_df_copy['Image']. apply(lambda x : np.flip(x , axis = 0)) ",facial-key-points-detection-using-residual-network.ipynb
values of Image,"img = augmented_df[: , 30] ",facial-key-points-detection-using-residual-network.ipynb
Normalize the image,img = img / 255. ,facial-key-points-detection-using-residual-network.ipynb
"empty array to feed the model of shape 96,96,1 ","X = np.empty(( len(img), 96 , 96 , 1)) ",facial-key-points-detection-using-residual-network.ipynb
"expanding dimensions to 96,96,1 ",for i in range(len(img)) : ,facial-key-points-detection-using-residual-network.ipynb
Converting array type to float,X = np.asarray(X). astype(np.float32) ,facial-key-points-detection-using-residual-network.ipynb
Residual Neural Network,"def res_block(X , filter , stage): ",facial-key-points-detection-using-residual-network.ipynb
Convolutional block, X_copy = X ,facial-key-points-detection-using-residual-network.ipynb
Main Path," X = Conv2D(f1 ,(1 , 1), strides =(1 , 1), name = 'res_' + str(stage)+ '_conv_a' , kernel_initializer = glorot_uniform(seed = 0))(X) ",facial-key-points-detection-using-residual-network.ipynb
Short path," X_copy = Conv2D(f3 , kernel_size =(1 , 1), strides =(1 , 1), name = 'res_' + str(stage)+ '_conv_copy' , kernel_initializer = glorot_uniform(seed = 0))(X_copy) ",facial-key-points-detection-using-residual-network.ipynb
ADD," X = Add ()([X , X_copy]) ",facial-key-points-detection-using-residual-network.ipynb
Identity Block 1, X_copy = X ,facial-key-points-detection-using-residual-network.ipynb
Main Path," X = Conv2D(f1 ,(1 , 1), strides =(1 , 1), name = 'res_' + str(stage)+ '_identity_1_a' , kernel_initializer = glorot_uniform(seed = 0))(X) ",facial-key-points-detection-using-residual-network.ipynb
ADD," X = Add ()([X , X_copy]) ",facial-key-points-detection-using-residual-network.ipynb
Identity Block 2, X_copy = X ,facial-key-points-detection-using-residual-network.ipynb
Main Path," X = Conv2D(f1 ,(1 , 1), strides =(1 , 1), name = 'res_' + str(stage)+ '_identity_2_a' , kernel_initializer = glorot_uniform(seed = 0))(X) ",facial-key-points-detection-using-residual-network.ipynb
ADD," X = Add ()([X , X_copy]) ",facial-key-points-detection-using-residual-network.ipynb
Input tensor shape,X_input = Input(input_shape) ,facial-key-points-detection-using-residual-network.ipynb
Zero padding,"X = ZeroPadding2D(( 3 , 3))(X_input) ",facial-key-points-detection-using-residual-network.ipynb
1 stage,"X = Conv2D(64 ,(7 , 7), strides =(2 , 2), name = 'conv1' , kernel_initializer = glorot_uniform(seed = 0))(X) ",facial-key-points-detection-using-residual-network.ipynb
2 stage,"X = res_block(X , filter =[64 , 64 , 256], stage = 2) ",facial-key-points-detection-using-residual-network.ipynb
3 stage,"X = res_block(X , filter =[128 , 128 , 512], stage = 3) ",facial-key-points-detection-using-residual-network.ipynb
Average Pooling,"X = AveragePooling2D(( 2 , 2), name = 'Averagea_Pooling')( X) ",facial-key-points-detection-using-residual-network.ipynb
Final layer,X = Flatten ()( X) ,facial-key-points-detection-using-residual-network.ipynb
Compile and Traning,"adam = tf.keras.optimizers.Adam(learning_rate = 0.0001, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)
model_1_facialKeyPoints.compile(loss = ""mean_squared_error"", optimizer = adam , metrics = ['accuracy'])",facial-key-points-detection-using-residual-network.ipynb
save the best model with least validation loss,"checkpointer = ModelCheckpoint(filepath = ""FacialKeyPoints_weights.hdf5"" , verbose = 1 , save_best_only = True) ",facial-key-points-detection-using-residual-network.ipynb
Import necessary packages,"import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import cv2
from math import sin, cos, pi

from keras.applications import ResNet50
from keras.layers import Conv2D, LeakyReLU, GlobalAveragePooling2D, Dropout, Dense
from keras.models import Sequential",facial-keypoint-detection-cnn-augmentation.ipynb
Loading Data,"!unzip ../input/facial-keypoints-detection/training.zip
!unzip ../input/facial-keypoints-detection/test.zip",facial-keypoint-detection-cnn-augmentation.ipynb
Training file include 31 columns represents for 30 features and 1 for the image,test_file.head(),facial-keypoint-detection-cnn-augmentation.ipynb
Check for the null values,train_file.isnull().sum(),facial-keypoint-detection-cnn-augmentation.ipynb
we use this for augmentation,clean_train_file = train_file.dropna () ,facial-keypoint-detection-cnn-augmentation.ipynb
Load images and keypints,"def load_images(image_data):
 images = []
 for idx, sample in image_data.iterrows():
 image = np.array(sample['Image'].split(' '), dtype=int)
 image = np.reshape(image, (96,96,1))
 images.append(image)
 images = np.array(images)/255.
 return images

def load_keypoints(keypoint_data):
 keypoint_data = keypoint_data.drop(['Image'], axis=1)
 keypoint_features = []
 for idx, features in keypoint_data.iterrows():
 keypoint_features.append(features)
 keypoint_features = np.array(keypoint_features, dtype=float)
 return keypoint_features

train_images = load_images(train_file)
images = load_images(clean_train_file)
train_keypoints = load_keypoints(train_file)
keypoints = load_keypoints(clean_train_file)
test_images = load_images(test_file)",facial-keypoint-detection-cnn-augmentation.ipynb
Augmentation,"class aug_config:
 rotation_augmentation = True
 brightness_augmentation = True
 shift_augmentation = True
 random_noise_augmentation = True
 rotation_angles = [15]
 pixel_shifts = [15]",facial-keypoint-detection-cnn-augmentation.ipynb
Rotation,"def rotate_augmentation(images, keypoints, rotation_angles):
 rotated_images = []
 rotated_keypoints = []
 for angle in rotation_angles:
 for angle in [angle, -angle]:
 M = cv2.getRotationMatrix2D((48,48), angle, 1.)
 angle_rad = -angle*pi/180.
 for image in images:
 rotated_image = cv2.warpAffine(image, M, (96,96), flags=cv2.INTER_CUBIC)
 rotated_images.append(rotated_image)
 for keypoint in keypoints:
 rotated_keypoint = keypoint - 48.
 for idx in range(0, len(rotated_keypoint), 2):
 rotated_keypoint[idx] = rotated_keypoint[idx]*cos(angle_rad)-rotated_keypoint[idx+1]*sin(angle_rad)
 rotated_keypoint[idx+1] = rotated_keypoint[idx]*sin(angle_rad)+rotated_keypoint[idx+1]*cos(angle_rad)
 rotated_keypoint += 48. 
 rotated_keypoints.append(rotated_keypoint)
 
 return np.reshape(rotated_images,(-1,96,96,1)), rotated_keypoints

if aug_config.rotation_augmentation:
 rotated_train_images, rotated_train_keypoints = rotate_augmentation(images, keypoints, aug_config.rotation_angles)
 train_images = np.concatenate((train_images, rotated_train_images))
 train_keypoints = np.concatenate((train_keypoints, rotated_train_keypoints))
 fig, axis = plt.subplots()
 plot_sample(rotated_train_images[19], rotated_train_keypoints[19], axis, ""Rotation Augmentation"")",facial-keypoint-detection-cnn-augmentation.ipynb
Change Brightness,"def alter_brightness(images, keypoints):
 altered_brightness_images = []
 inc_brightness_images = np.clip(images*1.2, 0.0, 1.0) 
 dec_brightness_images = np.clip(images*0.6, 0.0, 1.0) 
 altered_brightness_images.extend(inc_brightness_images)
 altered_brightness_images.extend(dec_brightness_images)
 return altered_brightness_images, np.concatenate((keypoints, keypoints))

if aug_config.brightness_augmentation:
 altered_brightness_images, altered_brightness_keypoints = alter_brightness(images, keypoints)
 train_images = np.concatenate((train_images, altered_brightness_images))
 train_keypoints = np.concatenate((train_keypoints, altered_brightness_keypoints))
 fig, axis = plt.subplots()
 plot_sample(altered_brightness_images[19], altered_brightness_keypoints[19], axis, ""Alter Brightness Augmentation"")",facial-keypoint-detection-cnn-augmentation.ipynb
Shift images,"def shift_images(images, keypoints, pixel_shifts):
 shifted_images = []
 shifted_keypoints = []
 for shift in pixel_shifts: 
 for (shift_x,shift_y) in [(-shift,-shift),(-shift,shift),(shift,-shift),(shift,shift)]:
 M = np.float32([[1,0,shift_x],[0,1,shift_y]])
 for image, keypoint in zip(images, keypoints):
 shifted_image = cv2.warpAffine(image, M, (96,96), flags=cv2.INTER_CUBIC)
 shifted_keypoint = np.array([(point+shift_x) if idx%2==0 else (point+shift_y) for idx, point in enumerate(keypoint)])
 if np.all(0.0<shifted_keypoint) and np.all(shifted_keypoint<96.0):
 shifted_images.append(shifted_image.reshape(96,96,1))
 shifted_keypoints.append(shifted_keypoint)
 shifted_keypoints = np.clip(shifted_keypoints,0.0,96.0)
 return shifted_images, shifted_keypoints

if aug_config.shift_augmentation:
 shifted_train_images, shifted_train_keypoints = shift_images(images, keypoints, aug_config.pixel_shifts)
 train_images = np.concatenate((train_images, shifted_train_images))
 train_keypoints = np.concatenate((train_keypoints, shifted_train_keypoints))
 fig, axis = plt.subplots()
 plot_sample(shifted_train_images[19], shifted_train_keypoints[19], axis, ""Shift Augmentation"")",facial-keypoint-detection-cnn-augmentation.ipynb
Add noise,def add_noise(images): ,facial-keypoint-detection-cnn-augmentation.ipynb
"Adding random normal noise to the input image clip the resulting noisy image between 1,1 "," noisy_image = cv2.add(image , 0.008 * np.random.randn(96 , 96 , 1)) ",facial-keypoint-detection-cnn-augmentation.ipynb
Modeling,"model = Sequential()
pretrained_model = ResNet50(input_shape=(96,96,3), include_top=False, weights='imagenet')
pretrained_model.trainable = True

model.add(Conv2D(3, (1,1), padding='same', input_shape=(96,96,1)))
model.add(LeakyReLU(alpha=0.1))
model.add(pretrained_model)
model.add(GlobalAveragePooling2D())
model.add(Dropout(0.1))
model.add(Dense(30))
model.summary()",facial-keypoint-detection-cnn-augmentation.ipynb
Training the Model,"from keras.callbacks import EarlyStopping, ReduceLROnPlateau
earlyStopping = EarlyStopping(monitor='loss', patience=30, mode='min',
 baseline=None)

rlp = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=1e-15, mode='min', verbose=1)

model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])

history = model.fit(train_images, train_keypoints, epochs=200, batch_size=64, validation_split=0.15, callbacks=[earlyStopping, rlp])",facial-keypoint-detection-cnn-augmentation.ipynb
Predicting on Test Set and Submission,test_preds = model.predict(test_images),facial-keypoint-detection-cnn-augmentation.ipynb
linear algebra,import numpy as np ,facial-keypoint-detection-udacity.ipynb
"For example, running this by clicking run or pressing Shift Enter will list the files in the input directory",import os ,facial-keypoint-detection-udacity.ipynb
Any results you write to the current directory are saved as output.,from pandas.io.parsers import read_csv ,facial-keypoint-detection-udacity.ipynb
"I changed the X dimension structure to have Nsample, Nrows in frame, N columns in frame, 1 in load2d.","def load2d(test = False , cols = None): ",facial-keypoint-detection-udacity.ipynb
A Fully connected model,"%%time
from keras.models import Sequential
from keras.layers import Dense, Activation
from keras.optimizers import SGD
from keras.layers import Dropout

model = Sequential()
model.add(Dense(128,input_dim=X.shape[1]))
model.add(Activation('relu'))
model.add(Dropout(0.1))
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dense(30))

model.summary()",facial-keypoint-detection-udacity.ipynb
converting the images back to 96 96 pixels so i can check the performance of my model on the image dataset,"def plot_sample(x , y , axis): ",facial-keypoint-detection-udacity.ipynb
creates a HDF5 file my model.h5 ,model.save('my_model.h5') ,facial-keypoint-detection-udacity.ipynb
 identical to the previous one,model = load_model('my_model.h5') ,facial-keypoint-detection-udacity.ipynb
Convolutional Neural Network,"X,y = load2d()",facial-keypoint-detection-udacity.ipynb
model.add Activation relu ," model2.add(MaxPooling2D(pool_size =(2 , 2), strides =(2 , 2), border_mode = ""valid"")) ",facial-keypoint-detection-udacity.ipynb
model.add Activation relu ," model2.add(MaxPooling2D(pool_size =(2 , 2), strides =(2 , 2), border_mode = ""valid"")) ",facial-keypoint-detection-udacity.ipynb
model.add Activation relu ," model2.add(MaxPooling2D(pool_size =(2 , 2), strides =(2 , 2), border_mode = ""valid"")) ",facial-keypoint-detection-udacity.ipynb
Comparing model1 and model2,"plt.figure(figsize =(4 , 4)) ",facial-keypoint-detection-udacity.ipynb
Comparing model1 and model2 on images,"fig = plt.figure(figsize =(10 , 10)) ",facial-keypoint-detection-udacity.ipynb
creates a HDF5 file my model.h5 ,model2.save('my_model2.h5') ,facial-keypoint-detection-udacity.ipynb
 identical to the previous one,model2 = load_model('my_model2.h5') ,facial-keypoint-detection-udacity.ipynb
the keypoints on the face,from keras.preprocessing.image import ImageDataGenerator ,facial-keypoint-detection-udacity.ipynb
splitting the data,from sklearn.model_selection import train_test_split ,facial-keypoint-detection-udacity.ipynb
"Comparing mode1, model2 and model3 using pyplot","plt.figure(figsize =(8 , 8)) ",facial-keypoint-detection-udacity.ipynb
creates a HDF5 file my model.h5 ,model3.save('my_model3.h5') ,facial-keypoint-detection-udacity.ipynb
 identical to the previous one,model3 = load_model('my_model3.h5') ,facial-keypoint-detection-udacity.ipynb
"Specialist SettingI have divided my dataset into 6 different groups. I will train my model on each of these 6 groups separately. All 6 models contains the same CNN architecture but the final output layer is adjusted for different number of outputs: for example we have a model for left eye and right eye center landmark prediction. As there are x and y coordinates for both eye centers, we have 4 nodes in the output layer of this model.","SPECIALIST_SETTINGS = [
 dict(
 columns=(
 'left_eye_center_x', 'left_eye_center_y',
 'right_eye_center_x', 'right_eye_center_y',
 ),
 flip_indices=((0, 2), (1, 3)),
 ),

 dict(
 columns=(
 'nose_tip_x', 'nose_tip_y',
 ),
 flip_indices=(),
 ),

 dict(
 columns=(
 'mouth_left_corner_x', 'mouth_left_corner_y',
 'mouth_right_corner_x', 'mouth_right_corner_y',
 'mouth_center_top_lip_x', 'mouth_center_top_lip_y',
 ),
 flip_indices=((0, 2), (1, 3)),
 ),

 dict(
 columns=(
 'mouth_center_bottom_lip_x',
 'mouth_center_bottom_lip_y',
 ),
 flip_indices=(),
 ),

 dict(
 columns=(
 'left_eye_inner_corner_x', 'left_eye_inner_corner_y',
 'right_eye_inner_corner_x', 'right_eye_inner_corner_y',
 'left_eye_outer_corner_x', 'left_eye_outer_corner_y',
 'right_eye_outer_corner_x', 'right_eye_outer_corner_y',
 ),
 flip_indices=((0, 2), (1, 3), (4, 6), (5, 7)),
 ),

 dict(
 columns=(
 'left_eyebrow_inner_end_x', 'left_eyebrow_inner_end_y',
 'right_eyebrow_inner_end_x', 'right_eyebrow_inner_end_y',
 'left_eyebrow_outer_end_x', 'left_eyebrow_outer_end_y',
 'right_eyebrow_outer_end_x', 'right_eyebrow_outer_end_y',
 ),
 flip_indices=((0, 2), (1, 3), (4, 6), (5, 7)),
 ),
 ]
",facial-keypoint-detection-udacity.ipynb
Training special model with model3,from collections import OrderedDict ,facial-keypoint-detection-udacity.ipynb
prediction with model 3,y_pred3 = model3.predict(X_test) ,facial-keypoint-detection-udacity.ipynb
prediction with specialist model,"def predict_specialist(specialists1 , X_test): ",facial-keypoint-detection-udacity.ipynb
Creating submission files for both the models,FIdLookup = '../input/IdLookupTable.csv' ,facial-keypoint-detection-udacity.ipynb
plt.show ,"df_y_pred_s = df_y_pred_s[df_y_pred3.columns]
df_compare = {}
df_compare[""difference""] = ((df_y_pred_s - df_y_pred3)**2).mean(axis=1)
df_compare[""RowId""] = range(df_y_pred_s.shape[0])
df_compare = DataFrame(df_compare)
df_compare = df_compare.sort_values(""difference"",ascending=False)",facial-keypoint-detection-udacity.ipynb
good model 3," ax = fig.add_subplot(Nsample , 4 , count , xticks = [], yticks = []) ",facial-keypoint-detection-udacity.ipynb
good special," ax = fig.add_subplot(Nsample , 4 , count , xticks = [], yticks = []) ",facial-keypoint-detection-udacity.ipynb
bad model 3," ax = fig.add_subplot(Nsample , 4 , count , xticks = [], yticks = []) ",facial-keypoint-detection-udacity.ipynb
bad special," ax = fig.add_subplot(Nsample , 4 , count , xticks = [], yticks = []) ",facial-keypoint-detection-udacity.ipynb
Training special model with model2,"%%time
specialists2 = fit_specialists(freeze=True,
 print_every=10,
 epochs=100,
 name_transfer_model=""my_model2.h5"")",facial-keypoint-detection-udacity.ipynb
prediction with model 2,y_pred2 = model2.predict(X_test) ,facial-keypoint-detection-udacity.ipynb
Creating submission files for both the models,FIdLookup = '../input/IdLookupTable.csv' ,facial-keypoint-detection-udacity.ipynb
good model 2," ax = fig.add_subplot(Nsample , 4 , count , xticks = [], yticks = []) ",facial-keypoint-detection-udacity.ipynb
good special," ax = fig.add_subplot(Nsample , 4 , count , xticks = [], yticks = []) ",facial-keypoint-detection-udacity.ipynb
bad model 2," ax = fig.add_subplot(Nsample , 4 , count , xticks = [], yticks = []) ",facial-keypoint-detection-udacity.ipynb
bad special," ax = fig.add_subplot(Nsample , 4 , count , xticks = [], yticks = []) ",facial-keypoint-detection-udacity.ipynb
checking where the files are,import os ,facial-keypoints-detection-basic-keras-model.ipynb
importing the rquired libraries,import pandas as pd ,facial-keypoints-detection-basic-keras-model.ipynb
filling the nan values,"train.fillna(method = 'ffill' , inplace = True) ",facial-keypoints-detection-basic-keras-model.ipynb
Preparing the training data,"X = train.Image.values
del train['Image']
Y = train.values",facial-keypoints-detection-basic-keras-model.ipynb
Splitting the data into 90 10 train test split,"from sklearn.model_selection import train_test_split as tts
x_train,x_test,y_train,y_test = tts(x,Y,random_state = 69,test_size = 0.1)",facial-keypoints-detection-basic-keras-model.ipynb
Model,"from keras.layers.advanced_activations import LeakyReLU
from keras.models import Sequential, Model
from keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D",facial-keypoints-detection-basic-keras-model.ipynb
model.add BatchNormalization ,model.add(LeakyReLU(alpha = 0.1)) ,facial-keypoints-detection-basic-keras-model.ipynb
return K.sqrt K.mean K.square y pred y true ,"model.compile(optimizer = 'adam',loss = 'mean_squared_error', metrics = ['mae','acc'])
model.fit(x_train,y_train,batch_size=256, epochs=50,validation_data=(x_test,y_test))",facial-keypoints-detection-basic-keras-model.ipynb
Training on the complete Dataset now,"model.compile(optimizer = 'adam',loss = 'mean_squared_error', metrics = ['mae'])
model.fit(x,Y,batch_size=64, epochs=100)
model.fit(x,Y,batch_size=128, epochs=50)
model.fit(x,Y,batch_size=256, epochs=50)",facial-keypoints-detection-basic-keras-model.ipynb
Predicting for test data,"test = pd.read_csv(""/kaggle/input/test/test.csv"")
test.head()",facial-keypoints-detection-basic-keras-model.ipynb
Should be version 0.4.6,import albumentations as A ,facial-keypoints-detection-keras-albumentations.ipynb
Reproducible results set to None if you want to keep the randomness ,import os ,facial-keypoints-detection-keras-albumentations.ipynb
 Data extraction ,"!mkdir ../data/
!unzip -q /kaggle/input/facial-keypoints-detection/training.zip -d ../data/train
!unzip -q /kaggle/input/facial-keypoints-detection/test.zip -d ../data/test",facial-keypoints-detection-keras-albumentations.ipynb
Helper functions of this section,def format_dataset(dataframe): ,facial-keypoints-detection-keras-albumentations.ipynb
Read data,train_dir = '../data/train/training.csv' ,facial-keypoints-detection-keras-albumentations.ipynb
Data overview,train_data.sample(5). T ,facial-keypoints-detection-keras-albumentations.ipynb
Check the target keypoints statistics,train_data.describe () ,facial-keypoints-detection-keras-albumentations.ipynb
Check for missing training data,print(f'Train sample: {len(train_data)}') ,facial-keypoints-detection-keras-albumentations.ipynb
Solution 3 Replace NaN values with each feature median,"train_data.fillna(train_data.describe (). T['50%'], inplace = True) ",facial-keypoints-detection-keras-albumentations.ipynb
Check imputed data,train_data.sample(5). T ,facial-keypoints-detection-keras-albumentations.ipynb
Format the data,"X_train , y_train = format_dataset(train_data) ",facial-keypoints-detection-keras-albumentations.ipynb
Display a random subset of training samples,"show_random_samples(X_train , y_train) ",facial-keypoints-detection-keras-albumentations.ipynb
Helper functions of this section,"def show_random_preds(model , X , n_rows = 2 , n_cols = 5): ",facial-keypoints-detection-keras-albumentations.ipynb
Architecture 1,def create_small_dense_network (): ,facial-keypoints-detection-keras-albumentations.ipynb
Train model,model1 = create_small_dense_network () ,facial-keypoints-detection-keras-albumentations.ipynb
Show training and validation loss,"plot_loss(hist1 , metric = 'mae') ",facial-keypoints-detection-keras-albumentations.ipynb
Visualize prediction on random test samples,model1.load_weights('best_model1.h5') ,facial-keypoints-detection-keras-albumentations.ipynb
Architecture 2,def create_convnet(n_outputs = 30): ,facial-keypoints-detection-keras-albumentations.ipynb
Train model,model2 = create_convnet () ,facial-keypoints-detection-keras-albumentations.ipynb
1.377 1.298,"plot_loss(hist2 , metric = 'mae') ",facial-keypoints-detection-keras-albumentations.ipynb
Visualize prediction on random test samples,model2.load_weights('best_model2.h5') ,facial-keypoints-detection-keras-albumentations.ipynb
Helper functions of this section,class DataLoader(Sequence): ,facial-keypoints-detection-keras-albumentations.ipynb
Convert grayscale to RGB if needed if you want to use a pre trained ResNet for example , if self.as_rgb : ,facial-keypoints-detection-keras-albumentations.ipynb
Apply transformations on both images and keypoints, if self.augment is not None : ,facial-keypoints-detection-keras-albumentations.ipynb
Add splitting to avoid leakage,"X_train2 , X_valid , y_train2 , y_valid = train_test_split(X_train , y_train , test_size = 0.10 , shuffle = True) ",facial-keypoints-detection-keras-albumentations.ipynb
Define augmentation strategy,"es = EarlyStopping(monitor='val_loss', patience=20)
mc = ModelCheckpoint('best_model3.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)

model3 = create_convnet()
model3.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])
hist3 = model3.fit(train_loader, steps_per_epoch=len(train_loader),
 validation_data=(X_valid, y_valid),
 epochs=500, verbose=0, callbacks=[es, mc])",facial-keypoints-detection-keras-albumentations.ipynb
 Hierarchical approach with training specialistsKaggle score public private : 1.89 2.06Idea originated and adapted from this blogWe divide the facial keypoints into 5 regions and train one model per region. Each model is initialized with the weights of a model trained on the whole dataset see section 3 . ,"def create_specialist(n_outputs = 30 , weights = None , freeze = False): ",facial-keypoints-detection-keras-albumentations.ipynb
Prepare dataset, train_data = pd.read_csv(train_dir) ,facial-keypoints-detection-keras-albumentations.ipynb
Define augmentation strategy,"specialist_keypoints = {'eyes_centers':(0,4), 'eyes_corners':(4,12), 'eyebrows': (12,20), 'nose': (20,22), 'mouth': (22,30)}
models = {}

for region, keypoint_ids in specialist_keypoints.items():
 print(f'Training model {region}...')
 model = create_specialist(n_outputs=keypoint_ids[1]-keypoint_ids[0], weights='best_model3.h5', freeze=False)
 models[region] = train_specialist(model, specialist_keypoints, region)",facial-keypoints-detection-keras-albumentations.ipynb
 Visualizing Intermediate Representations Inspired from this tutorial. ,model = model3 ,facial-keypoints-detection-keras-albumentations.ipynb
number of random filters to display per layer,n = 10 ,facial-keypoints-detection-keras-albumentations.ipynb
"Just do this for the conv maxpool layers, not the fully connected layers", n_features = feature_map.shape[- 1] ,facial-keypoints-detection-keras-albumentations.ipynb
We will tile our images in this matrix," display_grid = np.zeros(( size , size * n)) ",facial-keypoints-detection-keras-albumentations.ipynb
Postprocess the feature to be visually palatable," for i , idx in enumerate(np.random.randint(0 , n_features , n)) : ",facial-keypoints-detection-keras-albumentations.ipynb
Tile each filter into a horizontal grid," display_grid[: , i * size :(i + 1)* size]= x ",facial-keypoints-detection-keras-albumentations.ipynb
Display the grid, scale = 3 ,facial-keypoints-detection-keras-albumentations.ipynb
 Submission ,"def create_submission_file(model , X_test , save_name = 'model_preds'): ",facial-keypoints-detection-keras-albumentations.ipynb
Post process predictions, predictions[predictions > 96]= 96 ,facial-keypoints-detection-keras-albumentations.ipynb
Lookup table filters out the expected prediction points landmarks for each test image, lookid_data = pd.read_csv('/kaggle/input/facial-keypoints-detection/IdLookupTable.csv') ,facial-keypoints-detection-keras-albumentations.ipynb
linear algebra,import numpy as np ,facial-keypoints-detection.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,facial-keypoints-detection.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,facial-keypoints-detection.ipynb
Import Dependencies,% matplotlib inline ,facial-keypoints-detection.ipynb
Start Python Imports,"import math , time , random , datetime ",facial-keypoints-detection.ipynb
Data Manipulation,import numpy as np ,facial-keypoints-detection.ipynb
Visualization,import matplotlib.pyplot as plt ,facial-keypoints-detection.ipynb
Preprocessing,"from sklearn.preprocessing import OneHotEncoder , LabelEncoder , label_binarize ",facial-keypoints-detection.ipynb
Machine learning,import catboost ,facial-keypoints-detection.ipynb
Let s be rebels and ignore warnings for now,import warnings ,facial-keypoints-detection.ipynb
Train,"
display(train_data.isnull().sum())
",facial-keypoints-detection.ipynb
test,"display(test_data.isnull().sum())
",facial-keypoints-detection.ipynb
IdLookupTable,"display(IdLookupTable.isnull().sum())
",facial-keypoints-detection.ipynb
Visualize Data,"Vis = []

for i in range(len(train_data)):
 Vis.append(train_data['Image'][i].split(' '))
",facial-keypoints-detection.ipynb
prepare data x train,"array_float = np.array(Vis, dtype='float')

X_train = array_float.reshape(-1,96,96,1)",facial-keypoints-detection.ipynb
show Photo,"photo_visualize = array_float[1].reshape(96,96)

plt.imshow(photo_visualize, cmap='gray')
plt.title(""Visualize Image"")
plt.show()",facial-keypoints-detection.ipynb
Facial Keypoints,"
Facial_Keypoints_Data = train_data.drop(['Image'], axis=1)
facial_pnts = []

for i in range(len(Facial_Keypoints_Data)):
 facial_pnts.append(Facial_Keypoints_Data.iloc[i])
 ",facial-keypoints-detection.ipynb
prepare data y train,"training_data = train_data.drop('Image',axis = 1)

y_train = []
for i in range(len(train_data)):
 y = training_data.iloc[i,:]
 y_train.append(y)
 
 
y_train = np.array(y_train,dtype = 'float')",facial-keypoints-detection.ipynb
Show Photo image with facial points,"photo_visualize_pnts = Facial_Keypoints_Data.iloc[1].values

plt.imshow(photo_visualize, cmap='gray')
plt.scatter(photo_visualize_pnts[0::2], photo_visualize_pnts[1::2], c='Pink', marker='*')
plt.title(""Image with Facial Keypoints"")
plt.show()",facial-keypoints-detection.ipynb
Prepare and Split Data,train_data.shape,facial-keypoints-detection.ipynb
Keras CNN,model = Sequential () ,facial-keypoints-detection.ipynb
layer 1,"model.add(Convolution2D(32 ,(3 , 3), activation = 'relu' , padding = 'same' , use_bias = False , input_shape =(96 , 96 , 1))) ",facial-keypoints-detection.ipynb
layer 2,"model.add(Convolution2D(32 ,(3 , 3), activation = 'relu' , padding = 'same' , use_bias = False)) ",facial-keypoints-detection.ipynb
layer 3,"model.add(Convolution2D(64 ,(3 , 3), activation = 'relu' , padding = 'same' , use_bias = False)) ",facial-keypoints-detection.ipynb
layer 4,"model.add(Convolution2D(128 ,(3 , 3), activation = 'relu' , padding = 'same' , use_bias = False)) ",facial-keypoints-detection.ipynb
train Data,"model.fit(X_train,y_train,epochs = 5,batch_size = 32,validation_split = 0.2)",facial-keypoints-detection.ipynb
prepare data test,"test_images = []
for i in range(len(test_data)):
 item = test_data['Image'][i].split(' ')
 test_images.append(item)",facial-keypoints-detection.ipynb
predict,"predict = model.predict(X_test)
",facial-keypoints-detection.ipynb
from IdLookupTable table,FeatureName = list(IdLookupTable['FeatureName']) ,facial-keypoints-detection.ipynb
predict results,predict = list(predict) ,facial-keypoints-detection.ipynb
linear algebra,import numpy as np ,facial-keypoints-fastai-image-regression.ipynb
"For example, running this by clicking run or pressing Shift Enter will list the files in the input directory",from fastai.vision import * ,facial-keypoints-fastai-image-regression.ipynb
Control Variables,"train_path = Path('/tmp/train')
test_path = Path('/tmp/test')",facial-keypoints-fastai-image-regression.ipynb
Read CSV data,root = Path('../input'),facial-keypoints-fastai-image-regression.ipynb
train csv pd.read csv root training training.csv .dropna axis 0 ,train_csv = pd.read_csv(root / 'training/training.csv') ,facial-keypoints-fastai-image-regression.ipynb
Save train images,"def save_str_img(strimg,w,h,flpath):
 px=255-np.array(strimg.split(),dtype=int)
 if(len(px)==w*h and len(px)%w==0 and len(px)%h==0):
 cpx = list(px.reshape(w,h))
 img = Image(Tensor([cpx,cpx,cpx]))
 img.save(flpath)
 return img
 else:
 raise Exception(""Invalid height and width"")",facial-keypoints-fastai-image-regression.ipynb
shutil.rmtree train path ,train_path.mkdir(exist_ok = True) ,facial-keypoints-fastai-image-regression.ipynb
for each row,"for index , train_row in train_csv.iterrows (): ",facial-keypoints-fastai-image-regression.ipynb
for each row,"for index , test_row in test_csv.iterrows (): ",facial-keypoints-fastai-image-regression.ipynb
Make Data bunch,def get_locs(flname): ,facial-keypoints-fastai-image-regression.ipynb
TODO remove transforms,"data.show_batch(3,figsize=(6,6))",facial-keypoints-fastai-image-regression.ipynb
calculates distance between true and predictions,"def mloss(y_true , y_pred): ",facial-keypoints-fastai-image-regression.ipynb
Fine tune model,learn.load('s1');,facial-keypoints-fastai-image-regression.ipynb
Predict and display in one image,"def flp(npa):
 for i in range(npa.shape[0]):
 if(i%2==1):
 tmp=npa[i]
 npa[i]=npa[i-1]
 npa[i-1]=tmp
 return npa",facial-keypoints-fastai-image-regression.ipynb
Make predictions and save dataframe,"a=list(train_csv.columns.values)
a.remove('Image')
a.append('ImageId')",facial-keypoints-fastai-image-regression.ipynb
instantiate the bar,"f = IntProgress(min = 0 , max = test_csv.count ()[ 0]) ",facial-keypoints-fastai-image-regression.ipynb
"For example, here s several helpful packages to load",import random ,facial-keypoints-transferlearning-resnet50v2.ipynb
linear algebra,import numpy as np ,facial-keypoints-transferlearning-resnet50v2.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,facial-keypoints-transferlearning-resnet50v2.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,facial-keypoints-transferlearning-resnet50v2.ipynb
"You can also write temporary files to kaggle temp , but they won t be saved outside of the current session","!unzip ../input/facial-keypoints-detection/training.zip
!unzip ../input/facial-keypoints-detection/test.zip",facial-keypoints-transferlearning-resnet50v2.ipynb
Step 2: Data cleaning,"x_train = np.array([to_image(train.Image[i]) for i in range(len(train))]) / 255
x_test = np.array([to_image(test.Image[i]) for i in range(len(test))]) / 255",facial-keypoints-transferlearning-resnet50v2.ipynb
Step 3: Data augmentation,"from skimage.transform import SimilarityTransform, warp, rotate, resize
from skimage.exposure import adjust_gamma
from skimage.util import random_noise
import math",facial-keypoints-transferlearning-resnet50v2.ipynb
random noise," x = random_noise(x , var = var) ",facial-keypoints-transferlearning-resnet50v2.ipynb
adjust gamma," x = adjust_gamma(x , gamma) ",facial-keypoints-transferlearning-resnet50v2.ipynb
rotation," x = rotate(x , angle) ",facial-keypoints-transferlearning-resnet50v2.ipynb
translation, tform = SimilarityTransform(translation = translation) ,facial-keypoints-transferlearning-resnet50v2.ipynb
zoom," a = np.zeros(( 96 , 96)) ",facial-keypoints-transferlearning-resnet50v2.ipynb
to 3 channel,"x_train_clean = np.expand_dims(x_train_clean , axis = - 1) ",facial-keypoints-transferlearning-resnet50v2.ipynb
Step 4: Build model,preprocess_input = tf.keras.applications.resnet_v2.preprocess_input ,facial-keypoints-transferlearning-resnet50v2.ipynb
Step 5: Train model,"my_callbacks = tf.keras.callbacks.EarlyStopping(patience=30, 
 monitor='val_loss', 
 restore_best_weights=True)

history = model.fit(x_train_clean_aug, 
 y_train_clean_aug, 
 epochs=500, 
 batch_size=16, 
 validation_data=(x_val_clean, y_val_clean), 
 callbacks=[my_callbacks])",facial-keypoints-transferlearning-resnet50v2.ipynb
Step 6: Show high MSE data,"y_pred = model.predict(x_train_clean)
MSE = tf.math.reduce_mean(tf.square(y_train_clean - y_pred), axis=1)
_filter = MSE.numpy() > 5
_filter.sum()",facial-keypoints-transferlearning-resnet50v2.ipynb
Step 7: Submit the results,"x_test = np.expand_dims(x_test, axis=-1)
x_test = np.tile(x_test, [1, 1, 1, 3])",facial-keypoints-transferlearning-resnet50v2.ipynb
Load and Examine the Data,"datadir = Path.cwd().parent / 'input' / 'street-view-getting-started-with-julia'
datadir.ls()",fastai-cv.ipynb
Build Data Block and Train Model,"def label_func(fname):
 the_id = int(re.findall(""[0-9]+"",fname.name)[0])
 label = train_labels.loc[train_labels['ID'] == the_id, ""Class""].values.item()
 return label",fastai-cv.ipynb
Interpret the Model,interp = ClassificationInterpretation.from_learner(learn),fastai-cv.ipynb
Make Prediction,"file_extract(datadir/""test.zip"", dest=output_dir)
test_image_dir = output_dir/'test'",fastai-cv.ipynb
"This notebook is designed to show how EfficientNetB7 and DenseNet201 models ensemble can be used with TPU. It is based on my earlier notebook, and it uses external data to increase model s accuracy.Some cool notebooks I ve used to write this one: Getting started with 100 flowers on TPU by Martin Goerner TPU Flowers by Tushar Kendre and Shreyaansh Gupta random blockout random erasing augmentation TPU: ENet B7 DenseNet by Wojtek RosaI also recommend Rotation Augmentation GPU TPU 0.96 by Chris DeotteHave any suggestions? Feel free to comment.If you liked this kernel, please don t forget to upvote it!",!pip install -qU efficientnet wandb,fc-ensemble-external-data-effnet-densenet.ipynb
"We will use wandb for logging purposes. If you want to write logging results to your Wandb account, use Add ons Secrets to set wandb key variable to your Wandb API key.","try:
 from kaggle_secrets import UserSecretsClient
 user_secrets = UserSecretsClient()
 api_key = user_secrets.get_secret('wandb_key')
 wandb.login(key=api_key)
 anonymous = None
except:
 wandb.login(anonymous='must')
 print('To use your W&B account,\nGo to Add-ons -> Secrets and provide your \
 W&B access token. Use the Label name as WANDB. \nGet your W&B access \
 token from here: https://wandb.ai/authorize')",fc-ensemble-external-data-effnet-densenet.ipynb
TPU detection,AUTO = tf.data.experimental.AUTOTUNE ,fc-ensemble-external-data-effnet-densenet.ipynb
"Detect hardware, return appropriate distribution strategy",try : ,fc-ensemble-external-data-effnet-densenet.ipynb
TPU detection, tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect () ,fc-ensemble-external-data-effnet-densenet.ipynb
for GPU or multi GPU machines, strategy = tf.distribute.MirroredStrategy () ,fc-ensemble-external-data-effnet-densenet.ipynb
"192, 224, 331, 512","IMAGE_SIZE =[224 , 224] ",fc-ensemble-external-data-effnet-densenet.ipynb
"0, 1, 2, 3, 4, 5, 6 or 7",EFN = 7 ,fc-ensemble-external-data-effnet-densenet.ipynb
 imagenet or noisy student ,EFN_WEIGHTS = 'noisy-student' ,fc-ensemble-external-data-effnet-densenet.ipynb
"121, 169 or 201",DN = 201 ,fc-ensemble-external-data-effnet-densenet.ipynb
"TPUs read data directly from Google Cloud Storage GCS , so we need to copy the dataset to a GCS bucket co located with the TPU. To do that, pass the name of a specific dataset to the get gcs path function. The name of the dataset is the name of the directory it is mounted in. ",GCS_DS_PATH = KaggleDatasets (). get_gcs_path('tpu-getting-started') ,fc-ensemble-external-data-effnet-densenet.ipynb
numpy and matplotlib defaults,"np.set_printoptions(threshold = 15 , linewidth = 80) ",fc-ensemble-external-data-effnet-densenet.ipynb
"binary string in this case, these are image ID strings", if numpy_labels.dtype == object : ,fc-ensemble-external-data-effnet-densenet.ipynb
"If no labels, only image IDs, return None for labels this is the case for test data "," return numpy_images , numpy_labels ",fc-ensemble-external-data-effnet-densenet.ipynb
Random erasing blockout augmentation,"def random_erasing(img , sl = 0.1 , sh = 0.2 , rl = 0.4 , p = 0.3): ",fc-ensemble-external-data-effnet-densenet.ipynb
CONVERT DEGREES TO RADIANS, rotation = math.pi * rotation / 180. ,fc-ensemble-external-data-effnet-densenet.ipynb
ROTATION MATRIX, c1 = tf.math.cos(rotation) ,fc-ensemble-external-data-effnet-densenet.ipynb
SHEAR MATRIX, c2 = tf.math.cos(shear) ,fc-ensemble-external-data-effnet-densenet.ipynb
ZOOM MATRIX," zoom_matrix = tf.reshape(tf.concat ([one / height_zoom , zero , zero , zero , one / width_zoom , zero , zero , zero , one], axis = 0),[3 , 3]) ",fc-ensemble-external-data-effnet-densenet.ipynb
SHIFT MATRIX," shift_matrix = tf.reshape(tf.concat ([one , zero , height_shift , zero , one , width_shift , zero , zero , one], axis = 0),[3 , 3]) ",fc-ensemble-external-data-effnet-densenet.ipynb
"output image randomly rotated, sheared, zoomed, and shifted", DIM = IMAGE_SIZE[0] ,fc-ensemble-external-data-effnet-densenet.ipynb
fix for size 331, XDIM = DIM % 2 ,fc-ensemble-external-data-effnet-densenet.ipynb
GET TRANSFORMATION MATRIX," m = get_mat(rot , shr , h_zoom , w_zoom , h_shift , w_shift) ",fc-ensemble-external-data-effnet-densenet.ipynb
LIST DESTINATION PIXEL INDICES," x = tf.repeat(tf.range(DIM // 2 , - DIM // 2 , - 1), DIM) ",fc-ensemble-external-data-effnet-densenet.ipynb
ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS," idx2 = K.dot(m , tf.cast(idx , dtype = 'float32')) ",fc-ensemble-external-data-effnet-densenet.ipynb
FIND ORIGIN PIXEL VALUES," idx3 = tf.stack ([DIM // 2 - idx2[0 ,], DIM // 2 - 1 + idx2[1 ,]]) ",fc-ensemble-external-data-effnet-densenet.ipynb
Dataset functions,def decode_image(image_data): ,fc-ensemble-external-data-effnet-densenet.ipynb
"convert image to floats in 0, 1 range"," image = tf.cast(image , tf.float32)/ 255.0 ",fc-ensemble-external-data-effnet-densenet.ipynb
explicit size needed for TPU," image = tf.reshape(image ,[* IMAGE_SIZE , 3]) ",fc-ensemble-external-data-effnet-densenet.ipynb
data dump,"print(""Training data shapes:"") ",fc-ensemble-external-data-effnet-densenet.ipynb
U unicode string,"print(""Test data IDs:"" , idnum.numpy (). astype('U')) ",fc-ensemble-external-data-effnet-densenet.ipynb
Peek at training data,training_dataset = get_training_dataset () ,fc-ensemble-external-data-effnet-densenet.ipynb
run this cell again for next set of images,display_batch_of_images(next(train_batch)) ,fc-ensemble-external-data-effnet-densenet.ipynb
peer at test data,test_dataset = get_test_dataset () ,fc-ensemble-external-data-effnet-densenet.ipynb
run this cell again for next set of images,display_batch_of_images(next(test_batch)) ,fc-ensemble-external-data-effnet-densenet.ipynb
Starting with a high LR would break the pre trained weights.,def get_lr_callback(plot_schedule = False): ,fc-ensemble-external-data-effnet-densenet.ipynb
Train both models,models = [],fc-ensemble-external-data-effnet-densenet.ipynb
Let s train EfficientNet...,models.append(load_and_fit_model('efficientnet')),fc-ensemble-external-data-effnet-densenet.ipynb
...and DenseNet!,models.append(load_and_fit_model('densenet')),fc-ensemble-external-data-effnet-densenet.ipynb
"Let s find best alpha, the coefficient we will use for ensembling predictions from both models","def find_best_alpha(valid_dataset , model_lst): ",fc-ensemble-external-data-effnet-densenet.ipynb
get everything as one batch, y_true = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))). numpy () ,fc-ensemble-external-data-effnet-densenet.ipynb
"since we are splitting the dataset and iterating separately on images and labels, order matters.",valid_ds = get_validation_dataset(ordered = True) ,fc-ensemble-external-data-effnet-densenet.ipynb
Plot confusion matrix and predict on test dataset,"def predict_ensemble(dataset, model_lst, alpha, steps):
 print('Calculating predictions...')
 images_ds = dataset.map(lambda image, idnum: image)
 probs = []
 for model in model_lst:
 p = model.predict(images_ds,verbose=0, steps=steps)
 probs.append(p)
 preds = np.argmax(alpha*probs[0] + (1-alpha)*probs[1], axis=-1)
 return preds",fc-ensemble-external-data-effnet-densenet.ipynb
"cmdataset get validation dataset ordered True since we are splitting the dataset and iterating separately on images and labels, order matters.","cm_predictions = predict_ensemble(valid_ds , models , alpha , steps = VALIDATION_STEPS) ",fc-ensemble-external-data-effnet-densenet.ipynb
get everything as one batch,cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))). numpy () ,fc-ensemble-external-data-effnet-densenet.ipynb
cmat cmat.T cmat.sum axis 1 .T normalized,"display_confusion_matrix(cmat , score , precision , recall) ",fc-ensemble-external-data-effnet-densenet.ipynb
"since we are splitting the dataset and iterating separately on images and ids, order matters.",test_ds = get_test_dataset(ordered = True) ,fc-ensemble-external-data-effnet-densenet.ipynb
all in one batch,test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))). numpy (). astype('U') ,fc-ensemble-external-data-effnet-densenet.ipynb
linear algebra,import numpy as np ,first-kaggle-notebook-following-ts-tutorial.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,first-kaggle-notebook-following-ts-tutorial.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,first-kaggle-notebook-following-ts-tutorial.ipynb
"You can also write temporary files to kaggle temp , but they won t be saved outside of the current session","path = '/kaggle/input/store-sales-time-series-forecasting/'
os.listdir(path)",first-kaggle-notebook-following-ts-tutorial.ipynb
Equador s economy is dependent on the crude oil price. Let s examine first the relationship between crude oil and grocery sales and transactions.,"ax = data_oil.set_index('date').plot(figsize = (16, 8))
ax.set_xlabel('Date', fontsize = 'large')
ax.set_ylabel(""Crude Oil"", fontsize = 'large')",first-kaggle-notebook-following-ts-tutorial.ipynb
daily avg sales weekly avg sales daily avg sales sales .rolling window 7 .mean ,"avg_sales['weekly_avg_sales']= avg_sales['sales']. ewm(span = 7 , adjust = False). mean () ",first-kaggle-notebook-following-ts-tutorial.ipynb
"ax daily avg sales.set index date .plot figsize 16, 8 ","ax1 = avg_sales.plot(x = 'date' , y =['sales' , 'weekly_avg_sales'], figsize =(18 , 6)) ",first-kaggle-notebook-following-ts-tutorial.ipynb
avg transaction weekly avg sales avg transaction transactions .rolling window 7 .mean ,"avg_transactions['weekly_avg_transactions']= avg_transactions['transactions']. ewm(span = 7 , adjust = False). mean () ",first-kaggle-notebook-following-ts-tutorial.ipynb
print data oil.head ,data_oil.corr () ,first-kaggle-notebook-following-ts-tutorial.ipynb
Let s check items that are most sold and the promotion to see which items influence the most for the total sales.,"print(train_data.family.unique())
print(len(train_data.family.unique()))
train_data['family'] = train_data['family'].astype('category')
train_data['family_category'] = train_data['family'].cat.codes

family_category = dict( zip( train_data['family'].cat.codes, train_data['family']))
family_category",first-kaggle-notebook-following-ts-tutorial.ipynb
Let s check sales in different time frames.,"train_data['date'] = pd.to_datetime(train_data['date'])
train_data['day_of_week'] = train_data['date'].dt.dayofweek
train_data['month'] = train_data['date'].dt.month
train_data['year'] = train_data['date'].dt.year",first-kaggle-notebook-following-ts-tutorial.ipynb
Check sales for holidays,print(data_holi['type']. unique ()) ,first-kaggle-notebook-following-ts-tutorial.ipynb
print avg sales.head ,"df = pd.merge_asof(day_type , avg_sales , on = 'date') ",first-kaggle-notebook-following-ts-tutorial.ipynb
print df.head ,df_1 = df.groupby (['type']).mean ()[ 'sales'] ,first-kaggle-notebook-following-ts-tutorial.ipynb
print df 1.head ,print(f'average holiday sales is {average_holiday_sales}') ,first-kaggle-notebook-following-ts-tutorial.ipynb
Let s follow the template provided by Kaggle moderator for future sales prediction. Start with linear regression ,"avg_sales = train_data.groupby('date').agg({'sales': 'mean'}).reset_index()
avg_sales['Time'] = np.arange(len(avg_sales.index))
avg_sales.head()",first-kaggle-notebook-following-ts-tutorial.ipynb
features,"X = avg_sales.loc[: ,['Time']] ",first-kaggle-notebook-following-ts-tutorial.ipynb
target,"y = avg_sales.loc[: , 'sales'] ",first-kaggle-notebook-following-ts-tutorial.ipynb
Train the model,model = LinearRegression () ,first-kaggle-notebook-following-ts-tutorial.ipynb
the training data,"y_pred = pd.Series(model.predict(X), index = X.index) ",first-kaggle-notebook-following-ts-tutorial.ipynb
drop missing values in the feature set,X.dropna(inplace = True) ,first-kaggle-notebook-following-ts-tutorial.ipynb
create the target,"y = avg_sales.loc[: , 'sales'] ",first-kaggle-notebook-following-ts-tutorial.ipynb
drop corresponding values in target,"y , X = y.align(X , join = 'inner') ",first-kaggle-notebook-following-ts-tutorial.ipynb
Trend,from pathlib import Path ,first-kaggle-notebook-following-ts-tutorial.ipynb
ignore warnings to clean up output cells,"simplefilter(""ignore"") ",first-kaggle-notebook-following-ts-tutorial.ipynb
Set Matplotlib defaults,"plt.style.use(""seaborn-whitegrid"") ",first-kaggle-notebook-following-ts-tutorial.ipynb
the target,"y = avg_sales[""sales""] ",first-kaggle-notebook-following-ts-tutorial.ipynb
"features, so we need to be sure to exclude it here.",model = LinearRegression(fit_intercept = False) ,first-kaggle-notebook-following-ts-tutorial.ipynb
Seasonality,"from statsmodels.tsa.deterministic import CalendarFourier , DeterministicProcess ",first-kaggle-notebook-following-ts-tutorial.ipynb
Set Matplotlib defaults,"plt.style.use(""seaborn-whitegrid"") ",first-kaggle-notebook-following-ts-tutorial.ipynb
the x axis freq ,X['day']= X.index.dayofweek ,first-kaggle-notebook-following-ts-tutorial.ipynb
the seasonal period period ,X['week']= X.index.week ,first-kaggle-notebook-following-ts-tutorial.ipynb
days within a year,X['dayofyear']= X.index.dayofyear ,first-kaggle-notebook-following-ts-tutorial.ipynb
plot periodogram avg sales.sales ,y_deseason = y - y_pred ,first-kaggle-notebook-following-ts-tutorial.ipynb
10 sin cos pairs for A nnual seasonality,"fourier = CalendarFourier(freq = ""A"" , order = 10) ",first-kaggle-notebook-following-ts-tutorial.ipynb
Scikit learn solution,from sklearn.preprocessing import OneHotEncoder ,first-kaggle-notebook-following-ts-tutorial.ipynb
1 54,STORE_NBR = '1' ,first-kaggle-notebook-following-ts-tutorial.ipynb
display store sales.index.get level values family .unique ,"ax = y.loc(axis = 1)[ 'sales' , STORE_NBR , FAMILY]. plot(** plot_params) ",first-kaggle-notebook-following-ts-tutorial.ipynb
plot style settings,from learntools.time_series.style import * ,first-kaggle-notebook-following-ts-tutorial.ipynb
YOUR CODE HERE,"y_ma = y.rolling(7 , center = True). mean () ",first-kaggle-notebook-following-ts-tutorial.ipynb
Plot,ax = y_ma.plot () ,first-kaggle-notebook-following-ts-tutorial.ipynb
Drop the New Year outlier,"plot_lags(x = onpromotion.iloc[1 :], y = y_deseason.iloc[1 :], lags = 3 , leads = 3 , nrows = 1); ",first-kaggle-notebook-following-ts-tutorial.ipynb
YOUR CODE HERE: Make features from y deseason,"X_lags = make_lags(y_deseason , lags = 1) ",first-kaggle-notebook-following-ts-tutorial.ipynb
You may want to use pd.concat,"from sklearn.model_selection import train_test_split

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=30, shuffle=False)

model = LinearRegression(fit_intercept=False).fit(X_train, y_train)
y_fit = pd.Series(model.predict(X_train), index=X_train.index).clip(0.0)
y_pred = pd.Series(model.predict(X_valid), index=X_valid.index).clip(0.0)

rmsle_train = mean_squared_log_error(y_train, y_fit) ** 0.5
rmsle_valid = mean_squared_log_error(y_valid, y_pred) ** 0.5
print(f'Training RMSLE: {rmsle_train:.5f}')
print(f'Validation RMSLE: {rmsle_valid:.5f}')

ax = y.plot(**plot_params, alpha=0.5, title=""Average Sales"", ylabel=""items sold"")
ax = y_fit.plot(ax=ax, label=""Fitted"", color='C0')
ax = y_pred.plot(ax=ax, label=""Forecast"", color='C3')
ax.legend();",first-kaggle-notebook-following-ts-tutorial.ipynb
28 day mean of lagged target,mean_7 = y_lag.rolling(7). mean () ,first-kaggle-notebook-following-ts-tutorial.ipynb
YOUR CODE HERE: 14 day median of lagged target,median_14 = y_lag.rolling(14). median () ,first-kaggle-notebook-following-ts-tutorial.ipynb
YOUR CODE HERE: 7 day rolling standard deviation of lagged target,std_7 = y_lag.rolling(7). std () ,first-kaggle-notebook-following-ts-tutorial.ipynb
YOUR CODE HERE: 7 day sum of promotions with centered window,"promo_7 = onpromo.rolling(7 , center = True). sum () ",first-kaggle-notebook-following-ts-tutorial.ipynb
Hybrids Models,"family_sales = (
 store_sales
 .groupby(['family', 'date'])
 .mean()
 .unstack('family')
 .loc['2017']
)

family_sales.head()",first-kaggle-notebook-following-ts-tutorial.ipynb
You ll add fit and predict methods to this minimal class,class BoostedHybrid : ,first-kaggle-notebook-following-ts-tutorial.ipynb
store column names from fit method, self.y_columns = None ,first-kaggle-notebook-following-ts-tutorial.ipynb
Train model 1," self.model_1.fit(X_1 , y) ",first-kaggle-notebook-following-ts-tutorial.ipynb
Make predictions,"def predict(self , X_1 , X_2): ",first-kaggle-notebook-following-ts-tutorial.ipynb
Predict with model 1,from sklearn.preprocessing import LabelEncoder ,first-kaggle-notebook-following-ts-tutorial.ipynb
Target series,"y = family_sales.loc[: , 'sales'] ",first-kaggle-notebook-following-ts-tutorial.ipynb
X 1: Features for Linear Regression,"dp = DeterministicProcess(index = y.index , order = 1) ",first-kaggle-notebook-following-ts-tutorial.ipynb
onpromotion feature,"X_2 = family_sales.drop('sales' , axis = 1). stack () ",first-kaggle-notebook-following-ts-tutorial.ipynb
from sklearn.preprocessing,le = LabelEncoder () ,first-kaggle-notebook-following-ts-tutorial.ipynb
values are day of the month,"X_2[""day""]= X_2.index.day ",first-kaggle-notebook-following-ts-tutorial.ipynb
Model 1 trend ,from pyearth import Earth ,first-kaggle-notebook-following-ts-tutorial.ipynb
Model 2,"from sklearn.ensemble import ExtraTreesRegressor , RandomForestRegressor ",first-kaggle-notebook-following-ts-tutorial.ipynb
YOUR CODE HERE: Try different combinations of the algorithms above,"y_train , y_valid = y[: ""2017-07-01""], y[""2017-07-02"" :] ",first-kaggle-notebook-following-ts-tutorial.ipynb
just a demo.,"model.fit(X1_train , X2_train , y_train) ",first-kaggle-notebook-following-ts-tutorial.ipynb
family sales.head ,"import ipywidgets as widgets
from learntools.time_series.utils import (create_multistep_example,
 load_multistep_data,
 make_lags,
 make_multistep_target,
 plot_multistep)

datasets = load_multistep_data()

data_tabs = widgets.Tab([widgets.Output() for _ in enumerate(datasets)])
for i, df in enumerate(datasets):
 data_tabs.set_title(i, f'Dataset {i+1}')
 with data_tabs.children[i]:
 display(df)

display(data_tabs)",first-kaggle-notebook-following-ts-tutorial.ipynb
"print Test Data , n 9 n , test ","y = family_sales.loc[:, 'sales']

X = make_lags(y, lags=4).dropna()

y = make_multistep_target(y, steps=16).dropna()

y, X = y.align(X, join='inner', axis=0)",first-kaggle-notebook-following-ts-tutorial.ipynb
"First of all let s take a note about the goal of the competition, which is: Predict the price of products in the next 15 days from the lastday of the training data.Let s take a look at test.csv and sample submission.csv",from warnings import simplefilter ,first-project-store-sales.ipynb
ignore warnings to clean up output cells,"simplefilter(""ignore"") ",first-project-store-sales.ipynb
for garbage cleaning,import gc ,first-project-store-sales.ipynb
for autocomplete,% config Completer.use_jedi = False ,first-project-store-sales.ipynb
" The data explorationHere, I planed to explore these areas within all the datasets. In order to get the idea of what might be an interesting features to train the model.Task: Total sales : Hope to see an overall trend or the some spike in sales perhaps the earthquake would have a big effect. Daily sales by each stores : Hope to see the trend and pattern in sales throughout the timeframe. Sales by product family, by time : Hope to see the trend and pattern in sales throughout the timeframe. Store Location Cluster Effect on sales : This might be a good feature in further sale prediction. Onpromotion effect on sales Cycle and Seasonal Effect on total sales : Hope to see some bi weekly effect or something interesting. Map the holiday to the sales record: I guess that holiday will boost the sales. Calculate the correlation in change in oil price to change in total sales: At this point I assume that the fluctuation in oil price won t affect the sales in short term. Since the change in oil price take times to effect the national income and ultimately business activites.. Explore the transaction data ",train_data.head () ,first-project-store-sales.ipynb
seems like the store is close on new year day since they are all 0 sales in 1 Jan.,train_data.info () ,first-project-store-sales.ipynb
over 3m. lines of data with a date formatted as a string dtype,"print('there are {} different product families'.format(train_data.family.nunique()))
",first-project-store-sales.ipynb
let s format the datetime and use as index and drop the id column.,process_train = train_data.copy () ,first-project-store-sales.ipynb
to reduce ram usage,"process_train[[ 'store_nbr' , 'family']].astype('category') ",first-project-store-sales.ipynb
let s check the possible missing data observation.,process_train.isna().sum(),first-project-store-sales.ipynb
check the missing date missing the entire observation .Let s begin by checking the time lag between each observation.,"def count_day_gap(df):
 temp = df.reset_index().groupby(['date']).sales.sum()
 return (temp.index[1:]-temp.index[:-1]).value_counts()",first-project-store-sales.ipynb
4 rows of the data are missing as we see that the 2 days time gap are presented 4 time. Where are thay?,temp = process_train.reset_index (). groupby (['date']).sales.sum (). to_frame () ,first-project-store-sales.ipynb
add to gap list to have the same length as temp to combine it together,"gap.insert(0 , 'first day') ",first-project-store-sales.ipynb
print temp.gap.unique ,day_skip = temp.groupby('gap'). get_group(temp.gap.unique ()[ 2]) ,first-project-store-sales.ipynb
2013 2014 2015 2016 Christmas days 20xx 12 25 are all missing. Let s see if there are other days that the total sales across country is zero. ,process_train.groupby('date').sales.sum().sort_values().head(10),first-project-store-sales.ipynb
"First of all, let s see total sales as a stackpolot. I could get the idea of what s selling well and the major trend of total sales. Also, I will mark the earthquake to see if the quake noticably affect the sales.","date_fam_sale = process_train.groupby(['date','family']).sum().sales
unstack = date_fam_sale.unstack()
unstack = unstack.resample('1M').sum()",first-project-store-sales.ipynb
resample to monthly sales,month_family = process_train.groupby('family'). resample('M'). sales.sum () ,first-project-store-sales.ipynb
let s calculte the proportaion of sales by category,"total_sale = month_family.sum()
family_sale = month_family.groupby('family').sum().sort_values()
proportion = ((family_sale/total_sale)*100).sort_values(ascending=False)
proportion = pd.DataFrame(proportion)
proportion.head()",first-project-store-sales.ipynb
Let s take a look at daily total sales.,"import matplotlib.dates as mdates

fig, ax = plt.subplots(figsize=(18, 7))
ax.set(title=""'Total Sales Across All Stores"")
total_sales = process_train.sales.groupby(""date"").sum()
plt.plot(total_sales)

ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))
plt.xticks(rotation=70)
plt.axvline(x=pd.Timestamp('2016-04-16'),color='r',linestyle='--',linewidth=4,alpha=0.3)
plt.text(pd.Timestamp('2016-04-20'),1400000,'The Earthquake',rotation=360,c='r')


plt.show()",first-project-store-sales.ipynb
create a dic that contain each store total daily sale.,daily_sale_dict = { } ,first-project-store-sales.ipynb
mark the earthquake," plt.axvline(x = pd.Timestamp('2016-04-16'), color = 'r' , linestyle = '--' , linewidth = 2 , alpha = 0.3) ",first-project-store-sales.ipynb
What about the sales by Product Family.,"by_fam_dic = {}
fam_list = process_train.family.unique()

for fam in fam_list:
 by_fam_dic[fam] = process_train[process_train['family']==fam].sales",first-project-store-sales.ipynb
mark the earthquake," plt.axvline(x = pd.Timestamp('2016-04-16'), color = 'r' , linestyle = '--' , linewidth = 2 , alpha = 0.3) ",first-project-store-sales.ipynb
"Interpretation: Each family has their own selling paterns. However, Frozen Food and School and Office Supplies shown highly seasonal cycle. Frozen Food : Sell more on New Year School and Office Supplies: Sale more around AUG","del by_fam_dic
del fam_list",first-project-store-sales.ipynb
What about the stores themselves.The competition data description explicitly describe that cluster is a grouping of similar store . This might explan some of the different sales patterns in the subplots above. Let s see what can I get from this.,store_data.head(3),first-project-store-sales.ipynb
add the cluster to the process train df,"join_df = process_train.merge(store_data , on = 'store_nbr') ",first-project-store-sales.ipynb
get the missing time after merge back in place,join_df.set_index(process_train.index) ,first-project-store-sales.ipynb
I m curious about the store data Let s compare the average sales between types and alo between cluster. Hopefully I can see some pattern. ," def show_type_df(join_store_type_df):
 mean_sales_type = join_store_type_df.groupby('type').sales.mean()
 median_sales_type = join_store_type_df.groupby('type').sales.median()
 number=join_store_type_df.groupby('type').store_nbr.nunique()

 type_df = pd.DataFrame((mean_sales_type,median_sales_type,number))
 type_df = type_df.T
 type_df.columns = ['mean','median','number of store']

 return type_df",first-project-store-sales.ipynb
What about the store cluster.,"def show_cluster_summary(join_store_type_df):
 mean_sales_cluster = join_store_type_df.groupby('cluster').sales.mean()
 median_sales_cluster = join_store_type_df.groupby('cluster').sales.median()
 number=join_store_type_df.groupby('cluster').store_nbr.nunique()

 cluster_df = pd.DataFrame((mean_sales_cluster,median_sales_cluster,number))
 cluster_df = cluster_df.T
 cluster_df.columns = ['mean','median','number of store']

 return cluster_df.sort_values('mean', ascending=False)",first-project-store-sales.ipynb
let s see the City characteristic,"def show_city_df(join_store_type_df): 
 mean_sales_city = join_store_type_df.groupby('city').sales.mean()
 median_sales_city = join_store_type_df.groupby('city').sales.median()
 number=join_store_type_df.groupby('city').store_nbr.nunique()

 city_df = pd.DataFrame((mean_sales_city,median_sales_city,number))
 city_df = city_df.T
 city_df.columns = ['mean','median','number of store']

 return city_df.sort_values('mean', ascending=False)",first-project-store-sales.ipynb
Last thing I d like to explore within stores is a heatmap coorelation between the sales pattern of each stores.,import seaborn as sns ,first-project-store-sales.ipynb
slice only the two columns,"a = process_train[[ ""store_nbr"" , ""sales""]] ",first-project-store-sales.ipynb
create a column of ones,"a[""ind""]= 1 ",first-project-store-sales.ipynb
"cumulate the 1 column til the last observation. Ultimately, count the lines for a store.","a[""ind""]= a.groupby(""store_nbr""). ind.cumsum (). values ",first-project-store-sales.ipynb
create a corelation pivot table between sales of each stores,"a = pd.pivot(a , index = ""ind"" , columns = ""store_nbr"" , values = ""sales""). corr () ",first-project-store-sales.ipynb
slice the top triangle of the a.corr dataFrame,mask = np.triu(a.corr ()) ,first-project-store-sales.ipynb
let s check the first entry of store 20 to make sure.,"store_20 = process_train.groupby(['store_nbr','date']).sales.sum().loc[20]
store_20.loc['2013-01-01':'2015-01-01']",first-project-store-sales.ipynb
Effect of promotionthe competition overview state that the onpromotion column gives the total number of items in a product family that were being promoted at a store at a given date. let s take a look with the corelation between onpromotion and sales.,"print('Spearman Rank Correlation = {}'.format(
 process_train.sales.corr(process_train.onpromotion,method='spearman')))",first-project-store-sales.ipynb
"Let s explopre trend, seasonal, and cyclesJust for simplicity of the first attempt, I m not gonna use any determininstic process or Fourier series to determine the cycle of the sales. Instead, I will used rolling window average to project a trend and used as a feature for trend.As for cycle, I don t think I can overlook this feature since the day of the week or month of year are crucial in retail business. I will use one hot encoding for and feed as a feature.First, let s make sure that the seasonal effect exist.","from datetime import datetime as dt

def show_dow_sales():
 day_group = process_train.reset_index()[['date','sales']]
 day_group = day_group.groupby('date')
 day_group = day_group.sales.mean().to_frame()
 day_group['dow'] = day_group.index.day_of_week
 day_group = day_group.groupby('dow').sum()
 plt.bar(day_group.index,day_group['sales'])
 plt.title('Average sales on day of week')
 plt.show()",first-project-store-sales.ipynb
"Holiday Effect on sales NOTE: Pay special attention to the transferred column. A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is Transfer. For example, the holiday Independencia de Guayaquil was transferred from 2012 10 09 to 2012 10 12, which means it was celebrated on 2012 10 12. Days that are type Bridge are extra days that are added to a holiday e.g., to extend the break across a long weekend . These are frequently made up by the type Work Day which is a day not normally scheduled for work e.g., Saturday that is meant to payback the Bridge. Additional holidays are days added a regular calendar holiday, for example, as typically happens around Christmas making Christmas Eve a holiday . ","holiday_data = pd.read_csv(""/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv"",index_col='date',parse_dates=['date'])
holiday_data",first-project-store-sales.ipynb
Let s check if Quito the main operating area is here ,holiday_data.locale_name.value_counts().head(),first-project-store-sales.ipynb
Didn t see New Year and Christmas day in the holiday data. This day is so impactful since the sales across the counrty is dropped to about zero. Let s add it back.,"ny_dic = {'type': 'Holiday','locale':'National','locale_name':'Ecuador','description': 'New Year Day','transferred':'False'}
ny_date = pd.to_datetime(['2012-01-01','2013-01-01','2014-01-01','2015-01-01','2016-01-01','2017-01-01','2018-01-01'])

cm_dic = {'type': 'Holiday','locale':'National','locale_name':'Ecuador','description': 'Christmas Day','transferred':'False'}
cm_date = pd.to_datetime(['2012-12-25','2013-12-25','2014-12-25','2015-12-25','2016-12-25','2017-12-25','2018-12-25'])",first-project-store-sales.ipynb
My guide for formatting holiday s here :,"calendar = pd.DataFrame(index = pd.date_range('2013-01-01','2017-08-31'))
calendar = calendar.join(holiday_data).fillna(0)
del holiday_data
calendar",first-project-store-sales.ipynb
make work day false for sat and sun 6 7 in dow ,"calendar.loc[calendar['dow']> 5 , 'workday']= False ",first-project-store-sales.ipynb
"Next, I m setting workday in each National Holiday to ba False no matter dayofweek. Local holiday maybe a normal working day for the nation.","calendar.loc[( calendar['type']== 'Holiday')&(calendar['locale']. str.contains('National')) , 'workday']= False ",first-project-store-sales.ipynb
some holiday are explicitly said it is workday,"calendar.loc[calendar['type']== 'Work Day' , 'workday']= True ",first-project-store-sales.ipynb
Let s check the transfered holiday. They supposed to become working day if they are not Sat and Sun. Let s see that.,calendar.where(calendar['transferred'] == True).dropna(),first-project-store-sales.ipynb
"They are all weekday since dow are all less than 5 . Therefore, format the work day to True","calendar.loc[(calendar['transferred'] == True), 'workday'] = True",first-project-store-sales.ipynb
Format the football to be the one same kind of event. Do the same for the earthquake that last multiple days.,calendar.where(calendar['description'].str.contains('futbol')).dropna(),first-project-store-sales.ipynb
just checking,calendar.where(calendar['is_football']== 1). dropna (). head () ,first-project-store-sales.ipynb
just checking,calendar.where(calendar['is_eq']== 1). dropna (). head () ,first-project-store-sales.ipynb
let s test if the event such as football earthquake Black Friday affect sales.,"calendar.loc[calendar['is_football']== 1 , 'description']= 'football' ",first-project-store-sales.ipynb
for simplicity just format the description of the football event and earthquake to be the same,"sales = process_train.groupby('date').sales.sum()
event = calendar[calendar['type']=='Event']

event_merge = event.merge(sales,how='left',left_index=True,right_index=True)
event_merge

del sales
del event",first-project-store-sales.ipynb
"comparing those means, black friday and Dia de la Madre seems to not effect total sales much while all the other event affect the sales more.Now, from the plan Group Football Events They might have the different effect from other ceremonial event. Deal with transfered events Emphasize Cyber Monday and Black Friday Since these are practically shopping day. Creat a dummy DF that contain the date type of event one hot is football is Earthquake is cyber monday is black friday are other events where the event was held one hot is workday day of week let s do the dummy DF beginnig by drop the transfered holidays.",calendar.head(),first-project-store-sales.ipynb
check the christmas,calendar.loc['2014-12-25']. to_frame (). T ,first-project-store-sales.ipynb
don t need description anymore we have dummied them all,"calendar_checkpoint = calendar_checkpoint.drop('description' , axis = 1) ",first-project-store-sales.ipynb
The effect of oil price on the salesI put an assumption that the short term effect from change in oil price to the sales is negligible. Let s see if my guessy assumption holds. ,"oil_data = pd.read_csv(""/kaggle/input/store-sales-time-series-forecasting/oil.csv"")",first-project-store-sales.ipynb
let s check the missing date. remember that the training data are from 2013 01 01 to 2017 08 15 ,"pd.date_range(start = '2013-01-01' , end = '2017-08-15'). difference(oil_data.index) ",first-project-store-sales.ipynb
So much missing let s interpolate the missing data.,"oil_data['date'] = pd.to_datetime(oil_data['date'])
oil_data = oil_data.set_index('date')",first-project-store-sales.ipynb
No missing date now. Let s interpolate,"oil_data['dcoilwtico'] = np.where(oil_data['dcoilwtico']==0, np.nan, oil_data['dcoilwtico'])
oil_data['interpolated_price'] = oil_data.dcoilwtico.interpolate()",first-project-store-sales.ipynb
Now let s calculate the correlation between oil price and total sales. Let s begin by a scatterplot,daily_total_sales = total_sales.copy(),first-project-store-sales.ipynb
"Cosidering the scatterplot there seems to be no obvious linear correlation between both of them. the most I could say is that higher the oil price, lower the sales. However let s consider another angle. The oil price changes vs total sales changes. We already have oil price change and percentage change. Let s prepare the daily total sales.","daily_total_sales = pd.DataFrame(daily_total_sales)
daily_total_sales['sales_chg'] = daily_total_sales['sales']-daily_total_sales['sales'].shift(1)
daily_total_sales['sales_pct_chg'] = daily_total_sales['sales_chg']/daily_total_sales['sales'].shift(-1)

daily_total_sales.head()",first-project-store-sales.ipynb
Let s find correlation betwenn the amount change and percentage change of both oil and sales.,"print('Spearman Rank Correlation = {}'.format(
 process_train.sales.corr(process_train.onpromotion,method='spearman')))",first-project-store-sales.ipynb
for partial autocorrelation,import statsmodels.graphics.tsaplots ,first-project-store-sales.ipynb
"Oil price lag 10,15,26,27 is good to used. Let s see if there are any corelation to the sales.","oil_lag = [10,15,26,27]
for lag in oil_lag:
 oil_data['price_lag_{}'.format(lag)] = oil_data.interpolated_price.shift(lag)",first-project-store-sales.ipynb
"Noting Obvoius again. Higher the lag price, lower the sales. This just mightbe the result from the fact that the oil price had been dropping while the sales is getting better. A correlation, maybe, a causation, I don t know. PS. If someone could shed some light on this topic, please elaborate on the discussion panel please.As of now, I woudn t use the oil price as a feature.","del [oil_lag,oil_data,oil_for_lag_coor,lag_col]

gc.collect()",first-project-store-sales.ipynb
The Transaction record,"transactions = transaction_data.copy()
transactions = transactions.set_index('date')

del transaction_data",first-project-store-sales.ipynb
I really feel like this must be a very powerful feature. The transaction amount must be highly corelated with the sales. Let s process it by 1. Make a dic cotain store nbr as a key and transaction as elements 2. Interpolate the missing date in the dic 3. scatterplot the transaction with total sales record 4. decide if assumption is good to be used in the model ,transactions.index = pd.to_datetime(transactions.index),first-project-store-sales.ipynb
just for checking the outcome,merged_sales_transaction[1] ,first-project-store-sales.ipynb
for partial autocorrelation,import statsmodels.graphics.tsaplots ,first-project-store-sales.ipynb
for partial autocorrelation,import statsmodels.graphics.tsaplots ,first-project-store-sales.ipynb
just checking for store 1 dic,daily_store_sale_dict[1]. head () ,first-project-store-sales.ipynb
remove store nbt from index leaving only date,"for i in daily_store_sale_dict.keys():

 ax = statsmodels.graphics.tsaplots.plot_pacf(daily_store_sale_dict[i],lags=14, 
 title = 'store {} PA'.format(i))",first-project-store-sales.ipynb
Let s combine the process train and test for the ease of formatting,process_train,first-project-store-sales.ipynb
the city column is the same scale as holiday location.,"store_location = store_data.drop (['state' , 'type' , 'cluster'], axis = 1) ",first-project-store-sales.ipynb
merge the store location into inputs DF ,"inputs = merged_train.reset_index().merge(store_location,how='outer',left_on='store_nbr',right_on=store_location.index)",first-project-store-sales.ipynb
"Next, lag features: sales lag, total daily sales lag, transaction lag. I have prepared the store sales lag and tranasation lag, let s do the total country sales lag.",total_sales,first-project-store-sales.ipynb
Need to project the index further to 2017 08 31.,"total_sales_to_scale = pd.DataFrame(index=pd.date_range(start='2013-01-01',end='2017-08-31'))
total_sales_to_scale = total_sales_to_scale.merge(total_sales,how='left',left_index=True,right_index=True)
total_sales_to_scale = total_sales_to_scale.rename(columns={'sales':'national_sales'})",first-project-store-sales.ipynb
for partial autocorrelation,import statsmodels.graphics.tsaplots ,first-project-store-sales.ipynb
"lag 16 24 and 27,28 seems good to be used here.","lags= [16,17,18,19,20,21,22,23,24,27,28]
for lag in lags:
 total_sales_to_scale['nat_scaled_sales_lag{}'.format(lag)] = total_sales_to_scale['scaled_nat_sales'].shift(lag)",first-project-store-sales.ipynb
reset index for ease of merge,total_sales_to_scale.reset_index (). tail () ,first-project-store-sales.ipynb
just checking what are in the df,inputs.columns ,first-project-store-sales.ipynb
column named index in dt format don t need it anymore,"inputs.drop (['index'], axis = 1 , inplace = True) ",first-project-store-sales.ipynb
"Now, let s continue on individual store sales lag. From the lag ananyses above, I have conclude that the lags which will be used here are lag 1 2 3 4 5 6 7 8 13 14 . Let s put it to good use.",inputs,first-project-store-sales.ipynb
Now add the transaction lag to the inputs.,transactions,first-project-store-sales.ipynb
"still left the row from 2017 08 16 to 2017 08 31 and notice that someday, some store won t open such as new year that we only have store 25. How do I map this to the inputs dataframe?? ","store_nbr = range(1,55)
dates = pd.date_range('2013-01-01','2017-08-31')
mul_index = pd.MultiIndex.from_product([dates,store_nbr],names=['date','store_nbr'])
df = pd.DataFrame(index=mul_index)",first-project-store-sales.ipynb
just checking,df_transaction.loc[30020 : 30026] ,first-project-store-sales.ipynb
"df transaction.rename columns level 0 : date , level 1 : store nbr , inplace True ",df_transaction,first-project-store-sales.ipynb
"Now adding the lag. Remember the meaningful lag for transaction are lag 21,22,28 ","lags = [21,22,28]
for lag in lags:
 df_transaction['trans_lag_{}'.format(lag)] = df_transaction['transactions'].shift(lag)",first-project-store-sales.ipynb
in real life daily transaction came after day s end,"df_transaction = df_transaction.drop('transactions' , axis = 1) ",first-project-store-sales.ipynb
ust checking,df_transaction.loc[30030 : 30040] ,first-project-store-sales.ipynb
"Next, merge the df transaction to the inputs df.",inputs,first-project-store-sales.ipynb
"Next, add the holiday features into the inputs DF",calendar_checkpoint.reset_index().tail(),first-project-store-sales.ipynb
let s check for missing data.,"pd.set_option('display.max_rows',None)
inputs.isna().sum()",first-project-store-sales.ipynb
Drop all the NaN which are all lagging features ,"inputs.dropna(inplace = True)
inputs.isna().sum().sum()",first-project-store-sales.ipynb
Let s make the training data and testing data.,"y_train = inputs.loc['2013-01-01':'2017-08-15', 'sales']
y_train.tail()",first-project-store-sales.ipynb
Keep for later,test_id = x_test['id'] ,first-project-store-sales.ipynb
" Built model s and Validate model s Finally it s time for the model training. As I mentioned above, this very first attempt will be for the simple linear regression model and the metrics will be the Root Mean Squared Logarithmic Error RMSLE , in other words, the submission score itself. I am aimimg to surpass the benchmark score of 0.5109 from the Kaggle time series course. I didn t expect anythin more than abysmal for the result in this first attempt with simple model and inexperience feature eugineering. However, I hope that the next iterations to come will yield better results. Let s do it. ","from sklearn.linear_model import LinearRegression

ln = LinearRegression()
ln.fit(x_train,y_train)",first-project-store-sales.ipynb
" Test the models and SubmissionSince this is a regression prediction, I think I won t need a validation data. I will just measure the model by the score. ",len(y_pred),first-project-store-sales.ipynb
"First, make sure that the id are matched between the train data and y pred.",sample,first-project-store-sales.ipynb
The id are matched. Let s submit,"sample['sales'] = y_pred
sample",first-project-store-sales.ipynb
Submit,"sample.to_csv('submission.csv' , index = False) ",first-project-store-sales.ipynb
Calculate the Coeficients of the model.,"coef = pd.DataFrame(data = ln.coef_, index=x_train.columns,columns=['coef'])
coef['abs_value'] = abs(coef['coef'])

coef = coef.sort_values(by='abs_value',ascending=False)


pd.set_option('display.max_rows',None)
display(coef)
pd.reset_option('display.max_rows')
",first-project-store-sales.ipynb
Next the intercept.,ln.intercept_,first-project-store-sales.ipynb
In intercept is really strange. begining at a super big number. This is a sigh of really big residuals.Let s plot the predictions. I might find something even worse.,"result = x_test
result['predicted_sales'] = y_pred
result = result['predicted_sales'].groupby(['date','store_nbr']).sum()

result",first-project-store-sales.ipynb
Data PreparationImporting Dependencies Back to the table of contents,"!pip install -q efficientnet
import efficientnet.tfkeras as efn
from tensorflow.keras.applications import DenseNet201

import math, os, re, warnings
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from IPython.display import SVG

from kaggle_datasets import KaggleDatasets
from sklearn.utils import class_weight
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras import optimizers, applications, Sequential, layers
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler
import tensorflow as tf, tensorflow.keras.backend as K
from tensorflow.keras.models import Model


import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
from plotly.subplots import make_subplots

def seed_everything(seed):
 np.random.seed(seed)
 tf.random.set_seed(seed)
 os.environ['PYTHONHASHSEED'] = str(seed)
 os.environ['TF_DETERMINISTIC_OPS'] = '1'

seed = 42
seed_everything(seed)
warnings.filterwarnings(""ignore"")",flower-classification-augmentations-eda.ipynb
The below code is used to detect hardware and check whether our TPU is working or not... The output returned tells us about appropriate distribution strategy If the output is 8 replicas then TPU is switched on and working fine. If the output is 1 replica then TPU is not switched on and you can switch it on through Settings Accelerator TPU v3 8,try : ,flower-classification-augmentations-eda.ipynb
default distribution strategy in Tensorflow. Works on CPU and single GPU., strategy = tf.distribute.get_strategy () ,flower-classification-augmentations-eda.ipynb
Setting the parameters Back to the table of contents,"EPOCHS = 20
BATCH_SIZE = 16 * strategy.num_replicas_in_sync
WARMUP_LEARNING_RATE = 1e-4 * strategy.num_replicas_in_sync
WARMUP_EPOCHS = 3
HEIGHT = 512
WIDTH = 512
IMAGE_SIZE = [224, 224]
CHANNELS = 3
N_CLASSES = 104
ES_PATIENCE = 6
RLROP_PATIENCE = 3
DECAY_DROP = 0.3

model_path = 'DenseNet201_%sx%s.h5' % (HEIGHT, WIDTH)

GCS_PATH = KaggleDatasets().get_gcs_path() + '/tfrecords-jpeg-%sx%s' % (HEIGHT, WIDTH)

TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')
VALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')
TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec')",flower-classification-augmentations-eda.ipynb
Below defined are the 104 classes of flowers which most of the humans cannot classify but a machine can... Such advanced is the field of Computer Vision.,"CLASSES = [
 'pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 
 'wild geranium', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 
 'globe thistle', 'snapdragon', ""colt's foot"", 'king protea', 'spear thistle', 
 'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', 
 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 
 'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', 
 'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william', 
 'carnation', 'garden phlox', 'love in the mist', 'cosmos', 'alpine sea holly', 
 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 
 'lenten rose', 'barberton daisy', 'daffodil', 'sword lily', 'poinsettia', 
 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'daisy', 
 'common dandelion', 'petunia', 'wild pansy', 'primula', 'sunflower', 
 'lilac hibiscus', 'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 
 'pink-yellow dahlia', 'cautleya spicata', 'japanese anemone', 'black-eyed susan', 
 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'iris', 
 'windflower', 'tree poppy', 'gazania', 'azalea', 'water lily', 'rose', 
 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily', 
 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 
 'tree mallow', 'magnolia', 'cyclamen ', 'watercress', 'canna lily', 
 'hippeastrum ', 'bee balm', 'pink quill', 'foxglove', 'bougainvillea', 
 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', 
 'trumpet creeper', 'blackberry lily', 'common tulip', 'wild rose']",flower-classification-augmentations-eda.ipynb
Below is the code for the Learning Rate that will be used to train the model.. Note that we are not starting with a high learning rate because we are fine tuning a model and if we use high learning rate at the beginning then it might break the pretrained weights... To know more about Learning rate warm up please refer to this answer,"learning_rate = 3e-5 * strategy.num_replicas_in_sync
lr_start = 0.00000001
lr_min = 0.000001
lr_max = 3e-5 * strategy.num_replicas_in_sync
lr_rampup_epochs = 3
lr_sustain_epochs = 0
lr_exp_decay = .8

def lrfn(epoch):
 if epoch < lr_rampup_epochs:
 lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start
 elif epoch < lr_rampup_epochs + lr_sustain_epochs:
 lr = lr_max
 else:
 lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min
 return lr
 
 
lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)

rng = [i for i in range(21 if EPOCHS<21 else EPOCHS)]
y = [lrfn(x) for x in rng]


print(""Learning rate schedule: {:.3g} to {:.3g} to {:.3g}"".format(y[0], max(y), y[-1]))",flower-classification-augmentations-eda.ipynb
Visualizing anything makes understanding better so to understand how learning rate will change over the epochs please hover over the plot,"fig = go.Figure()
fig.add_trace(go.Scatter(x=rng, y=y,
 mode='lines+markers',
 line=dict(color='royalblue', width=4)))
fig.update_layout(
 title='Learning Rate Schedule',
 title_x=0.5,
 xaxis_title=""Range of epochs"",
 yaxis_title=""Learning rate in 10^-6"",
 paper_bgcolor='rgb(252, 252, 255)',
 plot_bgcolor='rgb(248, 248, 255)',
 font=dict(
 size=18,
 color=""red""
 )
)
fig.show()",flower-classification-augmentations-eda.ipynb
instructs the API to read from multiple files if available.,AUTO = tf.data.experimental.AUTOTUNE ,flower-classification-augmentations-eda.ipynb
Visualization functions Back to the table of contents,"np.set_printoptions(threshold = 15 , linewidth = 80) ",flower-classification-augmentations-eda.ipynb
"binary string in this case, these are image ID strings", if numpy_labels.dtype == object : ,flower-classification-augmentations-eda.ipynb
"If no labels, only image IDs, return None for labels this is the case for test data "," return numpy_images , numpy_labels ",flower-classification-augmentations-eda.ipynb
"Augmentation Functions Back to the table of contentsI will also be adding cutmix, mix up in the future versions if this notebook recieves a good response","def get_mat(rotation , shear , height_zoom , width_zoom , height_shift , width_shift): ",flower-classification-augmentations-eda.ipynb
CONVERT DEGREES TO RADIANS, rotation = math.pi * rotation / 180. ,flower-classification-augmentations-eda.ipynb
ROTATION MATRIX, c1 = tf.math.cos(rotation) ,flower-classification-augmentations-eda.ipynb
SHEAR MATRIX, c2 = tf.math.cos(shear) ,flower-classification-augmentations-eda.ipynb
ZOOM MATRIX," zoom_matrix = tf.reshape(tf.concat ([one / height_zoom , zero , zero , zero , one / width_zoom , zero , zero , zero , one], axis = 0),[3 , 3]) ",flower-classification-augmentations-eda.ipynb
SHIFT MATRIX," shift_matrix = tf.reshape(tf.concat ([one , zero , height_shift , zero , one , width_shift , zero , zero , one], axis = 0),[3 , 3]) ",flower-classification-augmentations-eda.ipynb
"output image randomly rotated, sheared, zoomed, and shifted", DIM = IMAGE_SIZE[0] ,flower-classification-augmentations-eda.ipynb
fix for size 331, XDIM = DIM % 2 ,flower-classification-augmentations-eda.ipynb
GET TRANSFORMATION MATRIX," m = get_mat(rot , shr , h_zoom , w_zoom , h_shift , w_shift) ",flower-classification-augmentations-eda.ipynb
LIST DESTINATION PIXEL INDICES," x = tf.repeat(tf.range(DIM // 2 , - DIM // 2 , - 1), DIM) ",flower-classification-augmentations-eda.ipynb
ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS," idx2 = K.dot(m , tf.cast(idx , dtype = 'float32')) ",flower-classification-augmentations-eda.ipynb
FIND ORIGIN PIXEL VALUES," idx3 = tf.stack ([DIM // 2 - idx2[0 ,], DIM // 2 - 1 + idx2[1 ,]]) ",flower-classification-augmentations-eda.ipynb
Train data,NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES) ,flower-classification-augmentations-eda.ipynb
Validation data,NUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES) ,flower-classification-augmentations-eda.ipynb
Test data,NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES) ,flower-classification-augmentations-eda.ipynb
Visualizations Training Images Back to the table of contents,disp_images(next(iter(train_dataset.unbatch().batch(8)))),flower-classification-augmentations-eda.ipynb
Validation Images Back to the table of contents,disp_images(next(iter(valid_dataset.unbatch().batch(8)))),flower-classification-augmentations-eda.ipynb
Test Images Back to the table of contents,disp_images(next(iter(test_dataset.unbatch().batch(8)))),flower-classification-augmentations-eda.ipynb
Augmentations Back to the table of contents,"for i in range(2):
 row = 2; col = 4;
 all_elements = get_training_dataset(do_aug=False).unbatch()
 one_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )
 augmented_element = one_element.repeat().map(transform).batch(row*col)

 for (img,label) in augmented_element:
 plt.figure(figsize=(16,int(16*row/col)))
 for j in range(row*col):
 plt.subplot(row,col,j+1)
 plt.axis('off')
 plt.imshow(img[j,])
 plt.show()
 break",flower-classification-augmentations-eda.ipynb
Now let s look at how many flowers of each type are there in the test and train dataset,"train_agg = np.asarray([[label, (y_train == index).sum()] for index, label in enumerate(CLASSES)])
valid_agg = np.asarray([[label, (y_valid == index).sum()] for index, label in enumerate(CLASSES)])
fig = go.Figure(data=[
 go.Bar(name='Train', x=train_agg[...,1], y=train_agg[...,0],orientation='h',
 marker=dict(color='rgba(102, 255, 102, 0.5)')),
 go.Bar(name='Validation',x=valid_agg[...,1], y=valid_agg[...,0],orientation='h',
 marker=dict(color='rgba(255, 102, 102, 0.5)'))
])
fig.update_layout(
 title='Train and Validation Class distribution',
 title_x=0.5,
 barmode='stack',
 xaxis_title="""",
 yaxis_title="""",
 font=dict(
 size=10,
 color=""royalblue""
 ),
 paper_bgcolor='rgb(252, 252, 255)',
 plot_bgcolor='rgb(248, 248, 255)',
 autosize=False,
 width=800,
 height=1500,
 margin=dict(
 l=0,
 r=0,
 b=0,
 t=40,
 pad=4
 ))

fig.show()",flower-classification-augmentations-eda.ipynb
"The first model we will be dealing with is DenseNet201 Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. DenseNet connects each layer to every other layer in a feed forward fashion. Whereas traditional convolutional networks with L layers have L connections one between each layer and its subsequent layer DenseNet network has L L 1 2 direct connections. For each layer, the feature maps of all preceding layers are used as inputs, and its own feature maps are used as inputs into all subsequent layers.For more information on DenseNet201 please follow this link","def create_model(input_shape , N_CLASSES): ",flower-classification-augmentations-eda.ipynb
Warm up the top layers Back to the table of contents,"with strategy.scope():
 model = create_model((None, None, CHANNELS), N_CLASSES)
 
metric_list = ['sparse_categorical_accuracy']

optimizer = optimizers.Adam(lr=WARMUP_LEARNING_RATE)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=metric_list)
model.summary()",flower-classification-augmentations-eda.ipynb
As I said earlier in the notebook that anything that is visualized makes understanding things more better so let s visualize the model architecture Fundamental DenseNet Block Back to the table of contents,"SVG(tf.keras.utils.model_to_dot(Model(model.layers[0].input, model.layers[0].layers[13].output), dpi=75).create(prog='dot', format='svg'))",flower-classification-augmentations-eda.ipynb
"The above image shows the fundamental block in the DenseNet architecture. The architecture mainly involves Convolution, Maxpooling, ReLU, and concatenation. Model Architecture Back to the table of contents","SVG(tf.keras.utils.model_to_dot(model, dpi=75).create(prog='dot', format='svg'))",flower-classification-augmentations-eda.ipynb
Remember the learning rate plot where there was a linear increase for first three epochs... The above code is warming up the top layers so that we can fine tune it in the next step Fine Tuning all the layers Back to the table of contents,for layer in model.layers : ,flower-classification-augmentations-eda.ipynb
Unfreeze layers, layer.trainable = True ,flower-classification-augmentations-eda.ipynb
Visualize the results Back to the table of contents,"def display_training_curves(training, validation):
 fig = go.Figure()
 
 fig.add_trace(
 go.Scatter(x=np.arange(1, EPOCHS+1), mode='lines+markers', y=training, marker=dict(color=""dodgerblue""),
 name=""Train""))
 
 fig.add_trace(
 go.Scatter(x=np.arange(1, EPOCHS+1), mode='lines+markers', y=validation, marker=dict(color='red'),
 name=""Val""))
 if training != history['loss']:
 fig.update_layout(title_x=0.5,title_text='Accuracy vs Epochs', 
 yaxis_title='Accuracy', xaxis_title=""Epochs"",
 paper_bgcolor='rgb(252, 252, 255)',
 plot_bgcolor='rgb(248, 248, 255)',)
 else:
 fig.update_layout(title_x=0.5,title_text='Loss vs Epochs', 
 yaxis_title='Loss', xaxis_title=""Epochs"",
 paper_bgcolor='rgb(252, 252, 255)',
 plot_bgcolor='rgb(248, 248, 255)',)
 fig.show()",flower-classification-augmentations-eda.ipynb
Please click the PLAY button to understand how Train accuracy and validation accuracy change over the time,"acc_df = pd.DataFrame(np.transpose([[*np.arange(1, 20).tolist()*2], [""Train""]*19 + [""Val""]*19,
 history['loss'] + history['val_loss'] ]))
acc_df.columns = [""Epochs"", ""Stage"", ""Loss""]",flower-classification-augmentations-eda.ipynb
Dependencies,"!pip install --quiet efficientnet

import math, os, re, warnings, random
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from kaggle_datasets import KaggleDatasets
from sklearn.utils import class_weight
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
import tensorflow.keras.layers as L
from tensorflow.keras import optimizers, applications, Sequential, losses
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler
import efficientnet.tfkeras as efn

def seed_everything(seed=0):
 random.seed(seed)
 np.random.seed(seed)
 tf.random.set_seed(seed)
 os.environ['PYTHONHASHSEED'] = str(seed)
 os.environ['TF_DETERMINISTIC_OPS'] = '1'

seed = 0
seed_everything(seed)
warnings.filterwarnings(""ignore"")",flower-classification-with-tpus-eda-and-baseline.ipynb
"Detect hardware, return appropriate distribution strategy",try : ,flower-classification-with-tpus-eda-and-baseline.ipynb
Model parameters,"BATCH_SIZE = 16 * REPLICAS
WARMUP_EPOCHS = 3
WARMUP_LEARNING_RATE = 1e-4 * REPLICAS
EPOCHS = 30
LEARNING_RATE = 3e-5 * REPLICAS
HEIGHT = 512
WIDTH = 512
CHANNELS = 3
N_CLASSES = 104
ES_PATIENCE = 5",flower-classification-with-tpus-eda-and-baseline.ipynb
instructs the API to read from multiple files if available.,AUTO = tf.data.experimental.AUTOTUNE ,flower-classification-with-tpus-eda-and-baseline.ipynb
Visualization utility functions,"np.set_printoptions(threshold = 15 , linewidth = 80) ",flower-classification-with-tpus-eda-and-baseline.ipynb
"binary string in this case, these are image ID strings", if numpy_labels.dtype == object : ,flower-classification-with-tpus-eda-and-baseline.ipynb
"If no labels, only image IDs, return None for labels this is the case for test data "," return numpy_images , numpy_labels ",flower-classification-with-tpus-eda-and-baseline.ipynb
Train data,NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES) ,flower-classification-with-tpus-eda-and-baseline.ipynb
Validation data,NUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES) ,flower-classification-with-tpus-eda-and-baseline.ipynb
Test data,NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES) ,flower-classification-with-tpus-eda-and-baseline.ipynb
First let s look at some samples from each setTrain samples,display_batch_of_images(next(iter(train_dataset.unbatch().batch(20)))),flower-classification-with-tpus-eda-and-baseline.ipynb
Validation samples,display_batch_of_images(next(iter(valid_dataset.unbatch().batch(20)))),flower-classification-with-tpus-eda-and-baseline.ipynb
Test samples,display_batch_of_images(next(iter(test_dataset.unbatch().batch(20)))),flower-classification-with-tpus-eda-and-baseline.ipynb
Label distribution,"train_agg = np.asarray([[label, (y_train == index).sum()] for index, label in enumerate(CLASSES)])
valid_agg = np.asarray([[label, (y_valid == index).sum()] for index, label in enumerate(CLASSES)])

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 64))

ax1 = sns.barplot(x=train_agg[...,1], y=train_agg[...,0], order=CLASSES, ax=ax1)
ax1.set_title('Train', fontsize=30)
ax1.tick_params(labelsize=16)

ax2 = sns.barplot(x=valid_agg[...,1], y=valid_agg[...,0], order=CLASSES, ax=ax2)
ax2.set_title('Validation', fontsize=30)
ax2.tick_params(labelsize=16)

plt.show()",flower-classification-with-tpus-eda-and-baseline.ipynb
Model,"def create_model(input_shape , N_CLASSES): ",flower-classification-with-tpus-eda-and-baseline.ipynb
Warmup top layers,"with strategy.scope():
 model = create_model((None, None, CHANNELS), N_CLASSES)
 
metric_list = ['sparse_categorical_accuracy']

optimizer = optimizers.Adam(lr=WARMUP_LEARNING_RATE)
model.compile(optimizer=optimizer, 
 loss=losses.SparseCategoricalCrossentropy(), 
 metrics=metric_list)
model.summary()",flower-classification-with-tpus-eda-and-baseline.ipynb
Learning rate schedule,"LR_START = 0.00000001
LR_MIN = 0.000001
LR_MAX = LEARNING_RATE
LR_RAMPUP_EPOCHS = 3
LR_SUSTAIN_EPOCHS = 0
LR_EXP_DECAY = .8

def lrfn(epoch):
 if epoch < LR_RAMPUP_EPOCHS:
 lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START
 elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:
 lr = LR_MAX
 else:
 lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN
 return lr
 
rng = [i for i in range(EPOCHS)]
y = [lrfn(x) for x in rng]

sns.set(style='whitegrid')
fig, ax = plt.subplots(figsize=(20, 6))
plt.plot(rng, y)

print(f'{EPOCHS} total epochs and {NUM_TRAINING_IMAGES//BATCH_SIZE} steps per epoch')
print(f'Learning rate schedule: {y[0]:.3g} to {max(y):.3g} to {y[-1]:.3g}')",flower-classification-with-tpus-eda-and-baseline.ipynb
Fine tune all layers,for layer in model.layers : ,flower-classification-with-tpus-eda-and-baseline.ipynb
Unfreeze layers, layer.trainable = True ,flower-classification-with-tpus-eda-and-baseline.ipynb
Model loss graph,"def plot_metrics(history, metric_list):
 fig, axes = plt.subplots(len(metric_list), 1, sharex='col', figsize=(24, 12))
 axes = axes.flatten()
 
 for index, metric in enumerate(metric_list):
 axes[index].plot(history[metric], label=f'Train {metric}')
 axes[index].plot(history[f'val_{metric}'], label=f'Validation {metric}')
 axes[index].legend(loc='best', fontsize=16)
 axes[index].set_title(metric)

 plt.xlabel('Epochs', fontsize=16)
 sns.despine()
 plt.show()

plot_metrics(history, metric_list=['loss', 'sparse_categorical_accuracy'])",flower-classification-with-tpus-eda-and-baseline.ipynb
Model evaluationTrain set,"x_train = train_dataset.map(lambda image, label: image)
train_preds = model.predict(x_train)
train_preds = np.argmax(train_preds, axis=-1)

print(classification_report(y_train, train_preds, target_names=CLASSES))",flower-classification-with-tpus-eda-and-baseline.ipynb
Validation set,"x_valid = valid_dataset.map(lambda image, label: image)
valid_preds = model.predict(x_valid)
valid_preds = np.argmax(valid_preds, axis=-1)

print(classification_report(y_valid, valid_preds, target_names=CLASSES))",flower-classification-with-tpus-eda-and-baseline.ipynb
"Confusion matrixTrainI have split confusion matrices into 3 parts to make it clearer, the first plot has classes 1 from 34, the second plot 35 to 69 and the third has the remaining ones.","fig, ax = plt.subplots(1, 1, figsize=(20, 45))
train_cfn_matrix = confusion_matrix(y_train, train_preds, labels=range(len(CLASSES)))
train_cfn_matrix = (train_cfn_matrix.T / train_cfn_matrix.sum(axis=1)).T
train_df_cm = pd.DataFrame(train_cfn_matrix, index=CLASSES, columns=CLASSES)
ax = sns.heatmap(train_df_cm, cmap='Blues').set_title('Train', fontsize=30)
plt.show()",flower-classification-with-tpus-eda-and-baseline.ipynb
Validation,"fig, ax = plt.subplots(1, 1, figsize=(20, 45))
valid_cfn_matrix = confusion_matrix(y_valid, valid_preds, labels=range(len(CLASSES)))
valid_cfn_matrix = (valid_cfn_matrix.T / valid_cfn_matrix.sum(axis=1)).T
valid_df_cm = pd.DataFrame(valid_cfn_matrix, index=CLASSES, columns=CLASSES)
ax = sns.heatmap(valid_df_cm, cmap=sns.cubehelix_palette(8)).set_title('Validation', fontsize=30)
plt.show()",flower-classification-with-tpus-eda-and-baseline.ipynb
Visualize predictionsTrain set,"x_train_samp, y_train_samp = dataset_to_numpy_util(train_dataset, 9)
train_samp_preds = model.predict(x_train_samp, batch_size=9)
display_9_images_with_predictions(x_train_samp, train_samp_preds, y_train_samp)",flower-classification-with-tpus-eda-and-baseline.ipynb
Validation set,"x_valid_samp, y_valid_samp = dataset_to_numpy_util(valid_dataset, 9)
valid_samp_preds = model.predict(x_valid_samp, batch_size=9)
display_9_images_with_predictions(x_valid_samp, valid_samp_preds, y_valid_samp)",flower-classification-with-tpus-eda-and-baseline.ipynb
Test set predictions,"x_test = test_dataset.map(lambda image, idnum: image)
test_preds = model.predict(x_test)
test_preds = np.argmax(test_preds, axis=-1)",flower-classification-with-tpus-eda-and-baseline.ipynb
Dependencies,"!pip install --quiet efficientnet

import numpy as np
import pandas as pd
import seaborn as sns
import os, re, math, warnings, random
from matplotlib import pyplot as plt
from kaggle_datasets import KaggleDatasets
from sklearn.model_selection import KFold
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
import tensorflow.keras.layers as L
import tensorflow.keras.backend as K
from tensorflow.keras import optimizers, applications, Sequential, losses
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler
import efficientnet.tfkeras as efn

def seed_everything(seed=0):
 random.seed(seed)
 np.random.seed(seed)
 tf.random.set_seed(seed)
 os.environ['PYTHONHASHSEED'] = str(seed)
 os.environ['TF_DETERMINISTIC_OPS'] = '1'

seed = 0
seed_everything(seed)
warnings.filterwarnings('ignore')",flower-with-tpus-advanced-augmentations.ipynb
"Detect hardware, return appropriate distribution strategy",try : ,flower-with-tpus-advanced-augmentations.ipynb
Model parameters,"BATCH_SIZE = 32 * REPLICAS
LEARNING_RATE = 3e-5 * REPLICAS
EPOCHS = 20
HEIGHT = 331
WIDTH = 331
CHANNELS = 3
N_CLASSES = 104
ES_PATIENCE = 5
N_FOLDS = 5
FOLDS_USED = 5",flower-with-tpus-advanced-augmentations.ipynb
About the datasets,def count_data_items(filenames): ,flower-with-tpus-advanced-augmentations.ipynb
the number of data items is written in the name of the .tfrec files.," n =[int(re.compile(r""-([0-9]*)\.""). search(filename). group(1)) for filename in filenames] ",flower-with-tpus-advanced-augmentations.ipynb
Train data,NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES) ,flower-with-tpus-advanced-augmentations.ipynb
Test data,NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES) ,flower-with-tpus-advanced-augmentations.ipynb
"Advanced augmentationsHere is where we can do fine control of the augmentation methods like we are used to doing with albumentations. I like better this kind of control because we can split the transformations into groups and only perform some of them, this is more effective on pixel level transforms because if you apply more than once on the same image things can be confusing. One advantage of using probabilities like this is that you can apply transformation from tf.image that are not random like tf.image.adjust gamma or any other. ","def data_augment(image , label): ",flower-with-tpus-advanced-augmentations.ipynb
Flips, if p_spatial >= .2 : ,flower-with-tpus-advanced-augmentations.ipynb
Rotates, if p_rotate > .75 : ,flower-with-tpus-advanced-augmentations.ipynb
rotate 270," image = tf.image.rot90(image , k = 3) ",flower-with-tpus-advanced-augmentations.ipynb
rotate 180," image = tf.image.rot90(image , k = 2) ",flower-with-tpus-advanced-augmentations.ipynb
rotate 90," image = tf.image.rot90(image , k = 1) ",flower-with-tpus-advanced-augmentations.ipynb
Rotation, if p_rotation >= .3 : ,flower-with-tpus-advanced-augmentations.ipynb
Shift, if p_shift >= .3 : ,flower-with-tpus-advanced-augmentations.ipynb
Shear, if p_shear >= .3 : ,flower-with-tpus-advanced-augmentations.ipynb
Crops, if p_crop > .4 : ,flower-with-tpus-advanced-augmentations.ipynb
Pixel level transforms, if p_pixel >= .2 : ,flower-with-tpus-advanced-augmentations.ipynb
data augmentation kernel: ,"def transform_rotation(image , height , rotation): ",flower-with-tpus-advanced-augmentations.ipynb
output image randomly rotated, DIM = height ,flower-with-tpus-advanced-augmentations.ipynb
fix for size 331, XDIM = DIM % 2 ,flower-with-tpus-advanced-augmentations.ipynb
CONVERT DEGREES TO RADIANS, rotation = math.pi * rotation / 180. ,flower-with-tpus-advanced-augmentations.ipynb
ROTATION MATRIX, c1 = tf.math.cos(rotation) ,flower-with-tpus-advanced-augmentations.ipynb
LIST DESTINATION PIXEL INDICES," x = tf.repeat(tf.range(DIM // 2 , - DIM // 2 , - 1), DIM) ",flower-with-tpus-advanced-augmentations.ipynb
ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS," idx2 = K.dot(rotation_matrix , tf.cast(idx , dtype = 'float32')) ",flower-with-tpus-advanced-augmentations.ipynb
FIND ORIGIN PIXEL VALUES," idx3 = tf.stack ([DIM // 2 - idx2[0 ,], DIM // 2 - 1 + idx2[1 ,]]) ",flower-with-tpus-advanced-augmentations.ipynb
output image randomly sheared, DIM = height ,flower-with-tpus-advanced-augmentations.ipynb
fix for size 331, XDIM = DIM % 2 ,flower-with-tpus-advanced-augmentations.ipynb
SHEAR MATRIX," one = tf.constant ([1], dtype = 'float32') ",flower-with-tpus-advanced-augmentations.ipynb
LIST DESTINATION PIXEL INDICES," x = tf.repeat(tf.range(DIM // 2 , - DIM // 2 , - 1), DIM) ",flower-with-tpus-advanced-augmentations.ipynb
ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS," idx2 = K.dot(shear_matrix , tf.cast(idx , dtype = 'float32')) ",flower-with-tpus-advanced-augmentations.ipynb
FIND ORIGIN PIXEL VALUES," idx3 = tf.stack ([DIM // 2 - idx2[0 ,], DIM // 2 - 1 + idx2[1 ,]]) ",flower-with-tpus-advanced-augmentations.ipynb
output image randomly shifted, DIM = height ,flower-with-tpus-advanced-augmentations.ipynb
fix for size 331, XDIM = DIM % 2 ,flower-with-tpus-advanced-augmentations.ipynb
SHIFT MATRIX," shift_matrix = tf.reshape(tf.concat ([one , zero , height_shift , zero , one , width_shift , zero , zero , one], axis = 0),[3 , 3]) ",flower-with-tpus-advanced-augmentations.ipynb
LIST DESTINATION PIXEL INDICES," x = tf.repeat(tf.range(DIM // 2 , - DIM // 2 , - 1), DIM) ",flower-with-tpus-advanced-augmentations.ipynb
ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS," idx2 = K.dot(shift_matrix , tf.cast(idx , dtype = 'float32')) ",flower-with-tpus-advanced-augmentations.ipynb
FIND ORIGIN PIXEL VALUES," idx3 = tf.stack ([DIM // 2 - idx2[0 ,], DIM // 2 - 1 + idx2[1 ,]]) ",flower-with-tpus-advanced-augmentations.ipynb
Datasets utility functions,def decode_image(image_data): ,flower-with-tpus-advanced-augmentations.ipynb
Visualization utility functions,"np.set_printoptions(threshold = 15 , linewidth = 80) ",flower-with-tpus-advanced-augmentations.ipynb
Look at some augmented samples,"train_dataset_aug = get_dataset(TRAINING_FILENAMES, labeled=True, ordered=False, repeated=True, shufled=True, augmented=True)
display_batch_of_images(next(iter(train_dataset_aug.unbatch().batch(20))))
display_batch_of_images(next(iter(train_dataset_aug.unbatch().batch(20))))
display_batch_of_images(next(iter(train_dataset_aug.unbatch().batch(20))))",flower-with-tpus-advanced-augmentations.ipynb
Model EfficientNet EfficientNet has proven to be a very good baseline for many computer vision tasks,"def create_model(input_shape, N_CLASSES):
 base_model = efn.EfficientNetB4(weights='noisy-student', 
 include_top=False,
 input_shape=input_shape)

 model = tf.keras.Sequential([
 base_model,
 L.GlobalAveragePooling2D(),
 L.Dense(N_CLASSES, activation='softmax')
 ])
 
 
 optimizer = optimizers.Adam(lr=LEARNING_RATE)
 model.compile(optimizer=optimizer, 
 loss=losses.SparseCategoricalCrossentropy(), 
 metrics=['sparse_categorical_accuracy'])
 
 return model",flower-with-tpus-advanced-augmentations.ipynb
Learning rate schedule Exponential decay with warmup ,"def exponential_schedule_with_warmup(epoch):
 '''
 Create a schedule with a learning rate that decreases exponentially after linearly increasing during a warmup period.
 '''
 
 warmup_epochs=3
 hold_max_epochs=0
 lr_start=1e-6
 lr_max=LEARNING_RATE
 lr_min=1e-6
 decay=0.8
 
 
 if epoch < warmup_epochs:
 lr = (lr_max - lr_start) / warmup_epochs * epoch + lr_start
 elif epoch < warmup_epochs + hold_max_epochs:
 lr = lr_max
 else:
 lr = lr_max * (decay ** (epoch - warmup_epochs - hold_max_epochs))
 if lr_min is not None:
 lr = tf.math.maximum(lr_min, lr)
 
 return lr

 
rng = [i for i in range(EPOCHS)]
y = [exponential_schedule_with_warmup(x) for x in rng]

sns.set(style='whitegrid')
fig, ax = plt.subplots(figsize=(20, 6))
plt.plot(rng, y)

print(f'{EPOCHS} total epochs and {NUM_TRAINING_IMAGES//BATCH_SIZE} steps per epoch')
print(f'Learning rate schedule: {y[0]:.3g} to { max(y):.3g} to { y[-1]:.3g}')",flower-with-tpus-advanced-augmentations.ipynb
Train,"kfold = KFold(N_FOLDS , shuffle = True , random_state = seed) ",flower-with-tpus-advanced-augmentations.ipynb
Datasets,"test_dataset = get_dataset(TEST_FILENAMES , labeled = False , ordered = True) ",flower-with-tpus-advanced-augmentations.ipynb
Data, fold_train_filenames = np.asarray(TRAINING_FILENAMES)[ trn_ind] ,flower-with-tpus-advanced-augmentations.ipynb
Train model, K.clear_session () ,flower-with-tpus-advanced-augmentations.ipynb
Model loss graph,"for index, history in enumerate(history_list):
 print(f'FOLD {index+1}')
 plot_metrics(history, metric_list=['loss', 'sparse_categorical_accuracy'])",flower-with-tpus-advanced-augmentations.ipynb
Model evaluation,"print(classification_report(y_valid, valid_pred, target_names=CLASSES))",flower-with-tpus-advanced-augmentations.ipynb
Confusion matrix,"fig, ax = plt.subplots(1, 1, figsize=(20, 45))
cfn_matrix = confusion_matrix(y_valid, valid_pred, labels=range(len(CLASSES)))
cfn_matrix = (cfn_matrix.T / cfn_matrix.sum(axis=1)).T
df_cm = pd.DataFrame(cfn_matrix, index=CLASSES, columns=CLASSES)
ax = sns.heatmap(df_cm, cmap='Blues').set_title('Labels', fontsize=30)
plt.show()",flower-with-tpus-advanced-augmentations.ipynb
Visualize predictions,"x_samp, y_samp = dataset_to_numpy_util(oof_ds, 9)
samp_preds = model.predict(x_samp, batch_size=9)
display_9_images_with_predictions(x_samp, samp_preds, y_samp)",flower-with-tpus-advanced-augmentations.ipynb
Test set predictions,"test_ids_ds = test_dataset.map(lambda image, idnum: idnum).unbatch()
test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')

submission = pd.DataFrame(test_ids, columns=['id'])
submission['label'] = test_preds
submission.to_csv('submission.csv', index=False)
display(submission.head(10))",flower-with-tpus-advanced-augmentations.ipynb
"IntroductionIn Lessons 2 and 3, we treated forecasting as a simple regression problem with all of our features derived from a single input, the time index. We could easily create forecasts for any time in the future by just generating our desired trend and seasonal features.When we added lag features in Lesson 4, however, the nature of the problem changed. Lag features require that the lagged target value is known at the time being forecast. A lag 1 feature shifts the time series forward 1 step, which means you could forecast 1 step into the future but not 2 steps.In Lesson 4, we just assumed that we could always generate lags up to the period we wanted to forecast every prediction was for just one step forward, in other words . Real world forecasting typically demands more than this, so in this lesson we ll learn how to make forecasts for a variety of situations.Defining the Forecasting TaskThere are two things to establish before designing a forecasting model: what information is available at the time a forecast is made features , and, the time period during which you require forecasted values target .The forecast origin is time at which you are making a forecast. Practically, you might consider the forecast origin to be the last time for which you have training data for the time being predicted. Everything up to he origin can be used to create features.The forecast horizon is the time for which you are making a forecast. We often describe a forecast by the number of time steps in its horizon: a 1 step forecast or 5 step forecast, say. The forecast horizon describes the target. A three step forecast horizon with a two step lead time, using four lag features. The figure represents what would be a single row of training data data for a single prediction, in other words. The time between the origin and the horizon is the lead time or sometimes latency of the forecast. A forecast s lead time is described by the number of steps from origin to horizon: a 1 step ahead or 3 step ahead forecast, say. In practice, it may be necessary for a forecast to begin multiple steps ahead of the origin because of delays in data acquisition or processing.Preparing Data for ForecastingIn order to forecast time series with ML algorithms, we need to transform the series into a dataframe we can use with those algorithms. Unless, of course, you are only using deterministic features like trend and seasonality. We saw the first half of this process in Lesson 4 when we created a feature set out of lags. The second half is preparing the target. How we do this depends on the forecasting task.Each row in a dataframe represents a single forecast. The time index of the row is the first time in the forecast horizon, but we arrange values for the entire horizon in the same row. For multistep forecasts, this means we are requiring a model to produce multiple outputs, one for each step.",import numpy as np ,forecasting-with-machine-learning.ipynb
"The above illustrates how a dataset would be prepared similar to the Defining a Forecast figure: a three step forecasting task with a two step lead time using five lag features. The original time series is y step 1. The missing values we could either fill in or drop.Multistep Forecasting StrategiesThere are a number of strategies for producing the multiple target steps required for a forecast. We ll outline four common strategies, each with strengths and weaknesses.Multioutput modelUse a model that produces multiple outputs naturally. Linear regression and neural networks can both produce multiple outputs. This strategy is simple and efficient, but not possible for every algorithm you might want to use. XGBoost can t do this, for instance. Direct strategyTrain a separate model for each step in the horizon: one model forecasts 1 step ahead, another 2 steps ahead, and so on. Forecasting 1 step ahead is a different problem than 2 steps ahead and so on , so it can help to have a different model make forecasts for each step. The downside is that training lots of models can be computationally expensive. Recursive strategyTrain a single one step model and use its forecasts to update the lag features for the next step. With the recursive method, we feed a model s 1 step forecast back in to that same model to use as a lag feature for the next forecasting step. We only need to train one model, but since errors will propagate from step to step, forecasts can be inaccurate for long horizons. DirRec strategyA combination of the direct and recursive strategies: train a model for each step and use forecasts from previous steps as new lag features. Step by step, each model gets an additional lag input. Since each model always has an up to date set of lag features, the DirRec strategy can capture serial dependence better than Direct, but it can also suffer from error propagation like Recursive. Example Flu TrendsIn this example we ll apply the MultiOutput and Direct strategies to the Flu Trends data from Lesson 4, this time making true forecasts for multiple weeks beyond the training period.We ll define our forecasting task to have an 8 week horizon with a 1 week lead time. In other words, we ll be forecasting eight weeks of flu cases starting with the following week.The hidden cell sets up the example and defines a helper function plot multistep.",from pathlib import Path ,forecasting-with-machine-learning.ipynb
Set Matplotlib defaults,"plt.style.use(""seaborn-whitegrid"") ",forecasting-with-machine-learning.ipynb
"First we ll prepare our target series weekly office visits for the flu for multistep forecasting. Once this is done, training and prediction will be very straightfoward.","def make_lags(ts , lags , lead_time = 1): ",forecasting-with-machine-learning.ipynb
Create splits,"X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.25 , shuffle = False) ",forecasting-with-machine-learning.ipynb
"Remember that a multistep model will produce a complete forecast for each instance used as input. There are 269 weeks in the training set and 90 weeks in the test set, and we now have an 8 step forecast for each of these weeks.","
train_rmse = mean_squared_error(y_train, y_fit, squared=False)
test_rmse = mean_squared_error(y_test, y_pred, squared=False)
print((f""Train RMSE: {train_rmse:.2f}\n"" f""Test RMSE: {test_rmse:.2f}""))

palette = dict(palette='husl', n_colors=64)
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 6))
ax1 = flu_trends.FluVisits[y_fit.index].plot(**plot_params, ax=ax1)
ax1 = plot_multistep(y_fit, ax=ax1, palette_kwargs=palette)
_ = ax1.legend(['FluVisits (train)', 'Forecast'])
ax2 = flu_trends.FluVisits[y_pred.index].plot(**plot_params, ax=ax2)
ax2 = plot_multistep(y_pred, ax=ax2, palette_kwargs=palette)
_ = ax2.legend(['FluVisits (test)', 'Forecast'])",forecasting-with-machine-learning.ipynb
"Direct strategyXGBoost can t produce multiple outputs for regression tasks. But by applying the Direct reduction strategy, we can still use it to produce multi step forecasts. This is as easy as wrapping it with scikit learn s MultiOutputRegressor.","from sklearn.multioutput import MultiOutputRegressor

model = MultiOutputRegressor(XGBRegressor())
model.fit(X_train, y_train)

y_fit = pd.DataFrame(model.predict(X_train), index=X_train.index, columns=y.columns)
y_pred = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)",forecasting-with-machine-learning.ipynb
XGBoost here is clearly overfitting on the training set. But on the test set it seems it was able to capture some of the dynamics of the flu season better than the linear regression model. It would likely do even better with some hyperparameter tuning.,"
train_rmse = mean_squared_error(y_train, y_fit, squared=False)
test_rmse = mean_squared_error(y_test, y_pred, squared=False)
print((f""Train RMSE: {train_rmse:.2f}\n"" f""Test RMSE: {test_rmse:.2f}""))

palette = dict(palette='husl', n_colors=64)
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 6))
ax1 = flu_trends.FluVisits[y_fit.index].plot(**plot_params, ax=ax1)
ax1 = plot_multistep(y_fit, ax=ax1, palette_kwargs=palette)
_ = ax1.legend(['FluVisits (train)', 'Forecast'])
ax2 = flu_trends.FluVisits[y_pred.index].plot(**plot_params, ax=ax2)
ax2 = plot_multistep(y_pred, ax=ax2, palette_kwargs=palette)
_ = ax2.legend(['FluVisits (test)', 'Forecast'])",forecasting-with-machine-learning.ipynb
linear algebra,import numpy as np ,gaussian-mixture-and-grid-search.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,gaussian-mixture-and-grid-search.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,gaussian-mixture-and-grid-search.ipynb
"Reading the dataSince there is no header in the data itself, just adding random headers to not lose the first data.This data serves no purpose so no need to store as pandas dataframe and we just turned it into a numpy array.","X_train = pd.read_csv(""/kaggle/input/data-science-london-scikit-learn/train.csv"", names=list(range(40))).values
y_train = pd.read_csv(""/kaggle/input/data-science-london-scikit-learn/trainLabels.csv"", names=[0]).values.ravel()

X_test = pd.read_csv(""/kaggle/input/data-science-london-scikit-learn/test.csv"", names=list(range(40))).values",gaussian-mixture-and-grid-search.ipynb
Balanced Data,"np.unique(y_train , return_counts = True) ",gaussian-mixture-and-grid-search.ipynb
Plain Classification with Grid Search,"from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV",gaussian-mixture-and-grid-search.ipynb
Classification with Gaussian Mixture and Grid Search,"from sklearn.mixture import GaussianMixture

X = np.r_[X_train, X_test]
X.shape",gaussian-mixture-and-grid-search.ipynb
Hello everyone!One way to generate new paintings is to transfer the style of the target artist Claude Monet to already existings images of real life. Here I randomly picked a real life image from Open Images Dataset and converted it using the painting style of Monet.This is a work in progress. Please upvote to keep this going.Thank you and stay safe!,"import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
import time
from PIL import Image
%matplotlib inline",generate-paintings-by-image-style-transfer.ipynb
A function to load the input images and set its dimensions to 1024 x 768,def load_image(image_path): ,generate-paintings-by-image-style-transfer.ipynb
decodes the image into a tensor," img = tf.image.decode_image(img , channels = 3) ",generate-paintings-by-image-style-transfer.ipynb
broadcasting the image array so that it has a batch dimension," img = img[tf.newaxis , :] ",generate-paintings-by-image-style-transfer.ipynb
"suppose dim is like 1,2,4,2,2,1... it removes the ones so that only 3 values remain W,H,c", if(len(image.shape)> 3): ,generate-paintings-by-image-style-transfer.ipynb
if there s a title mention it, if(title): ,generate-paintings-by-image-style-transfer.ipynb
Let s see the images,content_img = load_image('../input/gan-getting-started/photo_jpg/00068bc07f.jpg') ,generate-paintings-by-image-style-transfer.ipynb
needs preprocessing for the model to be initialized,x = tf.keras.applications.vgg19.preprocess_input(content_img * 255) ,generate-paintings-by-image-style-transfer.ipynb
the vgg19 model takes images in 256,"x = tf.image.resize(x ,(256 , 256)) ",generate-paintings-by-image-style-transfer.ipynb
Chooose the content and style layers,content_layers =['block4_conv2'] ,generate-paintings-by-image-style-transfer.ipynb
Build the model,def my_model(layer_names): ,generate-paintings-by-image-style-transfer.ipynb
Retrieve the output layers corresponding to the content and style layers," vgg_model = tf.keras.applications.VGG19(include_top = False , weights = 'imagenet') ",generate-paintings-by-image-style-transfer.ipynb
"input tensor is of shape ch, n H, n W",def gram_matrix(input_tensor): ,generate-paintings-by-image-style-transfer.ipynb
Unrolls n H and n W," num_locations = tf.cast(input_shape[1]* input_shape[2], tf.float32) ",generate-paintings-by-image-style-transfer.ipynb
Scale back the pixel values, inputs = inputs * 255.0 ,generate-paintings-by-image-style-transfer.ipynb
Pass the preprocessed input to my model, outputs = self.vgg(preprocessed_input) ,generate-paintings-by-image-style-transfer.ipynb
Separate the representations of style and content," style_outputs , content_outputs =(outputs[: self.num_style_layers], outputs[self.num_style_layers :]) ",generate-paintings-by-image-style-transfer.ipynb
Calculate the gram matrix for each layer in the style output. This will be the final style representation, style_outputs =[gram_matrix(layer)for layer in style_outputs] ,generate-paintings-by-image-style-transfer.ipynb
Now we extract the style and content features by calling the above class,"extractor = entire_model(style_layers , content_layers) ",generate-paintings-by-image-style-transfer.ipynb
Custom weights for different style layers,def total_cost(outputs): ,generate-paintings-by-image-style-transfer.ipynb
Define a tf.Variable to contain the image to optimize,generate_image = tf.Variable(content_img) ,generate-paintings-by-image-style-transfer.ipynb
"Since this is a float image, define a function to keep the pixel values between 0 and 1",def clip_0_1(image): ,generate-paintings-by-image-style-transfer.ipynb
random input data,target = pd.DataFrame(train['claim']) ,gentle-introduction-to-gan.ipynb
"new train1, new target1 OriginalGenerator .generate data pipe train, target, test, ","new_train3.to_csv('train.csv',index=False) 
new_target3.to_csv('targetcsv',index=False) ",gentle-introduction-to-gan.ipynb
IMPORT LIBRARIES,"import tensorflow as tf
import numpy as np
import math
import matplotlib.pyplot as plt
import os
from tensorflow import keras
from tensorflow.keras import layers",gentle-introduction-to-gan.ipynb
"Buiding a Generator Model Build a model using layers of BatchNorm ReLU Conv2DTranpose to generate fake imagesThe Generator network implemented hereThe network has 4 convolutional layers, all followed by BN except for the output layer and Rectified Linear unit ReLU activations.It takes as an input a random vector z drawn from a normal distribution . After reshaping z to have a 4D shape, we feed it to the generator that starts a series of upsampling layers.Each upsampling layer represents a transpose convolution operation with strides 2. Transpose convolutions are similar to the regular convolutions.This final output shape is defined by the size of the training images. In this case for MNIST, it would generate a 28x28 greyscale image.WHAT IS UP SAMPLING In the Upsampling network, the abstract image representations are upsampled using various techniques to make their spatial dimensions equal to the input image.","def build_generator(image_size = 28 , input_size = 100): ",gentle-introduction-to-gan.ipynb
Build an input layer," gen_input = keras.Input(shape =(input_size ,)) ",gentle-introduction-to-gan.ipynb
Increase dimensions and resize to 3D to feed it to Conv2DTranspose layer, x = layers.Dense(7 * 7 * 128)( gen_input) ,gentle-introduction-to-gan.ipynb
Use ConvTranspose, x = layers.BatchNormalization ()( x) ,gentle-introduction-to-gan.ipynb
Output layer for Generator, x = layers.Activation('sigmoid')( x) ,gentle-introduction-to-gan.ipynb
Build model using Model API," generator = keras.Model(gen_input , x , name = 'generator') ",gentle-introduction-to-gan.ipynb
"Discriminatorplease note generator and dicriminator network seems to be same. the difference is that in generator we were using convolution2DTranspose while in discrimator Convolution2D.for generator we need to increase size from input 100 to target image 28,28 for discriminator we need to decrease size from 28,28 to 1The discriminator is also a 4 layer CNN with BN except its input layer and leaky ReLU activations. Many activation functions will work fine with this basic GAN architecture. However, leaky ReLUs are very popular because they help the gradients flow easier through the architecture.A regular ReLU function works by truncating negative values to 0. This has the effect of blocking the gradients to flow through the network. Instead of the function being zero, leaky ReLUs allow a small negative value to pass through. That is, the function computes the greatest value between the features and a small factor.Finally, the discriminator needs to output probabilities. For that, we use the Logistic Sigmoid activation function on the final logits.","def build_discriminator(data_shape =[28 , 28 , 1 ,]) : ",gentle-introduction-to-gan.ipynb
Build the network, dis_input = keras.Input(data_shape) ,gentle-introduction-to-gan.ipynb
Flatten the output and build an output layer, x = layers.Flatten ()( x) ,gentle-introduction-to-gan.ipynb
Build Model," discriminator = keras.Model(dis_input , x , name = 'discriminator') ",gentle-introduction-to-gan.ipynb
"Build Adversarial modeltill now we have only created skeleton for our GAN. as of we need to add compiling part, optimizer, forward pass we need parameters like noise learning rate decay.Adversarial generator discriminatorwe will be calling build generator and build discriminator functions, i am using Adam optimizer which is popular one, experement with using different optimizers here is a link to know more abut optimizersIMPORTANT i have not compiled generator, while i have compiled discriminator why because generator will be using some learning of dicriminator. so we will be using frozen disciminator to train the whole adversarial network. so the ultimate equation be like adversarial network generator frozen discriminator.",def build_models (): ,gentle-introduction-to-gan.ipynb
Build Base Discriminator model," base_discriminator = build_discriminator(data_shape =(28 , 28 , 1 ,)) ",gentle-introduction-to-gan.ipynb
"NOW COMES THE TRAINING PARTJUST IN CASE this notebook is not about classification, its about generating fake images from existing ones.right now load the data and then start training, carefully if you see we have only loaded train part from dataset not the test part and labels. right because we are only interested in creating new images.first we will be training disciminator followed by training the Adversial network training generator next at evrey iteration we see how well our generator is doing. specifically after 500 iteration we will see the output from generator.so before begining the training check the architecture of Generator, discriminator and adversarial network and try to analyse how inputs are conveyed in generator network.discriminator and adversarial network are general purpose neural network.","def train_gan(generator , discriminator , adversarial , noise_size = 100): ",gentle-introduction-to-gan.ipynb
Training parameters, batch_size = 64 ,gentle-introduction-to-gan.ipynb
load MNIST dataset," (train_x , _),(_ , _)= tf.keras.datasets.mnist.load_data () ",gentle-introduction-to-gan.ipynb
Make it 3D dataset," train_x = np.reshape(train_x ,[- 1 , image_size , image_size , 1]) ",gentle-introduction-to-gan.ipynb
Standardize data : 0 to 1, train_x = train_x.astype('float32')/ 255 ,gentle-introduction-to-gan.ipynb
"Input for testing generator at different intervals, we will generate 16 images"," test_noise_input = np.random.uniform(- 1.0 , 1.0 , size =[16 , noise_size]) ",gentle-introduction-to-gan.ipynb
Start training, for i in range(train_steps): ,gentle-introduction-to-gan.ipynb
1. Get fake images from Generator," noise_input = np.random.uniform(- 1.0 , 1.0 , size =[batch_size , noise_size]) ",gentle-introduction-to-gan.ipynb
2. Get real images from training set," img_indexes = np.random.randint(0 , train_x.shape[0], size = batch_size) ",gentle-introduction-to-gan.ipynb
3. Prepare input for training Discriminator," X = np.concatenate(( real_images , fake_images)) ",gentle-introduction-to-gan.ipynb
4. Labels for training," y_real = np.ones(( batch_size , 1)) ",gentle-introduction-to-gan.ipynb
5. Train Discriminator," d_loss , d_acc = discriminator.train_on_batch(X , y) ",gentle-introduction-to-gan.ipynb
1. Prepare input create a new batch of noise," X = noise_input = np.random.uniform(- 1.0 , 1.0 , size =[batch_size , noise_size]) ",gentle-introduction-to-gan.ipynb
2. Prepare labels training Adversarial network to lie : All 1s," y = np.ones(( batch_size , 1)) ",gentle-introduction-to-gan.ipynb
3. Train Pls note Discrimator is not getting trained here," a_loss , a_acc = adversarial.train_on_batch(X , y) ",gentle-introduction-to-gan.ipynb
Print loss and Accuracy for both networks," print(""%s [Discriminator loss: %f, acc: %f, Adversarial loss: %f, acc: %f]"" %(i , d_loss , d_acc , a_loss , a_acc)) ",gentle-introduction-to-gan.ipynb
Save generated images to see how well Generator is doing, if(i + 1)% 500 == 0 : ,gentle-introduction-to-gan.ipynb
Generate 16 images, fake_images = generator.predict(test_noise_input) ,gentle-introduction-to-gan.ipynb
Display images," plot_images(fake_images , i + 1) ",gentle-introduction-to-gan.ipynb
Save Generator model, generator.save('mnist_generator_dcgan.h5') ,gentle-introduction-to-gan.ipynb
This is a utility function to draw images,"def plot_images(fake_images, step):
 
 plt.figure(figsize=(2.5,2.5))
 num_images = fake_images.shape[0]
 
 image_size = fake_images.shape[1]
 rows = int(math.sqrt(fake_images.shape[0]))
 
 for i in range(num_images):
 plt.subplot(rows, rows, i + 1)
 image = np.reshape(fake_images[i], [image_size, image_size])
 plt.imshow(image, cmap='gray')
 plt.axis('off')
 plt.show()",gentle-introduction-to-gan.ipynb
MODEL SUMMARY,"G, D, A = build_models()
G.summary()
D.summary()
A.summary()
",gentle-introduction-to-gan.ipynb
"Lets begin the training first iteration initially the generator have no idea, after 500 epochs the generated images doesnt seems like digits. second iteration Onward the generator have developed little bit understanding of data, geerated images have so similarity to minst datasetafter every cycle it can be visually confirmed that quality of generated images are increasing.THANKS FOR PATIENTLY READING TILL END","train_gan(G, D, A)",gentle-introduction-to-gan.ipynb
"For example, here s several helpful packages to load",import random ,getting-1000-score-using-only-minimax.ipynb
linear algebra,import numpy as np ,getting-1000-score-using-only-minimax.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,getting-1000-score-using-only-minimax.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,getting-1000-score-using-only-minimax.ipynb
"Okay so the most important thing for the algorithm to work properly and effeciently is defining the heuristic properly and then we can define the rest of the functions to use this heuristic in order to achieve the task of playing the game.So the most important part is to decide how we want to motivate the AI. For that these are the points we need to keep in mind: 1. Getting 2 in a row is difficult than getting 1 in a row. Getting 3 in a row is difficult than getting 2 in a row and so on. Thus we want to reward the AI when it gets higher number of pieces in a row than when it gets lower number of pieces in a row. Now the question is how much more score we want to give? So for that we imagine a grid randomly filled with 1s and 0s. In such a grid there would me exponentially more number of x pieces of 1s in a row than x 1 pieces of ones in a row thus we want to give exponentially greater score when AI makes higher and higher number of pieces in a row. VVIP Very Very Important Point :Now lets talk about the enemy. Lets say we are the color red and before dropping our piece on the board we have max 2 in a row and the enemy has max 3 in a row Taking the objective of the game to be to connect 4 in a row . If we can drop the piece in such a way that we can only either extend our 3 in a row or stop the enemy from making 4 in a row, it is more important to stop the enemy. Thus when we run the hueristic after dropping the piece the heuristic shoud penalise more if both the players have 3 in a row since we have already dropped our piece and now it will be the enemy s turn to play. Keeping these points in mind we can define the heuristic as follows:","def get_heuristic(grid, mark, config):
 score = 0
 for i in range(config.inarow):
 num = count_windows (grid,i+1,mark,config)
 score += (4**(i+1))*num
 for i in range(config.inarow):
 num_opp = count_windows (grid,i+1,mark%2+1,config)
 score-= (2**((2*i)+3))*num_opp
 return score",getting-1000-score-using-only-minimax.ipynb
Nothing much to explain here. Most of the functions are the same as compared to the tutorial notebook.,"def count_windows(grid , num_discs , piece , config): ",getting-1000-score-using-only-minimax.ipynb
horizontal, for row in range(config.rows): ,getting-1000-score-using-only-minimax.ipynb
vertical, for row in range(config.rows -(config.inarow - 1)) : ,getting-1000-score-using-only-minimax.ipynb
positive diagonal, for row in range(config.rows -(config.inarow - 1)) : ,getting-1000-score-using-only-minimax.ipynb
negative diagonal," for row in range(config.inarow - 1 , config.rows): ",getting-1000-score-using-only-minimax.ipynb
Here we have used 2 functions score move a and score move b that are calling each other recursively.,"def score_move_a(grid, col, mark, config,n_steps=1):
 next_grid = drop_piece(grid, col, mark, config)
 valid_moves = [col for col in range (config.columns) if next_grid[0][col]==0]
 if len(valid_moves)==0 or n_steps ==0:
 score = get_heuristic(next_grid, mark, config)
 return score
 else :
 scores = [score_move_b(next_grid,col,mark,config,n_steps-1) for col in valid_moves]
 score = min(scores)
 return score

def score_move_b(grid, col, mark, config,n_steps):
 next_grid = drop_piece(grid,col,(mark%2)+1,config)
 valid_moves = [col for col in range (config.columns) if next_grid[0][col]==0]
 if len(valid_moves)==0 or n_steps ==0:
 score = get_heuristic(next_grid, mark, config)
 return score
 else :
 scores = [score_move_a(next_grid,col,mark,config,n_steps-1) for col in valid_moves]
 score = max(scores)
 return score",getting-1000-score-using-only-minimax.ipynb
Here we have implemented a 1 step look ahead agent. ,"def agent(obs, config):
 valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]
 grid = np.asarray(obs.board).reshape(config.rows, config.columns)
 scores = dict(zip(valid_moves, [score_move_a(grid, col, obs.mark, config,1) for col in valid_moves]))
 max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]
 return random.choice(max_cols)",getting-1000-score-using-only-minimax.ipynb
Testing the agent against negamax,"from kaggle_environments import make, evaluate",getting-1000-score-using-only-minimax.ipynb
Since one iteration is not enough to conclude which agent is better we run 100 iterations.,"def get_win_percentages(agent1, agent2, n_rounds=100):
 config = {'rows': 10, 'columns': 7, 'inarow': 4}
 outcomes = evaluate(""connectx"", [agent1, agent2], config, [], n_rounds//2)
 outcomes += [[b,a] for [a,b] in evaluate(""connectx"", [agent2, agent1], config, [], n_rounds-n_rounds//2)]
 print(""Agent 1 Win Percentage:"", np.round(outcomes.count([1,-1])/len(outcomes), 2))
 print(""Agent 2 Win Percentage:"", np.round(outcomes.count([-1,1])/len(outcomes), 2))
 print(""Number of Invalid Plays by Agent 1:"", outcomes.count([None, 0]))
 print(""Number of Invalid Plays by Agent 2:"", outcomes.count([0, None]))",getting-1000-score-using-only-minimax.ipynb
Tweaking the hueristic,"def get_heuristic(grid, mark, config):
 score = 0
 for i in range(config.inarow):
 num = count_windows (grid,i+1,mark,config)
 if (i==(config.inarow-1) and num >= 1):
 return float(""inf"")
 score += (4**(i+1))*num
 for i in range(config.inarow):
 num_opp = count_windows (grid,i+1,mark%2+1,config)
 if (i==(config.inarow-1) and num_opp >= 1):
 return float (""-inf"")
 score-= (2**((2*i)+3))*num_opp
 return score",getting-1000-score-using-only-minimax.ipynb
Tweaking the minimax functions,"def score_move_a(grid , col , mark , config , n_steps = 1): ",getting-1000-score-using-only-minimax.ipynb
Thus score can only be infinity.," if len(valid_moves)== 0 or n_steps == 0 or score == float(""inf""): ",getting-1000-score-using-only-minimax.ipynb
Thus score can only be infinity.," if len(valid_moves)== 0 or n_steps == 0 or score == float(""-inf""): ",getting-1000-score-using-only-minimax.ipynb
Defining the agent with new functions,"def agent(obs, config):
 valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]
 grid = np.asarray(obs.board).reshape(config.rows, config.columns)
 scores = dict(zip(valid_moves, [score_move_a(grid, col, obs.mark, config,1) for col in valid_moves]))
 max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]
 return random.choice(max_cols)",getting-1000-score-using-only-minimax.ipynb
Testing against negamax,"env = make(""connectx"", debug=True)
env.run([agent,""negamax""])
env.render(mode=""ipython"")",getting-1000-score-using-only-minimax.ipynb
Obtaining the win percentage,"get_win_percentages(agent1=agent, agent2=""negamax"",n_rounds = 100)",getting-1000-score-using-only-minimax.ipynb
linear algebra,import numpy as np ,getting-started-with-gans.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,getting-started-with-gans.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,getting-started-with-gans.ipynb
Imports ,"import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import tensorflow as tf
import tensorflow_addons as tfa
import cv2

from colorama import Fore, Back, Style
from tensorflow import keras
from tensorflow.keras import layers

y_ = Fore.YELLOW
r_ = Fore.RED
g_ = Fore.GREEN
b_ = Fore.BLUE
m_ = Fore.MAGENTA

try:
 tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
 print('Device:', tpu.master())
 tf.config.experimental_connect_to_cluster(tpu)
 tf.tpu.experimental.initialize_tpu_system(tpu)
 strategy = tf.distribute.experimental.TPUStrategy(tpu)
except:
 strategy = tf.distribute.get_strategy()
print('Number of replicas:', strategy.num_replicas_in_sync)

AUTOTUNE = tf.data.experimental.AUTOTUNE
 
print(tf.__version__)",getting-started-with-gans.ipynb
Getting image paths from the directory ,"def getImagePaths(path):
 image_names = []
 for dirname, _, filenames in os.walk(path):
 for filename in filenames:
 fullpath = os.path.join(dirname, filename)
 image_names.append(fullpath)
 return image_names",getting-started-with-gans.ipynb
Number of images in each directory ,"print(f""{y_}Number of Monet images: {g_} {len(monet_images_path)}\n"")
print(f""{y_}Number of Photo images: {g_} {len(photo_images_path)}\n"")",getting-started-with-gans.ipynb
Checking if images in each directory have the same shape ,"def getShape(images_paths):
 shape = cv2.imread(images_paths[0]).shape
 for image_path in images_paths:
 image_shape=cv2.imread(image_path).shape
 if (image_shape!=shape):
 return ""Different image shape""
 else:
 return ""Same image shape "" + str(shape)",getting-started-with-gans.ipynb
Monet images ,"display_multiple_img(monet_images_path, 4, 4)",getting-started-with-gans.ipynb
Photo images ,"display_multiple_img(photo_images_path, 4, 4)",getting-started-with-gans.ipynb
Colour Histograms ,"def styling():
 for spine in plt.gca().spines.values():
 spine.set_visible(False)
 plt.xticks([])
 plt.yticks([])",getting-started-with-gans.ipynb
For Monet images ,display_hist(monet_images_path),getting-started-with-gans.ipynb
For Photo images ,display_hist(photo_images_path),getting-started-with-gans.ipynb
 Importing the necessary libraries ,import numpy as np ,getting-started-with-nlp-a-general-intro.ipynb
text processing libraries,import re ,getting-started-with-nlp-a-general-intro.ipynb
XGBoost,import xgboost as xgb ,getting-started-with-nlp-a-general-intro.ipynb
sklearn,from sklearn import model_selection ,getting-started-with-nlp-a-general-intro.ipynb
matplotlib and seaborn for plotting,import matplotlib.pyplot as plt ,getting-started-with-nlp-a-general-intro.ipynb
File system manangement,import os ,getting-started-with-nlp-a-general-intro.ipynb
Suppress warnings,import warnings ,getting-started-with-nlp-a-general-intro.ipynb
List files available,"print(os.listdir(""../input/"")) ",getting-started-with-nlp-a-general-intro.ipynb
Training data,train = pd.read_csv('../input/nlp-getting-started/train.csv') ,getting-started-with-nlp-a-general-intro.ipynb
Testing data,test = pd.read_csv('../input/nlp-getting-started/test.csv') ,getting-started-with-nlp-a-general-intro.ipynb
Missing values in training set,train.isnull (). sum () ,getting-started-with-nlp-a-general-intro.ipynb
Missing values in test set,test.isnull (). sum () ,getting-started-with-nlp-a-general-intro.ipynb
"Exploring the Target Column Distribution of the Target Column We have to predict whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.",train['target'].value_counts(),getting-started-with-nlp-a-general-intro.ipynb
A disaster tweet,disaster_tweets = train[train['target']== 1][ 'text'] ,getting-started-with-nlp-a-general-intro.ipynb
not a disaster tweet,non_disaster_tweets = train[train['target']== 0][ 'text'] ,getting-started-with-nlp-a-general-intro.ipynb
Exploring the keyword column The keyword column denotes a keyword from the tweet.Let s look at the top 20 keywords in the training data,"sns.barplot(y=train['keyword'].value_counts()[:20].index,x=train['keyword'].value_counts()[:20],
 orient='h')",getting-started-with-nlp-a-general-intro.ipynb
Let s see how often the word disaster come in the dataset and whether this help us in determining whether a tweet belongs to a disaster category or not.,"train.loc[train['text'].str.contains('disaster', na=False, case=False)].target.value_counts()",getting-started-with-nlp-a-general-intro.ipynb
A quick glance over the existing data,train['text'][ : 5] ,getting-started-with-nlp-a-general-intro.ipynb
Applying a first round of text cleaning techniques,def clean_text(text): ,getting-started-with-nlp-a-general-intro.ipynb
Just for fun let s create a wordcloud of the clean text to see the most dominating words in the tweets.,"from wordcloud import WordCloud
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])
wordcloud1 = WordCloud( background_color='white',
 width=600,
 height=400).generate("" "".join(disaster_tweets))
ax1.imshow(wordcloud1)
ax1.axis('off')
ax1.set_title('Disaster Tweets',fontsize=40);

wordcloud2 = WordCloud( background_color='white',
 width=600,
 height=400).generate("" "".join(non_disaster_tweets))
ax2.imshow(wordcloud2)
ax2.axis('off')
ax2.set_title('Non Disaster Tweets',fontsize=40);",getting-started-with-nlp-a-general-intro.ipynb
" TokenizationTokenization is a process that splits an input sequence into so called tokens where the tokens can be a word, sentence, paragraph etc. Base upon the type of tokens we want, tokenization can be of various types, for instance ","text = ""Are you coming , aren't you""
tokenizer1 = nltk.tokenize.WhitespaceTokenizer()
tokenizer2 = nltk.tokenize.TreebankWordTokenizer()
tokenizer3 = nltk.tokenize.WordPunctTokenizer()
tokenizer4 = nltk.tokenize.RegexpTokenizer(r'\w+')

print(""Example Text: "",text)
print(""------------------------------------------------------------------------------------------------"")
print(""Tokenization by whitespace:- "",tokenizer1.tokenize(text))
print(""Tokenization by words using Treebank Word Tokenizer:- "",tokenizer2.tokenize(text))
print(""Tokenization by punctuation:- "",tokenizer3.tokenize(text))
print(""Tokenization by regular expression:- "",tokenizer4.tokenize(text))",getting-started-with-nlp-a-general-intro.ipynb
Tokenizing the training and the test set,tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+') ,getting-started-with-nlp-a-general-intro.ipynb
" Stopwords RemovalNow, let s get rid of the stopwords i.e words which occur very frequently but have no possible value like a, an, the, are etc. ","def remove_stopwords(text):
 """"""
 Removing stopwords belonging to english language
 
 """"""
 words = [w for w in text if w not in stopwords.words('english')]
 return words


train['text'] = train['text'].apply(lambda x : remove_stopwords(x))
test['text'] = test['text'].apply(lambda x : remove_stopwords(x))
train.head()",getting-started-with-nlp-a-general-intro.ipynb
Stemming and Lemmatization examples,"text = ""feet cats wolves talked"" ",getting-started-with-nlp-a-general-intro.ipynb
Stemmer,stemmer = nltk.stem.PorterStemmer () ,getting-started-with-nlp-a-general-intro.ipynb
Lemmatizer,lemmatizer = nltk.stem.WordNetLemmatizer () ,getting-started-with-nlp-a-general-intro.ipynb
"After preprocessing, the text format",def combine_text(list_of_text): ,getting-started-with-nlp-a-general-intro.ipynb
text preprocessing function,def text_preprocessing(text): ,getting-started-with-nlp-a-general-intro.ipynb
" Transforming tokens to a vector After the initial preprocessing phase, we need to transform text into a meaningful vector or array of numbers. This can be done by a number of tecniques:Bag of WordsThe bag of words is a representation of text that describes the occurrence of words within a document. It involves two things: A vocabulary of known words. A measure of the presence of known words. Why is it is called a bag of words? That is because any information about the order or structure of words in the document is discarded and the model is only concerned with whether the known words occur in the document, not where they occur in the document.For example, source:Natural Language Processing course on courseraWe can do this using scikit learn s CountVectorizer, where every row will represent a different tweet and every column will represent a different word.Bag of Words Countvectorizer FeaturesCountvectorizer converts a collection of text documents to a matrix of token counts. It is important to note here that CountVectorizer comes with a lot of options to automatically do preprocessing, tokenization, and stop word removal.However, i did all the process manually above to just get a better understanding. Let s use a vanilla implementation of the countvectorizer without specifying any parameters. ",count_vectorizer = CountVectorizer () ,getting-started-with-nlp-a-general-intro.ipynb
Keeping only non zero elements to preserve space,print(train_vectors[0]. todense ()) ,getting-started-with-nlp-a-general-intro.ipynb
"TFIDF FeaturesA problem with the Bag of Words approach is that highly frequent words start to dominate in the document e.g. larger score , but may not contain as much informational content . Also, it will give more weight to longer documents than shorter documents.One approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like the that are also frequent across all documents are penalized. This approach to scoring is called Term Frequency Inverse Document Frequency, or TF IDF for short, where:Term Frequency: is a scoring of the frequency of the word in the current document.TF Number of times term t appears in a document Number of terms in the document Inverse Document Frequency: is a scoring of how rare the word is across documents.IDF 1 log N n , where, N is the number of documents and n is the number of documents a term t has appeared in.","tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))
train_tfidf = tfidf.fit_transform(train['text'])
test_tfidf = tfidf.transform(test[""text""])
",getting-started-with-nlp-a-general-intro.ipynb
Fitting a simple Logistic Regression on Counts,clf = LogisticRegression(C = 1.0) ,getting-started-with-nlp-a-general-intro.ipynb
Fitting a simple Logistic Regression on TFIDF,clf_tfidf = LogisticRegression(C = 1.0) ,getting-started-with-nlp-a-general-intro.ipynb
Fitting a simple Naive Bayes on Counts,clf_NB = MultinomialNB () ,getting-started-with-nlp-a-general-intro.ipynb
Fitting a simple Naive Bayes on TFIDF,clf_NB_TFIDF = MultinomialNB () ,getting-started-with-nlp-a-general-intro.ipynb
well the naive bayes on TFIDF features scores much better than logistic regression model. ,"clf_NB_TFIDF.fit(train_tfidf, train[""target""])",getting-started-with-nlp-a-general-intro.ipynb
XGBoost,"import xgboost as xgb
clf_xgb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, 
 subsample=0.8, nthread=10, learning_rate=0.1)
scores = model_selection.cross_val_score(clf_xgb, train_vectors, train[""target""], cv=5, scoring=""f1"")
scores",getting-started-with-nlp-a-general-intro.ipynb
Making the submission,"def submission(submission_file_path,model,test_vectors):
 sample_submission = pd.read_csv(submission_file_path)
 sample_submission[""target""] = model.predict(test_vectors)
 sample_submission.to_csv(""submission.csv"", index=False)",getting-started-with-nlp-a-general-intro.ipynb
"Introduction and SetupFor this tutorial, we will be using the TFRecord dataset. Import the following packages and change the accelerator to TPU. Because TPUs are pretty awesome.","import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa
import tensorflow_datasets as tfds

from kaggle_datasets import KaggleDatasets
import matplotlib.pyplot as plt
import numpy as np

from functools import partial
from albumentations import (
 Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip,
 Rotate
)

try:
 tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
 print('Device:', tpu.master())
 tf.config.experimental_connect_to_cluster(tpu)
 tf.tpu.experimental.initialize_tpu_system(tpu)
 strategy = tf.distribute.experimental.TPUStrategy(tpu)
except:
 strategy = tf.distribute.get_strategy()
print('Number of replicas:', strategy.num_replicas_in_sync)

AUTOTUNE = tf.data.experimental.AUTOTUNE

print(tf.__version__)",getting-started-with-standard-gans-tutorial.ipynb
"Load in the dataWe want to keep our photo dataset and our Monet dataset separate. First, load in the filenames of the TFRecords. We ll load both for the CycleGAN. For the first GAN we only need the Monets as training data.All the images for the competition are already sized to 256 x 256. As these images are RGB images, set the channel to 3. Additionally, we need to scale the images to a 1, 1 scale. Because we are building a generative model, we don t need the labels or the image id so we ll only return the image from the TFRecord.","GCS_PATH = KaggleDatasets().get_gcs_path()

MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))
print('Monet TFRecord Files:', len(MONET_FILENAMES))",getting-started-with-standard-gans-tutorial.ipynb
"You can see I put down a bit of augmentation using random jitter and flip to increase our data set, because we simply don t have enough data for","IMAGE_SIZE =[256 , 256] ",getting-started-with-standard-gans-tutorial.ipynb
"image tf.reshape image, 256, 256, 3 "," image = tf.image.decode_jpeg(image , channels = 3) ",getting-started-with-standard-gans-tutorial.ipynb
"image tf.cast image, tf.float32 127.5 1"," image = tf.reshape(image ,[* IMAGE_SIZE , 3]) ",getting-started-with-standard-gans-tutorial.ipynb
"Then load the data and display the first images to see if it all worked out. Which of course it does, because it s taken directly from the tutorial.","monet_ds = load_dataset(MONET_FILENAMES, labeled=True, repeats=100).batch(100, drop_remainder=True)",getting-started-with-standard-gans-tutorial.ipynb
extract 1 batch from the dataset, image = next(iter(ds)) ,getting-started-with-standard-gans-tutorial.ipynb
"Build the DCGAN Network Upsample and Downsample The downsample, as the name suggests, reduces the 2D dimensions, the width and height, of the image by the stride. The stride is the length of the step the filter takes. Since the stride is 2, the filter is applied to every other pixel, hence reducing the weight and height by 2.We ll be using an instance normalization instead of batch normalization. As the instance normalization is not standard in the TensorFlow API, we ll use the layer from TensorFlow Add ons.","def downsample(filters, size, apply_instancenorm=True):
 initializer = tf.random_normal_initializer(0., 0.02)
 gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)

 result = keras.Sequential()
 result.add(layers.Conv2D(filters, size, padding='same',
 kernel_initializer=initializer, use_bias=False))
 result.add(layers.MaxPool2D())

 if apply_instancenorm:
 result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))

 result.add(layers.LeakyReLU())

 return result",getting-started-with-standard-gans-tutorial.ipynb
Upsample does the opposite of downsample and increases the dimensions of the of the image. Conv2DTranspose does basically the opposite of a Conv2D layer.,"def upsample(filters, size, apply_dropout=False):
 initializer = tf.random_normal_initializer(0., 0.02)
 gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)

 result = keras.Sequential()
 result.add(layers.Conv2DTranspose(filters, size, strides=2,
 padding='same',
 kernel_initializer=initializer,
 use_bias=False))

 result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))

 if apply_dropout:
 result.add(layers.Dropout(0.5))

 result.add(layers.LeakyReLU())

 return result",getting-started-with-standard-gans-tutorial.ipynb
"Build the generatorThis generator samples from noise, reshapes it and upsamples the entire thing. That s basically it.","def Generator(LATENT_DIM = 128 , OUTPUT_CHANNELS = 3): ",getting-started-with-standard-gans-tutorial.ipynb
"Build the discriminatorThe discriminator takes in the input image and classifies it as real or fake generated . Instead of outputing a single node, the discriminator outputs a smaller 2D image with higher pixel values indicating a real classification and lower values indicating a fake classification.",def Discriminator (): ,getting-started-with-standard-gans-tutorial.ipynb
" bs, 128, 128, 64 "," down1 = downsample(64 , 4 , False)( x) ",getting-started-with-standard-gans-tutorial.ipynb
" bs, 64, 64, 128 "," down2 = downsample(128 , 4)( down1) ",getting-started-with-standard-gans-tutorial.ipynb
" bs, 32, 32, 256 "," down3 = downsample(256 , 4)( down2) ",getting-started-with-standard-gans-tutorial.ipynb
" bs, 34, 34, 256 ", zero_pad1 = layers.ZeroPadding2D ()( down3) ,getting-started-with-standard-gans-tutorial.ipynb
Define the Least Squares Loss,"with strategy.scope():
 def discriminator_loss(predictions_real, predictions_gen, labels_real):
 gen_loss = tf.reduce_mean((predictions_gen - tf.reduce_mean(predictions_real) + labels_real) ** 2)
 real_loss = tf.reduce_mean((predictions_real - tf.reduce_mean(predictions_gen) - labels_real) ** 2)
 return (gen_loss + real_loss) / 2
 
 def generator_loss(predictions_real, predictions_gen, labels_real):
 gen_loss = tf.reduce_mean((predictions_gen - tf.reduce_mean(predictions_real) - labels_real) ** 2)
 real_loss = tf.reduce_mean((predictions_real - tf.reduce_mean(predictions_gen) + labels_real) ** 2)
 return (gen_loss + real_loss) / 2",getting-started-with-standard-gans-tutorial.ipynb
Throw it on the TPU And build a GAN with the help of the keras documentation. Notice that I changed the loss function from the Binary Crossentropy used in the CycleGAN to a Least Squares approach taken from the RaLSGAN for dogs. The DCGAN would not converge on BCE whatsoever and the LSGAN works surprisingly well for the amount of data we have.,OUTPUT_CHANNELS = 3 ,getting-started-with-standard-gans-tutorial.ipynb
generates Monet esque paintings," monet_generator = Generator(LATENT_DIM , 3) ",getting-started-with-standard-gans-tutorial.ipynb
differentiates real Monet paintings and generated Monet paintings, monet_discriminator = Discriminator () ,getting-started-with-standard-gans-tutorial.ipynb
Sample random points in the latent space, batch_size = tf.shape(images_real)[ 0] ,getting-started-with-standard-gans-tutorial.ipynb
"labels real tf.fill batch size, 1 , self.real label "," labels_gen = tf.zeros(( batch_size , 1)) + self.fake_label ",getting-started-with-standard-gans-tutorial.ipynb
Add random noise to the labels important trick!, labels_gen += 0.05 * tf.random.uniform(tf.shape(labels_gen)) ,getting-started-with-standard-gans-tutorial.ipynb
Train the generator," images_gen = self.generator(random_latent_vectors , training = False) ",getting-started-with-standard-gans-tutorial.ipynb
Train the discriminator," predictions_real = self.discriminator(images_real , training = True) ",getting-started-with-standard-gans-tutorial.ipynb
tf.reduce mean predictions real tf.reduce mean predictions gen labels real 2 2," d_loss = self.d_loss_fn(predictions_real , predictions_gen , labels_real) ",getting-started-with-standard-gans-tutorial.ipynb
Train the generator," images_gen = self.generator(random_latent_vectors , training = True) ",getting-started-with-standard-gans-tutorial.ipynb
Train the discriminator," predictions_real = self.discriminator(images_real , training = False) ",getting-started-with-standard-gans-tutorial.ipynb
tf.reduce mean predictions gen tf.reduce mean predictions real labels real 2 2," g_loss = self.g_loss_fn(predictions_real , predictions_gen , labels_real) ",getting-started-with-standard-gans-tutorial.ipynb
"Things to note We re using a few tricks here including soft labeling, differing learning rates, augmentation, a better loss function, etc. So this is where we compile and train the model.","EPOCHS = 50

LR_G = 0.001
LR_D = 0.0005
beta_1 = .5

real_label = .66
fake_label = 0

with strategy.scope():
 monet_gan = MonetGan(monet_discriminator=monet_discriminator, 
 monet_generator=monet_generator, 
 latent_dim=LATENT_DIM,
 real_label=real_label,
 fake_label=fake_label)
 
 monet_gan.compile(
 d_opt = tf.keras.optimizers.Adam(learning_rate=LR_D, beta_1=beta_1),
 g_opt = tf.keras.optimizers.Adam(learning_rate=LR_G, beta_1=beta_1),
 d_loss_fn=discriminator_loss,
 g_loss_fn=generator_loss
 )
",getting-started-with-standard-gans-tutorial.ipynb
Create submission files,"import PIL
! mkdir ../images",getting-started-with-standard-gans-tutorial.ipynb
linear algebra,import numpy as np ,google-movie-reviews-sentiment-deep-stack-models.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,google-movie-reviews-sentiment-deep-stack-models.ipynb
"For example, running this by clicking run or pressing Shift Enter will list the files in the input directory",import os ,google-movie-reviews-sentiment-deep-stack-models.ipynb
First use pandas pd.read csv for reading these tabulated files and our basic process will be like importing data cleaning them visualizing them our stack models for deep leaning with ,"df_train = pd.read_csv(""../input/word2vec-nlp-tutorial/labeledTrainData.tsv"", header=0, delimiter=""\t"", quoting=3)
df_train.head()",google-movie-reviews-sentiment-deep-stack-models.ipynb
"dropping unecessary columns from additional dataset and combine as one , step 1 done now after this cleaning will startdown","df_train1=df_train1.drop([""type"",'file'],axis=1)",google-movie-reviews-sentiment-deep-stack-models.ipynb
"removing html tags re.sub a zA Z , , raw review in this line we will keep all the alphabetical words which are present in the file name raw review all special characters are replaced by a space. spliting of words with normalizing it taking stopwords into account checking words alphanumeric or not then after checking we stopwords finding and removing them joinning meaningful words ",from bs4 import BeautifulSoup ,google-movie-reviews-sentiment-deep-stack-models.ipynb
1. Remove HTML," review_text = BeautifulSoup(raw_review , 'lxml'). get_text () ",google-movie-reviews-sentiment-deep-stack-models.ipynb
2. Remove non letters with regex," letters_only = re.sub(""[^a-zA-Z]"" , "" "" , review_text) ",google-movie-reviews-sentiment-deep-stack-models.ipynb
"3. Convert to lower case, split into individual words", words = letters_only.lower (). split () ,google-movie-reviews-sentiment-deep-stack-models.ipynb
4. Create set of stopwords," stops = set(stopwords.words(""english"")) ",google-movie-reviews-sentiment-deep-stack-models.ipynb
5. Remove stop words, meaningful_words =[w for w in words if not w in stops] ,google-movie-reviews-sentiment-deep-stack-models.ipynb
and return the result.," return("" "".join(meaningful_words)) ",google-movie-reviews-sentiment-deep-stack-models.ipynb
using above function and store the filter things in array,"from wordcloud import WordCloud , STOPWORDS ",google-movie-reviews-sentiment-deep-stack-models.ipynb
checking nullity in the data of train and test,"new_train.isnull (). sum (), df_test.isnull (). sum () ",google-movie-reviews-sentiment-deep-stack-models.ipynb
"importing keras files tokenizer and padding is necessary so that long and short reviews must be of same length what is stacking idea , first to create word2vec dictionary , word embbeding apply cnn with maxpooling to find out features of neg and positive sentiment apply lstm bi directional unit to for need of good memoryBackground of the Techniques Convolution Neural Networks CNN :CNN s are efficient for sentence classification tasks as the convolution layers can extract features horizontally from multiple words . These characteristics are essential for classification tasks as it is tricky to find clues about class memberships especially when these clues can appear in different orders in the input. CNN has also been used for document topic classifications where a single local phrase could aid in establishing the topic regardless of the position where it appears in the document. They found that CNN is powerful enough to find these local indicators due to the powerful combination of the convolution and pooling layers. Long Short Term Memory LSTM :An example of LSTM s effectiveness is its ability to capture changing sentiment in a tweet. A sentence such as The movie was fine but not to my expectation contains words with conflicting sentiments which is not able to be inferred accurately by a typical neural network. However, LSTM will learn that the sentiments expressed towards the end of the sentence would carry more important context compared to the words at the start.CNN LSTM Model:The final model architecture is . We initialized the model with Keras Sequential layer and added the embedding layer as the first layer. By using the embedding layer, the positive integers is turned into a dense vector of fixed size and this new representations will be passed to the CNN layer. Each filter in the CNN will detect specific features or patterns and then it will be pooled to a smaller dimension in the max pooling layer. These features are then passed into a single LSTM layer of 100 units. Then, the LSTM outputs are then fed to a Fully Connected Layer FCL which was built using Keras s Dense layer. As there are five labels to be predicted, a softmax activation function was used at the output layer.","from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras import initializers, regularizers, constraints, optimizers, layers",google-movie-reviews-sentiment-deep-stack-models.ipynb
tokenize upto max 6000 words then using keras function of preprocessing of tokenizing and padding,"max_features = 6000
tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(list(list_sentences_train))
list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)
list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)",google-movie-reviews-sentiment-deep-stack-models.ipynb
checking distribution of word length,totalNumWords =[len(one_comment)for one_comment in list_tokenized_train] ,google-movie-reviews-sentiment-deep-stack-models.ipynb
" 0,50,100,150,200,250,300,350,400 ,450,500,550,600,650,700,750,800,850,900 ","plt.hist(totalNumWords , bins = np.arange(0 , 410 , 10)) ",google-movie-reviews-sentiment-deep-stack-models.ipynb
maxlen of review is 400 words,"maxlen = 370
X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)
X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)",google-movie-reviews-sentiment-deep-stack-models.ipynb
ouput submission file,"df_test = df_test[[ 'id' , 'sentiment']] ",google-movie-reviews-sentiment-deep-stack-models.ipynb
Packages and functions,% pip install - Uq upgini ,guide-external-data-features-for-multivariatets.ipynb
visualization tools,"from matplotlib import pyplot as plt , style ",guide-external-data-features-for-multivariatets.ipynb
Reading train and test datasets,"train = pd.read_csv('../input/store-sales-time-series-forecasting/train.csv',
 parse_dates = ['date'], infer_datetime_format = True,
 dtype = {'store_nbr' : 'category',
 'family' : 'category'},
 usecols = ['date', 'store_nbr', 'family', 'sales'])
train['date'] = train.date.dt.to_period('D')
train = train.set_index(['date', 'store_nbr', 'family']).sort_index()
print(train.shape)
train.head()",guide-external-data-features-for-multivariatets.ipynb
Oil price lag features,"calendar = pd.DataFrame(index = pd.date_range('2013-01-01', '2017-08-31')).to_period('D')
oil = pd.read_csv('../input/store-sales-time-series-forecasting/oil.csv',
 parse_dates = ['date'], infer_datetime_format = True,
 index_col = 'date').to_period('D')
oil['avg_oil'] = oil['dcoilwtico'].rolling(7).mean()
calendar = calendar.join(oil.avg_oil)
calendar['avg_oil'].fillna(method = 'ffill', inplace = True)
calendar.dropna(inplace = True)",guide-external-data-features-for-multivariatets.ipynb
"We concatenated calendar with oil price and replaced NaN with last valid observation price. Now, let s calculate partial autocorrelation to select proper lags for oil price features. The lag feature shifting a time series forward one step or more.","_ = plot_pacf(calendar.avg_oil, lags = 10)",guide-external-data-features-for-multivariatets.ipynb
Oil price lags for the first 3 days should be enough:,"n_lags = 3
for l in range(1, n_lags + 1):
 calendar[f'oil_lags{l}'] = calendar.avg_oil.shift(l)
calendar.dropna(inplace = True)",guide-external-data-features-for-multivariatets.ipynb
Joining calendar with holiday dataset,calendar = calendar.join(hol) ,guide-external-data-features-for-multivariatets.ipynb
Day of week,calendar['dofw']= calendar.index.dayofweek ,guide-external-data-features-for-multivariatets.ipynb
If it s saturday or sunday then it s not Weekday,"calendar.loc[calendar.dofw > 4 , 'wd']= 0 ",guide-external-data-features-for-multivariatets.ipynb
If it s Work Day event then it s a workday,"calendar.loc[calendar.type == 'Work Day' , 'wd']= 1 ",guide-external-data-features-for-multivariatets.ipynb
If it s Transfer event then it s not a work day,"calendar.loc[calendar.type == 'Transfer' , 'wd']= 0 ",guide-external-data-features-for-multivariatets.ipynb
If it s Bridge event then it s not a work day,"calendar.loc[calendar.type == 'Bridge' , 'wd']= 0 ",guide-external-data-features-for-multivariatets.ipynb
If it s holiday and the holiday is not transferred then it s non working day,"calendar.loc[( calendar.type == 'Holiday')&(calendar.transferred == False), 'wd']= 0 ",guide-external-data-features-for-multivariatets.ipynb
If it s holiday and transferred then it s working day,"calendar.loc[( calendar.type == 'Holiday')&(calendar.transferred == True), 'wd']= 1 ",guide-external-data-features-for-multivariatets.ipynb
One hot encoding Make sure to drop one of the columns by drop first True ,"calendar = pd.get_dummies(calendar , columns =['dofw'], drop_first = True) ",guide-external-data-features-for-multivariatets.ipynb
One hot encoding for type holiday No need to drop one of the columns because there s a No holiday already ,"calendar = pd.get_dummies(calendar , columns =['type']) ",guide-external-data-features-for-multivariatets.ipynb
Unused columns,"calendar.drop (['locale' , 'locale_name' , 'description' , 'transferred'], axis = 1 , inplace = True) ",guide-external-data-features-for-multivariatets.ipynb
Time dependent features Wages in the public sector are paid every two weeks on the 15th and on the last day of the month,"calendar['wageday']=0
calendar.loc[(calendar.index.to_timestamp().is_month_end) | (calendar.index.day == 15), 'wageday'] = 1",guide-external-data-features-for-multivariatets.ipynb
"School seasons April and May, than August and September. Let s put this information into the flag:","school_season = []
for i, r in calendar.iterrows():
 if i.month in [4, 5, 8, 9] :
 school_season.append(1)
 else :
 school_season.append(0)
calendar['school_season'] = school_season",guide-external-data-features-for-multivariatets.ipynb
"Zero forecasting Some stores do not sell specific products at all OR probably stop selling products after some time assortiment enhancement trials? . Which looks like the case for HOME APPLIANCES in certain stores. So let s make it as a hard coded prediction with following logic: 15 days of zero sales from 2017 08 01 till 2017 08 15 will lead to zero forecast. Except SCHOOL AND OFFICE SUPPLIES products, which has very specific seasonality pattern. Code credits to Bayar Note: Zero forecasting is one of the approaches to reduce compute memory consumption. That definitley will be usefull for this notebook, keeping in mind amount of computations in CustomRegressor. ","c = train.groupby([""store_nbr"",""family""]).tail(15).groupby([""store_nbr"",""family""]).sales.sum().reset_index()
c = c[c.sales == 0].drop(""sales"",axis = 1)
c = c[c.family != ""SCHOOL AND OFFICE SUPPLIES""]
c.shape",guide-external-data-features-for-multivariatets.ipynb
"As you can see now we removed zero sales from train data, so lets save it into separate dataframe. And then append to test set on submission phase","zero_prediction = []
for i in range(0, len(c)):
 zero_prediction.append(
 pd.DataFrame({
 ""date"":pd.date_range(""2017-08-16"", ""2017-08-31"").tolist(),
 ""store_nbr"":c.store_nbr.iloc[i],
 ""family"":c.family.iloc[i],
 ""sales"":0
 })
 )
zero_prediction = pd.concat(zero_prediction)
zero_prediction['date'] = zero_prediction.date.dt.to_period('D')
del c
gc.collect()
zero_prediction = zero_prediction.set_index(['date', 'store_nbr', 'family'])
zero_prediction.head()",guide-external-data-features-for-multivariatets.ipynb
"It s important to select training period correctly. Let s check stores opening, as we want a period where ALL data for all stores are available, so there is no zero sales values as store hasn t been opened yet","a = train.groupby([""date"",""store_nbr""]).sum().reset_index()
a = a[a[""sales""] > 0].groupby(""store_nbr"")[[""date""]].min().sort_values(by=""date"",ascending = False).head(5)
a.rename(columns = {'date':'open_date'}, inplace = True)
a",guide-external-data-features-for-multivariatets.ipynb
"Store 52 was the latest opened store open date is 2017 04 20 , so let s take next date as start 2017 04 21: ","y = train.unstack (['store_nbr' , 'family']).loc[""2017-04-21"" :] ",guide-external-data-features-for-multivariatets.ipynb
from 2017 04 30 for faster calculation,"a = train[""2017-04-30"" :]. reset_index () ",guide-external-data-features-for-multivariatets.ipynb
"BABY CARE, BOOKS, HOME APPLIANCES, LADIESWEAR, LAWN AND GARDEN, PET SUPPLIES are not correlated with other products sales at all. SCHOOL AND OFFICE SUPPLIES, LINGERIE has a very low correlation. But the rest of the product families sales are all correlated, so we clearly have a multivariate time series task. Note: We can use sales forecast of other products families in the SAME store as A FEATURE to improve prediction accuracy of specific product, as there s an influence on sales numbers between products. This is what we ll do later. Now let s check trends, seasonality and anomalies for these low correlated product families:","fig, ax = plt.subplots(1,2,figsize = (20,4))
train.loc[""2016-08-01"":].filter(like = 'BABY CARE', axis=0).groupby([""date""]).sales.sum().plot(ax = ax[0], title = ""BABY CARE"")
train.loc[""2016-08-01"":].filter(like = 'BOOKS', axis=0).groupby([""date""]).sales.sum().plot(ax = ax[1], title = ""BOOKS"")
plt.show()
fig, ax = plt.subplots(1,2,figsize = (20,4))
train.loc[""2016-01-01"":].filter(like = 'HOME APPLIANCES', axis=0).groupby([""date""]).sales.sum().plot(ax = ax[0], title = ""HOME APPLIANCES"")
train.loc[""2016-08-01"":].filter(like = 'LADIESWEAR', axis=0).groupby([""date""]).sales.sum().plot(ax = ax[1], title = ""LADIESWEAR"")
plt.show()
fig, ax = plt.subplots(1,2,figsize = (20,4))
train.loc[""2016-08-01"":].filter(like = 'LAWN AND GARDEN', axis=0).groupby([""date""]).sales.sum().plot(ax = ax[0], title = ""LAWN AND GARDEN"")
train.loc[""2016-08-01"":].filter(like = 'PET SUPPLIES', axis=0).groupby([""date""]).sales.sum().plot(ax = ax[1], title = ""PET SUPPLIES"")
plt.show()
fig, ax = plt.subplots(1,2,figsize = (20,4))
train.loc[""2016-08-01"":].filter(like = 'SCHOOL AND OFFICE SUPPLIES', axis=0).groupby([""date""]).sales.sum().plot(ax = ax[0], title = ""SCHOOL AND OFFICE SUPPLIES"")
train.loc[""2016-08-01"":].filter(like = 'LINGERIE', axis=0).groupby([""date""]).sales.sum().plot(ax = ax[1], title = ""LINGERIE"")
plt.show()",guide-external-data-features-for-multivariatets.ipynb
"What we see here: 1st of January has zero sales ie stores are closed on 1st of Jan. Make sense to start train period after 2017.01.01, just to avoid extra feature engineering on closing dates SCHOOL AND OFFICE SUPPLIES looks like a non stationary TS, let s check that using Augmented Dickey Fuller test further. Looks like this y will be difficult to predict by linear models, tree based models might be a better fit BOOKS this product category might be some kind of an assortiment trial and on clearance since the begining of 2017. Sharp sales decline might be result of assortiment decline, which for books lead to near zero daily sales. Same tree based models might have a better prediction results LAWN AND GARDEN there is an uplift in sales numbers from Dec 2016, most probably from introduction of this product category in more stores around the country. Feel free to check that . But in overall no specific ideas in terms of model type selection ie linear models should be ok, as soon as you ll start train period at least from 2017 01 02 HOME APPLIANCES looks like a non stationary TS, let s check that using Augmented Dickey Fuller test further. Tree based models might be a better fit for this product family PET SUPPLIES some monotonic trend component in TS, non stationary TS LINGERIE same, some monotonic trend component in TS, non stationary TS Let s check our time series for Stationarity using Augmented Dickey Fuller test with a significance level of less than 5 . The intuition behind this test is that it determines how strongly a time series is defined by a trend. As an example, let s take PET SUPPLIES, feel free to do the same for the rest of the suspicious product families:","result = adfuller(
 np.log1p(
 y.loc[""2016-08-01"":, y.columns.get_level_values(""family"").isin([""PET SUPPLIES""])].mean(axis=""columns"")
 )
)
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
 print('\t%s: %.3f' % (key, value))

if result[0] < result[4][""1%""]:
 print (""Reject Ho - Time Series is Stationary"")
else:
 print (""Failed to Reject Ho - Time Series is Non-Stationary"")",guide-external-data-features-for-multivariatets.ipynb
"Ok, now we want to test tree based regressors not linear for non satitionary TS with a low multivariate correlation. I did it already and found out, that only SCHOOL AND OFFICE SUPPLIES , BOOKS families will have an improvement from Tree based regressors vs. baseline Ridge. For these product families I ll use GradientBoostingRegressor ExtraTreesRegressor ensemble model:","non_st_ts = [""SCHOOL AND OFFICE SUPPLIES"",""BOOKS""]",guide-external-data-features-for-multivariatets.ipynb
"In FMCG high sales product categories usually have a low price elastisity and strong intra week correlation. So, let s take TOP 40 most sold product families personal guess, feel free to improve that and add K nearest neighbors regressor for strong intra week week seasonality and Bayesian regressor into ensemble model for them. To simplify the things in CustomRegressor I ll use inverse logic, with bottom 60 of low sales products in the separate list.","low_sales_ts = [""MAGAZINES"",""LAWN AND GARDEN"",""BABY CARE"",
 ""CELEBRATION"",""GROCERY II"",""HARDWARE"",""AUTOMOTIVE"",
 ""HOME AND KITCHEN I"",""HOME AND KITCHEN II"",
 ""HOME APPLIANCES"",""LINGERIE"",
 ""LADIESWEAR"",""SEAFOOD"",""PLAYERS AND ELECTRONICS"",
 ""PET SUPPLIES"",""BEAUTY"",""PREPARED FOODS"",
 ""HOME CARE"",""CLEANING""]",guide-external-data-features-for-multivariatets.ipynb
Let s take start date with some margin to stabilize sales numbers after opening 2017 04 30:,"sdate = '2017-04-30'
x=x.loc[sdate:]
y=y.loc[sdate:]",guide-external-data-features-for-multivariatets.ipynb
"3 Find relevant external features for several TS components in Multivariate Time Series Step 3.1 Select columns for external features searchTo find new features we ll use Upgini Feature search and enrichment library for supervised machine learning applications.To initiate search with Upgini library, you need to define so called search keys a set of columns to join external data sources and features. In this competition we can use the following keys: Column date should be used as SearchKey.DATE. Country as EC ISO 3166 country code for Ecuador , as all the data came from Favorita retailer in Ecuador. With this set of search keys, our dataset will be matched with different time specific features such as weather data, calendar data, financial data, etc , taking into account the country where sales happened. Than relevant selection and ranking will be done. As a result, we ll add new, only relevant features with additional information about specific dates in Ecuador. We already have date as index in train and test datasets, so let s use it. Index name is date, so column will have the same name. To start the search, we need to initiate scikit learn compartible FeaturesEnricher transformer with appropriate search parameters and cross validation type here we use TimeSeries CV, because our target variable strongly depends on time, ie we have TS prediction task . After that, we can call the fit or fit transform method of features enricher to start the search. Step 4","from upgini import FeaturesEnricher , SearchKey , ModelTaskType ",guide-external-data-features-for-multivariatets.ipynb
"That s just a quick guess, feel free to test other product families by fork change of this kernel ",enriched_ts_map = { } ,guide-external-data-features-for-multivariatets.ipynb
"log transform to reshape y distribution closer to normal, as we ll use same approach in the final model",y_fe_1 = np.log1p(y_fe_1) ,guide-external-data-features-for-multivariatets.ipynb
"Step 3.3 Initiate feature search and enrichment FeaturesEnricher.fit transform has a flag calculate metrics for the quick estimation of quality improvement from the new external features on cross validation and eval sets. This step is quite similar to sklearn.model selection.cross val score, so you can pass exact metric with scoring parameter: Built in scoring functions Custom scorer. Notice that you should pass X train as the first argument and y train as the second argument for FeaturesEnricher.fit transform , just like in scikit learn. Step will take around 2 minutes",% % time ,guide-external-data-features-for-multivariatets.ipynb
"We ve got 2 new relevant external features, which might improve accuracy of the model ranked by SHAP values. Initial features from search dataset will be checked for relevancy as well, so you don t need an extra feature selection step.MSE uplift after enrichment using all of the new external features is positive and significant more than 30 . We enriched initial feature space with only TOP 2 most important features, according to the SHAP values parameter max features 2 . Generally it s a bad idea to put a lot of features with unknown structure and possibly high pairwise correlation into a linear model, even with regularization like Ridge or Lasso, without careful selection and pre processing. Let s add these TOP 2 features to external features map enriched ts map with key LIQUOR,WINE,BEER:","enriched_ts_map[""LIQUOR,WINE,BEER""] = list(set(X_enriched.columns) - set(x.columns))",guide-external-data-features-for-multivariatets.ipynb
Next step is to enrich test dataset with the TOP 2 new features as well. And that s all for enrichment step. Step will take around 2 minutes,"X_test_enriched = enricher.transform(
 xtest.copy().reset_index(),
 keep_input=True,
 max_features=2, 
).set_index(""date"")",guide-external-data-features-for-multivariatets.ipynb
"Again, let s add most important features to external features map with key CLEANING:","enriched_ts_map[""CLEANING""] = list(set(X_enriched2.columns) - set(X_enriched.columns))",guide-external-data-features-for-multivariatets.ipynb
Summary on external feature search enrichment,"print(""Number of features, initial -> after enrichment:"",x.shape[1],""->"",X_enriched2.shape[1])
int_features = set(x.columns.to_list())
ext_features = [col for ext_features_ in enriched_ts_map.values() for col in ext_features_]

x = X_enriched2
xtest = X_test_enriched2
del X_enriched, X_enriched2
del X_test_enriched, X_test_enriched2
del y_fe_1
del y_fe_2
_ = gc.collect()",guide-external-data-features-for-multivariatets.ipynb
"4 Train final model 4.1 Same store sales predictions as new features not so blending Once again it s a very well known fact, that for the FMCG goods there is a strong correlation between product categories when buying product X, consumer buys product Y as well. And we already check what product families are correlated and what is not. Most of the product famlilies are correlated 25 of 33 . So let s try to use sales PREDICTION by product family as a new features, to catch information on joint sales Note on original notebook approach: This approach was called blending in the original notebook. But there was no holdout set to avoid overfitting and blending linear model are being trained on the same data as the ensembling final model. I strongly recommend you to read about blending with great visualisation here. In addition to that, in a final Custom Regressor there was no filter same store sales predictions and sales from ALL stores was used as features for every single store, so I fixed that. Step 5","from sklearn.linear_model import LinearRegression
from sklearn.compose import TransformedTargetRegressor, ColumnTransformer
from sklearn.preprocessing import PowerTransformer
from sklearn.pipeline import make_pipeline

lnr_reg = TransformedTargetRegressor(
 regressor = LinearRegression(fit_intercept = True, n_jobs = -1),
 func=np.log1p,
 inverse_func=np.expm1
)
lnr = make_pipeline(
 ColumnTransformer([(""drop_f"", ""drop"", ext_features)], remainder=""passthrough""),
 PowerTransformer(),
 lnr_reg
)

lnr.fit(x, y)
yfit_lnr = pd.DataFrame(lnr.predict(x), index = x.index, columns = y.columns).clip(0.)
ypred_lnr = pd.DataFrame(lnr.predict(xtest), index = xtest.index, columns = y.columns).clip(0.)

y_ = y.stack(['store_nbr', 'family'])
y_['lnr'] = yfit_lnr.stack(['store_nbr', 'family'])['sales']",guide-external-data-features-for-multivariatets.ipynb
from 30.04.17 till 31.08.17,ylnr = yfit_lnr.append(ypred_lnr) ,guide-external-data-features-for-multivariatets.ipynb
"What CustomRegressor is doing here it s fiting individual model on EVERY component of Multivariate TS single TS as combination FAMILY x STORE. And there will be 1658 independent models excluding Zero forecast . In our analysis on cross product correlations, we decided to introduce 3 groups of models in CustomRegressor, customized for specific product family set: Selected non stationary time series with low correlations: GradientBoostingRegressor ExtraTreesRegressor voting ensemble model High sales product categories TOP 40 by sales : K nearest neighbors regressor Bayesian regressor Ridge SVR voting ensemble model Product families enriched with new external features: simpe Ridge regressor Low sales product categories BOTTOM 60 by sales : Ridge regressor SVR voting ensemble",import warnings ,guide-external-data-features-for-multivariatets.ipynb
SEED for reproducible result,SEED = 5 ,guide-external-data-features-for-multivariatets.ipynb
As these features won t be relevant for the rest of product families and might decrease accuracy," remove_ext_features = ColumnTransformer ([( ""drop_f"" , ""drop"" , self.ext_features )], remainder = ""passthrough"") ",guide-external-data-features-for-multivariatets.ipynb
Be patient training may take up to 15 minutes:,% % time ,guide-external-data-features-for-multivariatets.ipynb
manual selection for KNN regression,"knn_features = list(int_features - set (['oil_lags2' , 'oil_lags1' , ""trend""])) ",guide-external-data-features-for-multivariatets.ipynb
5 Submit prediction with enriched features and calculate final leaderbord progress Let s quickly estimate model accuracy no cross validation! and submit:,"from sklearn.metrics import mean_squared_log_error as msle
y_pred = y_pred.stack(['store_nbr', 'family']).clip(0.)
y_ = y.stack(['store_nbr', 'family']).clip(0.)
y_['pred'] = y_pred.values
print(y_.groupby('family').apply(lambda r : np.sqrt(np.sqrt(msle(r['sales'], r['pred'])))))
print('RMSLE : ', np.sqrt(np.sqrt(msle(y_['sales'], y_['pred']))))",guide-external-data-features-for-multivariatets.ipynb
"This tutorial is part Level 2 in the Learn Machine Learning curriculum. This tutorial picks up where Level 1 finished, so you will get the most out of it if you ve done the exercise from Level 1.In this step, you will learn three approaches to dealing with missing values. You will then learn to compare the effectiveness of these approaches on any given dataset. IntroductionThere are many ways data can end up with missing values. For example A 2 bedroom house wouldn t include an answer for How large is the third bedroom Someone being surveyed may choose not to share their incomePython libraries represent missing numbers as nan which is short for not a number . You can detect which cells have missing values, and then count how many there are in each column with the command: missing val count by column data.isnull .sum print missing val count by column missing val count by column 0Most libraries including scikit learn will give you an error if you try to build a model using data with missing values. So you ll need to choose one of the strategies below. Solutions1 A Simple Option: Drop Columns with Missing Values If your data is in a DataFrame called original data, you can drop columns with missing values. One way to do that is data without missing values original data.dropna axis 1 In many cases, you ll have both a training dataset and a test dataset. You will want to drop the same columns in both DataFrames. In that case, you would writecols with missing col for col in original data.columns if original data col .isnull .any reduced original data original data.drop cols with missing, axis 1 reduced test data test data.drop cols with missing, axis 1 If those columns had useful information in the places that were not missing , your model loses access to this information when the column is dropped. Also, if your test data has missing values in places where your training data did not, this will result in an error. So, it s somewhat usually not the best solution. However, it can be useful when most values in a column are missing.2 A Better Option: Imputation Imputation fills in the missing value with some number. The imputed value won t be exactly right in most cases, but it usually gives more accurate models than dropping the column entirely.This is done with from sklearn.impute import SimpleImputer my imputer SimpleImputer data with imputed values my imputer.fit transform original data The default behavior fills in the mean value for imputation. Statisticians have researched more complex strategies, but those complex strategies typically give no benefit once you plug the results into sophisticated machine learning models.One of many nice things about Imputation is that it can be included in a scikit learn Pipeline. Pipelines simplify model building, model validation and model deployment.3 An Extension To Imputation Imputation is the standard approach, and it usually works well. However, imputed values may by systematically above or below their actual values which weren t collected in the dataset . Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing. Here s how it might look: make copy to avoid changing original data when Imputing new data original data.copy make new columns indicating what will be imputed cols with missing col for col in new data.columns if new data col .isnull .any for col in cols with missing: new data col was missing new data col .isnull Imputation my imputer SimpleImputer new data pd.DataFrame my imputer.fit transform new data new data.columns original data.columns In some cases this approach will meaningfully improve results. In other cases, it doesn t help at all. Example Comparing All Solutions We will see am example predicting housing prices from the Melbourne Housing data. To master missing value handling, fork this notebook and repeat the same steps with the Iowa Housing data. Find information about both in the Data section of the header menu.Basic Problem Set up",import pandas as pd ,handling-missing-values.ipynb
Load data,melb_data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv') ,handling-missing-values.ipynb
"For the sake of keeping the example simple, we ll use only numeric predictors.",melb_numeric_predictors = melb_predictors.select_dtypes(exclude =['object']) ,handling-missing-values.ipynb
"Create Function to Measure Quality of An Approach We divide our data into training and test. If the reason for this is unfamiliar, review Welcome to Data Science.We ve loaded a function score dataset X train, X test, y train, y test to compare the quality of diffrent approaches to missing values. This function reports the out of sample MAE score from a RandomForest.","from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(melb_numeric_predictors, 
 melb_target,
 train_size=0.7, 
 test_size=0.3, 
 random_state=0)

def score_dataset(X_train, X_test, y_train, y_test):
 model = RandomForestRegressor()
 model.fit(X_train, y_train)
 preds = model.predict(X_test)
 return mean_absolute_error(y_test, preds)",handling-missing-values.ipynb
Get Model Score from Dropping Columns with Missing Values,"cols_with_missing = [col for col in X_train.columns 
 if X_train[col].isnull().any()]
reduced_X_train = X_train.drop(cols_with_missing, axis=1)
reduced_X_test = X_test.drop(cols_with_missing, axis=1)
print(""Mean Absolute Error from dropping columns with Missing Values:"")
print(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))",handling-missing-values.ipynb
Get Model Score from Imputation,"from sklearn.impute import SimpleImputer

my_imputer = SimpleImputer()
imputed_X_train = my_imputer.fit_transform(X_train)
imputed_X_test = my_imputer.transform(X_test)
print(""Mean Absolute Error from Imputation:"")
print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))",handling-missing-values.ipynb
Get Score from Imputation with Extra Columns Showing What Was Imputed,imputed_X_train_plus = X_train.copy () ,handling-missing-values.ipynb
linear algebra,import numpy as np ,hello-scikit-learn.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,hello-scikit-learn.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,hello-scikit-learn.ipynb
" 999, 1 ","train_labels = pd.read_csv('../input/data-science-london-scikit-learn/trainLabels.csv' , header = None) ",hello-scikit-learn.ipynb
" 999, 40 ","train = pd.read_csv('../input/data-science-london-scikit-learn/train.csv' , header = None) ",hello-scikit-learn.ipynb
" 8999, 40 ","test = pd.read_csv('../input/data-science-london-scikit-learn/test.csv' , header = None) ",hello-scikit-learn.ipynb
Summary,"import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.ensemble import VotingClassifier

X_train = train
y_train = train_labels
X_test = test

X_train = np.asarray(X_train)
y_train = np.asarray(y_train)
X_test = np.asarray(X_test)
y_train = y_train.ravel()

print(""Training Data Shape: "", X_train.shape)
print(""Training Target Shape: "", y_train.shape)
print(""Testing Data Shape: "", X_test.shape)",hello-scikit-learn.ipynb
linear algebra,import numpy as np ,house-price-calculation-methods-for-beginners.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,house-price-calculation-methods-for-beginners.ipynb
"For example, runninga this by clicking run or pressing Shift Enter will list the files in the input directory",from subprocess import check_output ,house-price-calculation-methods-for-beginners.ipynb
Any results you write to the current directory are saved as output.,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.utils import shuffle

%matplotlib inline
import warnings
warnings.filterwarnings('ignore')",house-price-calculation-methods-for-beginners.ipynb
I want you to look at this thread of mine which tells you what to do first in BASIC EDA. Dont forget to upvote please.,"train.shape,test.shape",house-price-calculation-methods-for-beginners.ipynb
check for dupes for Id,idsUnique = len(set(train.Id)) ,house-price-calculation-methods-for-beginners.ipynb
drop id col,"train.drop (['Id'], axis = 1 , inplace = True) ",house-price-calculation-methods-for-beginners.ipynb
correlation matrix,corrmat = train.corr () ,house-price-calculation-methods-for-beginners.ipynb
most correlated features,corrmat = train.corr () ,house-price-calculation-methods-for-beginners.ipynb
"most of the features are correlated with each other like Garage Cars and Garage Area, isnt it? OverallQual is highly correlated with target feature SalePrice 0.79 can you see. we ll see how it effected the saleprice in below graph.","sns.barplot(train.OverallQual,train.SalePrice)",house-price-calculation-methods-for-beginners.ipynb
Here you can see how each feature is correlated with SalePrice.,"sns.set()
cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']
sns.pairplot(train[cols], size = 2.5)
plt.show();",house-price-calculation-methods-for-beginners.ipynb
SalePrice is the variable we need to predict. So let s do some analysis on this variable first.,from scipy import stats ,house-price-calculation-methods-for-beginners.ipynb
for some statistics,"from scipy.stats import norm , skew ",house-price-calculation-methods-for-beginners.ipynb
Get the fitted parameters used by the function,"( mu , sigma)= norm.fit(train['SalePrice']) ",house-price-calculation-methods-for-beginners.ipynb
for more info click Here,"train.SalePrice = np.log1p(train.SalePrice )
y = train.SalePrice",house-price-calculation-methods-for-beginners.ipynb
Pre processing,"plt.scatter(y = train.SalePrice , x = train.GrLivArea , c = 'black') ",house-price-calculation-methods-for-beginners.ipynb
we can see the outlier in the below image,"train_nas = train.isnull().sum()
train_nas = train_nas[train_nas>0]
train_nas.sort_values(ascending=False)",house-price-calculation-methods-for-beginners.ipynb
Differentiate numerical features minus the target and categorical features,categorical_features = train.select_dtypes(include =['object']).columns ,house-price-calculation-methods-for-beginners.ipynb
Differentiate numerical features minus the target and categorical features,"categorical_features = train.select_dtypes(include =[""object""]).columns ",house-price-calculation-methods-for-beginners.ipynb
Handle remaining missing values for numerical features by using median as replacement,"print(""NAs for numerical features in train : "" + str(train_num.isnull (). values.sum ())) ",house-price-calculation-methods-for-beginners.ipynb
we can treat skewness of a feature with the help fof log transformation.so we ll apply the same here.,skew_features = np.log1p(skew_features) ,house-price-calculation-methods-for-beginners.ipynb
Create dummy features for categorical values via one hot encoding,train_cat.shape ,house-price-calculation-methods-for-beginners.ipynb
Modeling,"import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV
from sklearn.metrics import mean_squared_error, make_scorer
import matplotlib.pyplot as plt
import seaborn as sns",house-price-calculation-methods-for-beginners.ipynb
earlier we split the train set into categorical and numerical features. Now after transformation preprocessing we ll join them to get the whole train set back.,"train = pd.concat([train_cat,train_num],axis=1)
train.shape",house-price-calculation-methods-for-beginners.ipynb
split the data to train the model,"X_train , X_test , y_train , y_test = train_test_split(train , y , test_size = 0.3 , random_state = 0) ",house-price-calculation-methods-for-beginners.ipynb
Defining cross val score function for both train and test sets separately,"n_folds = 5
from sklearn.metrics import make_scorer
from sklearn.model_selection import KFold
scorer = make_scorer(mean_squared_error,greater_is_better = False)
def rmse_CV_train(model):
 kf = KFold(n_folds,shuffle=True,random_state=42).get_n_splits(train.values)
 rmse = np.sqrt(-cross_val_score(model,X_train,y_train,scoring =""neg_mean_squared_error"",cv=kf))
 return (rmse)
def rmse_CV_test(model):
 kf = KFold(n_folds,shuffle=True,random_state=42).get_n_splits(train.values)
 rmse = np.sqrt(-cross_val_score(model,X_test,y_test,scoring =""neg_mean_squared_error"",cv=kf))
 return (rmse)",house-price-calculation-methods-for-beginners.ipynb
Linear model without Regularization,"lr = LinearRegression()
lr.fit(X_train,y_train)
test_pre = lr.predict(X_test)
train_pre = lr.predict(X_train)
print('rmse on train',rmse_CV_train(lr).mean())
print('rmse on train',rmse_CV_test(lr).mean())
",house-price-calculation-methods-for-beginners.ipynb
plot between predicted values and residuals,"plt.scatter(train_pre , train_pre - y_train , c = ""blue"" , label = ""Training data"") ",house-price-calculation-methods-for-beginners.ipynb
Plot predictions Real values,"plt.scatter(train_pre , y_train , c = ""blue"" , label = ""Training data"") ",house-price-calculation-methods-for-beginners.ipynb
"Regularization is a very useful method to handle collinearity, filter out noise from data, and eventually prevent overfitting.The concept behind regularization is to introduce additional information bias to penalize extreme parameter weights.I know it is confusing for beginner you can find the simple tutorial here","ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])
ridge.fit(X_train,y_train)
alpha = ridge.alpha_
print('best alpha',alpha)

print(""Try again for more precision with alphas centered around "" + str(alpha))
ridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, 
 alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,
 alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4],cv = 5)
ridge.fit(X_train, y_train)
alpha = ridge.alpha_
print(""Best alpha :"", alpha)
print(""Ridge RMSE on Training set :"", rmse_CV_train(ridge).mean())
print(""Ridge RMSE on Test set :"", rmse_CV_test(ridge).mean())
y_train_rdg = ridge.predict(X_train)
y_test_rdg = ridge.predict(X_test)",house-price-calculation-methods-for-beginners.ipynb
Plot residuals,"plt.scatter(y_train_rdg , y_train_rdg - y_train , c = ""blue"" , label = ""Training data"") ",house-price-calculation-methods-for-beginners.ipynb
Plot predictions Real values,"plt.scatter(y_train_rdg , y_train , c = ""blue"" , label = ""Training data"") ",house-price-calculation-methods-for-beginners.ipynb
"Model PerformanceWe can observe from the graph below that the blended model far outperforms the other models, with an RMSLE of 0.075. This is the model I used for making the final predictions.","from IPython.display import Image
Image(""../input/kernel-files/model_training_advanced_regression.png"")",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Essentials,import numpy as np ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Plots,import seaborn as sns ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Models,"from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor , AdaBoostRegressor , BaggingRegressor ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Stats,"from scipy.stats import skew , norm ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Misc,from sklearn.model_selection import GridSearchCV ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Ignore useless warnings,import warnings ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Read in the dataset as a dataframe,train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv') ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Preview the data we re working with,train.head () ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
SalePrice: the variable we re trying to predict,"sns.set_style(""white"") ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Check the new distribution,"sns.distplot(train['SalePrice'], color = ""b""); ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Skew and kurt,"print(""Skewness: %f"" % train['SalePrice']. skew ()) ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Finding numeric features,"numeric_dtypes =['int16' , 'int32' , 'int64' , 'float16' , 'float32' , 'float64'] ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
visualising some more outliers in the data values,"fig , axs = plt.subplots(ncols = 2 , nrows = 0 , figsize =(12 , 120)) ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
"and plot how the features are correlated to each other, and to SalePrice","corr = train.corr()
plt.subplots(figsize=(15,12))
sns.heatmap(corr, vmax=0.9, cmap=""Blues"", square=True)",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Let s plot how SalePrice relates to some of the features in the dataset,"data = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)
f, ax = plt.subplots(figsize=(8, 6))
fig = sns.boxplot(x=train['OverallQual'], y=""SalePrice"", data=data)
fig.axis(ymin=0, ymax=800000);",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
"Remove the Ids from train and test, as they are unique for each row and hence not useful for the model",train_ID = train['Id'] ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Let s take a look at the distribution of the SalePrice.,"sns.set_style(""white"") ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Check the new distribution,"sns.distplot(train['SalePrice'], color = ""b""); ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
log 1 x transform,"train[""SalePrice""]= np.log1p(train[""SalePrice""]) ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Let s plot the SalePrice again.,"sns.set_style(""white"") ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Check the new distribution,"sns.distplot(train['SalePrice'], fit = norm , color = ""b""); ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Get the fitted parameters used by the function,"( mu , sigma)= norm.fit(train['SalePrice']) ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Remove outliers,"train.drop(train[( train['OverallQual']< 5)&(train['SalePrice']> 200000 )]. index , inplace = True) ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Split features and labels,train_labels = train['SalePrice']. reset_index(drop = True) ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Combine train and test features in order to apply the feature transformation pipeline to the entire dataset,"all_features = pd.concat ([train_features , test_features]).reset_index(drop = True) ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
determine the threshold for missing values,def percent_missing(df): ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Visualize missing values,"sns.set_style(""white"") ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Tweak the visual presentation,ax.xaxis.grid(False) ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Some of the non numeric predictors are stored as numbers convert them into strings,all_features['MSSubClass']= all_features['MSSubClass']. apply(str) ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
the data description states that NA refers to typical Typ values, features['Functional']= features['Functional']. fillna('Typ') ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Replace the missing values in each of the columns below with their mode," features['Electrical']= features['Electrical']. fillna(""SBrkr"") ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
the data description stats that NA refers to No Pool ," features[""PoolQC""]= features[""PoolQC""]. fillna(""None"") ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
"Replacing the missing values with 0, since no garage no cars in garage"," for col in('GarageYrBlt' , 'GarageArea' , 'GarageCars'): ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Replacing the missing values with None," for col in['GarageType' , 'GarageFinish' , 'GarageQual' , 'GarageCond']: ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
"NaN values for these categorical basement features, means there s no basement"," for col in('BsmtQual' , 'BsmtCond' , 'BsmtExposure' , 'BsmtFinType1' , 'BsmtFinType2'): ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
"Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood", features['LotFrontage']= features.groupby('Neighborhood')[ 'LotFrontage']. transform(lambda x : x.fillna(x.median ())) ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
So we replace their missing values with None, objects = [] ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
"And we do the same thing for numerical features, but this time with 0s"," numeric_dtypes =['int16' , 'int32' , 'int64' , 'float16' , 'float32' , 'float64'] ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Let s make sure we handled all the missing values,missing = percent_missing(all_features) ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Fetch all numeric features,"numeric_dtypes =['int16' , 'int32' , 'int64' , 'float16' , 'float32' , 'float64'] ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Create box plots for all numeric features,"sns.set_style(""white"") ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Find skewed numerical features,skew_features = all_features[numeric]. apply(lambda x : skew(x)).sort_values(ascending = False) ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Normalize skewed features,for i in skew_index : ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Let s make sure we handled all the skewed values,"sns.set_style(""white"") ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
"ML models have trouble recognizing more complex patterns and we re staying away from neural nets for this competition , so let s help our models out by creating a few features based on our intuition about the dataset, e.g. total area of floors, bathrooms and porch area of each house.","all_features['BsmtFinType1_Unf'] = 1*(all_features['BsmtFinType1'] == 'Unf')
all_features['HasWoodDeck'] = (all_features['WoodDeckSF'] == 0) * 1
all_features['HasOpenPorch'] = (all_features['OpenPorchSF'] == 0) * 1
all_features['HasEnclosedPorch'] = (all_features['EnclosedPorch'] == 0) * 1
all_features['Has3SsnPorch'] = (all_features['3SsnPorch'] == 0) * 1
all_features['HasScreenPorch'] = (all_features['ScreenPorch'] == 0) * 1
all_features['YearsSinceRemodel'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)
all_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']
all_features = all_features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)
all_features['TotalSF'] = all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']
all_features['YrBltAndRemod'] = all_features['YearBuilt'] + all_features['YearRemodAdd']

all_features['Total_sqr_footage'] = (all_features['BsmtFinSF1'] + all_features['BsmtFinSF2'] +
 all_features['1stFlrSF'] + all_features['2ndFlrSF'])
all_features['Total_Bathrooms'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']) +
 all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath']))
all_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +
 all_features['EnclosedPorch'] + all_features['ScreenPorch'] +
 all_features['WoodDeckSF'])
all_features['TotalBsmtSF'] = all_features['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)
all_features['2ndFlrSF'] = all_features['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)
all_features['GarageArea'] = all_features['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)
all_features['GarageCars'] = all_features['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)
all_features['LotFrontage'] = all_features['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)
all_features['MasVnrArea'] = all_features['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)
all_features['BsmtFinSF1'] = all_features['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)

all_features['haspool'] = all_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)
all_features['has2ndfloor'] = all_features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)
all_features['hasgarage'] = all_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)
all_features['hasbsmt'] = all_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)
all_features['hasfireplace'] = all_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
"Feature transformations Let s create more features by calculating the log and square transformations of our numerical features. We do this manually, because ML models won t be able to reliably tell if log feature or feature 2 is a predictor of the SalePrice.","def logs(res, ls):
 m = res.shape[1]
 for l in ls:
 res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values) 
 res.columns.values[m] = l + '_log'
 m += 1
 return res

log_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',
 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',
 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',
 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',
 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']

all_features = logs(all_features, log_features)",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Numerically encode categorical features because most models can only handle numerical features.,"all_features = pd.get_dummies(all_features).reset_index(drop=True)
all_features.shape",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Remove any duplicated column names,"all_features = all_features.loc[: , ~ all_features.columns.duplicated()] ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Recreate training and test sets,"X = all_features.iloc[:len(train_labels), :]
X_test = all_features.iloc[len(train_labels):, :]
X.shape, train_labels.shape, X_test.shape",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Finding numeric features,"numeric_dtypes =['int16' , 'int32' , 'int64' , 'float16' , 'float32' , 'float64'] ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
visualising some more outliers in the data values,"fig , axs = plt.subplots(ncols = 2 , nrows = 0 , figsize =(12 , 150)) ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Setup cross validation folds,"kf = KFold(n_splits = 12 , random_state = 42 , shuffle = True) ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Define error metrics,"def rmsle(y , y_pred): ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Get cross validation scores for each model,"scores = {}

score = cv_rmse(lightgbm)
print(""lightgbm: {:.4f} ({:.4f})"".format(score.mean(), score.std()))
scores['lgb'] = (score.mean(), score.std())",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Fit the models,"print('stack_gen')
stack_gen_model = stack_gen.fit(np.array(X), np.array(train_labels))",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Blend models in order to make the final predictions more robust to overfitting,def blended_predictions(X): ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Get final precitions from the blended model,"blended_score = rmsle(train_labels , blended_predictions(X)) ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Plot the predictions for each model,"sns.set_style(""white"") ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Read in sample submission dataframe,"submission = pd.read_csv(""../input/house-prices-advanced-regression-techniques/sample_submission.csv"") ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Append predictions from blended models,"submission.iloc[: , 1]= np.floor(np.expm1(blended_predictions(X_test))) ",how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Fix outleir predictions,q1 = submission['SalePrice']. quantile(0.0045) ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
Scale predictions,submission['SalePrice']*= 1.001619 ,how-i-made-top-0-3-on-a-kaggle-competition.ipynb
"Loading packages First of all, let s load some packages...","from sklearn.linear_model import LogisticRegression
from sklearn import datasets as dt
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

import pandas as pd
import numpy as np
import seaborn as sns


import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go
import plotly.tools as tls

import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import matplotlib
%matplotlib inline

from skimage.io import imread, imshow

import warnings
warnings.filterwarnings(""ignore"", category=DeprecationWarning)
warnings.filterwarnings(""ignore"", category=FutureWarning)
warnings.filterwarnings(""ignore"", category=UserWarning)",how-to-attack-a-machine-learning-model.ipynb
Loading dataNow we will use the digits of the digit recognizer competition. Let s check: ,"from subprocess import check_output
print(check_output([""ls"", ""../input""]).decode(""utf8""))",how-to-attack-a-machine-learning-model.ipynb
"Ok, the label holds the true digit and the other columns all 784 pixel of an image with 28 times 28 pixels. Let s split our data intro train and test. This way we can measure our model performance on the test set and we can see how this score breaks down during the attack.","y = df.label.values
X = df.drop(""label"",axis=1).values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)",how-to-attack-a-machine-learning-model.ipynb
"Before we start with building targeted and non targeted attacks, let s have a look at the first digits of the test set:","fig1, ax1 = plt.subplots(1,15, figsize=(15,10))
for i in range(15):
 ax1[i].imshow(X_test[i].reshape((28,28)), cmap=""gray_r"")
 ax1[i].axis('off')
 ax1[i].set_title(y_test[i])",how-to-attack-a-machine-learning-model.ipynb
Attack classI have written a small class that performs the attack of logistic regression:,"class Attack:

 def __init__(self, model):
 self.fooling_targets = None
 self.model = model
 
 def prepare(self, X_train, y_train, X_test, y_test):
 self.images = X_test
 self.true_targets = y_test
 self.num_samples = X_test.shape[0]
 self.train(X_train, y_train)
 print(""Model training finished."")
 self.test(X_test, y_test)
 print(""Model testing finished. Initial accuracy score: "" + str(self.initial_score))
 
 def set_fooling_targets(self, fooling_targets):
 self.fooling_targets = fooling_targets
 
 def train(self, X_train, y_train):
 self.model.fit(X_train, y_train)
 self.weights = self.model.coef_
 self.num_classes = self.weights.shape[0]

 def test(self, X_test, y_test):
 self.preds = self.model.predict(X_test)
 self.preds_proba = self.model.predict_proba(X_test)
 self.initial_score = accuracy_score(y_test, self.preds)
 
 def create_one_hot_targets(self, targets):
 self.one_hot_targets = np.zeros(self.preds_proba.shape)
 for n in range(targets.shape[0]):
 self.one_hot_targets[n, targets[n]] = 1
 
 def attack(self, attackmethod, epsilon):
 perturbed_images, highest_epsilon = self.perturb_images(epsilon, attackmethod)
 perturbed_preds = self.model.predict(perturbed_images)
 score = accuracy_score(self.true_targets, perturbed_preds)
 return perturbed_images, perturbed_preds, score, highest_epsilon
 
 def perturb_images(self, epsilon, gradient_method):
 perturbed = np.zeros(self.images.shape)
 max_perturbations = []
 for n in range(self.images.shape[0]):
 perturbation = self.get_perturbation(epsilon, gradient_method, self.one_hot_targets[n], self.preds_proba[n])
 perturbed[n] = self.images[n] + perturbation
 max_perturbations.append(np.max(perturbation))
 highest_epsilon = np.max(np.array(max_perturbations))
 return perturbed, highest_epsilon
 
 def get_perturbation(self, epsilon, gradient_method, target, pred_proba):
 gradient = gradient_method(target, pred_proba, self.weights)
 inf_norm = np.max(gradient)
 perturbation = epsilon/inf_norm * gradient
 return perturbation
 
 def attack_to_max_epsilon(self, attackmethod, max_epsilon):
 self.max_epsilon = max_epsilon
 self.scores = []
 self.epsilons = []
 self.perturbed_images_per_epsilon = []
 self.perturbed_outputs_per_epsilon = []
 for epsilon in range(0, self.max_epsilon):
 perturbed_images, perturbed_preds, score, highest_epsilon = self.attack(attackmethod, epsilon)
 self.epsilons.append(highest_epsilon)
 self.scores.append(score)
 self.perturbed_images_per_epsilon.append(perturbed_images)
 self.perturbed_outputs_per_epsilon.append(perturbed_preds)",how-to-attack-a-machine-learning-model.ipynb
Attack methods,"def calc_output_weighted_weights(output, w):
 for c in range(len(output)):
 if c == 0:
 weighted_weights = output[c] * w[c]
 else:
 weighted_weights += output[c] * w[c]
 return weighted_weights

def targeted_gradient(foolingtarget, output, w):
 ww = calc_output_weighted_weights(output, w)
 for k in range(len(output)):
 if k == 0:
 gradient = foolingtarget[k] * (w[k]-ww)
 else:
 gradient += foolingtarget[k] * (w[k]-ww)
 return gradient

def non_targeted_gradient(target, output, w):
 ww = calc_output_weighted_weights(output, w)
 for k in range(len(target)):
 if k == 0:
 gradient = (1-target[k]) * (w[k]-ww)
 else:
 gradient += (1-target[k]) * (w[k]-ww)
 return gradient

def non_targeted_sign_gradient(target, output, w):
 gradient = non_targeted_gradient(target, output, w)
 return np.sign(gradient)",how-to-attack-a-machine-learning-model.ipynb
Training the model First of all we need a model for multiclass logistic regression:,"model = LogisticRegression(multi_class='multinomial', solver='lbfgs', fit_intercept=False)",how-to-attack-a-machine-learning-model.ipynb
And we will pass it to our class and call prepare. This way we train our model on training data and we will obtain the initial accuracy score on test data. Later on we want to break down this score by perturbing the test data.,"attack = Attack(model)
attack.prepare(X_train, y_train, X_test, y_test)",how-to-attack-a-machine-learning-model.ipynb
Let s check if we have as much weight vectors as classes:,"weights = attack.weights
weights.shape",how-to-attack-a-machine-learning-model.ipynb
"First of all we need to calculate the perturbations for each image in the test set. To do this we have to transform our true targets to one hot targets and call attack : . As I want to see, how much epsilon we need to create a good breakdown, I use the attack to max epsilon method. ","attack.create_one_hot_targets(y_test)
attack.attack_to_max_epsilon(non_targeted_gradient, 30)
non_targeted_scores = attack.scores",how-to-attack-a-machine-learning-model.ipynb
"Uii, the threshold is given by a max of 16 pixel that are allowed to be added as perturbation per pixel per image. Given this we would end up with a model that still predicts around 40 correctly. If we would use max the model would fail with almoast 90 digits in the test set : . Let s have a look at one example of successful fooling for a range of epsilons until max of .","eps = 16
attack.epsilons[eps]",how-to-attack-a-machine-learning-model.ipynb
We need the perturbed images as well as the fooling results of that epsilon:,"example_images = attack.perturbed_images_per_epsilon[eps]
example_preds = attack.perturbed_outputs_per_epsilon[eps]",how-to-attack-a-machine-learning-model.ipynb
And I will store results in a pandas dataframe such that we can easily find successful foolings:,"example_results = pd.DataFrame(data=attack.true_targets, columns=['y_true'])
example_results['y_fooled'] = example_preds
example_results['y_predicted'] = attack.preds
example_results['id'] = example_results.index.values
example_results.head()",how-to-attack-a-machine-learning-model.ipynb
"Ok, we will choose one of these successful examples and plot its related perturbed image over a range of epsilons: ","example_id = success_df.id.values[0]
example_id",how-to-attack-a-machine-learning-model.ipynb
"Yeah! : We can still see the true target and not the fooling target. That s amazing. But we can also see, that the background has increased intensitiy. Let s visualize the difference between the original true label and the adversarial image for : ","fig, (axA, axB, axC) = plt.subplots(1, 3, figsize=(15,5))
axB.imshow(example_images[example_id].reshape((28,28)), cmap='Greens')
axB.set_title(""Non-targeted attack result: "" + str(example_preds[example_id]))
axA.imshow(X_test[example_id].reshape((28,28)), cmap='Greens')
axA.set_title(""True label: "" + str(y_test[example_id]))
axC.imshow((X_test[example_id]-example_images[example_id]).reshape((28,28)), cmap='Reds')
axC.set_title(""Perturbation: epsilon 16"");",how-to-attack-a-machine-learning-model.ipynb
"The gradient travel guide natural fooling targetsI m happy that it was possible to fool our model but it s still diffuse and unclear where the one step gradient guides us through remember we do not iterate with gradient ascent, we just take one step and size is given by strength of gradient times eta . I assume that some numbers are closer to each other in weight space than to others. As the model training draws decision boundaries dependent on the quality of the input data and flexibility of model architecture, there will be regions where a 3 is not predicted as 3 but as 8. Those regions where the model makes an incorrect prediction. And I think, that there are preffered numbers to be wrong predictions given a digit input image. Perhaps the fooling gradients drives us to those natural fooling target numbers? ","plt.figure(figsize=(10,5))
sns.countplot(x='y_fooled', data=example_results[example_results.y_true != example_results.y_fooled])",how-to-attack-a-machine-learning-model.ipynb
"Ok, we see that 8 was selected most often as fooling target. But 9, 3, 5 and 2 have high counts as well in contrast to 0, 1, 6 and 7. If our assumption is true that the gradient drives us to targets where the model tends to fail in prediction we should see a similar pattern of counts for wrong predictions:","wrong_predictions = example_results[example_results.y_true != example_results.y_predicted]
wrong_predictions.shape",how-to-attack-a-machine-learning-model.ipynb
"Ok, so out of 16800 samples, the model failed to predict around 1600. That s why our intital accuracy score is close to 90 means 10 failing . Now, which digit was selected as wrong prediction result most often?","plt.figure(figsize=(10,5))
sns.countplot(x='y_predicted', data=wrong_predictions)",how-to-attack-a-machine-learning-model.ipynb
"Yes, that s the same pattern as for the fooling targets. As this is caused by the difficulty of our model to draw good decision boundaries we should see this pattern as well for the true labels of those digits that were wrong predicted:","plt.figure(figsize=(10,5))
sns.countplot(x='y_true', data=wrong_predictions)",how-to-attack-a-machine-learning-model.ipynb
Now I want to see it in more detail: Which are the natural fooling targets for successful foolings for each digit?,"attacktargets = example_results.loc[example_results.y_true != example_results.y_fooled].groupby(
 'y_true').y_fooled.value_counts()
counts = example_results.loc[example_results.y_true != example_results.y_fooled].groupby(
 'y_true').y_fooled.count()
attacktargets = attacktargets/counts * 100
attacktargets = attacktargets.unstack()
attacktargets = attacktargets.fillna(0.0) 
attacktargets = attacktargets.apply(np.round).astype(np.int)",how-to-attack-a-machine-learning-model.ipynb
"One example imageTo play around, let s select one input of and try to make targeted attacks for each class except for the true label target . ","example = X_test[0]
imshow(example.reshape((28,28)), cmap='Greens');",how-to-attack-a-machine-learning-model.ipynb
"First of all, we need some fooling targets. For our example digit all others are possible:","fooling_classes = []
for k in range(num_classes):
 if k != y_test[0]:
 fooling_classes.append(k)
fooling_classes",how-to-attack-a-machine-learning-model.ipynb
"Attacking the modelI will force the attack to success by allowing an epsilon high enough to yield all targets. This way we can still find out, if we can see the true label or the fooling target.","eps=100
targeted_perturbed_images = []
targeted_perturbed_predictions = []
for fooling_target in foolingtargets: 
 targeted_perturbation = attack.get_perturbation(eps, targeted_gradient, fooling_target, attack.preds_proba[0])
 targeted_perturbed_image = X_test[0] + targeted_perturbation
 targeted_perturbed_prediction = attack.model.predict(targeted_perturbed_image.reshape(1, -1))
 targeted_perturbed_images.append(targeted_perturbed_image)
 targeted_perturbed_predictions.append(targeted_perturbed_prediction)",how-to-attack-a-machine-learning-model.ipynb
Prepare natural and non natural fooling targetsThe gradient travel guide showed us the occurences of fooling target digits for each true digit. The highest count stands for the natural fooling target whereas the lowest corresponds to the non natural fooling target. Given the heatmap we could create the targets by argmin and argmax per row y true as follows:,"f, ax = plt.subplots(figsize=(10, 10))
sns.heatmap(attacktargets, annot=True, ax=ax, cbar=False, cmap=""Purples"", fmt=""g"");",how-to-attack-a-machine-learning-model.ipynb
Some small experiment: Let s only take the sign of the gradient of our attack. I think we will not be as goog as we could be with the full gradient:,"attack.create_one_hot_targets(y_test)
attack.attack_to_max_epsilon(non_targeted_sign_gradient, 30)",how-to-attack-a-machine-learning-model.ipynb
LOAD LIBRARIES,import pandas as pd ,how-to-choose-cnn-architecture-mnist.ipynb
LOAD THE DATA,"train = pd.read_csv(""../input/train.csv"") ",how-to-choose-cnn-architecture-mnist.ipynb
PREPARE DATA FOR NEURAL NETWORK,"Y_train = train[""label""] ",how-to-choose-cnn-architecture-mnist.ipynb
GLOBAL VARIABLES,"annealer = LearningRateScheduler(lambda x : 1e-3 * 0.95 ** x , verbose = 0) ",how-to-choose-cnn-architecture-mnist.ipynb
BUILD CONVOLUTIONAL NEURAL NETWORKS,nets = 3 ,how-to-choose-cnn-architecture-mnist.ipynb
CREATE VALIDATION SET,"X_train2 , X_val2 , Y_train2 , Y_val2 = train_test_split(X_train , Y_train , test_size = 0.333) ",how-to-choose-cnn-architecture-mnist.ipynb
TRAIN NETWORKS,history =[0]* nets ,how-to-choose-cnn-architecture-mnist.ipynb
PLOT ACCURACIES,"plt.figure(figsize =(15 , 5)) ",how-to-choose-cnn-architecture-mnist.ipynb
BUILD CONVOLUTIONAL NEURAL NETWORKS,nets = 6 ,how-to-choose-cnn-architecture-mnist.ipynb
CREATE VALIDATION SET,"X_train2 , X_val2 , Y_train2 , Y_val2 = train_test_split(X_train , Y_train , test_size = 0.333) ",how-to-choose-cnn-architecture-mnist.ipynb
TRAIN NETWORKS,history =[0]* nets ,how-to-choose-cnn-architecture-mnist.ipynb
PLOT ACCURACIES,"plt.figure(figsize =(15 , 5)) ",how-to-choose-cnn-architecture-mnist.ipynb
BUILD CONVOLUTIONAL NEURAL NETWORKS,nets = 8 ,how-to-choose-cnn-architecture-mnist.ipynb
CREATE VALIDATION SET,"X_train2 , X_val2 , Y_train2 , Y_val2 = train_test_split(X_train , Y_train , test_size = 0.333) ",how-to-choose-cnn-architecture-mnist.ipynb
TRAIN NETWORKS,history =[0]* nets ,how-to-choose-cnn-architecture-mnist.ipynb
PLOT ACCURACIES,"plt.figure(figsize =(15 , 5)) ",how-to-choose-cnn-architecture-mnist.ipynb
BUILD CONVOLUTIONAL NEURAL NETWORKS,nets = 8 ,how-to-choose-cnn-architecture-mnist.ipynb
CREATE VALIDATION SET,"X_train2 , X_val2 , Y_train2 , Y_val2 = train_test_split(X_train , Y_train , test_size = 0.333) ",how-to-choose-cnn-architecture-mnist.ipynb
TRAIN NETWORKS,history =[0]* nets ,how-to-choose-cnn-architecture-mnist.ipynb
PLOT ACCURACIES,"plt.figure(figsize =(15 , 5)) ",how-to-choose-cnn-architecture-mnist.ipynb
BUILD CONVOLUTIONAL NEURAL NETWORKS,nets = 5 ,how-to-choose-cnn-architecture-mnist.ipynb
CREATE VALIDATION SET,"X_train2 , X_val2 , Y_train2 , Y_val2 = train_test_split(X_train , Y_train , test_size = 0.2) ",how-to-choose-cnn-architecture-mnist.ipynb
"TRAIN NETWORKS 1,2,3,4",history =[0]* nets ,how-to-choose-cnn-architecture-mnist.ipynb
PLOT ACCURACIES,"plt.figure(figsize =(15 , 5)) ",how-to-choose-cnn-architecture-mnist.ipynb
TRAIN OUR BEST NET MORE,annealer = LearningRateScheduler(lambda x : 1e-3 * 0.95 **(x + epochs)) ,how-to-choose-cnn-architecture-mnist.ipynb
I tried and played around in the competition. I submitted it and got 0.00075.,"import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

import tensorflow as tf
print('Tensorflow version ' + tf.__version__)

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
 for filename in filenames:
 print(os.path.join(dirname, filename))",how-to-get-the-lowest-score.ipynb
"since we are splitting the dataset and iterating separately on images and ids, order matters.",test_ds = get_test_dataset(ordered = True) ,how-to-get-the-lowest-score.ipynb
all in one batch,test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))). numpy (). astype('U') ,how-to-get-the-lowest-score.ipynb
ConnectX environment was defined in v0.1.4,! pip install 'kaggle-environments>=0.1.4' ,how-to-play-with-computer-and-check-winner.ipynb
Create ConnectX Environment,"from kaggle_environments import evaluate, make

env = make(""connectx"", debug=True)
env.render()",how-to-play-with-computer-and-check-winner.ipynb
with this comand you can check values of enviroment,env.configuration ,how-to-play-with-computer-and-check-winner.ipynb
Create an User to Play with PC,def get_input(user): ,how-to-play-with-computer-and-check-winner.ipynb
This agent random chooses a non empty column.,"def my_agent(observation , configuration): ",how-to-play-with-computer-and-check-winner.ipynb
Change this line to Start Play,play = False ,how-to-play-with-computer-and-check-winner.ipynb
Play as first position against random agent.,"trainer = env.train ([None , ""random""]) ",how-to-play-with-computer-and-check-winner.ipynb
"print observation, reward, done, info ", if(check_winner(observation)== 1): ,how-to-play-with-computer-and-check-winner.ipynb
Run Ramdom, else : ,how-to-play-with-computer-and-check-winner.ipynb
This is the final played Game,"
env.render(mode=""ipython"", width=500, height=450)",how-to-play-with-computer-and-check-winner.ipynb
"IntroductionLinear regression excels at extrapolating trends, but can t learn interactions. XGBoost excels at learning interactions, but can t extrapolate trends. In this lesson, we ll learn how to create hybrid forecasters that combine complementary learning algorithms and let the strengths of one make up for the weakness of the other. Components and ResidualsSo that we can design effective hybrids, we need a better understanding of how time series are constructed. We ve studied up to now three patterns of dependence: trend, seasons, and cycles. Many time series can be closely described by an additive model of just these three components plus some essentially unpredictable, entirely random error:series trend seasons cycles errorEach of the terms in this model we would then call a component of the time series.The residuals of a model are the difference between the target the model was trained on and the predictions the model makes the difference between the actual curve and the fitted curve, in other words. Plot the residuals against a feature, and you get the left over part of the target, or what the model failed to learn about the target from that feature. The difference between the target series and the predictions blue gives the series of residuals. On the left of the figure above is a portion of the Tunnel Traffic series and the trend seasonal curve from Lesson 3. Subtracting out the fitted curve leaves the residuals, on the right. The residuals contain everything from Tunnel Traffic the trend seasonal model didn t learn.We could imagine learning the components of a time series as an iterative process: first learn the trend and subtract it out from the series, then learn the seasonality from the detrended residuals and subtract the seasons out, then learn the cycles and subtract the cycles out, and finally only the unpredictable error remains. Learning the components of Mauna Loa CO2 step by step. Subtract the fitted curve blue from its series to get the series in the next step. Add together all the components we learned and we get the complete model. This is essentially what linear regression would do if you trained it on a complete set of features modeling trend, seasons, and cycles. Add the learned components to get a complete model. Hybrid Forecasting with ResidualsIn previous lessons, we used a single algorithm linear regression to learn all the components at once. But it s also possible to use one algorithm for some of the components and another algorithm for the rest. This way we can always choose the best algorithm for each component. To do this, we use one algorithm to fit the original series and then the second algorithm to fit the residual series.In detail, the process is this: 1. Train and predict with first model model 1.fit X train 1, y train y pred 1 model 1.predict X train 2. Train and predict with second model on residuals model 2.fit X train 2, y train y pred 1 y pred 2 model 2.predict X train 2 3. Add to get overall predictions y pred y pred 1 y pred 2 We ll usually want to use different feature sets X train 1 and X train 2 above depending on what we want each model to learn. If we use the first model to learn the trend, we generally wouldn t need a trend feature for the second model, for example.While it s possible to use more than two models, in practice it doesn t seem to be especially helpful. In fact, the most common strategy for constructing hybrids is the one we ve just described: a simple usually linear learning algorithm followed by a complex, non linear learner like GBDTs or a deep neural net, the simple model typically designed as a helper for the powerful algorithm that follows.Designing HybridsThere are many ways you could combine machine learning models besides the way we ve outlined in this lesson. Successfully combining models, though, requires that we dig a bit deeper into how these algorithms operate.There are generally two ways a regression algorithm can make predictions: either by transforming the features or by transforming the target. Feature transforming algorithms learn some mathematical function that takes features as an input and then combines and transforms them to produce an output that matches the target values in the training set. Linear regression and neural nets are of this kind.Target transforming algorithms use the features to group the target values in the training set and make predictions by averaging values in a group a set of feature just indicates which group to average. Decision trees and nearest neighbors are of this kind.The important thing is this: feature transformers generally can extrapolate target values beyond the training set given appropriate features as inputs, but the predictions of target transformers will always be bound within the range of the training set. If the time dummy continues counting time steps, linear regression continues drawing the trend line. Given the same time dummy, a decision tree will predict the trend indicated by the last step of the training data into the future forever. Decision trees cannot extrapolate trends. Random forests and gradient boosted decision trees like XGBoost are ensembles of decision trees, so they also cannot extrapolate trends. A decision tree will fail to extrapolate a trend beyond the training set. This difference is what motivates the hybrid design in this lesson: use linear regression to extrapolate the trend, transform the target to remove the trend, and apply XGBoost to the detrended residuals. To hybridize a neural net a feature transformer , you could instead include the predictions of another model as a feature, which the neural net would then include as part of its own predictions. The method of fitting to residuals is actually the same method the gradient boosting algorithm uses, so we will call these boosted hybrids the method of using predictions as features is known as stacking , so we will call these stacked hybrids. Winning Hybrids from Kaggle Competitions For inspiration, here are a few top scoring solutions from past competitions: STL boosted with exponential smoothing Walmart Recruiting Store Sales Forecasting ARIMA and exponential smoothing boosted with GBDT Rossmann Store Sales An ensemble of stacked and boosted hybrids Web Traffic Time Series Forecasting Exponential smoothing stacked with LSTM neural net M4 non Kaggle Example US Retail SalesThe US Retail Sales dataset contains monthly sales data for various retail industries from 1992 to 2019, as collected by the US Census Bureau. Our goal will be to forecast sales in the years 2016 2019 given sales in the earlier years. In addition to creating a linear regression XGBoost hybrid, we ll also see how to set up a time series dataset for use with XGBoost.",from pathlib import Path ,hybrid-models.ipynb
Set Matplotlib defaults,"plt.style.use(""seaborn-whitegrid"") ",hybrid-models.ipynb
"First let s use a linear regression model to learn the trend in each series. For demonstration, we ll use a quadratic order 2 trend. The code here is basically the same as that in previous lessons. Though the fit isn t perfect, it will be enough for our needs.",y = retail.copy () ,hybrid-models.ipynb
pivot dataset wide to long,X = retail.stack () ,hybrid-models.ipynb
grab target series,y = X.pop('Sales') ,hybrid-models.ipynb
Turn row labels into categorical feature columns with a label encoding,X = X.reset_index('Industries') ,hybrid-models.ipynb
Label encoding for Industries feature,"for colname in X.select_dtypes ([""object"" , ""category""]) : ",hybrid-models.ipynb
"values are 1, 2, ..., 12","X[""Month""]= X.index.month ",hybrid-models.ipynb
Create splits,"X_train , X_test = X.loc[idx_train , :], X.loc[idx_test , :] ",hybrid-models.ipynb
trend from training set,y_fit = y_fit.stack (). squeeze () ,hybrid-models.ipynb
trend from test set,y_pred = y_pred.stack (). squeeze () ,hybrid-models.ipynb
Create residuals the collection of detrended series from the training set,y_resid = y_train - y_fit ,hybrid-models.ipynb
Train XGBoost on the residuals,xgb = XGBRegressor () ,hybrid-models.ipynb
Add the predicted residuals onto the predicted trends,y_fit_boosted = xgb.predict(X_train)+ y_fit ,hybrid-models.ipynb
"The fit appears quite good, though we can see how the trend learned by XGBoost is only as good as the trend learned by the linear regression in particular, XGBoost wasn t able to compensate for the poorly fit trend in the BuildingMaterials series.","
axs = y_train.unstack(['Industries']).plot(
 color='0.25', figsize=(11, 5), subplots=True, sharex=True,
 title=['BuildingMaterials', 'FoodAndBeverage'],
)
axs = y_test.unstack(['Industries']).plot(
 color='0.25', subplots=True, sharex=True, ax=axs,
)
axs = y_fit_boosted.unstack(['Industries']).plot(
 color='C0', subplots=True, sharex=True, ax=axs,
)
axs = y_pred_boosted.unstack(['Industries']).plot(
 color='C3', subplots=True, sharex=True, ax=axs,
)
for ax in axs: ax.legend([])",hybrid-models.ipynb
Import Library,"import numpy as np
import pandas as pd
from matplotlib import pyplot as plt, rcParams, style
style.use('seaborn-darkgrid')
import seaborn as sns
sns.set_style('darkgrid')
from plotly import express as px, graph_objects as go

from statsmodels.tsa.deterministic import DeterministicProcess, CalendarFourier
from statsmodels.graphics.tsaplots import plot_pacf
from sklearn.preprocessing import RobustScaler, StandardScaler, Normalizer, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor

import gc
gc.enable()
from warnings import filterwarnings, simplefilter
filterwarnings('ignore')
simplefilter('ignore')",hyperparamaters.ipynb
Konfigurasi jendela figure,"rcParams['figure.figsize']=(12 , 9) ",hyperparamaters.ipynb
Fetching dataset,"train = pd.read_csv('../input/store-sales-time-series-forecasting/train.csv',
 parse_dates = ['date'], infer_datetime_format = True,
 dtype = {'store_nbr' : 'category',
 'family' : 'category'},
 usecols = ['date', 'store_nbr', 'family', 'sales'])
train['date'] = train.date.dt.to_period('D')
train = train.set_index(['date', 'store_nbr', 'family']).sort_index()
train",hyperparamaters.ipynb
Calendar Engineering,"calendar = pd.DataFrame(index = pd.date_range('2013-01-01', '2017-08-31')).to_period('D')
oil = pd.read_csv('../input/store-sales-time-series-forecasting/oil.csv',
 parse_dates = ['date'], infer_datetime_format = True,
 index_col = 'date').to_period('D')
oil['avg_oil'] = oil['dcoilwtico'].rolling(7).mean()
calendar = calendar.join(oil.avg_oil)
calendar['avg_oil'].fillna(method = 'ffill', inplace = True)
calendar.dropna(inplace = True)",hyperparamaters.ipynb
Plotting oil price,_ = sns.lineplot(data = oil.dcoilwtico.to_timestamp ()) ,hyperparamaters.ipynb
Lagplot oil price Feature Engineering ,"_ = plot_pacf(calendar.avg_oil , lags = 12) ",hyperparamaters.ipynb
Adding lags,"n_lags = 3
for l in range(1, n_lags + 1) :
 calendar[f'oil_lags{l}'] = calendar.avg_oil.shift(l)
calendar.dropna(inplace = True)
calendar",hyperparamaters.ipynb
Correlation plot,"lag = 'oil_lags1'
plt.figure()
sns.regplot(x = calendar[lag], y = calendar.avg_oil)
plt.title(f'corr {calendar.avg_oil.corr(calendar[lag])}')
plt.show()",hyperparamaters.ipynb
Joining calendar with holiday dataset,calendar = calendar.join(hol) ,hyperparamaters.ipynb
Weekly day,calendar['dofw']= calendar.index.dayofweek ,hyperparamaters.ipynb
If it s saturday or sunday then it s not Weekday,"calendar.loc[calendar.dofw > 4 , 'wd']= 0 ",hyperparamaters.ipynb
If it s Work Day event then it s a workday,"calendar.loc[calendar.type == 'Work Day' , 'wd']= 1 ",hyperparamaters.ipynb
If it s Transfer event then it s not a work day,"calendar.loc[calendar.type == 'Transfer' , 'wd']= 0 ",hyperparamaters.ipynb
If it s Bridge event then it s not a work day,"calendar.loc[calendar.type == 'Bridge' , 'wd']= 0 ",hyperparamaters.ipynb
If it s holiday and the holiday is not transferred then it s holiday,"calendar.loc[( calendar.type == 'Holiday')&(calendar.transferred == False), 'wd']= 0 ",hyperparamaters.ipynb
If it s holiday and transferred then it s not holiday,"calendar.loc[( calendar.type == 'Holiday')&(calendar.transferred == True), 'wd']= 1 ",hyperparamaters.ipynb
One hot encoding Make sure to drop one of the columns by drop first True ,"calendar = pd.get_dummies(calendar , columns =['dofw'], drop_first = True) ",hyperparamaters.ipynb
One hot encoding for type holiday No need to drop one of the columns because there s a No holiday already ,"calendar = pd.get_dummies(calendar , columns =['type']) ",hyperparamaters.ipynb
Unused columns,"calendar.drop (['locale' , 'locale_name' , 'description' , 'transferred'], axis = 1 , inplace = True) ",hyperparamaters.ipynb
Dependent Variable Viz,"y = train.unstack(['store_nbr', 'family']).loc['2016-06':'2017']
family = {c[2] for c in train.index}
for f in family :
 ax = y.loc(axis = 1)['sales', :, f].plot(legend = None)
 ax.set_title(f)",hyperparamaters.ipynb
Start and end of training date,sdate = '2017-04-30' ,hyperparamaters.ipynb
Feature for school fluctuations,school_season = [] ,hyperparamaters.ipynb
DeterministicProcess,"y = train.unstack(['store_nbr', 'family']).loc[sdate:edate]
fourier = CalendarFourier(freq = 'W', order = 4)
dp = DeterministicProcess(index = y.index,
 order = 1,
 seasonal = False,
 constant = False,
 additional_terms = [fourier],
 drop = True)
x = dp.in_sample()
x = x.join(calendar)
x",hyperparamaters.ipynb
16 because we are predicting next 16 days,xtest = dp.out_of_sample(steps = 16) ,hyperparamaters.ipynb
Fungsi untuk membuat fitur lags,"def make_lags(x , lags = 1): ",hyperparamaters.ipynb
Using LinearRegression to make a generalized line It s usually called blending. ,"from joblib import Parallel, delayed
from tqdm.auto import tqdm
from sklearn.metrics import mean_squared_log_error as msle
from sklearn.model_selection import TimeSeriesSplit
from sklearn.svm import SVR
from sklearn.multioutput import MultiOutputRegressor

lnr = LinearRegression(fit_intercept = True, n_jobs = -1, normalize = True)
lnr.fit(x, y)

yfit_lnr = pd.DataFrame(lnr.predict(x), index = x.index, columns = y.columns).clip(0.)
ypred_lnr = pd.DataFrame(lnr.predict(xtest), index = xtest.index, columns = y.columns).clip(0.)

svr = MultiOutputRegressor(SVR(C = 0.2, kernel = 'rbf'), n_jobs = -1)
svr.fit(x, y)

yfit_svr = pd.DataFrame(svr.predict(x), index = x.index, columns = y.columns).clip(0.)
ypred_svr = pd.DataFrame(svr.predict(xtest), index = xtest.index, columns = y.columns).clip(0.)

yfit_mean = pd.DataFrame(np.mean([yfit_svr.values, yfit_lnr.values], axis = 0), index = x.index, columns = y.columns).clip(0.)
ypred_mean = pd.DataFrame(np.mean([ypred_lnr.values, ypred_svr.values], axis = 0), index = xtest.index, columns = y.columns).clip(0.)

y_ = y.stack(['store_nbr', 'family'])
y_['lnr'] = yfit_lnr.stack(['store_nbr', 'family'])['sales']
y_['svr'] = yfit_svr.stack(['store_nbr', 'family'])['sales']
y_['mean'] = yfit_mean.stack(['store_nbr', 'family'])['sales']

print('='*70, 'Linear Regression', '='*70)
print(y_.groupby('family').apply(lambda r : np.sqrt(msle(r['sales'], r['lnr']))))
print('LNR RMSLE :', np.sqrt(msle(y, yfit_lnr)))
print('='*70, 'SVR', '='*70)
print(y_.groupby('family').apply(lambda r : np.sqrt(msle(r['sales'], r['svr']))))
print('SVR RMSLE :', np.sqrt(msle(y, yfit_svr)))
print('='*70, 'Mean', '='*70)
print(y_.groupby('family').apply(lambda r : np.sqrt(msle(r['sales'], r['mean']))))
print('Mean RMSLE :', np.sqrt(msle(y, yfit_mean)))",hyperparamaters.ipynb
"Because in RMSLE we are applying log, that means higher the value, the lower the deviation.Let me show you","true_low = [2]
pred_low = [4]

print('RMSLE for low value :', np.sqrt(msle(true_low, pred_low)))
print('MAE for low value :', mae(true_low, pred_low))

true_high = [255]
pred_high = [269]

print('RMSLE for high value :', np.sqrt(msle(true_high, pred_high)))
print('MAE for high value :', mae(true_high, pred_high))",hyperparamaters.ipynb
I m not gonna use validation data because the data we have is not much and because we are using linear based algorithm so only using training would be fine.,"display(x, xtest)",hyperparamaters.ipynb
yfit mean.mean axis 1 .plot label Mean ,plt.legend () ,hyperparamaters.ipynb
"You can concat linear regression s prediction with the training data, this is called blending.",ymean = yfit_lnr.append(ypred_lnr) ,hyperparamaters.ipynb
I m also adding school lag for it s cyclic yearly.,"ymean = ymean.join(school.shift(1), rsuffix = 'lag1') ",hyperparamaters.ipynb
Concating linear result,x = x.join(ymean) ,hyperparamaters.ipynb
Model Creation,"from joblib import Parallel , delayed ",hyperparamaters.ipynb
Import necessary library,"from sklearn.linear_model import Ridge , LinearRegression , ElasticNet ",hyperparamaters.ipynb
SEED for reproducible result,SEED = 5 ,hyperparamaters.ipynb
"Because SCHOOL AND OFFICE SUPPLIES has weird trend, we use decision tree instead.", if y.name[2]== 'SCHOOL AND OFFICE SUPPLIES' : ,hyperparamaters.ipynb
Evaluation,from sklearn.metrics import mean_squared_log_error ,hyperparamaters.ipynb
Looking at error,"print('RMSLE : ' , np.sqrt(np.sqrt(msle(y_['sales'], y_['pred'])))) ",hyperparamaters.ipynb
All seems good.,y_pred.isna().sum(),hyperparamaters.ipynb
Submission,sub = pd.read_csv('../input/store-sales-time-series-forecasting/sample_submission.csv') ,hyperparamaters.ipynb
Submit,"sub.to_csv('submission.csv' , index = False) ",hyperparamaters.ipynb
"Introduction Image ProcessingAn image is a composed of pixels. Every Pixel is assigned a value between 0 and 255 in a rows and columns. So it is easy to represent in matrix format as below.Color Image A color image is a matrix that specifies the color of various pixels in terms of the amount of red, green and blue components.A set of one dot of each color form a pixel. Every pixel is assigned an RGB value, each components being a value between 0 and 255.Example: Red is rgb 255,0,0 , yellow is rgb 241,252,23 and white is rgb 255,255,255 .Now will see an example of reading an image and storing in a ndarray. OpenCV Python When reading a color image file, OpenCV imread reads as a NumPy array ndarray of row height x column width x color 3 . The order of color is BGR blue, green, red . On the other hand, the order of colors is assumed to be RGB red, green, blue . You can use the OpenCV function cvtColor or simply change the order of ndarray. Below image is 768 1024 pixels with blue, green and red components.",import matplotlib.pyplot as plt ,image-processing-face-keypoint-detection.ipynb
default value is BGR,plt.imshow(img) ,image-processing-face-keypoint-detection.ipynb
Now we will convert BGR to RGB component to see the actual image.,"fig = plt.figure(figsize =(20 , 20)) ",image-processing-face-keypoint-detection.ipynb
Converting BGR to RGB,"plt.imshow(img[: , : ,[2 , 1 , 0]]) ",image-processing-face-keypoint-detection.ipynb
plt.imshow im ,"fig.add_subplot(1 , 4 , 2) ",image-processing-face-keypoint-detection.ipynb
Select the pixels of the face,"plt.imshow(img[40 : 275 , 500 : 750 ,[2 , 1 , 0]]) ",image-processing-face-keypoint-detection.ipynb
Select the pixels of the eyes,"plt.imshow(img[120 : 160 , 550 : 600 ,[2 , 1 , 0]]) ",image-processing-face-keypoint-detection.ipynb
Select the pixels of the nose,"plt.imshow(img[140 : 190 , 600 : 650 ,[2 , 1 , 0]]) ",image-processing-face-keypoint-detection.ipynb
"Feature Maps Feature maps are the results we get after applying the filters.The shape of the feature map is influenced by: 1 Filter Kernals 2 Padding 3 Striding Filter Kernals: The filters are the neurons of the convolutional layers. They are used for feature detection. They are represented in the form of square matrix. Initial weights of the filters are assigned randomly, and during the training phase these weights gets updated based on backward propagations. Examples of image after we apply a filter: Striding The amount of movement between applications of the filter to the input image is referred to as the stride, and it is almost always symmetrical in height and width dimensions. The default stride or strides in two dimensions is 1,1 for the height and the width movement, performed when needed. And this default works well in most cases. The stride can be changed, which has an effect both on how the filter is applied to the image and, in turn, the size of the resulting feature map. Padding:The pixels on the edge of the input are only ever exposed to the edge of the filter. By starting the filter outside the frame of the image, it gives the pixels on the border of the image more of an opportunity for interacting with the filter, more of an opportunity for features to be detected by the filter, and in turn, an output feature map that has the same shape as the input image. This process of creating extra layers borders in image is known as padding. Padding are extremely usefull when the input dimension are small and when we don t want to have any information leakage.",import numpy as np ,image-processing-face-keypoint-detection.ipynb
showing max pooling," print(""shape before pooling"" , image.shape) ",image-processing-face-keypoint-detection.ipynb
"return cv2.copyMakeBorder image,top,bottom,left,right,cv2.BORDER CONSTANT,value values "," x , y = image.shape ",image-processing-face-keypoint-detection.ipynb
print image.shape ," arr = np.full(( x + top + bottom , y + left + right), values , dtype = float) ",image-processing-face-keypoint-detection.ipynb
"print y,y bottom "," arr[top : x + top , left : y + left]= image ",image-processing-face-keypoint-detection.ipynb
print arr top , return arr ,image-processing-face-keypoint-detection.ipynb
"including padding,striding and convolution"," print(""shape before padding/striding"" , image.shape) ",image-processing-face-keypoint-detection.ipynb
" how many rows, columns to be padded, and of what type "," image = padding(image , * pad_val) ",image-processing-face-keypoint-detection.ipynb
Below is the image pixel value represent in 96 96 matrix.,"import numpy as np
import matplotlib.pyplot as plt
import sys
np.set_printoptions(threshold=sys.maxsize)
img_txt_input = training['Image'][0]
print(""No of pixel values is :"",len(img_txt_input.split(' ')),""Converting the pixel values into rows and column:"",np.sqrt(len(img_txt_input.split(' '))),""*"",np.sqrt(len(img_txt_input.split(' '))),""\n"")
fn_reshape = lambda a: np.fromstring(a, dtype=int, sep=' ').reshape(96,96)
img = fn_reshape(img_txt_input)
print(""Below is the pixel value conveted into an image"")
plt.imshow(img,cmap='gray')
plt.show()",image-processing-face-keypoint-detection.ipynb
Applying Filters on image,samp_imag = img.copy () ,image-processing-face-keypoint-detection.ipynb
Laplacian filter,"lap_filter = np.array ([[ 0 , 1 , 0],[1 , - 4 , 1],[0 , 1 , 0]]) ",image-processing-face-keypoint-detection.ipynb
print shape of actual image: .format samp imag.shape ,"padded_image_5 = padding(samp_imag , *(5 , 5 , 5 , 5 , 1)) ",image-processing-face-keypoint-detection.ipynb
 print shape of padded image: .format padded image.shape ,"plt.figure(figsize =(10 , 10)) ",image-processing-face-keypoint-detection.ipynb
Applying Striding on image,"print(""Padding used is 1 for all the borders\nVertical filter is used"") ",image-processing-face-keypoint-detection.ipynb
Applying MaxPooling on image,"print(""Pooling example"") ",image-processing-face-keypoint-detection.ipynb
"Facial Keypoints Detection competition The goal of the competition is to locate specific keypoints on face images. Build a model that, given an image of a face, automatically locates where these keypoints are located.Given training.csv It contains x,y coordinates of 30 facial keypoints both left and right and pixel values of Images. test.csv It contains pixel values of images IdLookupTable.csv It contains required Feature Names along with ImageId for submission. Important points: In total, we have 7049 rows, each one with 31 columns. The first 30 columns are keypoint locations, which python correctly identified as numbers. The last one is a string representation of the image, identified as a string.","train_columns = training.columns[:-1].values
training.head().T",image-processing-face-keypoint-detection.ipynb
Exploring Data,"training[training.columns[:-1]].describe(percentiles = [0.05,0.1,.25, .5, .75,0.9,0.95]).T",image-processing-face-keypoint-detection.ipynb
Missing Data and outliers,"whisker_width = 1.5
total_rows = training.shape[0]
missing_col = 0
for col in training[training.columns[:-1]]:
 count = training[col].count()
 q1 = training[col].quantile(0.25)
 q3 = training[col].quantile(0.75)
 iqr = q3 - q1
 outliers = training[(training[col] < q1 - whisker_width*iqr)
 | (training[col] > q3 + whisker_width*iqr)][col].count()
 print (f""dv:{col}, dv_rows:{count}, missing_pct:{round(100.*(1-count/total_rows),2)}%, outliers:{outliers}, outlier_pct:{round(100.*outliers/count,2)}%"")
 if (100.*(1-count/total_rows)>65):
 missing_col+=1

print(f""DVs containing more than 65% of data missing : {missing_col} out of {len(training.columns[:-1])}"")",image-processing-face-keypoint-detection.ipynb
"From the above analysis we can see that, around 2 5 of the data are prone to outliers. And for detailed DV such as mouth right corner,left eyebrow outer end y etc, 65 of the data are missing.","def plot_loss(hist,name,plt,RMSE_TF=False):
 '''
 RMSE_TF: if True, then RMSE is plotted with original scale 
 '''
 loss = hist['loss']
 val_loss = hist['val_loss']
 if RMSE_TF:
 loss = np.sqrt(np.array(loss))*48 
 val_loss = np.sqrt(np.array(val_loss))*48 
 
 plt.plot(loss,""--"",linewidth=3,label=""train:""+name)
 plt.plot(val_loss,linewidth=3,label=""val:""+name)

def plot_sample_val(X,y,axs,pred):
 '''
 kaggle picture is 96 by 96
 y is rescaled to range between -1 and 1
 '''
 
 axs.imshow(X.reshape(96,96),cmap=""gray"")
 axs.scatter(48*y[0::2]+ 48,48*y[1::2]+ 48, label='Actual')
 axs.scatter(48*pred[0::2]+ 48,48*pred[1::2]+ 48, label='Prediction')

def plot_sample(X,y,axs):
 '''
 kaggle picture is 96 by 96
 y is rescaled to range between -1 and 1
 '''
 
 axs.imshow(X.reshape(96,96),cmap=""gray"")
 axs.scatter(48*y[0::2]+ 48,48*y[1::2]+ 48)",image-processing-face-keypoint-detection.ipynb
Manually splitting the training and validation data,def data_loader(data_frame): ,image-processing-face-keypoint-detection.ipynb
Load dataset file," data_frame['Image']= data_frame['Image']. apply(lambda i : np.fromstring(i , sep = ' ')) ",image-processing-face-keypoint-detection.ipynb
Get only the data with 15 keypoints, data_frame = data_frame.dropna () ,image-processing-face-keypoint-detection.ipynb
Extract Images pixel values, imgs_array = np.vstack(data_frame['Image']. values)/ 255.0 ,image-processing-face-keypoint-detection.ipynb
"Normalize, target values to 0, 1 ", imgs_array = imgs_array.astype(np.float32) ,image-processing-face-keypoint-detection.ipynb
Extract labels key point cords , labels_array = data_frame[data_frame.columns[: - 1]].values ,image-processing-face-keypoint-detection.ipynb
"Normalize, traget cordinates to 1, 1 ", labels_array =(labels_array - 48)/ 48 ,image-processing-face-keypoint-detection.ipynb
"imgs array, labels array shuffle imgs array, labels array, random state 9 "," return imgs_array , labels_array ",image-processing-face-keypoint-detection.ipynb
Load dataset file," data_frame['Image']= data_frame['Image']. apply(lambda i : np.fromstring(i , sep = ' ')) ",image-processing-face-keypoint-detection.ipynb
Extract Images pixel values, imgs_array = np.vstack(data_frame['Image']. values)/ 255.0 ,image-processing-face-keypoint-detection.ipynb
"Normalize, target values to 0, 1 ", imgs_array = imgs_array.astype(np.float32) ,image-processing-face-keypoint-detection.ipynb
"Data augmentationImage data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.Training deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.The Keras deep learning neural network library provides the capability to fit models using image data augmentation via the ImageDataGenerator class.Horizontal and Vertical Flip AugmentationAn image flip means reversing the rows or columns of pixels in the case of a vertical or horizontal flip respectively.Horizontal and Vertical Shift AugmentationA shift to an image means moving all pixels of the image in one direction, such as horizontally or vertically, while keeping the image dimensions the same.Flipping pictures can double the number of pictures twice. If we allow the pictures to shift by some pixcels within frames, this can increase the number of pictures substantially!",class DataModifier(object): ,image-processing-face-keypoint-detection.ipynb
Flipping pictures,"from keras.preprocessing.image import ImageDataGenerator

generator = ImageDataGenerator()
modifier = FlipPic_8()
fig = plt.figure(figsize=(20,20))
count = 1
for batch in generator.flow(X[:4],Y[:4]):
 X_batch, y_batch = modifier.fit(*batch)
 ax = fig.add_subplot(5,4, count,xticks=[],yticks=[]) 
 plot_sample(X_batch[0],y_batch[0],ax)
 count += 1
 if count == 10:
 break
plt.show()",image-processing-face-keypoint-detection.ipynb
Shifting pictures,"from keras.preprocessing.image import ImageDataGenerator
generator = ImageDataGenerator()
shiftFlipPic = ShiftFlipPic(prop=0.1)

fig = plt.figure(figsize=(20,20))

count = 1
for batch in generator.flow(X[:4],Y[:4]):
 X_batch, y_batch = shiftFlipPic.fit(*batch)

 ax = fig.add_subplot(5,4, count,xticks=[],yticks=[]) 
 plot_sample(X_batch[0],y_batch[0],ax)
 count += 1
 if count == 10:
 break
plt.show()",image-processing-face-keypoint-detection.ipynb
Rotation augmentation for a list of angle values, for angle in rotation_angles : ,image-processing-face-keypoint-detection.ipynb
"print f angle , end "," M = cv2.getRotationMatrix2D(( 48 , 48), angle , 1.0) ",image-processing-face-keypoint-detection.ipynb
Obtain angle in radians from angle in degrees notice negative sign for change in clockwise vs anti clockwise directions from conventional rotation to cv2 s image rotation , angle_rad = - angle * pi / 180. ,image-processing-face-keypoint-detection.ipynb
For train images, for image in images : ,image-processing-face-keypoint-detection.ipynb
For train keypoints, for keypoint in keypoints : ,image-processing-face-keypoint-detection.ipynb
Subtract the middle value of the image dimension, rotated_keypoint =(keypoint + 1)- 1 ,image-processing-face-keypoint-detection.ipynb
Add the earlier subtracted value, rotated_keypoint =(rotated_keypoint - 1)+ 1 ,image-processing-face-keypoint-detection.ipynb
Rotation angle in degrees includes both clockwise anti clockwise rotations ,"rotation_angles =[6 , 12] ",image-processing-face-keypoint-detection.ipynb
"Increased brightness by a factor of 1.2 clip any values outside the range of 1,1 "," inc_brightness_images = np.clip(images * 2 , 0.0 , 1.0) ",image-processing-face-keypoint-detection.ipynb
"Decreased brightness by a factor of 0.6 clip any values outside the range of 1,1 "," dec_brightness_images = np.clip(images * 0.1 , 0.0 , 1.0) ",image-processing-face-keypoint-detection.ipynb
"kernel np.ones 5,5 ,np.float32 25"," image = cv2.blur(image ,(blur_val , blur_val), cv2.BORDER_DEFAULT). reshape(96 , 96 , 1) ",image-processing-face-keypoint-detection.ipynb
"kernel np.ones 5,5 ,np.float32 25"," gauss = np.random.normal(0 , 1 , image.size) ",image-processing-face-keypoint-detection.ipynb
Add the Gaussian noise to the image," img_gauss = cv2.add(img , gauss) ",image-processing-face-keypoint-detection.ipynb
Image rotation,"from keras.preprocessing.image import ImageDataGenerator
generator = ImageDataGenerator()

fig = plt.figure(figsize=(20,20))

count = 1
for batch in generator.flow(X[:4],Y[:4]):
 X_batch, y_batch = add_blur(*batch,3)
 ax = fig.add_subplot(5,4, count,xticks=[],yticks=[]) 
 plot_sample(X_batch[0],y_batch[0],ax)
 count += 1
 if count == 10:
 break
plt.show()",image-processing-face-keypoint-detection.ipynb
Image Augmentation With ImageDataGenerator,modifier = FlipPic () ,image-processing-face-keypoint-detection.ipynb
"X train np.concatenate X train,X batch ", batches += 1 ,image-processing-face-keypoint-detection.ipynb
the generator loops indefinitely, break ,image-processing-face-keypoint-detection.ipynb
"Custom Metrics in KerasKaggle submissions are scored on the root mean squared error. RMSE is very common and is a suitable general purpose error metric. Compared to the Mean Absolute Error, RMSE punishes large errors:","from keras import backend
 
def rmse(y_true, y_pred):
 return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))",image-processing-face-keypoint-detection.ipynb
Define simple CNN function using Keras,def cnn(n_out): ,image-processing-face-keypoint-detection.ipynb
"Input dimensions: None, 96, 96, 32 "," model.add(TimeDistributed(Convolution2D(32 ,(3 , 3), padding = 'same' , use_bias = False , kernel_initializer = GlorotNormal))) ",image-processing-face-keypoint-detection.ipynb
"Input dimensions: None, 48, 48, 32 "," model.add(TimeDistributed(Convolution2D(64 ,(3 , 1), padding = 'same' , use_bias = False , kernel_initializer = GlorotNormal))) ",image-processing-face-keypoint-detection.ipynb
"Input dimensions: None, 48, 48, 64 "," model.add(TimeDistributed(Convolution2D(64 ,(1 , 3), padding = 'same' , use_bias = False , kernel_initializer = GlorotNormal))) ",image-processing-face-keypoint-detection.ipynb
"Input dimensions: None, 24, 24, 64 "," model.add(TimeDistributed(Convolution2D(96 ,(3 , 1), padding = 'same' , use_bias = False , kernel_initializer = GlorotNormal))) ",image-processing-face-keypoint-detection.ipynb
"Input dimensions: None, 24, 24, 96 "," model.add(TimeDistributed(Convolution2D(96 ,(1 , 3), padding = 'same' , use_bias = False , kernel_initializer = GlorotNormal))) ",image-processing-face-keypoint-detection.ipynb
"Input dimensions: None, 12, 12, 96 "," model.add(TimeDistributed(Convolution2D(128 ,(3 , 1), padding = 'same' , use_bias = False , kernel_initializer = GlorotNormal))) ",image-processing-face-keypoint-detection.ipynb
"Input dimensions: None, 12, 12, 128 "," model.add(TimeDistributed(Convolution2D(128 ,(1 , 3), padding = 'same' , use_bias = False , kernel_initializer = GlorotNormal))) ",image-processing-face-keypoint-detection.ipynb
"Input dimensions: None, 6, 6, 128 "," model.add(TimeDistributed(Convolution2D(256 ,(3 , 1), padding = 'same' , use_bias = False , kernel_initializer = GlorotNormal))) ",image-processing-face-keypoint-detection.ipynb
"Input dimensions: None, 6, 6, 256 "," model.add(TimeDistributed(Convolution2D(256 ,(1 , 3), padding = 'same' , use_bias = False , kernel_initializer = GlorotNormal))) ",image-processing-face-keypoint-detection.ipynb
"Input dimensions: None, 3, 3, 256 "," model.add(TimeDistributed(Convolution2D(512 ,(3 , 1), padding = 'same' , use_bias = False , kernel_initializer = GlorotNormal))) ",image-processing-face-keypoint-detection.ipynb
"Input dimensions: None, 3, 3, 512 "," model.add(TimeDistributed(Convolution2D(512 ,(1 , 3), padding = 'same' , use_bias = False , kernel_initializer = GlorotNormal))) ",image-processing-face-keypoint-detection.ipynb
"Input dimensions: None, 48, 48, 32 "," model.add(TimeDistributed(Convolution2D(64 ,(3 , 1), padding = 'same' , use_bias = False , kernel_initializer = GlorotNormal))) ",image-processing-face-keypoint-detection.ipynb
"Input dimensions: None, 48, 48, 64 "," model.add(TimeDistributed(Convolution2D(64 ,(1 , 3), padding = 'same' , use_bias = False , kernel_initializer = GlorotNormal))) ",image-processing-face-keypoint-detection.ipynb
"Input dimensions: None, 3, 3, 32 "," model.add(TimeDistributed(Convolution2D(32 ,(3 , 1), padding = 'same' , use_bias = False , kernel_initializer = GlorotNormal))) ",image-processing-face-keypoint-detection.ipynb
"Input dimensions: None, 3, 3, 32 "," model.add(TimeDistributed(Convolution2D(32 ,(1 , 3), padding = 'same' , use_bias = False , kernel_initializer = GlorotNormal))) ",image-processing-face-keypoint-detection.ipynb
Fit the model,model_30 = cnn(30) ,image-processing-face-keypoint-detection.ipynb
"model 30.compile optimizer SGD lr 0.01,momentum 0.9,decay 0.00006, nesterov True , loss rmse,metrics rmse, mse , mae ","model_30.compile(optimizer = Adam(lr = 0.001), loss = rmse , metrics =[rmse , 'mse' , 'mae']) ",image-processing-face-keypoint-detection.ipynb
"LR callback 30 ReduceLROnPlateau monitor val loss , verbose 10, factor .4, min lr .00001 ","EarlyStop_callback_30 = EarlyStopping(restore_best_weights = True , mode = 'min') ",image-processing-face-keypoint-detection.ipynb
"model 30.fit X train,y train,validation data X val, y val , epochs 120 ",from keras.preprocessing.sequence import TimeseriesGenerator ,image-processing-face-keypoint-detection.ipynb
fit model using fit generator instead of fit,"model_30.fit_generator(train_sequences , validation_data = test_sequences , epochs = 120) ",image-processing-face-keypoint-detection.ipynb
"model 30.compile optimizer Adam lr 0.001 ,loss rmse, metrics rmse, mse , mae ","hist = model_30.fit(X_train,y_train,validation_data=(X_val, y_val), epochs = 120, batch_size=20) ",image-processing-face-keypoint-detection.ipynb
"scores model 30.evaluate train sequences, verbose 0 ","print("" Train %s: %.2f%% %s: %.2f%% %s: %.2f%%"" %(model_30.metrics_names[1], scores[1]* 100 , model_30.metrics_names[2], scores[2]* 100 , model_30.metrics_names[3], scores[3]* 100)) ",image-processing-face-keypoint-detection.ipynb
"y hat 30 model 30.predict generator X test,350 ",1783/5,image-processing-face-keypoint-detection.ipynb
Plot val and train rmse on each epoch,"def plot_loss(hist,name,plt,RMSE_TF=False):
 '''
 RMSE_TF: if True, then RMSE is plotted with original scale 
 '''
 loss = hist['rmse']
 val_loss = hist['val_rmse']
 if RMSE_TF:
 loss = np.sqrt(np.array(loss))*48 
 val_loss = np.sqrt(np.array(val_loss))*48 
 
 plt.plot(loss,""--"",linewidth=3,label=""train:""+name)
 plt.plot(val_loss,linewidth=3,label=""val:""+name)

plot_loss(hist.history,""model 1"",plt)
plt.legend()
plt.grid()
plt.xlabel(""epoch"")
plt.ylabel(""RMSE"")
plt.show()",image-processing-face-keypoint-detection.ipynb
Plot actual and prediction keypoints on val sample," 
pred = model_30.predict(X_val)

fig = plt.figure(figsize=(7, 7))
fig.subplots_adjust(hspace=0.13,wspace=0.0001,
 left=0,right=1,bottom=0, top=1)
Npicture = 9
count = 1
for irow in range(Npicture):
 ipic = np.random.choice(X_val.shape[0])
 ax = fig.add_subplot(Npicture/3 , 3, count,xticks=[],yticks=[]) 
 plot_sample_val(X_val[ipic],y_val[ipic], ax,pred[ipic])
 ax.legend( ncol = 1)
 ax.set_title(""picture ""+ str(ipic))
 count += 1
plt.show()",image-processing-face-keypoint-detection.ipynb
Model with less feature,"import pandas as pd
training_8 = pd.read_csv('../input/facial-keypoints-detection/training.zip',usecols = main_features).dropna()
",image-processing-face-keypoint-detection.ipynb
the generator loops indefinitely, break ,image-processing-face-keypoint-detection.ipynb
instantiating the model in the strategy scope creates the model on the TPU,with tpu_strategy.scope (): ,image-processing-face-keypoint-detection.ipynb
"model 30.compile optimizer SGD lr 0.01,momentum 0.9,decay 0.00006, nesterov True , loss rmse,metrics rmse, mse , mae "," model_8.compile(optimizer = Adam(lr = 0.001), loss = rmse , metrics =[rmse , 'mse' , 'mae']) ",image-processing-face-keypoint-detection.ipynb
Create .csv files to submit to kaggle competition,y_hat_8 = model_8.predict(X_test),image-processing-face-keypoint-detection.ipynb
Merge 2 prediction from y hat 30 and y hat 8.,for i in range(8): ,image-processing-face-keypoint-detection.ipynb
linear algebra,import numpy as np ,imdb-review-data.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,imdb-review-data.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,imdb-review-data.ipynb
Loading the training data set ,"df = pd.read_csv('/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip', delimiter=""\t"")",imdb-review-data.ipynb
Loading the test data set ,"df1=pd.read_csv(""/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip"",delimiter= ""\t"")
df1.head()",imdb-review-data.ipynb
Visualizing the train and test data set ,"import matplotlib.pyplot as plt
import seaborn as sns
fig=plt.figure(figsize=(14,8))
fig.add_subplot(1,2,1)
sns.distplot((train_len),color='red')

fig.add_subplot(1,2,2)
sns.distplot((test_len),color='blue')",imdb-review-data.ipynb
Spitting the review in the words ,"df['word_n'] = df['review'].apply(lambda x : len(x.split(' ')))
df1[""word_n""]=df1[""review""].apply(lambda x : len(x.split("" "")))",imdb-review-data.ipynb
"Creating a word cloud to see, the words which appear mostly",from wordcloud import WordCloud ,imdb-review-data.ipynb
join function can help merge all words into one string. means space can be a seperator between words.,"plt.figure(figsize =(16 , 10)) ",imdb-review-data.ipynb
Remoiving unwanted HTML tags such as br which appears the maximum,"import re
import json
",imdb-review-data.ipynb
"Using regrex library, we can remove the html tags easily from the sentiments",TAG_RE = re.compile(r'<[^>]+>'),imdb-review-data.ipynb
join function can help merge all words into one string. means space can be a seperator between words.,"plt.figure(figsize =(16 , 10)) ",imdb-review-data.ipynb
Keeping only alphabets in the review segment ,"df['review']=df['review'].apply(lambda x: re.sub(""[^a-zA-Z]"","" "",x))
df1['review']=df1['review'].apply(lambda x: re.sub(""[^a-zA-Z]"","" "",x))",imdb-review-data.ipynb
from nltk.stem import WordNetLemmatizer,from nltk.corpus import stopwords ,imdb-review-data.ipynb
lemmatizer WordNetLemmatizer ,"df[""review""]=df['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))",imdb-review-data.ipynb
tokenizer to remove unwanted elements from out data like symbols and numbers,token = RegexpTokenizer(r'[a-zA-Z0-9]+') ,imdb-review-data.ipynb
Model Generation Using Multinomial Naive Bayes,"clf = MultinomialNB (). fit(X_train , y_train.values.ravel ()) ",imdb-review-data.ipynb
doctest: SKIP,from sklearn.preprocessing import StandardScaler ,imdb-review-data.ipynb
doctest: SKIP,>> > scaler = StandardScaler(with_mean = False) ,imdb-review-data.ipynb
Don t cheat fit only on training data,>> > ,imdb-review-data.ipynb
doctest: SKIP,>> > scaler.fit(X_train) ,imdb-review-data.ipynb
doctest: SKIP,>> > X_train = scaler.transform(X_train) ,imdb-review-data.ipynb
apply same transformation to test data,>> > ,imdb-review-data.ipynb
Import modules,import warnings ,imdb-review-word2vec-bilstm-99-acc.ipynb
Modules for data manipulation,import numpy as np ,imdb-review-word2vec-bilstm-99-acc.ipynb
Modules for visualization,import matplotlib.pyplot as plt ,imdb-review-word2vec-bilstm-99-acc.ipynb
Tools for preprocessing input data,from bs4 import BeautifulSoup ,imdb-review-word2vec-bilstm-99-acc.ipynb
Tools for creating ngrams and vectorizing input data,"from gensim.models import Word2Vec , Phrases ",imdb-review-word2vec-bilstm-99-acc.ipynb
Tools for building a model,from sklearn.model_selection import train_test_split ,imdb-review-word2vec-bilstm-99-acc.ipynb
Tools for assessing the quality of model prediction,"from sklearn.metrics import accuracy_score , confusion_matrix ",imdb-review-word2vec-bilstm-99-acc.ipynb
Set some matplotlib configs for visualization,"SMALL_SIZE = 12
MEDIUM_SIZE = 14
BIG_SIZE = 16
LARGE_SIZE = 20

params = {
 'figure.figsize': (16, 8),
 'font.size': SMALL_SIZE,
 'xtick.labelsize': MEDIUM_SIZE,
 'ytick.labelsize': MEDIUM_SIZE,
 'legend.fontsize': BIG_SIZE,
 'figure.titlesize': LARGE_SIZE,
 'axes.titlesize': MEDIUM_SIZE,
 'axes.labelsize': BIG_SIZE
}
plt.rcParams.update(params)",imdb-review-word2vec-bilstm-99-acc.ipynb
Import data Importing the existing datasets and also importing the IMDB dataset from another source. It helps us to increase maximal accuracy of our model from 87 to 90 .,"usecols = ['sentiment','review']
train_data = pd.read_csv(
 filepath_or_buffer='../input/word2vec-nlp-tutorial/labeledTrainData.tsv',
 usecols=usecols, sep='\t')
additional_data = pd.read_csv(
 filepath_or_buffer='../input/imdb-review-dataset/imdb_master_filtered.csv',
 sep='\t')[usecols]
unlabeled_data = pd.read_csv(
 filepath_or_buffer=""../input/word2vec-nlp-tutorial/unlabeledTrainData.tsv"", 
 error_bad_lines=False,
 sep='\t')
submission_data = pd.read_csv(
 filepath_or_buffer=""../input/word2vec-nlp-tutorial/testData.tsv"",
 sep='\t')",imdb-review-word2vec-bilstm-99-acc.ipynb
Check class balance,"plt.hist(train_data[train_data.sentiment == 1].sentiment,
 bins=2, color='green', label='Positive')
plt.hist(train_data[train_data.sentiment == 0].sentiment,
 bins=2, color='blue', label='Negative')
plt.title('Classes distribution in the train data', fontsize=LARGE_SIZE)
plt.xticks([])
plt.xlim(-0.5, 2)
plt.legend()
plt.show()",imdb-review-word2vec-bilstm-99-acc.ipynb
1. Remove HTML," review_text = BeautifulSoup(raw_review , ""lxml""). get_text () ",imdb-review-word2vec-bilstm-99-acc.ipynb
2. Remove non letters," letters_only = REPLACE_WITH_SPACE.sub("" "" , review_text) ",imdb-review-word2vec-bilstm-99-acc.ipynb
3. Convert to lower case, lowercase_letters = letters_only.lower () ,imdb-review-word2vec-bilstm-99-acc.ipynb
1. Lemmatize," tokens = list(map(lemmatizer.lemmatize , tokens)) ",imdb-review-word2vec-bilstm-99-acc.ipynb
2. Remove stop words," meaningful_words = list(filter(lambda x : not x in stop_words , lemmatized_tokens)) ",imdb-review-word2vec-bilstm-99-acc.ipynb
1. Clean text, review = clean_review(review) ,imdb-review-word2vec-bilstm-99-acc.ipynb
2. Split into individual words, tokens = word_tokenize(review) ,imdb-review-word2vec-bilstm-99-acc.ipynb
3. Lemmatize, lemmas = lemmatize(tokens) ,imdb-review-word2vec-bilstm-99-acc.ipynb
and return the result., return lemmas ,imdb-review-word2vec-bilstm-99-acc.ipynb
Now we can use gensim s phrases to find bigrams or trigrams,print(bigrams['space station near the solar system'.split()]),imdb-review-word2vec-bilstm-99-acc.ipynb
"And now we can use gensim s word2vec model to build a word embedding. Also we can use the word2vec model to define most similar words, calculate diffence between the words, etc.",trigrams_model.wv.most_similar('galaxy'),imdb-review-word2vec-bilstm-99-acc.ipynb
summarize history for accuracy,"axis1.plot(history.history['acc'], label = 'Train' , linewidth = 3) ",imdb-review-word2vec-bilstm-99-acc.ipynb
summarize history for loss,"axis2.plot(history.history['loss'], label = 'Train' , linewidth = 3) ",imdb-review-word2vec-bilstm-99-acc.ipynb
Make submission,"print('Convert sentences to sentences with ngrams...', end='\r')
X_submit = trigrams[bigrams[X_submission]]
print('Convert sentences to sentences with ngrams... (done)')
X_sub = pad_sequences(
 sequences=vectorize_data(X_submit, vocab=trigrams_model.wv.vocab),
 maxlen=input_length,
 padding='post')
print('Transform sentences to sequences... (done)')",imdb-review-word2vec-bilstm-99-acc.ipynb
Dependencies,"import os, random, json, PIL, shutil, re, imageio, glob
import numpy as np
import pandas as pd
import seaborn as sns
from PIL import ImageDraw
import matplotlib.pyplot as plt
from kaggle_datasets import KaggleDatasets
import tensorflow as tf
import tensorflow.keras.layers as L
import tensorflow.keras.backend as K
import tensorflow_addons as tfa
from tensorflow.keras import Model, losses, optimizers
from tensorflow.keras.callbacks import Callback


def seed_everything(seed=0):
 random.seed(seed)
 np.random.seed(seed)
 tf.random.set_seed(seed)
 os.environ['PYTHONHASHSEED'] = str(seed)
 os.environ['TF_DETERMINISTIC_OPS'] = '1'
 
SEED = 0
seed_everything(SEED)",improving-cyclegan-monet-paintings.ipynb
TPU configuration,"try:
 tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
 print(f'Running on TPU {tpu.master()}')
except ValueError:
 tpu = None

if tpu:
 tf.config.experimental_connect_to_cluster(tpu)
 tf.tpu.experimental.initialize_tpu_system(tpu)
 strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
 strategy = tf.distribute.get_strategy()


REPLICAS = strategy.num_replicas_in_sync
AUTO = tf.data.experimental.AUTOTUNE
print(f'REPLICAS: {REPLICAS}')",improving-cyclegan-monet-paintings.ipynb
Model parameters,"HEIGHT = 256
WIDTH = 256
HEIGHT_RESIZE = 128
WIDTH_RESIZE = 128
CHANNELS = 3
BATCH_SIZE = 16
EPOCHS = 120
TRANSFORMER_BLOCKS = 6
GENERATOR_LR = 2e-4
DISCRIMINATOR_LR = 2e-4",improving-cyclegan-monet-paintings.ipynb
Load data,GCS_PATH = KaggleDatasets (). get_gcs_path('gan-getting-started') ,improving-cyclegan-monet-paintings.ipynb
EXT PATH KaggleDatasets .get gcs path monet tfrecords 256x256 ,MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/monet*.tfrec')) ,improving-cyclegan-monet-paintings.ipynb
MONET FILENAMES tf.io.gfile.glob str EXT PATH monet .tfrec ,def count_data_items(filenames): ,improving-cyclegan-monet-paintings.ipynb
"AugmentationsData augmentation for GANs should be done very carefully, especially for tasks similar to style transfer, if we apply transformations that can change too much the style of the data e.g. brightness, contrast, saturation it can cause the generator to do not efficiently learn the base style, so in this case, we are using only spatial transformations like, flips, rotates and crops.",def data_augment(image): ,improving-cyclegan-monet-paintings.ipynb
90 rotations, if p_rotate > .8 : ,improving-cyclegan-monet-paintings.ipynb
rotate 270," image = tf.image.rot90(image , k = 3) ",improving-cyclegan-monet-paintings.ipynb
rotate 180," image = tf.image.rot90(image , k = 2) ",improving-cyclegan-monet-paintings.ipynb
rotate 90," image = tf.image.rot90(image , k = 1) ",improving-cyclegan-monet-paintings.ipynb
Flips, image = tf.image.random_flip_left_right(image) ,improving-cyclegan-monet-paintings.ipynb
Train on crops," image = tf.image.random_crop(image , size =[HEIGHT_RESIZE , WIDTH_RESIZE , CHANNELS]) ",improving-cyclegan-monet-paintings.ipynb
Auxiliar functions,def normalize_img(img): ,improving-cyclegan-monet-paintings.ipynb
"Map values in the range 1, 1 ", return(img / 127.5)- 1.0 ,improving-cyclegan-monet-paintings.ipynb
Auxiliar functions model Here we the building blocks of our models: Encoder block: Apply convolutional filters while also reducing data resolution and increasing features. Decoder block: Apply convolutional filters while also increasing data resolution and decreasing features. Transformer block: Apply convolutional filters to find relevant data patterns and keeps features constant.,"conv_initializer = tf.random_normal_initializer(mean = 0.0 , stddev = 0.02) ",improving-cyclegan-monet-paintings.ipynb
"Generator modelThe generator is responsible for generating images from a specific domain. CycleGAN architecture has two generators, in this context we will have one generator that will take photos and generate Monet paints, and the other generator will take Monet paintings and generate photos.Bellow, we have the architecture of the original CycleGAN generator, ours have some changes to improve performance on this task.","def generator_fn(height = HEIGHT , width = WIDTH , channels = CHANNELS , transformer_blocks = TRANSFORMER_BLOCKS): ",improving-cyclegan-monet-paintings.ipynb
" bs, 256, 256, 64 "," enc_1 = encoder_block(inputs , 64 , 7 , 1 , apply_instancenorm = False , activation = L.ReLU (), name = 'block_1') ",improving-cyclegan-monet-paintings.ipynb
" bs, 128, 128, 128 "," enc_2 = encoder_block(enc_1 , 128 , 3 , 2 , apply_instancenorm = True , activation = L.ReLU (), name = 'block_2') ",improving-cyclegan-monet-paintings.ipynb
" bs, 64, 64, 256 "," enc_3 = encoder_block(enc_2 , 256 , 3 , 2 , apply_instancenorm = True , activation = L.ReLU (), name = 'block_3') ",improving-cyclegan-monet-paintings.ipynb
Transformer, x = enc_3 ,improving-cyclegan-monet-paintings.ipynb
" bs, 64, 64, 256 "," x = transformer_block(x , 3 , 1 , name = f'block_{n+1}') ",improving-cyclegan-monet-paintings.ipynb
encoder decoder skip connection," x_skip = L.Concatenate(name = 'enc_dec_skip_1')([x , enc_3]) ",improving-cyclegan-monet-paintings.ipynb
" bs, 128, 128, 128 "," dec_1 = decoder_block(x_skip , 128 , 3 , 2 , apply_instancenorm = True , name = 'block_1') ",improving-cyclegan-monet-paintings.ipynb
encoder decoder skip connection," x_skip = L.Concatenate(name = 'enc_dec_skip_2')([dec_1 , enc_2]) ",improving-cyclegan-monet-paintings.ipynb
" bs, 256, 256, 64 "," dec_2 = decoder_block(x_skip , 64 , 3 , 2 , apply_instancenorm = True , name = 'block_2') ",improving-cyclegan-monet-paintings.ipynb
encoder decoder skip connection," x_skip = L.Concatenate(name = 'enc_dec_skip_3')([dec_2 , enc_1]) ",improving-cyclegan-monet-paintings.ipynb
"Discriminator modelThe discriminator is responsible for differentiating real images from images that have been generated by a generator model.Bellow, we have the architecture of the original CycleGAN discriminator, again, ours have some changes to improve performance on this task.","def discriminator_fn(height = HEIGHT , width = WIDTH , channels = CHANNELS): ",improving-cyclegan-monet-paintings.ipynb
" bs, 128, 128, 64 "," x = encoder_block(inputs , 64 , 4 , 2 , apply_instancenorm = False , activation = L.LeakyReLU(0.2), name = 'block_1') ",improving-cyclegan-monet-paintings.ipynb
" bs, 64, 64, 128 "," x = encoder_block(x , 128 , 4 , 2 , apply_instancenorm = True , activation = L.LeakyReLU(0.2), name = 'block_2') ",improving-cyclegan-monet-paintings.ipynb
" bs, 32, 32, 256 "," x = encoder_block(x , 256 , 4 , 2 , apply_instancenorm = True , activation = L.LeakyReLU(0.2), name = 'block_3') ",improving-cyclegan-monet-paintings.ipynb
" bs, 32, 32, 512 "," x = encoder_block(x , 512 , 4 , 1 , apply_instancenorm = True , activation = L.LeakyReLU(0.2), name = 'block_4') ",improving-cyclegan-monet-paintings.ipynb
" bs, 29, 29, 1 "," outputs = L.Conv2D(1 , 4 , strides = 1 , padding = 'valid' , kernel_initializer = conv_initializer)( x) ",improving-cyclegan-monet-paintings.ipynb
Build model CycleGAN ,with strategy.scope (): ,improving-cyclegan-monet-paintings.ipynb
transforms photos to Monet esque paintings," monet_generator = generator_fn(height = None , width = None , transformer_blocks = TRANSFORMER_BLOCKS) ",improving-cyclegan-monet-paintings.ipynb
transforms Monet paintings to be more like photos," photo_generator = generator_fn(height = None , width = None , transformer_blocks = TRANSFORMER_BLOCKS) ",improving-cyclegan-monet-paintings.ipynb
differentiates real Monet paintings and generated Monet paintings," monet_discriminator = discriminator_fn(height = None , width = None) ",improving-cyclegan-monet-paintings.ipynb
differentiates real photos and generated photos," photo_discriminator = discriminator_fn(height = None , width = None) ",improving-cyclegan-monet-paintings.ipynb
Loss functions,with strategy.scope (): ,improving-cyclegan-monet-paintings.ipynb
"Discriminator loss 0: fake, 1: real The discriminator loss outputs the average of the real and generated loss "," def discriminator_loss(real , generated): ",improving-cyclegan-monet-paintings.ipynb
Generator loss, def generator_loss(generated): ,improving-cyclegan-monet-paintings.ipynb
Cycle consistency loss measures if original photo and the twice transformed photo to be similar to one another , with strategy.scope (): ,improving-cyclegan-monet-paintings.ipynb
Identity loss compares the image with its generator i.e. photo with photo generator , with strategy.scope (): ,improving-cyclegan-monet-paintings.ipynb
"Learning rate scheduleThe original CycleGAN implementation used a constant learning rate schedule with a linear decay, I also found that the linear decay phase seems to be good at making the model more stable at the last epochs, you can check how the generator changes in a more conservative rate by the end looking at the gif images by the end.","@tf.function
def linear_schedule_with_warmup(step):
 """""" Create a schedule with a learning rate that decreases linearly after
 linearly increasing during a warmup period.
 """"""
 lr_start = 2e-4
 lr_max = 2e-4
 lr_min = 0.
 
 steps_per_epoch = int(max(n_monet_samples, n_photo_samples)//BATCH_SIZE)
 total_steps = EPOCHS * steps_per_epoch
 warmup_steps = 1
 hold_max_steps = total_steps * 0.8
 
 if step < warmup_steps:
 lr = (lr_max - lr_start) / warmup_steps * step + lr_start
 elif step < warmup_steps + hold_max_steps:
 lr = lr_max
 else:
 lr = lr_max * ((total_steps - step) / (total_steps - warmup_steps - hold_max_steps))
 if lr_min is not None:
 lr = tf.math.maximum(lr_min, lr)

 return lr

steps_per_epoch = int(max(n_monet_samples, n_photo_samples)//BATCH_SIZE)
total_steps = EPOCHS * steps_per_epoch
rng = [i for i in range(0, total_steps, 50)]
y = [linear_schedule_with_warmup(x) for x in rng]

sns.set(style=""whitegrid"")
fig, ax = plt.subplots(figsize=(20, 6))
plt.plot(rng, y)
print(f'{EPOCHS} total epochs and {steps_per_epoch} steps per epoch')
print(f'Learning rate schedule: {y[0]:.3g} to {max(y):.3g} to {y[-1]:.3g}')",improving-cyclegan-monet-paintings.ipynb
Train,with strategy.scope (): ,improving-cyclegan-monet-paintings.ipynb
Create generators," lr_monet_gen = lambda : linear_schedule_with_warmup(tf.cast(monet_generator_optimizer.iterations , tf.float32)) ",improving-cyclegan-monet-paintings.ipynb
Create discriminators," lr_monet_disc = lambda : linear_schedule_with_warmup(tf.cast(monet_discriminator_optimizer.iterations , tf.float32)) ",improving-cyclegan-monet-paintings.ipynb
Create dataset,"monet_ds = get_dataset(MONET_FILENAMES , augment = data_augment , batch_size = BATCH_SIZE) ",improving-cyclegan-monet-paintings.ipynb
Callbacks,class GANMonitor(Callback): ,improving-cyclegan-monet-paintings.ipynb
Create directories to save the generate images, if not os.path.exists(self.monet_path): ,improving-cyclegan-monet-paintings.ipynb
Monet generated images," for i , img in enumerate(photo_ds_eval.take(self.num_img)) : ",improving-cyclegan-monet-paintings.ipynb
Photo generated images," for i , img in enumerate(monet_ds_eval.take(self.num_img)) : ",improving-cyclegan-monet-paintings.ipynb
Create monet gif,"create_gif('/kaggle/working/monet/*.png' , 'monet.gif') ",improving-cyclegan-monet-paintings.ipynb
Create photo gif,"create_gif('/kaggle/working/photo/*.png' , 'photo.gif') ",improving-cyclegan-monet-paintings.ipynb
"Evaluating generator modelsHere we are going to evaluate the generator models including how good is the generator cycle, this means that we will get a photo to generate a Monet picture from it, then use the generated picture to generate the original photo.Photo input Monet generated Photo generated ","evaluate_cycle(photo_ds_eval.take(2), monet_generator, photo_generator, n_samples=2)",improving-cyclegan-monet-paintings.ipynb
Here we will do the same process but starting with a Monet picture.Monet input Photo generated Monet generated ,"evaluate_cycle(monet_ds_eval.take(2), photo_generator, monet_generator, n_samples=2)",improving-cyclegan-monet-paintings.ipynb
"Visualize predictionsA common issue with images generated by GANs is that the often show some undisered artifacts, a very common on is known as checkerboard artifacts , a good practice is to inspect some of the images to see its quality and if some of these undisered artifacts are present.","display_generated_samples(photo_ds_eval.take(8), monet_generator, 8)",improving-cyclegan-monet-paintings.ipynb
Make predictions,% % time ,improving-cyclegan-monet-paintings.ipynb
Create folder to save generated images,os.makedirs('../images/') ,improving-cyclegan-monet-paintings.ipynb
Submission file,"shutil.make_archive('/kaggle/working/images/', 'zip', '../images')

print(f""Generated samples: {len([name for name in os.listdir('../images/') if os.path.isfile(os.path.join('../images/', name))])}"")",improving-cyclegan-monet-paintings.ipynb
Output models,"monet_generator.save('monet_generator.h5')
photo_generator.save('photo_generator.h5')
monet_discriminator.save('monet_discriminator.h5')
photo_discriminator.save('photo_discriminator.h5')",improving-cyclegan-monet-paintings.ipynb
linear algebra,import numpy as np ,in-depth-guide-to-google-s-bert.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,in-depth-guide-to-google-s-bert.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,in-depth-guide-to-google-s-bert.ipynb
Any results you write to the current directory are saved as output.,import numpy as np ,in-depth-guide-to-google-s-bert.ipynb
Getting tokenizer,!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py,in-depth-guide-to-google-s-bert.ipynb
4.2 Helper Functions:,"def bert_encode(texts, tokenizer, max_len=512):
 all_tokens = []
 all_masks = []
 all_segments = []
 
 for text in texts:
 text = tokenizer.tokenize(text)
 
 text = text[:max_len-2]
 input_sequence = [""[CLS]""] + text + [""[SEP]""]
 pad_len = max_len - len(input_sequence)
 
 tokens = tokenizer.convert_tokens_to_ids(input_sequence)
 tokens += [0] * pad_len
 pad_masks = [1] * len(input_sequence) + [0] * pad_len
 segment_ids = [0] * max_len
 
 all_tokens.append(tokens)
 all_masks.append(pad_masks)
 all_segments.append(segment_ids)
 
 return np.array(all_tokens), np.array(all_masks), np.array(all_segments)",in-depth-guide-to-google-s-bert.ipynb
4.3 Loading BERT from the Tensorflow Hub:,"%%time
module_url = ""https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1""
bert_layer = hub.KerasLayer(module_url, trainable=True)",in-depth-guide-to-google-s-bert.ipynb
4.4 Loading data:,"train = pd.read_csv(""/kaggle/input/nlp-getting-started/train.csv"")
test = pd.read_csv(""/kaggle/input/nlp-getting-started/test.csv"")
submission = pd.read_csv(""/kaggle/input/nlp-getting-started/sample_submission.csv"")",in-depth-guide-to-google-s-bert.ipynb
4.5 Loading tokenizer from the bert layer:,"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)",in-depth-guide-to-google-s-bert.ipynb
"4.6 Encoding the text into tokens, masks, and segment flags:","train_input = bert_encode(train.text.values, tokenizer, max_len=160)
test_input = bert_encode(test.text.values, tokenizer, max_len=160)
train_labels = train.target.values",in-depth-guide-to-google-s-bert.ipynb
"4.7 Model: Build, Train, Predict, Submit:","model = build_model(bert_layer, max_len=160)
model.summary()",in-depth-guide-to-google-s-bert.ipynb
linear algebra,import numpy as np ,interactive-intro-to-dimensionality-reduction.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,interactive-intro-to-dimensionality-reduction.ipynb
Import the 3 dimensionality reduction methods,from sklearn.manifold import TSNE ,interactive-intro-to-dimensionality-reduction.ipynb
"MNIST DatasetFor the purposes of this interactive guide, the MNIST Mixed National Institute of Standards and Technology computer vision digit dataset was chosen partly due to its simplicity and also surprisingly deep and informative research that can be done with the dataset. So let s load the training data and see what we have","train = pd.read_csv('../input/train.csv')
train.head()",interactive-intro-to-dimensionality-reduction.ipynb
save the labels to a Pandas series target,target = train['label'] ,interactive-intro-to-dimensionality-reduction.ipynb
Drop the label feature,"train = train.drop(""label"" , axis = 1) ",interactive-intro-to-dimensionality-reduction.ipynb
Standardize the data,from sklearn.preprocessing import StandardScaler ,interactive-intro-to-dimensionality-reduction.ipynb
Calculating Eigenvectors and eigenvalues of Cov matirx,"mean_vec = np.mean(X_std , axis = 0) ",interactive-intro-to-dimensionality-reduction.ipynb
"Create a list of eigenvalue, eigenvector tuples","eig_pairs =[( np.abs(eig_vals[i]) , eig_vecs[: , i]) for i in range(len(eig_vals))] ",interactive-intro-to-dimensionality-reduction.ipynb
"Sort the eigenvalue, eigenvector pair from high to low","eig_pairs.sort(key = lambda x : x[0], reverse = True) ",interactive-intro-to-dimensionality-reduction.ipynb
Calculation of Explained Variance from the eigenvalues,tot = sum(eig_vals) ,interactive-intro-to-dimensionality-reduction.ipynb
Individual explained variance,"var_exp =[( i / tot)* 100 for i in sorted(eig_vals , reverse = True )] ",interactive-intro-to-dimensionality-reduction.ipynb
Cumulative explained variance,cum_var_exp = np.cumsum(var_exp) ,interactive-intro-to-dimensionality-reduction.ipynb
Invoke SKlearn s PCA method,n_components = 30 ,interactive-intro-to-dimensionality-reduction.ipynb
"eigenvalues pca.components .reshape n components, 28, 28 ",eigenvalues = pca.components_ ,interactive-intro-to-dimensionality-reduction.ipynb
Plot the first 8 eignenvalues,"plt.figure(figsize =(13 , 12)) ",interactive-intro-to-dimensionality-reduction.ipynb
plot some of the numbers,"plt.figure(figsize =(14 , 12)) ",interactive-intro-to-dimensionality-reduction.ipynb
reshape from 1d to 2d pixel array," grid_data = train.iloc[digit_num]. as_matrix (). reshape(28 , 28) ",interactive-intro-to-dimensionality-reduction.ipynb
Delete our earlier created X object,del X ,interactive-intro-to-dimensionality-reduction.ipynb
Taking only the first N rows to speed things up,X = train[: 6000]. values ,interactive-intro-to-dimensionality-reduction.ipynb
Standardising the values,X_std = StandardScaler (). fit_transform(X) ,interactive-intro-to-dimensionality-reduction.ipynb
Call the PCA method with 5 components.,pca = PCA(n_components = 5) ,interactive-intro-to-dimensionality-reduction.ipynb
"For cluster coloring in our Plotly plots, remember to also restrict the target values",Target = target[: 6000] ,interactive-intro-to-dimensionality-reduction.ipynb
KMeans clustering,from sklearn.cluster import KMeans ,interactive-intro-to-dimensionality-reduction.ipynb
Set a KMeans clustering with 9 components 9 chosen sneakily as hopefully we get back our 9 class labels ,kmeans = KMeans(n_clusters = 9) ,interactive-intro-to-dimensionality-reduction.ipynb
Compute cluster centers and predict cluster indices,X_clustered = kmeans.fit_predict(X_5d) ,interactive-intro-to-dimensionality-reduction.ipynb
" Linear Discriminant Analysis LDA LDA, much like PCA is also a linear transformation method commonly used in dimensionality reduction tasks. However unlike the latter which is an unsupervised learning algorithm, LDA falls into the class of supervised learning methods. As such the goal of LDA is that with available information about class labels, LDA will seek to maximise the separation between the different classes by computing the component axes linear discriminants which does this. ","from IPython.display import display, Math, Latex",interactive-intro-to-dimensionality-reduction.ipynb
"LDA Implementation via SklearnHaving gone through the nitty gritty details of the LDA implementation in theory, let us now implement the method in practise. Surprise, surprise we find that the Sklearn toolkit also comes with its own inbuilt LDA function and hence we invoke an LDA model as follows:",lda = LDA(n_components = 5) ,interactive-intro-to-dimensionality-reduction.ipynb
Taking in as second argument the Target as labels,"X_LDA_2D = lda.fit_transform(X_std , Target.values) ",interactive-intro-to-dimensionality-reduction.ipynb
Using the Plotly library again,8 ,interactive-intro-to-dimensionality-reduction.ipynb
Invoking the t SNE method,tsne = TSNE(n_components = 2) ,interactive-intro-to-dimensionality-reduction.ipynb
Having invoked the t SNE algorithm by simply calling TSNE we fit the digit data to the model and reduce its dimensions with fit transform. Finally let s plot the first two components in the new feature space in a scatter plot,"traceTSNE = go.Scatter(
 x = tsne_results[:,0],
 y = tsne_results[:,1],
 name = Target,
 hoveron = Target,
 mode = 'markers',
 text = Target,
 showlegend = True,
 marker = dict(
 size = 8,
 color = Target,
 colorscale ='Jet',
 showscale = False,
 line = dict(
 width = 2,
 color = 'rgb(255, 255, 255)'
 ),
 opacity = 0.8
 )
)
data = [traceTSNE]

layout = dict(title = 'TSNE (T-Distributed Stochastic Neighbour Embedding)',
 hovermode= 'closest',
 yaxis = dict(zeroline = False),
 xaxis = dict(zeroline = False),
 showlegend= False,

 )

fig = dict(data=data, layout=layout)
py.iplot(fig, filename='styled-scatter')",interactive-intro-to-dimensionality-reduction.ipynb
Differential Learning Rate, def is_backbone(name): ,interpreting-text-models-with-bert-on-tpu.ipynb
Train," batch_time = AverageMeter('Time' , ':6.3f') ",interpreting-text-models-with-bert-on-tpu.ipynb
Validation, model.eval () ,interpreting-text-models-with-bert-on-tpu.ipynb
Train Config,TRAIN_BATCH_SIZE = 16 ,interpreting-text-models-with-bert-on-tpu.ipynb
Scale learning rate to 8 TPU s,LR = 2e-5 * xm.xrt_world_size () ,interpreting-text-models-with-bert-on-tpu.ipynb
Train Validation Split,mask = np.random.rand(len(train)) < 0.95 ,interpreting-text-models-with-bert-on-tpu.ipynb
Run,def _run (): ,interpreting-text-models-with-bert-on-tpu.ipynb
Integrated Gradients,!pip install captum,interpreting-text-models-with-bert-on-tpu.ipynb
A token used for generating token reference,ref_token_id = tokenizer.pad_token_id ,interpreting-text-models-with-bert-on-tpu.ipynb
A token used as a separator between question and text and it is also added to the end of the text.,sep_token_id = tokenizer.sep_token_id ,interpreting-text-models-with-bert-on-tpu.ipynb
A token used for prepending to the concatenated question text word sequence,cls_token_id = tokenizer.cls_token_id ,interpreting-text-models-with-bert-on-tpu.ipynb
construct input token ids, input_ids =[cls_token_id]+ text_ids +[sep_token_id] ,interpreting-text-models-with-bert-on-tpu.ipynb
construct reference token ids, ref_input_ids =[cls_token_id]+[ref_token_id]* len(text_ids)+[sep_token_id] ,interpreting-text-models-with-bert-on-tpu.ipynb
 1," ref_token_type_ids = torch.zeros_like(token_type_ids , device = device) ",interpreting-text-models-with-bert-on-tpu.ipynb
"we could potentially also use random permutation with torch.randperm seq length, device device "," ref_position_ids = torch.zeros(seq_length , dtype = torch.long , device = device) ",interpreting-text-models-with-bert-on-tpu.ipynb
print text ," text = "" "".join(text) ",interpreting-text-models-with-bert-on-tpu.ipynb
ConnectX environment was defined in v0.1.4,! pip install 'kaggle-environments>=0.1.4' ,intro-to-connextx-env-and-minimax.ipynb
"Investigate ConnectX Environment Let s investigate, what Kaggle will be doing with this environment:","from kaggle_environments import evaluate, make

env = make(""connectx"", debug=True)",intro-to-connextx-env-and-minimax.ipynb
"There are some agents coming baked in, namely the random agent, that will create a baseline, if your final agent is doing better than flipping a coin. The negamax agent? We ll talk about that one below!",env.agents,intro-to-connextx-env-and-minimax.ipynb
"So clearly we can vary columns and rows, as well as the amount of tokens in a line to win. I guess this may be a nice test case for bigger games.But there s also the amount of steps and a timeout variable. I ll venture a guess and say that your move is a maximum of 2 seconds.With the specification commend, you ll be able to get a condensed dictionary that has most of the important information!",env.specification,intro-to-connextx-env-and-minimax.ipynb
This agent random chooses a non empty column.,"def my_agent(observation , configuration): ",intro-to-connextx-env-and-minimax.ipynb
Play as first position against random agent.,"trainer = env.train ([None , ""random""]) ",intro-to-connextx-env-and-minimax.ipynb
"env.render mode ipython , width 100, height 90, header False, controls False ","env.render(mode = ""ipython"" , width = 100 , height = 90 , header = False , controls = False) ",intro-to-connextx-env-and-minimax.ipynb
"From this it is clear that the board contains a flattened 1D array of the board as described in the specification. Zero contains non occupied spaces, our tokens represent a one and our opponent has value token 2.","def my_comatose_agent(observation, configuration):
 from random import choice
 from time import sleep
 sleep(2)
 return choice([c for c in range(configuration.columns) if observation.board[c] == 0])

def my_sleepy_agent(observation, configuration):
 from random import choice
 from time import sleep
 sleep(1)
 return choice([c for c in range(configuration.columns) if observation.board[c] == 0])",intro-to-connextx-env-and-minimax.ipynb
"Taking The HintIf you look in the starter notebook, you ll see that the evaluation is done against the following code:","def mean_reward(rewards):
 return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)

print(""My Agent vs Negamax Agent:"", mean_reward(evaluate(""connectx"", [my_agent, ""negamax""], num_episodes=3)))",intro-to-connextx-env-and-minimax.ipynb
"The important bit is the keyword negamax. It s a special version of the minimax strategy, that optimizes based on the symmetry, that in this two player game you are always doing better when your opponent is doing worse. So essentially, the game state is always I m at score 0.7 so my opponent is at 0.3 then you re at 0.4 and they re at 0.6. Just a fact from a two player game with perfect information. CC BY SA 3.0 MaschelosIf you re interested in implementing your own Negamax strategy with all the optimizations, I find this tutorial exceptional, despite being in C . These optimizations probably include Alpha Beta Pruning and Transposition Tables.Personally, I found the EasyAI implementation pretty understandable and worth diving into. Let s have a look at how kaggle approaches the negamax problem:","import inspect
import os

print(inspect.getsource(env.agents['negamax']))",intro-to-connextx-env-and-minimax.ipynb
How well is negamax doing against itself?,"neg_v_neg = evaluate(""connectx"", [env.agents['negamax'], ""negamax""], num_episodes=10)
print(neg_v_neg)
print(mean_reward(neg_v_neg))",intro-to-connextx-env-and-minimax.ipynb
"Let s Try Something Stupid How about, we try random choice, but just not take the step that will make us lose?And yes, this could be the first step toward implementing negamax. Considering, you have to simulate the games in a copy of the environment.","def try_not_to_loose_agent(observation , configuration): ",intro-to-connextx-env-and-minimax.ipynb
"We set the state of the environment, so we can experiment on it.", env.state[0][ 'observation']= observation ,intro-to-connextx-env-and-minimax.ipynb
Take a random column that is not full, my_action = choice ([c for c in cols if observation.board[c]== 0]) ,intro-to-connextx-env-and-minimax.ipynb
Simulate the next step," out = env.train ([None , ""negamax""]).step(my_action) ",intro-to-connextx-env-and-minimax.ipynb
"If the next step makes us lose, take a different step!", if out[2]: ,intro-to-connextx-env-and-minimax.ipynb
"If we run out of steps to take, we just loose with one step.", return 1 ,intro-to-connextx-env-and-minimax.ipynb
" IntroductionThis is a 5 layers Sequential Convolutional Neural Network for digits recognition trained on MNIST dataset. I choosed to build it with keras API Tensorflow backend which is very intuitive. Firstly, I will prepare the data handwritten digits images then i will focus on the CNN modeling and evaluation.I achieved 99.671 of accuracy with this CNN trained in 2h30 on a single CPU i5 2500k . For those who have a 3.0 GPU capabilites from GTX 650 to recent GPUs , you can use tensorflow gpu with keras. Computation will be much much faster !!!For computational reasons, i set the number of steps epochs to 2, if you want to achieve 99 of accuracy set it to 30.This Notebook follows three main parts: The data preparation The CNN modeling and evaluation The results prediction and submission ",import pandas as pd ,introduction-to-cnn-keras-0-997-top-6.ipynb
convert to one hot encoding,from keras.utils.np_utils import to_categorical ,introduction-to-cnn-keras-0-997-top-6.ipynb
Load the data,"train = pd.read_csv(""../input/train.csv"") ",introduction-to-cnn-keras-0-997-top-6.ipynb
Drop label column,"X_train = train.drop(labels =[""label""], axis = 1) ",introduction-to-cnn-keras-0-997-top-6.ipynb
free some space,del train ,introduction-to-cnn-keras-0-997-top-6.ipynb
Check the data,X_train.isnull (). any (). describe () ,introduction-to-cnn-keras-0-997-top-6.ipynb
Normalize the data,X_train = X_train / 255.0 ,introduction-to-cnn-keras-0-997-top-6.ipynb
"Reshape image in 3 dimensions height 28px, width 28px , canal 1 ","X_train = X_train.values.reshape(- 1 , 28 , 28 , 1) ",introduction-to-cnn-keras-0-997-top-6.ipynb
"Encode labels to one hot vectors ex : 2 0,0,1,0,0,0,0,0,0,0 ","Y_train = to_categorical(Y_train , num_classes = 10) ",introduction-to-cnn-keras-0-997-top-6.ipynb
Set the random seed,random_seed = 2 ,introduction-to-cnn-keras-0-997-top-6.ipynb
Split the train and the validation set for the fitting,"X_train , X_val , Y_train , Y_val = train_test_split(X_train , Y_train , test_size = 0.1 , random_state = random_seed) ",introduction-to-cnn-keras-0-997-top-6.ipynb
Some examples,"g = plt.imshow(X_train[0][ : , : , 0]) ",introduction-to-cnn-keras-0-997-top-6.ipynb
my CNN architechture is In Conv2D relu 2 MaxPool2D Dropout 2 Flatten Dense Dropout Out,model = Sequential () ,introduction-to-cnn-keras-0-997-top-6.ipynb
Define the optimizer,"optimizer = RMSprop(lr = 0.001 , rho = 0.9 , epsilon = 1e-08 , decay = 0.0) ",introduction-to-cnn-keras-0-997-top-6.ipynb
Compile the model,"model.compile(optimizer = optimizer , loss = ""categorical_crossentropy"" , metrics =[""accuracy""]) ",introduction-to-cnn-keras-0-997-top-6.ipynb
Turn epochs to 30 to get 0.9967 accuracy,epochs = 1 ,introduction-to-cnn-keras-0-997-top-6.ipynb
Plot the loss and accuracy curves for training and validation,"fig , ax = plt.subplots(2 , 1) ",introduction-to-cnn-keras-0-997-top-6.ipynb
Errors are difference between predicted labels and true labels,errors =(Y_pred_classes - Y_true != 0) ,introduction-to-cnn-keras-0-997-top-6.ipynb
Probabilities of the wrong predicted numbers,"Y_pred_errors_prob = np.max(Y_pred_errors , axis = 1) ",introduction-to-cnn-keras-0-997-top-6.ipynb
Predicted probabilities of the true values in the error set,"true_prob_errors = np.diagonal(np.take(Y_pred_errors , Y_true_errors , axis = 1)) ",introduction-to-cnn-keras-0-997-top-6.ipynb
Difference between the probability of the predicted label and the true label,delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors ,introduction-to-cnn-keras-0-997-top-6.ipynb
Sorted list of the delta prob errors,sorted_dela_errors = np.argsort(delta_pred_true_errors) ,introduction-to-cnn-keras-0-997-top-6.ipynb
Top 6 errors,most_important_errors = sorted_dela_errors[- 6 :] ,introduction-to-cnn-keras-0-997-top-6.ipynb
Show the top 6 errors,"display_errors(most_important_errors , X_val_errors , Y_pred_classes_errors , Y_true_errors) ",introduction-to-cnn-keras-0-997-top-6.ipynb
predict results,results = model.predict(test) ,introduction-to-cnn-keras-0-997-top-6.ipynb
select the indix with the maximum probability,"results = np.argmax(results , axis = 1) ",introduction-to-cnn-keras-0-997-top-6.ipynb
Dependencies,"import os, random, json, PIL, shutil, re
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from kaggle_datasets import KaggleDatasets
import tensorflow as tf
import tensorflow.keras.layers as L
import tensorflow_addons as tfa
from tensorflow.keras import Model, losses, optimizers",introduction-to-cyclegan-monet-paintings.ipynb
TPU configuration,"try:
 tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
 print(f'Running on TPU {tpu.master()}')
except ValueError:
 tpu = None

if tpu:
 tf.config.experimental_connect_to_cluster(tpu)
 tf.tpu.experimental.initialize_tpu_system(tpu)
 strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
 strategy = tf.distribute.get_strategy()


REPLICAS = strategy.num_replicas_in_sync
print(f'REPLICAS: {REPLICAS}')
AUTO = tf.data.experimental.AUTOTUNE",introduction-to-cyclegan-monet-paintings.ipynb
Model parameters,"HEIGHT = 256
WIDTH = 256
CHANNELS = 3
EPOCHS = 50
BATCH_SIZE = 1",introduction-to-cyclegan-monet-paintings.ipynb
Load data,"GCS_PATH = KaggleDatasets().get_gcs_path('monet-gan-getting-started')

MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))
PHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))

def count_data_items(filenames):
 n = [int(re.compile(r""-([0-9]*)\."").search(filename).group(1)) for filename in filenames]
 return np.sum(n)

n_monet_samples = count_data_items(MONET_FILENAMES)
n_photo_samples = count_data_items(PHOTO_FILENAMES)

print(f'Monet TFRecord files: {len(MONET_FILENAMES)}')
print(f'Monet image files: {n_monet_samples}')
print(f'Photo TFRecord files: {len(PHOTO_FILENAMES)}')
print(f'Photo image files: {n_photo_samples}')",introduction-to-cyclegan-monet-paintings.ipynb
Auxiliar functions,def decode_image(image): ,introduction-to-cyclegan-monet-paintings.ipynb
Looking at a few Monet paintings,"display_samples(load_dataset(MONET_FILENAMES).batch(1), 4, 6)",introduction-to-cyclegan-monet-paintings.ipynb
Looking at a few photo samples,"display_samples(load_dataset(PHOTO_FILENAMES).batch(1), 4, 6)",introduction-to-cyclegan-monet-paintings.ipynb
Generator model,OUTPUT_CHANNELS = 3 ,introduction-to-cyclegan-monet-paintings.ipynb
Discriminator model,def discriminator_fn (): ,introduction-to-cyclegan-monet-paintings.ipynb
" bs, 128, 128, 64 "," down1 = downsample(64 , 4 , False)( x) ",introduction-to-cyclegan-monet-paintings.ipynb
" bs, 64, 64, 128 "," down2 = downsample(128 , 4)( down1) ",introduction-to-cyclegan-monet-paintings.ipynb
" bs, 32, 32, 256 "," down3 = downsample(256 , 4)( down2) ",introduction-to-cyclegan-monet-paintings.ipynb
" bs, 34, 34, 256 ", zero_pad1 = L.ZeroPadding2D ()( down3) ,introduction-to-cyclegan-monet-paintings.ipynb
Build model CycleGAN ,with strategy.scope (): ,introduction-to-cyclegan-monet-paintings.ipynb
transforms photos to Monet esque paintings, monet_generator = generator_fn () ,introduction-to-cyclegan-monet-paintings.ipynb
transforms Monet paintings to be more like photos, photo_generator = generator_fn () ,introduction-to-cyclegan-monet-paintings.ipynb
differentiates real Monet paintings and generated Monet paintings, monet_discriminator = discriminator_fn () ,introduction-to-cyclegan-monet-paintings.ipynb
differentiates real photos and generated photos, photo_discriminator = discriminator_fn () ,introduction-to-cyclegan-monet-paintings.ipynb
Loss functions,with strategy.scope (): ,introduction-to-cyclegan-monet-paintings.ipynb
"Discriminator loss 0: fake, 1: real The discriminator loss outputs the average of the real and generated loss "," def discriminator_loss(real , generated): ",introduction-to-cyclegan-monet-paintings.ipynb
Generator loss, def generator_loss(generated): ,introduction-to-cyclegan-monet-paintings.ipynb
Cycle consistency loss measures if original photo and the twice transformed photo to be similar to one another , with strategy.scope (): ,introduction-to-cyclegan-monet-paintings.ipynb
Identity loss compares the image with its generator i.e. photo with photo generator , with strategy.scope (): ,introduction-to-cyclegan-monet-paintings.ipynb
Train,with strategy.scope (): ,introduction-to-cyclegan-monet-paintings.ipynb
Create generators," monet_generator_optimizer = optimizers.Adam(2e-4 , beta_1 = 0.5) ",introduction-to-cyclegan-monet-paintings.ipynb
Create discriminators," monet_discriminator_optimizer = optimizers.Adam(2e-4 , beta_1 = 0.5) ",introduction-to-cyclegan-monet-paintings.ipynb
Visualize predictions,"display_generated_samples(load_dataset(PHOTO_FILENAMES).batch(1), monet_generator, 8)",introduction-to-cyclegan-monet-paintings.ipynb
Create folder to save generated images,os.makedirs('../images/') ,introduction-to-cyclegan-monet-paintings.ipynb
Submission file,"shutil.make_archive('/kaggle/working/images/', 'zip', '../images')

print(f""Generated samples: {len([name for name in os.listdir('../images/') if os.path.isfile(os.path.join('../images/', name))])}"")",introduction-to-cyclegan-monet-paintings.ipynb
Load in our libraries,import pandas as pd ,introduction-to-ensembling-stacking-in-python.ipynb
Load in the train and test datasets,train = pd.read_csv('../input/train.csv') ,introduction-to-ensembling-stacking-in-python.ipynb
Store our passenger ID for easy access,PassengerId = test['PassengerId'] ,introduction-to-ensembling-stacking-in-python.ipynb
"Well it is no surprise that our task is to somehow extract the information out of the categorical variables Feature EngineeringHere, credit must be extended to Sina s very comprehensive and well thought out notebook for the feature engineering ideas so please check out his work Titanic Best Working Classfier 1 : by Sina","full_data =[train , test] ",introduction-to-ensembling-stacking-in-python.ipynb
Gives the length of the name,train['Name_length']= train['Name']. apply(len) ,introduction-to-ensembling-stacking-in-python.ipynb
Feature that tells whether a passenger had a cabin on the Titanic,"train['Has_Cabin']= train[""Cabin""]. apply(lambda x : 0 if type(x)== float else 1) ",introduction-to-ensembling-stacking-in-python.ipynb
Create new feature FamilySize as a combination of SibSp and Parch,for dataset in full_data : ,introduction-to-ensembling-stacking-in-python.ipynb
Create new feature IsAlone from FamilySize,for dataset in full_data : ,introduction-to-ensembling-stacking-in-python.ipynb
Remove all NULLS in the Embarked column,for dataset in full_data : ,introduction-to-ensembling-stacking-in-python.ipynb
Remove all NULLS in the Fare column and create a new feature CategoricalFare,for dataset in full_data : ,introduction-to-ensembling-stacking-in-python.ipynb
Create a New feature CategoricalAge,for dataset in full_data : ,introduction-to-ensembling-stacking-in-python.ipynb
Define function to extract titles from passenger names,def get_title(name): ,introduction-to-ensembling-stacking-in-python.ipynb
"If the title exists, extract and return it.", if title_search : ,introduction-to-ensembling-stacking-in-python.ipynb
"Create a new feature Title, containing the titles of passenger names",for dataset in full_data : ,introduction-to-ensembling-stacking-in-python.ipynb
Group all non common titles into one single grouping Rare ,for dataset in full_data : ,introduction-to-ensembling-stacking-in-python.ipynb
Mapping Sex," dataset['Sex']= dataset['Sex']. map({ 'female' : 0 , 'male' : 1 }). astype(int) ",introduction-to-ensembling-stacking-in-python.ipynb
Mapping titles," title_mapping = { ""Mr"" : 1 , ""Miss"" : 2 , ""Mrs"" : 3 , ""Master"" : 4 , ""Rare"" : 5 } ",introduction-to-ensembling-stacking-in-python.ipynb
Mapping Embarked," dataset['Embarked']= dataset['Embarked']. map({ 'S' : 0 , 'C' : 1 , 'Q' : 2 }). astype(int) ",introduction-to-ensembling-stacking-in-python.ipynb
Mapping Fare," dataset.loc[dataset['Fare']<= 7.91 , 'Fare']= 0 ",introduction-to-ensembling-stacking-in-python.ipynb
Mapping Age," dataset.loc[dataset['Age']<= 16 , 'Age']= 0 ",introduction-to-ensembling-stacking-in-python.ipynb
Feature selection,"drop_elements =['PassengerId' , 'Name' , 'Ticket' , 'Cabin' , 'SibSp'] ",introduction-to-ensembling-stacking-in-python.ipynb
"All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe hoVisualisations",train.head(3),introduction-to-ensembling-stacking-in-python.ipynb
"Pearson Correlation Heatmaplet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows","colormap = plt.cm.RdBu
plt.figure(figsize=(14,12))
plt.title('Pearson Correlation of Features', y=1.05, size=15)
sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, 
 square=True, cmap=colormap, linecolor='white', annot=True)",introduction-to-ensembling-stacking-in-python.ipynb
Takeaway from the PlotsOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn t much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch Parents and Children . I ll still leave both features in for the purposes of this exercise.PairplotsFinally let us generate some pairplots to observe the distribution of data from one feature to the other. Once again we use Seaborn to help us.,"g = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',
 u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )
g.set(xticklabels=[])",introduction-to-ensembling-stacking-in-python.ipynb
Some useful parameters which will come in handy later on,ntrain = train.shape[0] ,introduction-to-ensembling-stacking-in-python.ipynb
for reproducibility,SEED = 0 ,introduction-to-ensembling-stacking-in-python.ipynb
set folds for out of fold prediction,NFOLDS = 5 ,introduction-to-ensembling-stacking-in-python.ipynb
Class to extend the Sklearn classifier,class SklearnHelper(object): ,introduction-to-ensembling-stacking-in-python.ipynb
"Out of Fold PredictionsNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second level training. This runs the risk of your base model predictions already having seen the test set and therefore overfitting when feeding these predictions.","def get_oof(clf, x_train, y_train, x_test):
 oof_train = np.zeros((ntrain,))
 oof_test = np.zeros((ntest,))
 oof_test_skf = np.empty((NFOLDS, ntest))

 for i, (train_index, test_index) in enumerate(kf):
 x_tr = x_train[train_index]
 y_tr = y_train[train_index]
 x_te = x_train[test_index]

 clf.train(x_tr, y_tr)

 oof_train[test_index] = clf.predict(x_te)
 oof_test_skf[i, :] = clf.predict(x_test)

 oof_test[:] = oof_test_skf.mean(axis=0)
 return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)",introduction-to-ensembling-stacking-in-python.ipynb
Create 5 objects that represent our 4 models,"rf = SklearnHelper(clf = RandomForestClassifier , seed = SEED , params = rf_params) ",introduction-to-ensembling-stacking-in-python.ipynb
"Create Numpy arrays of train, test and target Survived dataframes to feed into our models",y_train = train['Survived']. ravel () ,introduction-to-ensembling-stacking-in-python.ipynb
Creates an array of the train data,x_train = train.values ,introduction-to-ensembling-stacking-in-python.ipynb
Creats an array of the test data,x_test = test.values ,introduction-to-ensembling-stacking-in-python.ipynb
Extra Trees,"et_oof_train , et_oof_test = get_oof(et , x_train , y_train , x_test) ",introduction-to-ensembling-stacking-in-python.ipynb
Random Forest,"rf_oof_train , rf_oof_test = get_oof(rf , x_train , y_train , x_test) ",introduction-to-ensembling-stacking-in-python.ipynb
AdaBoost,"ada_oof_train , ada_oof_test = get_oof(ada , x_train , y_train , x_test) ",introduction-to-ensembling-stacking-in-python.ipynb
Gradient Boost,"gb_oof_train , gb_oof_test = get_oof(gb , x_train , y_train , x_test) ",introduction-to-ensembling-stacking-in-python.ipynb
Support Vector Classifier,"svc_oof_train , svc_oof_test = get_oof(svc , x_train , y_train , x_test) ",introduction-to-ensembling-stacking-in-python.ipynb
"Feature importances generated from the different classifiersNow having learned our the first level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.As per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in .feature importances . Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such","rf_feature = rf.feature_importances(x_train,y_train)
et_feature = et.feature_importances(x_train, y_train)
ada_feature = ada.feature_importances(x_train, y_train)
gb_feature = gb.feature_importances(x_train,y_train)",introduction-to-ensembling-stacking-in-python.ipynb
So I have not yet figured out how to assign and store the feature importances outright. Therefore I ll print out the values from the code above and then simply copy and paste into Python lists as below sorry for the lousy hack ,"rf_features = [0.10474135, 0.21837029, 0.04432652, 0.02249159, 0.05432591, 0.02854371
 ,0.07570305, 0.01088129 , 0.24247496, 0.13685733 , 0.06128402]
et_features =[0.12165657, 0.37098307 ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157
 ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517, 0.08910063]
ada_features = [0.028 , 0.008 , 0.012 , 0.05866667, 0.032 , 0.008
 ,0.04666667 , 0. , 0.05733333, 0.73866667, 0.01066667]
gb_features =[0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395, 0.04778854
 ,0.05965792 , 0.02774745, 0.07462718, 0.4593142 , 0.01340093]",introduction-to-ensembling-stacking-in-python.ipynb
Create a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.,cols = train.columns.values ,introduction-to-ensembling-stacking-in-python.ipynb
axis 1 computes the mean row wise,feature_dataframe['mean']= feature_dataframe.mean(axis = 1) ,introduction-to-ensembling-stacking-in-python.ipynb
"Plotly Barplot of Average Feature ImportancesHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:",y = feature_dataframe['mean']. values ,introduction-to-ensembling-stacking-in-python.ipynb
"First level output as new featuresHaving now obtained our first level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first level predictions from our earlier classifiers and we train the next classifier on this.","base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),
 'ExtraTrees': et_oof_train.ravel(),
 'AdaBoost': ada_oof_train.ravel(),
 'GradientBoost': gb_oof_train.ravel()
 })
base_predictions_train.head()",introduction-to-ensembling-stacking-in-python.ipynb
Correlation Heatmap of the Second Level Training set,"data = [
 go.Heatmap(
 z= base_predictions_train.astype(float).corr().values ,
 x=base_predictions_train.columns.values,
 y= base_predictions_train.columns.values,
 colorscale='Viridis',
 showscale=True,
 reversescale = True
 )
]
py.iplot(data, filename='labelled-heatmap')",introduction-to-ensembling-stacking-in-python.ipynb
There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.,"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)
x_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)",introduction-to-ensembling-stacking-in-python.ipynb
ViT official repository by Google Research is cloned and appended in the system path. Check out the GitHub repo here. ,from IPython.display import clear_output ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
Updating to latest versioln of jaxlib,! pip3 install - U jaxlib ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
Updating to latest versioln of jax,! pip3 install - U jax ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
Updating to latest versioln of flax,! pip3 install - U flax ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
"This notebook works with all three accelerator TPU, GPU and CPU provided by Kaggle.But using TPU is recommended for faster training and evaluation. ","import os
if 'TPU_NAME' in os.environ:
 import requests
 if 'TPU_DRIVER_MODE' not in globals():
 url = 'http:' + os.environ['TPU_NAME'].split(':')[1] + ':8475/requestversion/tpu_driver_nightly'
 resp = requests.post(url)
 TPU_DRIVER_MODE = 1
 from jax.config import config
 config.FLAGS.jax_xla_backend = ""tpu_driver""
 config.FLAGS.jax_backend_target = os.environ['TPU_NAME']
 print(""\033[94m"")
 print(""TPU DETECTED!"")
 print('Registered TPU:', config.FLAGS.jax_backend_target)
else:
 print('No TPU detected.')
import jax 
DEVICE_COUNT = len(jax.local_devices())
TPU = DEVICE_COUNT==8

if TPU:
 print(""\033[94m"")
 print(""8 cores of TPU(Local devices in Jax ):"")
 print('\n'.join(map(str,jax.local_devices())))",jax-flax-tf-data-vision-transformers-tutorial.ipynb
Imports : ,import sys ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
VISION TRANSFORMER REPO IMPORTS,from vit_jax import utils ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
LIBRARIES IMPORT,import os ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
Setting up TF.data Pipeline : ,"IMAGE_SIZE = [512,512]
if TPU :
 from kaggle_datasets import KaggleDatasets
 GCS_DS_PATH = KaggleDatasets().get_gcs_path('tpu-getting-started')
 GCS_PATH = GCS_DS_PATH + '/tfrecords-jpeg-'+ str(IMAGE_SIZE[0])+""x"" + str(IMAGE_SIZE[1])
 PATH = GCS_PATH 
 
else: 
 PATH = ""../input/tpu-getting-started/tfrecords-jpeg-""+ str(IMAGE_SIZE[0])+""x"" + str(IMAGE_SIZE[1])",jax-flax-tf-data-vision-transformers-tutorial.ipynb
The data is provided serialized into TFRecords. This is a format convenient for distributing data to each of the TPUs cores. The tf.data input pipeline used in this notebook is a modified version of the wonderful beginner notebook Create Your First Submission of the Computer Vision course provided by Kaggle.,AUTO = tf.data.experimental.AUTOTUNE ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
Now Datasets are loaded and then mapped to a function that names the respective tensor for further processing.,"train_ds = load_dataset(TRAINING_FILENAMES , labeled = True) ",jax-flax-tf-data-vision-transformers-tutorial.ipynb
function for renaming,"def idx_name(image , label): ",jax-flax-tf-data-vision-transformers-tutorial.ipynb
"Data augmentation is a technique through which one can increase the size of the data for the training of the model without adding the new data. Techniques like padding, cropping, rotating, and flipping are the most common methods that are used over the images to increase the data size. There are six different augumentations transformations implemented in this notebook : Random Brighness Random Contrast Random Saturation Random Crop or Pad Random Rotate Sharpness","VIS_IMAGE = 4
aug_batch = next(iter(train_ds.batch(VIS_IMAGE).as_numpy_iterator()))
for image in aug_batch[""image""]: 
 seed = (SEED, SEED)
 image = image/ 255
 random_bright = tf.image.stateless_random_brightness(image, max_delta=1.0, 
 seed = seed)
 random_contrast = tf.image.stateless_random_contrast(image, 0.2, 2.0,
 seed = seed)
 random_saturation = tf.image.stateless_random_saturation(image, 0.2, 1.0,
 seed = seed)
 random = tf.random.uniform(shape=[],minval=-20, maxval=20).numpy().astype(""int"")
 random_crop_or_pad = tf.image.resize_with_crop_or_pad(image, 
 tf.shape(image).numpy()[0] + random, 
 tf.shape(image).numpy()[1] + random)
 random_rotate = tfa.image.rotate(image, tf.constant(tf.random.uniform((1,), 
 minval = 0.01,
 maxval = 0.4)))
 sharpness = tfa.image.sharpness(image, 5.1)

 plt.figure(figsize = (20, 20))
 
 plt.subplot(1, 7, 1)
 plt.imshow(image)
 plt.axis('off')
 plt.title('Original')
 
 plt.subplot(1, 7, 2)
 plt.imshow(random_bright.numpy())
 plt.title('Random Brightness')
 plt.axis('off')
 
 plt.subplot(1, 7, 3)
 plt.imshow(random_contrast.numpy())
 plt.title('Random Contrast')
 plt.axis('off')
 
 plt.subplot(1, 7, 4)
 plt.imshow(random_saturation.numpy())
 plt.title('Random Saturation')
 plt.axis('off')
 
 plt.subplot(1, 7, 5)
 plt.imshow(random_crop_or_pad.numpy())
 plt.title('Random Crop or Pad')
 plt.axis('off')
 
 plt.subplot(1, 7, 6)
 plt.imshow(random_rotate.numpy())
 plt.title('Random Rotate')
 plt.axis('off')
 
 plt.subplot(1, 7, 7)
 plt.imshow(sharpness.numpy())
 plt.title('Sharpness')",jax-flax-tf-data-vision-transformers-tutorial.ipynb
Transformations are divided into two groups with a 50 chance of being applied. The two groups are further divided into two sub groups with an equal chance of being applied. Each of the four sub groups transformations have a 25 chance of being applied ,def aug(data): ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
"The dataset passed on to the get data function gets augumented according to the preproces function passed, and then processed as required by the model and get reshaped as: IMAGE HEIGHT, IMAGE WIDTH , 3 DEVICE COUNT, BATCH, IMAGE HEIGHT, IMAGE WIDTH , 3 In case of TPU DEVICE COUNT 8, and in GPU and CPU DEVICE COUNT 1. ",AUTOTUNE = tf.data.experimental.AUTOTUNE ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
Extracting test ids for submission later,"test_ids_ds = test_ds.map(lambda data : data[""image_id""]) ",jax-flax-tf-data-vision-transformers-tutorial.ipynb
in case of GPU and CPU ,def shared_data(data): ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
Visualizing Processed Training Images : ,"def show_image_grid(images, labels ):
 """"""
 Create a plot of 32 images
 """"""
 rows = 4
 cols = 8
 fig = plt.figure(figsize=(20,11))
 for i in range(1, cols*rows+1 ):
 img = images[i-1]
 img = (img +1) / 2 
 fig.add_subplot(rows, cols, i)
 plt.axis('off')
 plt.title(CLASSES[labels[i-1]], fontsize=14)
 plt.imshow(img)
 plt.subplots_adjust(wspace=0.05, hspace=0.05)
 plt.show()",jax-flax-tf-data-vision-transformers-tutorial.ipynb
You need to change this code if you change batch size,train_batch = next(iter(train_ds)) ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
You need to change this code if you change batch size,valid_batch = next(iter(valid_ds)) ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
All available pre trained models: ,!gsutil ls -lh gs://vit_models/imagenet*,jax-flax-tf-data-vision-transformers-tutorial.ipynb
VIT B 32 model pretrained on imagenet21k and imagenet2012 is used in this notebook for training. ,"model_name = 'ViT-B_32' 
![ -e ""$model_name"".npz]|| gsutil cp gs://vit_models/imagenet21k+imagenet2012/""$model_name"".npz .
assert os.path.exists(f'{model_name}.npz')",jax-flax-tf-data-vision-transformers-tutorial.ipynb
"model config can be fetched used MODEL CONFIGfunction from the models config module. model config contains the configuration of the given model. After loading model definition conifg , random parameters are initialized using VisionTransformer function from the models module. ",model_name = 'ViT-B_32' ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
Intilizaing model,"model = models.VisionTransformer(num_classes = NUM_CLASSES , ** model_config) ",jax-flax-tf-data-vision-transformers-tutorial.ipynb
"After replication array type and shape changes as: DeviceArray ShardedDeviceArray NUM CLASSES DEVICE COUNT,NUM CLASSES NOTE: The above shape transformation demonstration is of the last output array of params ",params_repl = flax.jax_utils.replicate(params) ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
"jax.pmap: Applying pmap to a function will compile the function with XLA similarly to jit , then execute it in parallel on XLA devices, such as multiple GPUs or multiple TPU cores. pmap is different than vmap in how values are computer. vmap vectorizes a function by adding a batch dimension to every primitive operation in the function, whereas pmap replicates the function and executes each replica on its own XLA device in parallel. Learn more about jax.pmap here. Learn more about jax.vmap here.","vit_apply_repl = jax.pmap(
 lambda params, inputs: model.apply(dict(params=params), 
 inputs, 
 train=False))",jax-flax-tf-data-vision-transformers-tutorial.ipynb
Utility Functions: ,"def learning_rate_scheduler(base_learning_rate , total_steps , warmup_steps): ",jax-flax-tf-data-vision-transformers-tutorial.ipynb
total training steps,TOTAL_STEPS = 500 ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
Base learning rate,BASE_LR = 0.01 ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
Warmup steps for learning rate scheduler,WARMUP_STEPS = 100 ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
Glips gradient norm,GRAD_NORM_CLIP = 1 ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
Controls in how many forward passes the batch is split 8 for TPU ,ACCUM_STEPS = 8 ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
Verbosity,VERBOSE = 50 ,jax-flax-tf-data-vision-transformers-tutorial.ipynb
I have implemented a custom train function for ease and simplicity. The function provides an option to get the best parameter based on the minimum training loss. Verbosity can also be set manually. Option to plot loss vs steps graph at the end of training. See the docstring for more details ,"def fit(opt_repl,
 update_rng_repl, 
 total_steps, 
 data, 
 save_best_parameters=True, 
 verbose = 0 , 
 plot_loss_graph = True ):
 """"""
 Returns paramters after training
 Args:
 opt_repl : Optimier replicated across devices (8 devices for TPU).
 update_rng_rel : PRGs for dropout.
 total_steps : Int, Total training steps.
 data : Data for training.
 save_best_parameters : Bool, saves best parameters based on decreasing loss.
 verbose : Int, Verbosity of training 
 plot_loss_graph : Bool, plots loss vs steps graph at the end of training.
 """"""
 losses = []
 if save_best_parameters:
 best_loss = float('inf')
 best_opt = 0 
 
 for step, batch in zip(trange(1, total_steps + 1), data.as_numpy_iterator()):
 opt_repl, loss_repl, update_rng_repl = update_fn_repl(
 opt_repl, 
 flax.jax_utils.replicate(step), 
 batch, 
 update_rng_repl
 )
 
 if save_best_parameters:
 best_opt , best_loss = save_best_params(
 current_opt = opt_repl,
 best_opt = best_opt ,
 current_loss = loss_repl[0],
 best_loss = best_loss
 )
 
 if verbose:
 if step%verbose == 0 or step == 1:
 if save_best_parameters:
 print(f""\033[94mSTEP: {step}, LOSS: {loss_repl[0]:.2f}, MIN LOSS: {best_loss:.2f}"")
 else : 
 print(f""\033[94mSTEP: {step}, LOSS: {loss_repl[0]:.2f}"")
 
 losses.append(loss_repl[0])
 
 if plot_loss_graph:
 plt.plot(losses)
 plt.xlabel(""Steps"")
 plt.ylabel(""Loss"")
 plt.title(""Training Loss"")
 
 if save_best_parameters:
 return best_opt
 return opt_repl",jax-flax-tf-data-vision-transformers-tutorial.ipynb
Evaluating Predictions ,"valid_acc_jnp, valid_preds_jnp, valid_true_jnp = evaluate(
 opt_repl.target, 
 valid_ds , 
 NUM_VALIDATION_IMAGES , 
 BATCH_SIZE
)

val_acc = valid_acc_jnp.item()*100
print(f""Validation Accuracy = {val_acc:.2f}%"")",jax-flax-tf-data-vision-transformers-tutorial.ipynb
Confusion Matrix with various evaluation metrics : ,"y_true = valid_true_jnp.astype(""int32"")
y_preds = valid_preds_jnp.astype(""int32"")

labels = range(len(CLASSES))
cmat = confusion_matrix(y_true, y_preds, labels= labels)
score = f1_score(y_true, y_preds, labels=labels, average='macro')
precision = precision_score(y_true, y_preds, labels=labels, average='macro')
recall = recall_score(y_true, y_preds, labels=labels, average='macro')
display_confusion_matrix(cmat,score,precision,recall)",jax-flax-tf-data-vision-transformers-tutorial.ipynb
Making Predictions : ,"test_preds = predictions(opt_repl.target, test_ds, NUM_TEST_IMAGES, BATCH_SIZE)",jax-flax-tf-data-vision-transformers-tutorial.ipynb
Predictions to Submission: ,"submission = pd.DataFrame(
 data = np.array([test_ids, test_preds ]).T, 
 columns = [""id"", ""label""]
)
submission.to_csv(""submission.csv"", index = False)
submission.head()",jax-flax-tf-data-vision-transformers-tutorial.ipynb
EN Set rute into constants ES Se ponen las rutas de los archivos en constantes,"TEST_LABELS_RUTE = '/kaggle/input/street-view-getting-started-with-julia/trainLabels.csv'
TRAIN_IMG_DIR = '/kaggle/output/street-view-getting-started-with-julia/trainResized/'
TEST_IMG_DIR = '/kaggle/output/street-view-getting-started-with-julia/testResized/'
TRAIN_IMG_ZIP = '/kaggle/input/street-view-getting-started-with-julia/trainResized.zip'
TEST_IMG_ZIP = '/kaggle/input/street-view-getting-started-with-julia/testResized.zip'
BASE_DIR = '/kaggle/output/street-view-getting-started-with-julia/'",julia-en-es-prediction.ipynb
EN Create a function that allows us to extract and create a directory if it doesn t exist ES Creamos una funcin que nos permita extraer datos y crear un directorio en caso de que no exista,"def data_ext(Dir, Zip):
 if not os.path.exists(Dir):
 os.makedirs(Dir)
 zip_ref = zipfile.ZipFile(Zip)
 zip_ref.extractall(BASE_DIR)
 zip_ref.close()",julia-en-es-prediction.ipynb
"EN Function that we use later to see the characteristics of the data such as missing values NaN , all of features and number, records and columnsES Funcin que usamos ms tarde para ver las caractersticas de los datos, como valores faltantes NaN , todas las caractersticas y el nmero, registros y columnas.",def data_description(df): ,julia-en-es-prediction.ipynb
Get the datatype of features, for col in df.columns : ,julia-en-es-prediction.ipynb
Number of NaN values, n_miss = df.isna (). sum () ,julia-en-es-prediction.ipynb
Train data,"data_ext(TRAIN_IMG_DIR , TRAIN_IMG_ZIP) ",julia-en-es-prediction.ipynb
Test data,"data_ext(TEST_IMG_DIR , TEST_IMG_ZIP) ",julia-en-es-prediction.ipynb
EN Show general data of dataframe ES Mostrar datos generales de el dataframe,data_description(train_data),julia-en-es-prediction.ipynb
"EN As we can see, the difference with some characters is quite big ES Como se puede ver hay una gran diferencia entre algunos caracteres","X = X.drop(""ID"", axis = 1)",julia-en-es-prediction.ipynb
EN See the number of pixels in the image and the image number ES Mirar el numero de pixeles de la imagen y el numero de imagenes disponibles,print(f'Number of images: {X.shape[0]}\nNumer of pixels per image {X.shape[1]}'),julia-en-es-prediction.ipynb
EN Change dir to image and separe this in pixelsES Cambiar el directirio y poner la imagen separando los pixeles,"train_data_img = []
for img_path in train_data[""img""]:
 img = image.imread(img_path)
 data = np.asarray(img)
 if data.shape != (20, 20, 3):
 data = np.repeat(data[:, :, np.newaxis], 3, axis=2)
 train_data_img.append(data)
 
 
img_data = np.asarray(train_data_img, dtype=np.uint8)
print(""Completed"")",julia-en-es-prediction.ipynb
"final train cv2.resize image, 28,28 .flatten for image in train data img ",img_data.shape,julia-en-es-prediction.ipynb
See if the number of images its the same in X and in img data,"print(f""X: {X.shape[0]} size \nimg_data: {img_data.shape[0]} size"") ",julia-en-es-prediction.ipynb
EN Show non modified img ES Muestra las imagenes no modificadas,"for i in range(9):
 plt.subplot(3,3, i+1)
 plt.imshow(img_data[i])",julia-en-es-prediction.ipynb
Date generator,"datagen = ImageDataGenerator (
 zoom_range = 0.2,
 rescale = 1./255,
 rotation_range = 5.0,
 shear_range = 3.0,
 brightness_range = [0.0, 3.0]
)",julia-en-es-prediction.ipynb
Normalize the data,X = np.array(X). astype(float)/ 255 ,julia-en-es-prediction.ipynb
EN Show the fake dataES Muestra los datos falsos ,"for img, label in datagen.flow(X, y, batch_size = 10, shuffle = False):
 for i in range(10):
 plt.subplot(2,5, i+1)
 plt.xticks([])
 plt.yticks([])
 plt.imshow(img[i])
 break",julia-en-es-prediction.ipynb
Test size 25 of the data,"X_train , X_val , y_train , y_val = train_test_split(X , y , test_size = 0.25 , random_state = 0) ",julia-en-es-prediction.ipynb
"EN Generate extra data, with latest parameters datagen ES Generar datos extra mediante los anteriores parametros datagen ","data_gen_train = datagen.flow(X_train, y_train, batch_size = 20)",julia-en-es-prediction.ipynb
Number of total posible results,"num_pos = len(train_data[""Class""]. value_counts ()) ",julia-en-es-prediction.ipynb
Model creation and fit,"ACTIVATION_M = ""gelu""

model = tf.keras.models.Sequential([
 tf.keras.layers.Conv2D(32, (3,3), activation = ACTIVATION_M, input_shape=(20,20,3)),
 tf.keras.layers.Conv2D(64, (3,3), activation = ACTIVATION_M),
 tf.keras.layers.Dropout(0.25),
 tf.keras.layers.MaxPooling2D((2,2), padding='same'),
 
 tf.keras.layers.Conv2D(128, (3,3), activation = ACTIVATION_M),
 tf.keras.layers.Conv2D(256, (3,3), activation = ACTIVATION_M),
 tf.keras.layers.MaxPooling2D((2,2), padding='same'),
 tf.keras.layers.Dropout(0.2),
 
 tf.keras.layers.Flatten(),
 
 tf.keras.layers.Dense(125, activation = ACTIVATION_M),
 tf.keras.layers.Dropout(0.2),
 tf.keras.layers.Dense(250, activation = ACTIVATION_M),
 tf.keras.layers.Dense(500, activation = ACTIVATION_M),
 tf.keras.layers.Dropout(0.2),
 tf.keras.layers.Dense(num_pos, activation='softmax')
])
print(""Model created"")",julia-en-es-prediction.ipynb
EN Compile the modelES Compilar el modelo,"model.compile(optimizer = ""adam"", loss=""sparse_categorical_crossentropy"", metrics = [""accuracy""])
print(""Model compiled"")",julia-en-es-prediction.ipynb
"EN Create early stopping metod to imporve the model, the model stops if it sees that its performance is being lower during several epochsES Creamos un metodo early stopping para mejorar el modelo, el modelo para si ve que su desempeo esta siendo menor durante varias epocas","es_callbacks = [tf.keras.callbacks.EarlyStopping(patience = 10, monitor = ""accuracy"")]",julia-en-es-prediction.ipynb
EN fit the model and save the historialES Entrenamos el modelo y guardamos los datos,"history_frame = pd.DataFrame(history.history)
history_frame.loc[:, ['loss', 'val_loss']].plot()
history_frame.loc[:, ['accuracy', 'val_accuracy']].plot()",julia-en-es-prediction.ipynb
zipObj.extractall ,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns",just-the-basics-notebook.ipynb
EDA,"plt.figure(figsize=(20,20))
sns.heatmap(train_df.corr())",just-the-basics-notebook.ipynb
Data preprocessing,mean = {},just-the-basics-notebook.ipynb
Model defining,"from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier, XGBRFClassifier
from typing import Union",just-the-basics-notebook.ipynb
"scores.loc algorithm, F1 f1","def train(scaler: Union['none', 'Standard', 'MinMax']='none'):
 global models
 if scaler == 'none':
 for index, model in algorithms.items():
 models[index] = GridSearchCV(estimator=model, param_grid=param_grid[index], cv=10, scoring='roc_auc')
 models[index].fit(X, Y)
 scores.loc[index, ""AUC""] = models[index].best_score_
 print(index)
 
 elif scaler == 'Standard':
 for index, model in algorithms.items():
 models[index] = make_pipeline(StandardScaler(), GridSearchCV(estimator=model, param_grid=param_grid[index], cv=10, scoring='roc_auc', refit=True))
 models[index].fit(X, Y)
 scores.loc[index, ""AUC""] = models[index]['gridsearchcv'].best_score_
 print(index)
 elif scaler == 'MinMax':
 for index, model in algorithms.items():
 models[index] = make_pipeline(MinMaxScaler(), GridSearchCV(estimator=model, param_grid=param_grid[index], cv=10, scoring='roc_auc', refit=True))
 models[index].fit(X, Y)
 scores.loc[index, ""AUC""] = models[index]['gridsearchcv'].best_score_
 print(index)",just-the-basics-notebook.ipynb
linear algebra,import numpy as np ,keras-facial-keypoint-analysis.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,keras-facial-keypoint-analysis.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,keras-facial-keypoint-analysis.ipynb
Modeling,X_train.shape ,keras-facial-keypoint-analysis.ipynb
30 metrics,"import tensorflow as tf
from keras.layers.advanced_activations import LeakyReLU
from keras.layers import Conv2D,Dropout,Dense,Flatten
from keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Conv2D,MaxPool2D, ZeroPadding2D
from keras.models import Sequential, Model

model = Sequential([Flatten(input_shape=(96,96)),
 Dense(128, activation=""relu""),
 Dropout(0.1),
 Dense(64, activation=""relu""),
 Dense(30)
 ])",keras-facial-keypoint-analysis.ipynb
model.add BatchNormalization ,model.add(LeakyReLU(alpha = 0.1)) ,keras-facial-keypoint-analysis.ipynb
Fit the model!,"model.fit(X_train,y_train,epochs = 40,batch_size = 256,validation_split = 0.2)
",keras-facial-keypoint-analysis.ipynb
" Introduction 1.1 What is Knowledge Graph? A knowledge graph is a way of storing data that resulted from an information extraction task. Many basic implementations of knowledge graphs make use of a concept we call triple, that is a set of three items a subject, a predicate and an object that we can use to store information about something. We can define a graph as a set of nodes and edges. Node A and Node B here are two different entities. These nodes are connected by an edge that represents the relationship between the two nodes. Now, this is the smallest knowledge graph we can build it is also known as a triple.Knowledge Graph s come in a variety of shapes and sizes. 1.2 Data Representation in Knowledge Graph? Let s take this sentence as an example:London is the capital of England. Westminster is located in London.After some basic processing which we will see later, we would 2 triples like this: London, be capital, England , Westminster, locate, London So in this example we have three unique entities London, England and Westminster and two relations be capital, locate . To build a knowledge graph, we only have two associated nodes in the graph with the entities and vertices with the relations and we will get something like this: Manually building a knowledge graph is not scalable. Nobody is going to go through thousands of documents and extract all the entities and the relations between them!That s why machines are more suitable to perform this task as going through even hundreds or thousands of documents is child s play for them. But then there is another challenge machines do not understand natural language. This is where Natural Language Processing NLP comes into the picture.To build a knowledge graph from the text, it is important to make our machine understand natural language. This can be done by using NLP techniques such as sentence segmentation, dependency parsing, parts of speech tagging, and entity recognition. 1.3 Import Dependencies Load dataset ","import re
import pandas as pd
import bs4
import requests
import spacy
from spacy import displacy
nlp = spacy.load('en_core_web_sm')

from spacy.matcher import Matcher 
from spacy.tokens import Span 

import networkx as nx

import matplotlib.pyplot as plt
from tqdm import tqdm

pd.set_option('display.max_colwidth', 200)
%matplotlib inline",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
import wikipedia sentences,"candidate_sentences = pd.read_csv(""../input/wiki-sentences1/wiki_sentences_v2.csv"") ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"1.4 Sentence Segmentation The first step in building a knowledge graph is to split the text document or article into sentences. Then, we will shortlist only those sentences in which there is exactly 1 subject and 1 object.","
doc = nlp(""the drawdown process is governed by astm standard d823"")

for tok in doc:
 print(tok.text, ""..."", tok.dep_)",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"1.5 Entities Extraction The extraction of a single word entity from a sentence is not a tough task. We can easily do this with the help of parts of speech POS tags. The nouns and the proper nouns would be our entities.However, when an entity spans across multiple words, then POS tags alone are not sufficient. We need to parse the dependency tree of the sentence.To build a knowledge graph, the most important things are the nodes and the edges between them.These nodes are going to be the entities that are present in the Wikipedia sentences. Edges are the relationships connecting these entities to one another. We will extract these elements in an unsupervised manner, i.e., we will use the grammar of the sentences.The main idea is to go through a sentence and extract the subject and the object as and when they are encountered. However, there are a few challenges an entity can span across multiple words, eg., red wine , and the dependency parsers tag only the individual words as subjects or objects.So, I have created a function below to extract the subject and the object entities from a sentence while also overcoming the challenges mentioned above. I have partitioned the code into multiple chunks for your convenience:",def get_entities(sent): ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
chunk 1," ent1 = """" ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
dependency tag of previous token in the sentence," prv_tok_dep = """" ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
previous token in the sentence," prv_tok_text = """" ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
if token is a punctuation mark then move on to the next token," if tok.dep_ != ""punct"" : ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
check: token is a compound word or not," if tok.dep_ == ""compound"" : ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
if the previous word was also a compound then add the current word to it," if prv_tok_dep == ""compound"" : ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
check: token is a modifier or not," if tok.dep_.endswith(""mod"")== True : ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
if the previous word was also a compound then add the current word to it," if prv_tok_dep == ""compound"" : ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
chunk 3," if tok.dep_.find(""subj"")== True : ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
chunk 4," if tok.dep_.find(""obj"")== True : ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
update variables, prv_tok_dep = tok.dep_ ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Chunk 1Defined a few empty variables in this chunk. prv tok dep and prv tok text will hold the dependency tag of the previous word in the sentence and that previous word itself, respectively. prefix and modifier will hold the text that is associated with the subject or the object.Chunk 2Next, we will loop through the tokens in the sentence. We will first check if the token is a punctuation mark or not. If yes, then we will ignore it and move on to the next token. If the token is a part of a compound word dependency tag compound , we will keep it in the prefix variable. A compound word is a combination of multiple words linked to form a word with a new meaning example Football Stadium , animal lover .As and when we come across a subject or an object in the sentence, we will add this prefix to it. We will do the same thing with the modifier words, such as nice shirt , big house , etc.Chunk 3Here, if the token is the subject, then it will be captured as the first entity in the ent1 variable. Variables such as prefix, modifier, prv tok dep, and prv tok text will be reset.Chunk 4Here, if the token is the object, then it will be captured as the second entity in the ent2 variable. Variables such as prefix, modifier, prv tok dep, and prv tok text will again be reset.Chunk 5Once we have captured the subject and the object in the sentence, we will update the previous token and its dependency tag.Let s test this function on a sentence:","get_entities(""the film had 200 patents"")",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Great, it seems to be working as planned. In the above sentence, film is the subject and 200 patents is the object.Now we can use this function to extract these entity pairs for all the sentences in our data:","entity_pairs = []

for i in tqdm(candidate_sentences[""sentence""]):
 entity_pairs.append(get_entities(i))",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
The list entity pairs contains all the subject object pairs from the Wikipedia sentences. Let s have a look at a few of them:,entity_pairs[10:20],knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"As you can see, there are a few pronouns in these entity pairs such as we , it , she , etc. We d like to have proper nouns or nouns instead. Perhaps we can further improve the get entities function to filter out pronouns1.6 Relations Extraction Entity extraction is half the job done. To build a knowledge graph, we need edges to connect the nodes entities to one another. These edges are the relations between a pair of nodes.Our hypothesis is that the predicate is actually the main verb in a sentence.For example, in the sentence Sixty Hollywood musicals were released in 1929 , the verb is released in and this is what we are going to use as the predicate for the triple generated from this sentence.The function below is capable of capturing such predicates from the sentences. Here, I have used spaCy s rule based matching:",def get_relation(sent): ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Matcher class object, matcher = Matcher(nlp.vocab) ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"The pattern defined in the function tries to find the ROOT word or the main verb in the sentence. Once the ROOT is identified, then the pattern checks whether it is followed by a preposition prep or an agent word. If yes, then it is added to the ROOT word. Let me show you a glimpse of this function:","get_relation(""John completed the task"")",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Let s take a look at the most frequent relations or predicates that we have just extracted:,pd.Series(relations).value_counts()[:50],knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
extract subject,source =[i[0]for i in entity_pairs] ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
extract object,target =[i[1]for i in entity_pairs] ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Let s plot the network:,"plt.figure(figsize=(12,12))

pos = nx.spring_layout(G)
nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)
plt.show()",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"That s a much cleaner graph. Here the arrows point towards the composers. For instance, A.R. Rahman, who is a renowned music composer, has entities like soundtrack score , film score , and music connected to him in the graph above.Let s check out a few more relations.Now I would like to visualize the graph for the written by relation:","G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==""written by""], ""source"", ""target"", 
 edge_attr=True, create_using=nx.MultiDiGraph())

plt.figure(figsize=(12,12))
pos = nx.spring_layout(G, k = 0.5)
nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)
plt.show()",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"This knowledge graph is giving us some extraordinary information. Guys like Javed Akhtar, Krishna Chaitanya, and Jaideep Sahni are all famous lyricists and this graph beautifully captures this relationship.Let s see the knowledge graph of another important predicate, i.e., the released in :","G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==""released in""], ""source"", ""target"", 
 edge_attr=True, create_using=nx.MultiDiGraph())

plt.figure(figsize=(12,12))
pos = nx.spring_layout(G, k = 0.5)
nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)
plt.show()",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Let us install the pytorch interface for BERT by Hugging Face. This library contains interfaces for other pretrained language models like OpenAI s GPT and GPT 2. I have selected the pytorch interface because it strikes a nice balance between the high level APIs and tensorflow code .,!pip install pytorch-pretrained-bert pytorch-nlp,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Import Libraries,import tensorflow as tf ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Load Dataset:I will be using The Corpus of Linguistic Acceptability CoLA dataset for single sentence classification. It s a set of sentences labeled as grammatically correct or incorrect. The data is as follows:Column 1: the code representing the source of the sentence.Column 2: the acceptability judgment label 0 unacceptable, 1 acceptable .Column 3: the acceptability judgment as originally notated by the author.Column 4: the sentence.","df = pd.read_csv(""../input/cola-the-corpus-of-linguistic-acceptability/cola_public/raw/in_domain_train.tsv"", delimiter='\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Create sentence and label lists,sentences = df.sentence.values ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
We need to add special tokens at the beginning and end of each sentence for BERT to work properly,"sentences =[""[CLS] "" + sentence + "" [SEP]"" for sentence in sentences] ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Next, import the BERT tokenizer, used to convert our text into tokens that correspond to BERT s vocabulary.","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]
print (""Tokenize the first sentence:"")
print (tokenized_texts[0])",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"In the original paper, the authors used a length of 512.",MAX_LEN = 128 ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary,input_ids =[tokenizer.convert_tokens_to_ids(x)for x in tokenized_texts] ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Pad our input tokens,"input_ids = pad_sequences(input_ids , maxlen = MAX_LEN , dtype = ""long"" , truncating = ""post"" , padding = ""post"") ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Create attention masks,attention_masks = [] ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Create a mask of 1s for each token followed by 0s for padding,for seq in input_ids : ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Convert all of our data into torch tensors, the required datatype for our model",train_inputs = torch.tensor(train_inputs) ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Select a batch size for training. For fine tuning BERT on a specific task, the authors recommend a batch size of 16 or 32",batch_size = 32 ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
with an iterator the entire dataset does not need to be loaded into memory,"train_data = TensorDataset(train_inputs , train_masks , train_labels) ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.","model = BertForSequenceClassification.from_pretrained(""bert-base-uncased"" , num_labels = 2) ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.For the purposes of fine tuning, the authors recommend the following hyperparameter ranges:Batch size: 16, 32 Learning rate Adam : 5e 5, 3e 5, 2e 5 Number of epochs: 2, 3, 4","param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'gamma', 'beta']
optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay_rate': 0.01},
 {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
This variable contains all of the hyperparemeter information our training loop needs,"optimizer = BertAdam(optimizer_grouped_parameters , lr = 2e-5 , warmup = .1) ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Function to calculate the accuracy of our predictions vs labels,"def flat_accuracy(preds , labels): ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Store our loss and accuracy for plotting,train_loss_set = [] ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Number of training epochs,epochs = 2 ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
trange is a tqdm wrapper around the normal python range,"for _ in trange(epochs , desc = ""Epoch""): ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Set our model to training mode as opposed to evaluation mode , model.train () ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Tracking variables, tr_loss = 0 ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Train the data for one epoch," for step , batch in enumerate(train_dataloader): ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Unpack the inputs from our dataloader," b_input_ids , b_input_mask , b_labels = batch ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Clear out the gradients by default they accumulate , optimizer.zero_grad () ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Forward pass," loss = model(b_input_ids , token_type_ids = None , attention_mask = b_input_mask , labels = b_labels) ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Backward pass, loss.backward () ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Update parameters and take a step using the computed gradient, optimizer.step () ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Update tracking variables, tr_loss += loss.item () ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Put model in evaluation mode to evaluate loss on the validation set, model.eval () ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Tracking variables," eval_loss , eval_accuracy = 0 , 0 ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Evaluate data for one epoch, for batch in validation_dataloader : ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Unpack the inputs from our dataloader," b_input_ids , b_input_mask , b_labels = batch ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Telling the model not to compute or store gradients, saving memory and speeding up validation", with torch.no_grad (): ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Forward pass, calculate logit predictions"," logits = model(b_input_ids , token_type_ids = None , attention_mask = b_input_mask) ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Move logits and labels to CPU, logits = logits.detach (). cpu (). numpy () ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Training EvaluationLet s take a look at our training loss over all batches:,"plt.figure(figsize=(15,8))
plt.title(""Training loss"")
plt.xlabel(""Batch"")
plt.ylabel(""Loss"")
plt.plot(train_loss_set)
plt.show()",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Predict and Evaluate on Holdout Set Now we ll load the holdout dataset and prepare inputs just as we did with the training set. Then we ll evaluate predictions using Matthew s correlation coefficient because this is the metric used by the wider NLP community to evaluate performance on CoLA. With this metric, 1 is the best score, and 1 is the worst score. This way, we can see how well we perform against the state of the art models for this specific task.","df = pd.read_csv(""../input/cola-the-corpus-of-linguistic-acceptability/cola_public/raw/out_of_domain_dev.tsv"", delimiter='\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Create sentence and label lists,sentences = df.sentence.values ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
We need to add special tokens at the beginning and end of each sentence for BERT to work properly,"sentences =[""[CLS] "" + sentence + "" [SEP]"" for sentence in sentences] ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary,input_ids =[tokenizer.convert_tokens_to_ids(x)for x in tokenized_texts] ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Pad our input tokens,"input_ids = pad_sequences(input_ids , maxlen = MAX_LEN , dtype = ""long"" , truncating = ""post"" , padding = ""post"") ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Create attention masks,attention_masks = [] ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Create a mask of 1s for each token followed by 0s for padding,for seq in input_ids : ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Put model in evaluation mode,model.eval () ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Tracking variables,"predictions , true_labels = [], [] ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Predict,for batch in prediction_dataloader : ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Unpack the inputs from our dataloader," b_input_ids , b_input_mask , b_labels = batch ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Telling the model not to compute or store gradients, saving memory and speeding up prediction", with torch.no_grad (): ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Forward pass, calculate logit predictions"," logits = model(b_input_ids , token_type_ids = None , attention_mask = b_input_mask) ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Move logits and labels to CPU, logits = logits.detach (). cpu (). numpy () ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Store predictions and true labels, predictions.append(logits) ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Import and evaluate each test batch using Matthew s correlation coefficient,from sklearn.metrics import matthews_corrcoef ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"1.2 Installation Spacy, its data, and its models can be easily installed using python package index and setup tools. Use the following command to install spacy in your machine:",!pip install -U spacy,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"1.3 Statistical models Some of spaCy s features work independently, others require statistical models to be loaded, which enable spaCy to predict linguistic annotations for example, whether a word is a verb or a noun. spaCy currently offers statistical models for a variety of languages, which can be installed as individual Python modules. Models can differ in size, speed, memory usage, accuracy and the data they include. The model you choose always depends on your use case and the texts you re working with. For a general purpose use case, the small, default models are always a good start. They typically include the following components: Binary weights for the part of speech tagger, dependency parser and named entity recognizer to predict those annotations in context. Lexical entries in the vocabulary, i.e. words and their context independent attributes like the shape or spelling. Data files like lemmatization rules and lookup tables. Word vectors, i.e. multi dimensional meaning representations of words that let you determine how similar they are to each other. Configuration options, like the language and processing pipeline settings, to put spaCy in the correct state when you load in the model. These models are the power engines of spaCy. These models enable spaCy to perform several NLP related tasks, such as part of speech tagging, named entity recognition, and dependency parsing.I ve listed below the different statistical models in spaCy along with their specifications: en core web sm: English multi task CNN trained on OntoNotes. Size 11 MB en core web md: English multi task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Size 91 MB en core web lg: English multi task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Size 789 MB Importing these models is super easy. We can import a model by just executing spacy.load model name as shown below:",!python -m spacy download en_core_web_lg,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"1.4 Linguistic annotations spaCy provides a variety of linguistic annotations to give you insights into a text s grammatical structure. This includes the word types, like the parts of speech, and how the words are related to each other. For example, if you re analyzing text, it makes a huge difference whether a noun is the subject of a sentence, or the object or whether google is used as a verb, or refers to the website or company in a specific context.Once you have downloaded and installed a model, you can load it via spacy.load . This will return a Language object containing all components and data needed to process text. We usually call it nlp. Calling the nlp object on a string of text will return a processed Doc:","import spacy
nlp = spacy.load(""en_core_web_sm"")

doc = nlp(""Company Y is planning to acquire stake in X company for $23 billion"")
for token in doc:
 print(token.text, token.pos_, token.dep_)",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Even though a Doc is processed e.g. split into individual words and annotated it still holds all information of the original text, like whitespace characters. You can always get the offset of a token into the original string, or reconstruct the original by joining the tokens and their trailing whitespace. This way, you ll never lose any information when processing text with spaCy.spaCy s Processing Pipeline 1.5 spaCy s Processing Pipeline The first step for a text string, when working with spaCy, is to pass it to an NLP object. This object is essentially a pipeline of several text pre processing operations through which the input text string has to go through.As you can see in the figure above, the NLP pipeline has multiple components, such as tokenizer, tagger, parser, ner, etc. So, the input text string has to go through all these components before we can work on it.Let me show you how we can create an nlp object:",import spacy ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Create an nlp object,"doc = nlp(""He went to play cricket with friends in the stadium"") ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
You can use the below code to figure out the active pipeline components:,nlp.pipe_names,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Just in case you wish to disable the pipeline components and keep only the tokenizer up and running, then you can use the code below to disable the pipeline components:","nlp.disable_pipes('tagger', 'parser')",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Let s again check the active pipeline component:,nlp.pipe_names,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
" Features 2.1 Tokenization Segmenting text into words, punctuations marks etc. During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. ","import spacy

nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""Reliance is looking at buying U.K. based analytics startup for $7 billion"")
for token in doc:
 print(token.text)",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"2.2 Part Of Speech POS Tagging Part of speech or POS is a grammatical role that explains how a particular word is used in a sentence. There are eight parts of speech. Noun Pronoun Adjective Verb Adverb Preposition Conjunction Interjection Part of speech tagging is the process of assigning a POS tag to each token depending on its usage in the sentence. POS tags are useful for assigning a syntactic category like noun or verb to each word.After tokenization, spaCy can parse and tag a given Doc. This is where the statistical model comes in, which enables spaCy to make a prediction of which tag or label most likely applies in this context. A model consists of binary data and is produced by showing a system enough examples for it to make predictions that generalize across the language for example, a word following the in English is most likely a noun.Linguistic annotations are available as Token attributes. Like many NLP libraries, spaCy encodes all strings to hash values to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore to its name.In English grammar, the parts of speech tell us what is the function of a word and how it is used in a sentence. Some of the common parts of speech in English are Noun, Pronoun, Adjective, Verb, Adverb, etc.POS tagging is the task of automatically assigning POS tags to all the words of a sentence. It is helpful in various downstream tasks in NLP, such as feature engineering, language understanding, and information extraction.Performing POS tagging, in spaCy, is a cakewalk.In spaCy, POS tags are available as an attribute on the Token object:",import spacy ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Create an nlp object,"doc = nlp(""Reliance is looking at buying U.K. based analytics startup for $7 billion"") ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Iterate over the tokens,for token in doc : ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Print the token and its part of speech tag," print(token , token.tag_ , token.pos_ , spacy.explain(token.tag_)) ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Using spaCy s built in displaCy visualizer,The quickest way to visualize Doc is to use displacy.serve. This will spin up a simple web server and let you view the result straight from your browser. displaCy can either take a single Doc or a list of Doc objects as its first argument. This lets you construct them however you like using any model or modifications you like.Here s what our example sentence and its dependencies look like:","import spacy
from spacy import displacy

doc = nlp(""Reliance is looking at buying U.K. based analytics startup for $7 billion"")
displacy.render(doc, style=""dep"" , jupyter=True)",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
2.3 Dependency Parsing Dependency parsing is the process of extracting the dependency parse of a sentence to represent its grammatical structure. It defines the dependency relationship between headwords and their dependents. The head of a sentence has no dependency and is called the root of the sentence. The verb is usually the head of the sentence. All other words are linked to the headword.The dependencies can be mapped in a directed graph representation: Words are the nodes. The grammatical relationships are the edges. Dependency parsing helps you know what role a word plays in the text and how different words relate to each other. It s also used in shallow parsing and named entity recognition.Here s how you can use dependency parsing to see the relationships between words: Performing dependency parsing is again pretty easy in spaCy. We will use the same sentence here that we used for POS tagging:,import spacy ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Create an nlp object,"doc = nlp(""Reliance is looking at buying U.K. based analytics startup for $7 billion"") ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Iterate over the tokens,for token in doc : ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Print the token and its part of speech tag," print(token.text , ""-->"" , token.dep_) ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
The dependency tag ROOT denotes the main verb or action in the sentence. The other words are directly or indirectly connected to the ROOT word of the sentence. You can find out what other tags stand for by executing the code below:,"spacy.explain(""nsubj""), spacy.explain(""ROOT""), spacy.explain(""aux""), spacy.explain(""advcl""), spacy.explain(""dobj"")",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"2.4 Lemmatization Lemmatization is the process of reducing inflected forms of a word while still ensuring that the reduced form belongs to the language. This reduced form or root word is called a lemma.For example, organizes, organized and organizing are all forms of organize. Here, organize is the lemma. The inflection of a word allows you to express different grammatical categories like tense organized vs organize , number trains vs train , and so on. Lemmatization is necessary because it helps you reduce the inflected forms of a word so that they can be analyzed as a single item. It can also help you normalize the text.spaCy has the attribute lemma on the Token class. This attribute has the lemmatized form of a token:",import spacy ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Create an nlp object,"doc = nlp(""Reliance is looking at buying U.K. based analytics startup for $7 billion"") ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Iterate over the tokens,for token in doc : ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Print the token and its part of speech tag," print(token.text , ""-->"" , token.lemma_) ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"2.5 Sentence Boundary Detection SBD Sentence Boundary Detection is the process of locating the start and end of sentences in a given text. This allows you to you divide a text into linguistically meaningful units. You ll use these units when you re processing your text to perform tasks such as part of speech tagging and entity extraction.In spaCy, the sents property is used to extract sentences. Here s how you would extract the total number of sentences and the sentences for a given input text:",import spacy ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Create an nlp object,"doc = nlp(""Reliance is looking at buying U.K. based analytics startup for $7 billion.This is India.India is great"") ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"2.6 Named Entity Recognition NER A named entity is a real world object that s assigned a name for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn t always work perfectly and might need some tuning later, depending on your use case.Named entities are available as the ents property of a Doc:","import spacy

nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""Reliance is looking at buying U.K. based analytics startup for $7 billion"")

for ent in doc.ents:
 print(ent.text, ent.start_char, ent.end_char, ent.label_)",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"2.7 Entity Detection Entity detection, also called entity recognition, is a more advanced form of language processing that identifies important elements like places, people, organizations, and languages within an input string of text. This is really helpful for quickly extracting information from text, since you can quickly pick out important topics or indentify key sections of text.Let s try out some entity detection using a few paragraphs from this recent article in the Washington Post. We ll use .label to grab a label for each entity that s detected in the text, and then we ll take a look at these entities in a more visual format using spaCy s displaCy visualizer.","import spacy
from spacy import displacy
nlp = spacy.load(""en_core_web_sm"")
doc= nlp(u""""""The Amazon rainforest,[a] alternatively, the Amazon Jungle, also known in English as Amazonia, is a moist broadleaf tropical rainforest in the Amazon biome that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 km2 (2,700,000 sq mi), of which 5,500,000 km2 (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations.

The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Bolivia, Ecuador, French Guiana, Guyana, Suriname, and Venezuela. Four nations have ""Amazonas"" as the name of one of their first-level administrative regions and France uses the name ""Guiana Amazonian Park"" for its rainforest protected area. The Amazon represents over half of the planet's remaining rainforests,[2] and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.[3]

Etymology
The name Amazon is said to arise from a war Francisco de Orellana fought with the Tapuyas and other tribes. The women of the tribe fought alongside the men, as was their custom.[4] Orellana derived the name Amazonas from the Amazons of Greek mythology, described by Herodotus and Diodorus.[4]

History
See also: History of South America  Amazon, and Amazon River  History
Tribal societies are well capable of escalation to all-out wars between tribes. Thus, in the Amazonas, there was perpetual animosity between the neighboring tribes of the Jivaro. Several tribes of the Jivaroan group, including the Shuar, practised headhunting for trophies and headshrinking.[5] The accounts of missionaries to the area in the borderlands between Brazil and Venezuela have recounted constant infighting in the Yanomami tribes. More than a third of the Yanomamo males, on average, died from warfare.[6]"""""")

entities=[(i, i.label_, i.label) for i in doc.ents]
entities",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Using this technique, we can identify a variety of entities within the text. The spaCy documentation provides a full list of supported entity types, and we can see from the short example above that it s able to identify a variety of different entity types, including specific locations GPE , date related words DATE , important numbers CARDINAL , specific individuals PERSON , etc.Using displaCy we can also visualize our input text, with each identified entity highlighted by color and labeled. We ll use style ent to tell displaCy that we want to visualize entities here.","displacy.render(doc, style = ""ent"",jupyter = True)",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"2.8 Similarity Similarity is determined by comparing word vectors or word embeddings , multi dimensional meaning representations of a word. Word vectors can be generated using an algorithm like word2vec and usually look like this:Spacy also provides inbuilt integration of dense, real valued vectors representing distributional similarity information.Models that come with built in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalize vectors.","import spacy

nlp = spacy.load(""en_core_web_lg"")
tokens = nlp(""dog cat banana afskfsd"")

for token in tokens:
 print(token.text, token.has_vector, token.vector_norm, token.is_oov)",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"The words dog , cat and banana are all pretty common in English, so they re part of the model s vocabulary, and come with a vector. The word afskfsd on the other hand is a lot less common and out of vocabulary so its vector representation consists of 300 dimensions of 0, which means it s practically nonexistent. If your application will benefit from a large vocabulary with more vectors, you should consider using one of the larger models or loading in a full vector package, for example, en vectors web lg, which includes over 1 million unique vectors.spaCy is able to compare two objects, and make a prediction of how similar they are. Predicting similarity is useful for building recommendation systems or flagging duplicates. For example, you can suggest a user content that s similar to what they re currently looking at, or label a support ticket as a duplicate if it s very similar to an already existing one.Each Doc, Span and Token comes with a .similarity method that lets you compare it with another object, and determine the similarity. Of course similarity is always subjective whether dog and cat are similar really depends on how you re looking at it. spaCy s similarity model usually assumes a pretty general purpose definition of similarity.",import spacy ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
make sure to use larger model!,"nlp = spacy.load(""en_core_web_lg"") ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"2.9 Text Classification Assigning categories or labels to a whole document, or parts of a document. Text is an extremely rich source of information. Each minute, people send hundreds of millions of new emails and text messages. There s a veritable mountain of text data waiting to be mined for insights. But data scientists who want to glean meaning from all of that text data face a challenge: it is difficult to analyze and process because it exists in unstructured form.Quite often, we may find ourselves with a set of text data that we d like to classify according to some parameters perhaps the subject of each snippet, for example and text classification is what will help us to do this.The diagram below illustrates the big picture view of what we want to do when classifying text. First, we extract the features we want from our source text and any tags or metadata it came with , and then we feed our cleaned data into a machine learning algorithm that do the classification for us. We ll start by importing the libraries we ll need for this task. We ve already imported spaCy, but we ll also want pandas and scikit learn to help with our analysis.We will use a real world data set this set of Amazon Alexa product reviews.This data set comes as a tab separated file .tsv . It has has five columns: rating, date, variation, verified reviews, feedback.rating denotes the rating each user gave the Alexa out of 5 . date indicates the date of the review, and variation describes which model the user reviewed. verified reviews contains the text of each review, and feedback contains a sentiment label, with 1 denoting positive sentiment the user liked it and 0 denoting negative sentiment the user didn t .This dataset has consumer reviews of amazon Alexa products like Echos, Echo Dots, Alexa Firesticks etc. What we re going to do is develop a classification model that looks at the review text and predicts whether a review is positive or negative. Since this data set already includes whether a review is positive or negative in the feedback column, we can use those answers to train and test our model. Our goal here is to produce an accurate model that we could then use to process new user reviews and quickly determine whether they were positive or negative.Let s start by reading the data into a pandas dataframe and then using the built in functions of pandas to help us take a closer look at our data.",import pandas as pd ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Loading TSV file,"df_amazon = pd.read_csv(""../input/amazon-alexa-reviews/amazon_alexa.tsv"" , sep = ""\t"") ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Shape of dataframe,df_amazon.shape ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
View data information,df_amazon.info () ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Feedback Value count,df_amazon.feedback.value_counts () ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Let s create a custom tokenizer function using spaCy. We ll use this function to automatically strip information we don t need, like stopwords and punctuation, from each review.We ll start by importing the English models we need from spaCy, as well as Python s string module, which contains a helpful list of all punctuation marks that we can use in string.punctuation. We ll create variables that contain the punctuation marks and stopwords we want to remove, and a parser that runs input through spaCy s English module.Then, we ll create a spacy tokenizer function that accepts a sentence as input and processes the sentence into tokens, performing lemmatization, lowercasing, and removing stop words. This is similar to what we did in the examples earlier in this tutorial, but now we re putting it all together into a single function for preprocessing each user review we re analyzing.",import string ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Create our list of punctuation marks,punctuations = string.punctuation ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Create our list of stopwords,nlp = spacy.load('en') ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Load English tokenizer, tagger, parser, NER and word vectors",parser = English () ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Creating our tokenizer function,def spacy_tokenizer(sentence): ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Creating our token object, which is used to create documents with linguistic annotations.", mytokens = parser(sentence) ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Lemmatizing each token and converting each token into lowercase," mytokens =[word.lemma_.lower (). strip ()if word.lemma_ != ""-PRON-"" else word.lower_ for word in mytokens] ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Removing stop words, mytokens =[word for word in mytokens if word not in stop_words and word not in punctuations] ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
return preprocessed list of tokens, return mytokens ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Custom transformer using spaCy,class predictors(TransformerMixin): ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Cleaning Text, return[clean_text(text)for text in X] ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Basic function to clean the text,def clean_text(text): ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Removing spaces and converting text into lowercase, return text.strip (). lower () ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"When we classify text, we end up with text snippets matched with their respective labels. But we can t simply use text strings in our machine learning model we need a way to convert our text into something that can be represented numerically just like the labels 1 for positive and 0 for negative are. Classifying text in positive and negative labels is called sentiment analysis. So we need a way to represent our text numerically.One tool we can use for doing this is called Bag of Words. BoW converts text into the matrix of occurrence of words within a given document. It focuses on whether given words occurred or not in the document, and it generates a matrix that we might see referred to as a BoW matrix or a document term matrix.We can generate a BoW matrix for our text data by using scikit learn s CountVectorizer. In the code below, we re telling CountVectorizer to use the custom spacy tokenizer function we built as its tokenizer, and defining the ngram range we want.N grams are combinations of adjacent words in a given text, where n is the number of words that incuded in the tokens. for example, in the sentence Who will win the football world cup in 2022? unigrams would be a sequence of single words such as who , will , win and so on. Bigrams would be a sequence of 2 contiguous words such as who will , will win , and so on. So the ngram range parameter we ll use in the code below sets the lower and upper bounds of the our ngrams we ll be using unigrams . Then we ll assign the ngrams to bow vector.","bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"We ll also want to look at the TF IDF Term Frequency Inverse Document Frequency for our terms. This sounds complicated, but it s simply a way of normalizing our Bag of Words BoW by looking at each word s frequency in comparison to the document frequency. In other words, it s a way of representing how important a particular term is in the context of a given document, based on how many times the term appears and how many other documents that same term appears in. The higher the TF IDF, the more important that term is to that document.We can represent this with the following mathematical equation:idf W log documents documents containing W Of course, we don t have to calculate that by hand! We can generate TF IDF automatically using scikit learn s TfidfVectorizer. Again, we ll tell it to use the custom tokenizer that we built with spaCy, and then we ll assign the result to the variable tfidf vector.",tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer),knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"We re trying to build a classification model, but we need a way to know how it s actually performing. Dividing the dataset into a training set and a test set the tried and true method for doing this. We ll use half of our data set as our training set, which will include the correct answers. Then we ll test our model using the other half of the data set without giving it the answers, to see how accurately it performs.",from sklearn.model_selection import train_test_split ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
the features we want to analyze,X = df_amazon['verified_reviews'] ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"the labels, or answers, we want to test against",ylabels = df_amazon['feedback'] ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Logistic Regression Classifier,from sklearn.linear_model import LogisticRegression ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Let s take a look at how our model actually performs! We can do this using the metrics module from scikit learn. Now that we ve trained our model, we ll put our test data through the pipeline to come up with predictions. Then we ll use various functions of the metrics module to look at our model s accuracy, precision, and recall. Accuracy refers to the percentage of the total predictions our model makes that are completely correct. Precision describes the ratio of true positives to true positives plus false positives in our predictions. Recall describes the ratio of true positives to true positives plus false negatives in our predictions. ",from sklearn import metrics ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Predicting with a test dataset,predicted = pipe.predict(X_test) ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Model Accuracy,"print(""Logistic Regression Accuracy:"" , metrics.accuracy_score(y_test , predicted)) ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Natural Language Processing using NLTK1. Introduction to NLTKThe NLTK module is a massive tool kit, aimed at helping with the entire Natural Language Processing NLP methodology. NLTK will aid with everything from splitting sentences from paragraphs, splitting up words, recognizing the part of speech of those words, highlighting the main subjects, and then even with helping machine to understand what the text is all about. In this tutorial, we re going to tackle the field of opinion mining, or sentiment analysis.In our path to learning how to do sentiment analysis with NLTK, we re going to learn the following: Tokenizing Splitting sentences and words from the body of text. Part of Speech tagging Machine Learning with the Naive Bayes classifier How to tie in Scikit learn sklearn with NLTK Training classifiers with datasets Performing live, streaming, sentiment analysis with Twitter. ...and much more.In order to get started, you are going to need the NLTK module, as well as Python.",import nltk,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
" Tokenizing Words SentencesTokenization is the process of breaking up the given text into units called tokens. The tokens may be words or number or punctuation mark or even sentences. Tokenization does this task by locating word boundaries. Ending point of a word and beginning of the next word is called word boundaries. Tokenization is also known as word segmentation. Challenges in tokenization depends on the type of language. Languages such as English and French are referred to as space delimited as most of the words are separated from each other by white spaces. Languages such as Chinese and Thai are referred to as unsegmented as words do not have clear boundaries. Tokenising unsegmented language sentences requires additional lexical and morphological information. Tokenization is also affected by writing system and the typographical structure of the words. Structures of languges can be grouped into three categories: Isolating: Words do not divide into smaller units. Example: Mandarin Chinese Agglutinative: Words divide into smaller units. Example: Japanese, Tamil Inflectional: Boundaries between morphemes are not clear and ambiguous in terms of grammatical meaning. Example: Latin. Let us understand some more basic terminology. What is Corpora? It is a body of text e.g Medical journal, Presidential speech, English language What is Lexicon? Lexicon is nothing but words and their means .E.g Investor speak vs. Regular English speaki.e Investor talk about BULL as some stock going positive in the market which bullish as to the regular word of BULL describing the usual animal. So in simple for now let us look at Word Tokenizer and Sentence Tokenizer using NLTK. ","from nltk.tokenize import sent_tokenize,word_tokenize",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
As you can see that sentence tokenizer did split the above example text into seperate sentences.Now let us look at word tokenizer below,print(word_tokenize(example_text)),knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
" StopwordsStop words are natural language words which have very little meaning, such as and , the , a , an , and similar words.Basically during the pre processing of natural language text we eliminate the stopwords as they are redundant and do not convey any meaning insight in the data. ",from nltk.corpus import stopwords,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Now let set the stopwords for english language.Let us see what are all the stopwords in english,"stop_words = set(stopwords.words(""english""))
print(stop_words)",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Now let us tokenize the sample text and filter the sentence by removing the stopwords from it .,"example_text = ""Hi Mr.Pavan , How are you doing today?. Cool you got a nice job at IBM. Wow thats an awesome car. Weather is great.""
words = word_tokenize(example_text)
filtered_sentence = []
for w in words:
 if w not in stop_words:
 filtered_sentence.append(w)
print(filtered_sentence) ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
" Stemming Words Stemming is the process of reducing infected or derived words to their word stem,base or root form. It basically affixes to suffixes and prefixes or to the roots of words known as a lemma.It is also a preprocessing step in natural language processing.Examples: Words like organise, organising ,organisation the root of its stem is organis. intelligence,intelligently the root of its stem is intelligenSo stemming produces intermediate representation of the word which may not have any meaning.In this case intelligen has no meaning.The idea of stemming is a sort of normalizing method. Many variations of words carry the same meaning, other than when tense is involved.The reason why we stem is to shorten the lookup, and normalize sentences.One of the most popular stemming algorithms is the Porter stemmer, which has been around since 1979.First, we re going to grab and define our stemmer: ",from nltk.stem import PorterStemmer,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
" Lemmatization It is same as stemming process but the intermediate representation root has a meaning.It is also a preprocessing step in natural language processing.Examples: Words like going ,goes,gone when we do lemmatization we get go intelligence,intelligently when we do lemmatization we get intelligent .So lemmatization produces intermediate representation of the word which has a meaning.In this case intelligent has meaning. ",from nltk.stem import WordNetLemmatizer,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
" Part of Speech TaggingOne of the more powerful aspects of the NLTK is the Part of Speech tagging that it can do. This means labeling words in a sentence as nouns, adjectives, verbs...etc. Even more impressive, it also labels by tense, and more. Here s a list of the tags, what they mean, and some examples:POS tag list: CC coordinating conjunction CD cardinal digit DT determiner EX existential there like: there is ... think of it like there exists FW foreign word IN preposition subordinating conjunction JJ adjective big JJR adjective, comparative bigger JJS adjective, superlative biggest LS list marker 1 MD modal could, will NN noun, singular desk NNS noun plural desks NNP proper noun, singular Harrison NNPS proper noun, plural Americans PDT predeterminer all the kids POS possessive ending parent s PRP personal pronoun I, he, she PRPdollar possessive pronoun my, his, hers RB adverb very, silently, RBR adverb, comparative better RBS adverb, superlative best RP particle give up TO to go to the store. UH interjection errrrrrrrm VB verb, base form take VBD verb, past tense took VBG verb, gerund present participle taking VBN verb, past participle taken VBP verb, sing. present, non 3d take VBZ verb, 3rd person sing. present takes WDT wh determiner which WP wh pronoun who, what WPdollar possessive wh pronoun whose WRB wh abverb where, when Now let us use a new sentence tokenizer, called the PunktSentenceTokenizer. This tokenizer is capable of unsupervised machine learning, so we can actually train it on any body of text that we use. ",from nltk.tokenize import PunktSentenceTokenizer ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Now, let s create our training and testing data:","train_txt = ""Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes Tomistoma, is not used in this article. The term crocodile here applies to only the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes the alligators and caimans (family Alligatoridae), the gharial and false gharial (family Gavialidae), and all other living and fossil Crocodylomorpha."" ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Next, we can train the Punkt tokenizer like:",cust_tokenizer = PunktSentenceTokenizer(train_txt) ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Then we can actually tokenize, using:",tokenized = cust_tokenizer.tokenize(sample_text) ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
Now we can finish up this part of speech tagging script by creating a function that will run through and tag all of the parts of speech per sentence like so:,"print(""Speech Tagging Output"")
def process_text():
 try:
 for i in tokenized:
 words = nltk.word_tokenize(i)
 tagged = nltk.pos_tag(words)
 print(tagged)

 except Exception as e:
 print(str(e))

process_text()",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
" ChunkingNow that we know the parts of speech, we can do what is called chunking, and group words into hopefully meaningful chunks. One of the main goals of chunking is to group into what are known as noun phrases. These are phrases of one or more words that contain a noun, maybe some descriptive words, maybe a verb, and maybe something like an adverb. The idea is to group nouns with the words that are in relation to them.In order to chunk, we combine the part of speech tags with regular expressions. Mainly from regular expressions, we are going to utilize the following: match 1 or more ? match 0 or 1 repetitions. match 0 or MORE repetitions . Any character except a new lineThe last things to note is that the part of speech tags are denoted with the and and we can also place regular expressions within the tags themselves, so account for things like all nouns Let us take the same code from the above Speech Tagging section and modify it to include chunking for noun plural NNS and adjective JJ ",from nltk.tokenize import PunktSentenceTokenizer ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Now, let s create our training and testing data:","train_txt = ""Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes Tomistoma, is not used in this article. The term crocodile here applies to only the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes the alligators and caimans (family Alligatoridae), the gharial and false gharial (family Gavialidae), and all other living and fossil Crocodylomorpha."" ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Next, we can train the Punkt tokenizer like:",cust_tokenizer = PunktSentenceTokenizer(train_txt) ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Then we can actually tokenize, using:",tokenized = cust_tokenizer.tokenize(sample_text) ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
chunked.draw , print(chunked) ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
" ChinkingYou may find that, after a lot of chunking, you have some words in your chunk you still do not want, but you have no idea how to get rid of them by chunking. You may find that chinking is your solution.Chinking is a lot like chunking, it is basically a way for you to remove a chunk from a chunk. The chunk that you remove from your chunk is your chink.The code is very similar, you just denote the chink, after the chunk, with instead of the chunk s ",from nltk.tokenize import PunktSentenceTokenizer ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Now, let s create our training and testing data:","train_txt = ""Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes Tomistoma, is not used in this article. The term crocodile here applies to only the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes the alligators and caimans (family Alligatoridae), the gharial and false gharial (family Gavialidae), and all other living and fossil Crocodylomorpha."" ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Next, we can train the Punkt tokenizer like:",cust_tokenizer = PunktSentenceTokenizer(train_txt) ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Then we can actually tokenize, using:",tokenized = cust_tokenizer.tokenize(sample_text) ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
" Named Entity RecognitionOne of the most major forms of chunking in natural language processing is called Named Entity Recognition. The idea is to have the machine immediately be able to pull out entities like people, places, things, locations, monetary figures, and more.This can be a bit of a challenge, but NLTK is this built in for us. There are two major options with NLTK s named entity recognition: either recognize all named entities, or recognize named entities as their respective type, like people, places, locations, etc. ",from nltk.tokenize import PunktSentenceTokenizer ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Now, let s create our training and testing data:","train_txt = ""Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes Tomistoma, is not used in this article. The term crocodile here applies to only the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes the alligators and caimans (family Alligatoridae), the gharial and false gharial (family Gavialidae), and all other living and fossil Crocodylomorpha."" ",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Next, we can train the Punkt tokenizer like:",cust_tokenizer = PunktSentenceTokenizer(train_txt) ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Then we can actually tokenize, using:",tokenized = cust_tokenizer.tokenize(sample_text) ,knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
" The CorporaThe NLTK corpus is a massive dump of all kinds of natural language data sets that are definitely worth taking a look at.Almost all of the files in the NLTK corpus follow the same rules for accessing them by using the NLTK module, but nothing is magical about them. These files are plain text files for the most part, some are XML and some are other formats, but they are all accessible by manual, or via the module and Python. Let s talk about viewing them manually. ","import nltk
from nltk.tokenize import sent_tokenize
from nltk.corpus import gutenberg
sample = gutenberg.raw(""bible-kjv.txt"")
tok = sent_tokenize(sample)
print(tok[5:15])",knowledge-graph-nlp-tutorial-bert-spacy-nltk.ipynb
"Welcome to Time Series!Forecasting is perhaps the most common application of machine learning in the real world. Businesses forecast product demand, governments forecast economic and population growth, meteorologists forecast the weather. The understanding of things to come is a pressing need across science, government, and industry not to mention our personal lives! , and practitioners in these fields are increasingly applying machine learning to address this need.Time series forecasting is a broad field with a long history. This course focuses on the application of modern machine learning methods to time series data with the goal of producing the most accurate predictions. The lessons in this course were inspired by winning solutions from past Kaggle forecasting competitions but will be applicable whenever accurate forecasts are a priority.After finishing this course, you ll know how to: engineer features to model the major time series components trends, seasons, and cycles , visualize time series with many kinds of time series plots, create forecasting hybrids that combine the strengths of complementary models, and adapt machine learning methods to a variety of forecasting tasks.As part of the exercises, you ll get a chance to participate in our Store Sales Time Series Forecasting Getting Started competition. In this competition, you re tasked with forecasting sales for Corporacin Favorita a large Ecuadorian based grocery retailer in almost 1800 product categories.What is a Time Series?The basic object of forecasting is the time series, which is a set of observations recorded over time. In forecasting applications, the observations are typically recorded with a regular frequency, like daily or monthly.","
import pandas as pd

df = pd.read_csv(
 ""../input/ts-course-data/book_sales.csv"",
 index_col='Date',
 parse_dates=['Date'],
).drop('Paperback', axis=1)

df.head()",linear-regression-with-time-series.ipynb
"This series records the number of hardcover book sales at a retail store over 30 days. Notice that we have a single column of observations Hardcover with a time index Date.Linear Regression with Time SeriesFor the first part of this course, we ll use the linear regression algorithm to construct forecasting models. Linear regression is widely used in practice and adapts naturally to even complex forecasting tasks.The linear regression algorithm learns how to make a weighted sum from its input features. For two features, we would have:target weight 1 feature 1 weight 2 feature 2 biasDuring training, the regression algorithm learns values for the parameters weight 1, weight 2, and bias that best fit the target. This algorithm is often called ordinary least squares since it chooses values that minimize the squared error between the target and the predictions. The weights are also called regression coefficients and the bias is also called the intercept because it tells you where the graph of this function crosses the y axis.Time step featuresThere are two kinds of features unique to time series: time step features and lag features.Time step features are features we can derive directly from the time index. The most basic time step feature is the time dummy, which counts off time steps in the series from beginning to end.","
import numpy as np

df['Time'] = np.arange(len(df.index))

df.head()",linear-regression-with-time-series.ipynb
"Linear regression with the time dummy produces the model:target weight time biasThe time dummy then lets us fit curves to time series in a time plot, where Time forms the x axis.","
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use(""seaborn-whitegrid"")
plt.rc(
 ""figure"",
 autolayout=True,
 figsize=(11, 4),
 titlesize=18,
 titleweight='bold',
)
plt.rc(
 ""axes"",
 labelweight=""bold"",
 labelsize=""large"",
 titleweight=""bold"",
 titlesize=16,
 titlepad=10,
)
%config InlineBackend.figure_format = 'retina'

fig, ax = plt.subplots()
ax.plot('Time', 'Hardcover', data=df, color='0.75')
ax = sns.regplot(x='Time', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_title('Time Plot of Hardcover Sales');",linear-regression-with-time-series.ipynb
"Time step features let you model time dependence. A series is time dependent if its values can be predicted from the time they occured. In the Hardcover Sales series, we can predict that sales later in the month are generally higher than sales earlier in the month.Lag featuresTo make a lag feature we shift the observations of the target series so that they appear to have occured later in time. Here we ve created a 1 step lag feature, though shifting by multiple steps is possible too.","
df['Lag_1'] = df['Hardcover'].shift(1)
df = df.reindex(columns=['Hardcover', 'Lag_1'])

df.head()",linear-regression-with-time-series.ipynb
Linear regression with a lag feature produces the model:target weight lag biasSo lag features let us fit curves to lag plots where each observation in a series is plotted against the previous observation.,"
fig, ax = plt.subplots()
ax = sns.regplot(x='Lag_1', y='Hardcover', data=df, ci=None, scatter_kws=dict(color='0.25'))
ax.set_aspect('equal')
ax.set_title('Lag Plot of Hardcover Sales');",linear-regression-with-time-series.ipynb
"You can see from the lag plot that sales on one day Hardcover are correlated with sales from the previous day Lag 1 . When you see a relationship like this, you know a lag feature will be useful.More generally, lag features let you model serial dependence. A time series has serial dependence when an observation can be predicted from previous observations. In Hardcover Sales, we can predict that high sales on one day usually mean high sales the next day.Adapting machine learning algorithms to time series problems is largely about feature engineering with the time index and lags. For most of the course, we use linear regression for its simplicity, but these features will be useful whichever algorithm you choose for your forecasting task.Example Tunnel TrafficTunnel Traffic is a time series describing the number of vehicles traveling through the Baregg Tunnel in Switzerland each day from November 2003 to November 2005. In this example, we ll get some practice applying linear regression to time step features and lag features.The hidden cell sets everything up.",from pathlib import Path ,linear-regression-with-time-series.ipynb
ignore warnings to clean up output cells,"simplefilter(""ignore"") ",linear-regression-with-time-series.ipynb
Set Matplotlib defaults,"plt.style.use(""seaborn-whitegrid"") ",linear-regression-with-time-series.ipynb
"Time step featureProvided the time series doesn t have any missing dates, we can create a time dummy by counting out the length of the series.","df = tunnel.copy()

df['Time'] = np.arange(len(tunnel.index))

df.head()",linear-regression-with-time-series.ipynb
The procedure for fitting a linear regression model follows the standard steps for scikit learn.,from sklearn.linear_model import LinearRegression ,linear-regression-with-time-series.ipynb
features,"X = df.loc[: ,['Time']] ",linear-regression-with-time-series.ipynb
target,"y = df.loc[: , 'NumVehicles'] ",linear-regression-with-time-series.ipynb
Train the model,model = LinearRegression () ,linear-regression-with-time-series.ipynb
the training data,"y_pred = pd.Series(model.predict(X), index = X.index) ",linear-regression-with-time-series.ipynb
The model actually created is approximately : Vehicles 22.5 Time 98176. Plotting the fitted values over time shows us how fitting linear regression to the time dummy creates the trend line defined by this equation.,"
ax = y.plot(**plot_params)
ax = y_pred.plot(ax=ax, linewidth=3)
ax.set_title('Time Plot of Tunnel Traffic');",linear-regression-with-time-series.ipynb
"Lag featurePandas provides us a simple method to lag a series, the shift method.","df['Lag_1'] = df['NumVehicles'].shift(1)
df.head()",linear-regression-with-time-series.ipynb
"When creating lag features, we need to decide what to do with the missing values produced. Filling them in is one option, maybe with 0.0 or backfilling with the first known value. Instead, we ll just drop the missing values, making sure to also drop values in the target from corresponding dates.",from sklearn.linear_model import LinearRegression ,linear-regression-with-time-series.ipynb
drop missing values in the feature set,X.dropna(inplace = True) ,linear-regression-with-time-series.ipynb
create the target,"y = df.loc[: , 'NumVehicles'] ",linear-regression-with-time-series.ipynb
drop corresponding values in target,"y , X = y.align(X , join = 'inner') ",linear-regression-with-time-series.ipynb
The lag plot shows us how well we were able to fit the relationship between the number of vehicles one day and the number the previous day.,"
fig, ax = plt.subplots()
ax.plot(X['Lag_1'], y, '.', color='0.25')
ax.plot(X['Lag_1'], y_pred)
ax.set_aspect('equal')
ax.set_ylabel('NumVehicles')
ax.set_xlabel('Lag_1')
ax.set_title('Lag Plot of Tunnel Traffic');",linear-regression-with-time-series.ipynb
What does this prediction from a lag feature mean about how well we can predict the series across time? The following time plot shows us how our forecasts now respond to the behavior of the series in the recent past.,"
ax = y.plot(**plot_params)
ax = y_pred.plot()",linear-regression-with-time-series.ipynb
"Data Augmentation in batch form and running on GPU TPU.This notebook implements a batch form of Chris Deotte s great data augmention in Rotation Augmentation GPU TPU 0.96 .It also shows how to perform data augmentation on GPU TPU directly. Important Although running data augmentation directly on GPU TPU is faster, it loses the advantage of separating the data processing on CPU and model training on GPU TPU. Therefore, we don t encourage the idea of using tf.keras.layers.Layer for data augmentation shown in this kernel. However, the batch implementation is still beneficial when doing data augmentation on CPU by using tf.data.Dataset. In order to make the measurement of timing, we measure it with a dummy layer which does the data processing in GPU TPU, but return dummy tensor. This is to avoid the overhead of sending large tensor back to CPU. ","import math, re, os
import tensorflow as tf
import tensorflow.keras.backend as K
import numpy as np
from matplotlib import pyplot as plt
from kaggle_datasets import KaggleDatasets
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
import datetime
import tqdm
print(""Tensorflow version "" + tf.__version__)
AUTO = tf.data.experimental.AUTOTUNE",make-chris-deotte-s-data-augmentation-faster.ipynb
"Detect hardware, return appropriate distribution strategy",try : ,make-chris-deotte-s-data-augmentation-faster.ipynb
TPU detection. No parameters necessary if TPU NAME environment variable is set. On Kaggle this is always the case., tpu = tf.distribute.cluster_resolver.TPUClusterResolver () ,make-chris-deotte-s-data-augmentation-faster.ipynb
default distribution strategy in Tensorflow. Works on CPU and single GPU., strategy = tf.distribute.get_strategy () ,make-chris-deotte-s-data-augmentation-faster.ipynb
Configuration,"IMAGE_SIZE =[512 , 512] ",make-chris-deotte-s-data-augmentation-faster.ipynb
Data access,GCS_DS_PATH = KaggleDatasets (). get_gcs_path('tpu-getting-started') ,make-chris-deotte-s-data-augmentation-faster.ipynb
Dataset Functions From starter kernel 1 ,def decode_image(image_data): ,make-chris-deotte-s-data-augmentation-faster.ipynb
"convert image to floats in 0, 1 range"," image = tf.cast(image , tf.float32)/ 255.0 ",make-chris-deotte-s-data-augmentation-faster.ipynb
explicit size needed for TPU," image = tf.reshape(image ,[* IMAGE_SIZE , 3]) ",make-chris-deotte-s-data-augmentation-faster.ipynb
"Data Augmentation The following code does random rotations, shear, zoom, and shift using the GPU TPU. When an image gets moved away from an edge revealing blank space, the blank space is filled by stretching the colors on the original edge. Change the variables in function transform below to control the desired amount of augmentation. Here s a diagram illustrating the mathematics.","def get_mat(rotation , shear , height_zoom , width_zoom , height_shift , width_shift): ",make-chris-deotte-s-data-augmentation-faster.ipynb
CONVERT DEGREES TO RADIANS, rotation = math.pi * rotation / 180. ,make-chris-deotte-s-data-augmentation-faster.ipynb
ROTATION MATRIX, c1 = tf.math.cos(rotation) ,make-chris-deotte-s-data-augmentation-faster.ipynb
SHEAR MATRIX, c2 = tf.math.cos(shear) ,make-chris-deotte-s-data-augmentation-faster.ipynb
ZOOM MATRIX," zoom_matrix = tf.reshape(tf.concat ([one / height_zoom , zero , zero , zero , one / width_zoom , zero , zero , zero , one], axis = 0),[3 , 3]) ",make-chris-deotte-s-data-augmentation-faster.ipynb
SHIFT MATRIX," shift_matrix = tf.reshape(tf.concat ([one , zero , height_shift , zero , one , width_shift , zero , zero , one], axis = 0),[3 , 3]) ",make-chris-deotte-s-data-augmentation-faster.ipynb
"output image randomly rotated, sheared, zoomed, and shifted", DIM = IMAGE_SIZE[0] ,make-chris-deotte-s-data-augmentation-faster.ipynb
fix for size 331, XDIM = DIM % 2 ,make-chris-deotte-s-data-augmentation-faster.ipynb
GET TRANSFORMATION MATRIX," m = get_mat(rot , shr , h_zoom , w_zoom , h_shift , w_shift) ",make-chris-deotte-s-data-augmentation-faster.ipynb
LIST DESTINATION PIXEL INDICES," x = tf.repeat(tf.range(DIM // 2 , - DIM // 2 , - 1), DIM) ",make-chris-deotte-s-data-augmentation-faster.ipynb
ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS," idx2 = K.dot(m , tf.cast(idx , dtype = 'float32')) ",make-chris-deotte-s-data-augmentation-faster.ipynb
FIND ORIGIN PIXEL VALUES," idx3 = tf.stack ([DIM // 2 - idx2[0 ,], DIM // 2 - 1 + idx2[1 ,]]) ",make-chris-deotte-s-data-augmentation-faster.ipynb
Measure timing,"def get_training_dataset(dataset , batch_size = None , do_aug = True , advanced_aug = True , repeat = True , with_labels = True , drop_remainder = False): ",make-chris-deotte-s-data-augmentation-faster.ipynb
the training dataset must repeat for several epochs, dataset = dataset.repeat () ,make-chris-deotte-s-data-augmentation-faster.ipynb
prefetch next batch while training autotune prefetch buffer size , dataset = dataset.prefetch(AUTO) ,make-chris-deotte-s-data-augmentation-faster.ipynb
Timing without any data augmentation,"n_iter = 3
dataset = load_dataset(TRAINING_FILENAMES, labeled=True)",make-chris-deotte-s-data-augmentation-faster.ipynb
Iterate over the whole training dataset," for image , labe in training_dataset : ",make-chris-deotte-s-data-augmentation-faster.ipynb
Timing with advanced Chris Deotte s data augumentation only,"training_dataset = get_training_dataset(dataset , do_aug = False , advanced_aug = True , repeat = 1 , with_labels = True) ",make-chris-deotte-s-data-augmentation-faster.ipynb
Iterate over the whole training dataset," for image , labe in training_dataset : ",make-chris-deotte-s-data-augmentation-faster.ipynb
Using GPU TPU,"def get_batch_transformatioin_matrix(rotation , shear , height_zoom , width_zoom , height_shift , width_shift): ",make-chris-deotte-s-data-augmentation-faster.ipynb
An example usage,"input_layer = tf.keras.layers.InputLayer(
 input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3), name='input_layer'
)

data_augmentation_layer = Data_Augmentation_Dummy()

with strategy.scope():
 
 model = tf.keras.Sequential([
 input_layer,
 data_augmentation_layer
 ])",make-chris-deotte-s-data-augmentation-faster.ipynb
Timing with advanced data augumentation run in GPU TPU,"training_dataset = get_training_dataset(dataset, do_aug=False, advanced_aug=False, repeat=1, with_labels=False, drop_remainder=True)

start = datetime.datetime.now()

 
for i in tqdm.tqdm(range(n_iter)):
 model.predict(training_dataset)
 
end = datetime.datetime.now()
elapsed = (end - start).total_seconds()
average = elapsed / n_iter
print(""Average timing for 1 iteration = {}"".format(average))",make-chris-deotte-s-data-augmentation-faster.ipynb
Check data augmentation effect,def batch_to_numpy_images_and_labels(data): ,make-chris-deotte-s-data-augmentation-faster.ipynb
"If no labels, only image IDs, return None for labels this is the case for test data ", return numpy_images ,make-chris-deotte-s-data-augmentation-faster.ipynb
Use actual Data Augmentation layer,"training_dataset = get_training_dataset(dataset , batch_size = 16 , do_aug = False , advanced_aug = False , repeat = 1 , with_labels = False) ",make-chris-deotte-s-data-augmentation-faster.ipynb
Original images,display_batch_of_images(images),make-chris-deotte-s-data-augmentation-faster.ipynb
Transformed images,display_batch_of_images(new_images),make-chris-deotte-s-data-augmentation-faster.ipynb
8.1. Loading Modules,"import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns
from datetime import timedelta
import warnings
from scipy import stats
import random
import os
import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
import plotly.express as px",mastering-bias-variance-tradeoff.ipynb
8.2. Configuration,"warnings.filterwarnings('ignore')
init_notebook_mode(connected=True)",mastering-bias-variance-tradeoff.ipynb
9.1. Lets Generate some data,"rows = 1000
random_x = np.random.randn(rows)

random_y = (((random_x ** 5)) + (-2 * (random_x ** 4)) -4.2 * (random_x**3) +20 * (random_x**2)+ 6 ** random_x +2).reshape(rows, 1)",mastering-bias-variance-tradeoff.ipynb
Adding noise,"mylist = []

for i in range(0,320):
 x = random.randint(25,100)
 mylist.append(x)",mastering-bias-variance-tradeoff.ipynb
"9.2. Visualizing Distributions of X,y","sns.set(rc={'figure.figsize':(15,5)})
for i, column in enumerate([random_x,random_y], 1):
 plt.subplot(1,2,i)
 sns.distplot(column,color='tomato',fit_kws={""color"":""indigo""},fit=stats.gamma, label=""label 1"")",mastering-bias-variance-tradeoff.ipynb
Hold out 20 of the dataset for training,"test_size = int(np.round(rows * 0.2 , 0)) ",mastering-bias-variance-tradeoff.ipynb
Split dataset into training and testing sets,x_train = random_x[: - test_size] ,mastering-bias-variance-tradeoff.ipynb
9.3. Plot the training set data,"fig , ax = plt.subplots(figsize =(12 , 7)) ",mastering-bias-variance-tradeoff.ipynb
removing to and right border,ax.spines['top']. set_visible(False) ,mastering-bias-variance-tradeoff.ipynb
adding major gridlines,"ax.grid(color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.8) ",mastering-bias-variance-tradeoff.ipynb
10.1 Developing model,"linear_regression_model = np.polyfit(x_train, y_train, deg=1)
linear_model_predictions = np.polyval(linear_regression_model, x_test)
linear_model_predictions_train = np.polyval(linear_regression_model, x_train)",mastering-bias-variance-tradeoff.ipynb
Plot linear regression line,"fig , ax = plt.subplots(figsize =(12 , 7)) ",mastering-bias-variance-tradeoff.ipynb
removing to and right border,ax.spines['top']. set_visible(False) ,mastering-bias-variance-tradeoff.ipynb
adding major gridlines,"ax.grid(color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.7) ",mastering-bias-variance-tradeoff.ipynb
11.1. Calculate Bias,"def get_bias(predicted_values, true_values):
 return np.round(np.mean((predicted_values - true_values) ** 2), 0)",mastering-bias-variance-tradeoff.ipynb
11.2. Calculate Variance,"def get_variance(values):
 return np.round(np.var(values), 0)",mastering-bias-variance-tradeoff.ipynb
11.3. Calculate Other Metrics,"def get_metrics(target_train, target_test, model_train_predictions, model_test_predictions):
 training_mse = mean_squared_error(target_train, model_train_predictions)
 test_mse = mean_squared_error(target_test, model_test_predictions)
 bias = get_bias(model_test_predictions, target_test)
 variance = get_variance(model_test_predictions)
 linear_regression_model = np.polyfit(x_train, y_train, deg=1)
 
 return [training_mse,test_mse,bias,variance]",mastering-bias-variance-tradeoff.ipynb
Predicting values for the test set,"linear_model_predictions = np.polyval(linear_regression_model , x_test) ",mastering-bias-variance-tradeoff.ipynb
Predicting values for the training set,"training_linear_model_predictions = np.polyval(linear_regression_model , x_train) ",mastering-bias-variance-tradeoff.ipynb
deg 2 for 2nd degree,"polynomial_2nd_model = np.polyfit(x_train , y_train , deg = 2) ",mastering-bias-variance-tradeoff.ipynb
Predicting values for the training set,"training_polynomial_2nd_predictions = np.polyval(polynomial_2nd_model , x_train) ",mastering-bias-variance-tradeoff.ipynb
12.2. Testing metrics,"polynomial_2nd_training_mse, polynomial_2nd_test_mse, polynomial_2nd_bias, polynomial_2nd_variance = get_metrics(y_train, y_test, training_polynomial_2nd_predictions, polynomial_2nd_predictions)

degree.append(2)
training_mse.append(polynomial_2nd_training_mse)
test_mse.append(polynomial_2nd_test_mse)
bias.append(polynomial_2nd_bias)
variance.append(polynomial_2nd_variance)

print('2nd degree polynomial')
print('Training MSE %0.f' % polynomial_2nd_training_mse)
print('Test MSE %0.f' % polynomial_2nd_test_mse)
print('Bias %0.f' % polynomial_2nd_bias)
print('Variance %0.f' % polynomial_2nd_variance)",mastering-bias-variance-tradeoff.ipynb
Plot 2nd degree polynomial,"fig , ax = plt.subplots(figsize =(12 , 7)) ",mastering-bias-variance-tradeoff.ipynb
removing to and right border,ax.spines['top']. set_visible(False) ,mastering-bias-variance-tradeoff.ipynb
Adding major gridlines,"ax.grid(color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.5) ",mastering-bias-variance-tradeoff.ipynb
deg 4 for 4th degree,"polynomial_4thdeg_model = np.polyfit(x_train , y_train , deg = 4) ",mastering-bias-variance-tradeoff.ipynb
Predicting values for the training set,"training_polynomial_4thdeg_predictions = np.polyval(polynomial_4thdeg_model , x_train) ",mastering-bias-variance-tradeoff.ipynb
13.2. Testing metrics,"polynomial_4thdeg_training_mse, polynomial_4thdeg_test_mse, polynomial_4thdeg_bias, polynomial_4thdeg_variance = get_metrics(y_train, y_test, training_polynomial_4thdeg_predictions, polynomial_4thdeg_predictions)

degree.append(4)
training_mse.append(polynomial_4thdeg_training_mse)
test_mse.append(polynomial_4thdeg_test_mse)
bias.append(polynomial_4thdeg_bias)
variance.append(polynomial_4thdeg_variance)

print('4th degree polynomial')
print('Training MSE %0.f' % polynomial_4thdeg_training_mse)
print('Test MSE %0.f' % polynomial_4thdeg_test_mse)
print('Bias %0.f' % polynomial_4thdeg_bias)
print('Variance %0.f' % polynomial_4thdeg_variance)",mastering-bias-variance-tradeoff.ipynb
13.3. Ploting,"fig , ax = plt.subplots(figsize =(12 , 7)) ",mastering-bias-variance-tradeoff.ipynb
removing to and right border,ax.spines['top']. set_visible(False) ,mastering-bias-variance-tradeoff.ipynb
Adding major gridlines,"ax.grid(color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.5) ",mastering-bias-variance-tradeoff.ipynb
14.1. Making Predictions,"polynomial_5thdeg_model = np.polyfit(x_train, y_train, deg=5)
p_5th = np.poly1d(polynomial_5thdeg_model.reshape(1, 6)[0])
polynomial_5thdeg_predictions = np.polyval(polynomial_5thdeg_model, x_test)
training_polynomial_5thdeg_predictions = np.polyval(polynomial_5thdeg_model, x_train)",mastering-bias-variance-tradeoff.ipynb
14.2. Testing metrics,"polynomial_5thdeg_training_mse, polynomial_5thdeg_test_mse, polynomial_5thdeg_bias, polynomial_5thdeg_variance = get_metrics(y_train, y_test, training_polynomial_5thdeg_predictions, polynomial_5thdeg_predictions)

degree.append(5)
training_mse.append(polynomial_5thdeg_training_mse)
test_mse.append(polynomial_5thdeg_test_mse)
bias.append(polynomial_5thdeg_bias)
variance.append(polynomial_5thdeg_variance)

print('5th degree polynomial')
print('Training MSE %0.f' % polynomial_5thdeg_training_mse)
print('Test MSE %0.f' % polynomial_5thdeg_test_mse)
print('Bias %0.f' % polynomial_5thdeg_bias)
print('Variance %0.f' % polynomial_5thdeg_variance)",mastering-bias-variance-tradeoff.ipynb
14.3. Ploting,"fig , ax = plt.subplots(figsize =(12 , 7)) ",mastering-bias-variance-tradeoff.ipynb
removing to and right border,ax.spines['top']. set_visible(False) ,mastering-bias-variance-tradeoff.ipynb
Adding major gridlines,"ax.grid(color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.1) ",mastering-bias-variance-tradeoff.ipynb
15.1. Making Predictions,"polynomial_8thdeg_model = np.polyfit(x_train, y_train, deg=8)
p_8th = np.poly1d(polynomial_5thdeg_model.reshape(1, 6)[0])
polynomial_8thdeg_predictions = np.polyval(polynomial_8thdeg_model, x_test)
training_polynomial_8thdeg_predictions = np.polyval(polynomial_8thdeg_model, x_train)",mastering-bias-variance-tradeoff.ipynb
15.2. Testing metrics,"polynomial_8thdeg_training_mse, polynomial_8thdeg_test_mse, polynomial_8thdeg_bias, polynomial_8thdeg_variance = get_metrics(y_train, y_test, training_polynomial_8thdeg_predictions, polynomial_8thdeg_predictions)

degree.append(8)
training_mse.append(polynomial_8thdeg_training_mse)
test_mse.append(polynomial_8thdeg_test_mse)
bias.append(polynomial_8thdeg_bias)
variance.append(polynomial_8thdeg_variance)

print('8th degree polynomial')
print('Training MSE %0.f' % polynomial_8thdeg_training_mse)
print('Test MSE %0.f' % polynomial_8thdeg_test_mse)
print('Bias %0.f' % polynomial_8thdeg_bias)
print('Variance %0.f' % polynomial_8thdeg_variance)",mastering-bias-variance-tradeoff.ipynb
15.3. Ploting,"fig , ax = plt.subplots(figsize =(12 , 7)) ",mastering-bias-variance-tradeoff.ipynb
removing to and right border,ax.spines['top']. set_visible(False) ,mastering-bias-variance-tradeoff.ipynb
Adding major gridlines,"ax.grid(color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.5) ",mastering-bias-variance-tradeoff.ipynb
removing to and right border,ax.spines['top']. set_visible(False) ,mastering-bias-variance-tradeoff.ipynb
Adding major gridlines,"ax.grid(color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.5) ",mastering-bias-variance-tradeoff.ipynb
 Ploting Metrics ,"lst = [degree,training_mse,test_mse,bias,variance]
metrics_df = pd.DataFrame(lst)
metrics_df=metrics_df.T
metrics_df.rename(columns = {0:""degree"",1:""training_mse"",2:""test_mse"",3:""bias"",4:""variance""}, inplace = True)
metrics_df",mastering-bias-variance-tradeoff.ipynb
16.1. Plotting MSEs,"plt.figure(figsize=(10,10))
trace1 = go.Scatter(x=metrics_df.degree,
 y=metrics_df.training_mse,
 name = ""Training"",
 line = dict(color = 'green'),
 opacity = 0.9,
 marker=dict(
 color='white',
 size=10,
 line=dict(
 width=5
 )))

trace2 = go.Scatter(x=metrics_df.degree,
 y=metrics_df.test_mse,
 name = ""Test"",
 line = dict(color = 'red'),
 opacity = 0.9,
 marker=dict(
 color='white',
 size=10,
 line=dict(
 width=5
 )))

layout2 = dict(title='Mean Squared Error',)

fig2 = dict(data=[trace1, trace2], layout=layout2)

iplot(fig2)",mastering-bias-variance-tradeoff.ipynb
16.2. Potting Bias,"plt.figure(figsize=(10,10))
Bias = go.Scatter(x=metrics_df.degree,
 y=metrics_df.bias,
 name = ""Bias"",
 line = dict(color = 'darkorange'),
 opacity = 1,
 marker=dict(
 color='white',
 size=10,
 line=dict(
 width=12
 )))

layout2 = dict(title='Bias',)

fig = dict(data=[Bias], layout=layout2)

iplot(fig)",mastering-bias-variance-tradeoff.ipynb
16.2. Potting Variance,"plt.figure(figsize=(10,10))
Variance = go.Scatter(x=metrics_df.degree,
 y=metrics_df.variance,
 name = ""Variance"",
 line = dict(color = 'indigo'),
 opacity = 1,
 marker=dict(
 color='white',
 size=10,
 line=dict(
 width=12
 )))

layout2 = dict(title='Variance',)

fig = dict(data=[Variance], layout=layout2)

iplot(fig)",mastering-bias-variance-tradeoff.ipynb
16.2. Potting Bias Variance,"layout = dict(title='Bias - Variance',)
fig = dict(data=[Bias, Variance], layout=layout)
iplot(fig)",mastering-bias-variance-tradeoff.ipynb
"IntroductionIn Lesson 2 we began our discussion of how the base in a convnet performs feature extraction. We learned about how the first two operations in this process occur in a Conv2D layer with relu activation.In this lesson, we ll look at the third and final operation in this sequence: condense with maximum pooling, which in Keras is done by a MaxPool2D layer.Condense with Maximum PoolingAdding condensing step to the model we had before, will give us this:",from tensorflow import keras ,maximum-pooling.ipynb
"A MaxPool2D layer is much like a Conv2D layer, except that it uses a simple maximum function instead of a kernel, with the pool size parameter analogous to kernel size. A MaxPool2D layer doesn t have any trainable weights like a convolutional layer does in its kernel, however.Let s take another look at the extraction figure from the last lesson. Remember that MaxPool2D is the Condense step. Notice that after applying the ReLU function Detect the feature map ends up with a lot of dead space, that is, large areas containing only 0 s the black areas in the image . Having to carry these 0 activations through the entire network would increase the size of the model without adding much useful information. Instead, we would like to condense the feature map to retain only the most useful part the feature itself.This in fact is what maximum pooling does. Max pooling takes a patch of activations in the original feature map and replaces them with the maximum activation in that patch. When applied after the ReLU activation, it has the effect of intensifying features. The pooling step increases the proportion of active pixels to zero pixels.Example Apply Maximum PoolingLet s add the condense step to the feature extraction we did in the example in Lesson 2. This next hidden cell will take us back to where we left off.",import tensorflow as tf ,maximum-pooling.ipynb
"We ll use another one of the functions in tf.nn to apply the pooling step, tf.nn.pool. This is a Python function that does the same thing as the MaxPool2D layer you use when model building, but, being a simple function, is easier to use directly.",import tensorflow as tf ,maximum-pooling.ipynb
"Meets Bags of PopcornThis is my first published notebook on Kaggle About NLP, So I decided ofc to take a look at the Bag of Words Meets Bags of Popcorn Compeition I will be doing a EDA of review texts, some Visualization and Pre Processing. and finally modelling Your feedback is very welcome If you find this notebook useful, please don t forget to upvote it! ","import numpy as np
import pandas as pd

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import re
import json
import matplotlib.pyplot as plt
import seaborn as sns
from bs4 import BeautifulSoup as bs
from nltk.corpus import stopwords
from wordcloud import WordCloud
from nltk.stem import WordNetLemmatizer

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report",meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
Getting to know data,print(train.shape) ,meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
Sample review,print(train['review'][ 0]) ,meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
join function can help merge all words into one string. means space can be a sep between words.,"cloud = WordCloud(width = 800 , height = 600). generate("" "".join(train['review'])) ",meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
HTML tag like br should be removed words like movie or film are present in every review ,"fig, axe = plt.subplots(1,3, figsize=(23,5))
sns.countplot(train['sentiment'], ax=axe[0])
sns.boxenplot(x=train['sentiment'], y=train['length'], data=train, ax=axe[1])
sns.boxenplot(x=train['sentiment'], y=train['word_n'], data=train, ax=axe[2])",meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
English stopwords,"stopwords = stopwords.words(""english"") ",meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
Removing HTML Tags, html_removed_text = bs(raw_text). get_text () ,meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
Remove any non character," character_only_text = re.sub(""[^a-zA-Z]"" , "" "" , html_removed_text) ",meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
Lowercase and split, lower_text = character_only_text.lower (). split () ,meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
Get STOPWORDS and remove, stop_remove_text =[i for i in lower_text if not i in stopwords] ,meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
Remove one character words, lemma_removed_text =[word for word in stop_remove_text if len(word)> 1] ,meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
Lemmatization," lemma_removed_text =[wordnet_lemmatizer.lemmatize(word , 'v')for word in stop_remove_text] ",meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
Predictors,X = train['cleaned_review'] ,meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
Target,y = train['sentiment'] ,meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
RandomForestClassifier,model_RMC = RandomForestClassifier(n_estimators = 110) ,meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
Making a submission,"test_feature_vector = create_vector(vectorizer , test['review']) ",meets-bags-of-popcorn-a-beginner-s-notebook.ipynb
"Three Approaches1 A Simple Option: Drop Columns with Missing ValuesThe simplest option is to drop columns with missing values. Unless most values in the dropped columns are missing, the model loses access to a lot of potentially useful! information with this approach. As an extreme example, consider a dataset with 10,000 rows, where one important column is missing a single entry. This approach would drop the column entirely!2 A Better Option: ImputationImputation fills in the missing values with some number. For instance, we can fill in the mean value along each column. The imputed value won t be exactly right in most cases, but it usually leads to more accurate models than you would get from dropping the column entirely.3 An Extension To ImputationImputation is the standard approach, and it usually works well. However, imputed values may be systematically above or below their actual values which weren t collected in the dataset . Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing. In this approach, we impute the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries.In some cases, this will meaningfully improve results. In other cases, it doesn t help at all.ExampleIn the example, we will work with the Melbourne Housing dataset. Our model will use information such as the number of rooms and land size to predict home price.We won t focus on the data loading step. Instead, you can imagine you are at a point where you already have the training and validation data in X train, X valid, y train, and y valid. ",import pandas as pd ,missing-values.ipynb
Load the data,data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv') ,missing-values.ipynb
Select target,y = data.Price ,missing-values.ipynb
"To keep things simple, we ll use only numerical predictors","melb_predictors = data.drop (['Price'], axis = 1) ",missing-values.ipynb
Define Function to Measure Quality of Each ApproachWe define a function score dataset to compare different approaches to dealing with missing values. This function reports the mean absolute error MAE from a random forest model.,from sklearn.ensemble import RandomForestRegressor ,missing-values.ipynb
Function for comparing different approaches,"def score_dataset(X_train , X_valid , y_train , y_valid): ",missing-values.ipynb
"Score from Approach 2 Imputation Next, we use SimpleImputer to replace missing values with the mean value along each column.Although it s simple, filling in the mean value generally performs quite well but this varies by dataset . While statisticians have experimented with more complex ways to determine imputed values such as regression imputation, for instance , the complex strategies typically give no additional benefit once you plug the results into sophisticated machine learning models.",from sklearn.impute import SimpleImputer ,missing-values.ipynb
Imputation,my_imputer = SimpleImputer () ,missing-values.ipynb
Imputation removed column names put them back,imputed_X_train.columns = X_train.columns ,missing-values.ipynb
Make copy to avoid changing original data when imputing ,X_train_plus = X_train.copy () ,missing-values.ipynb
Make new columns indicating what will be imputed,for col in cols_with_missing : ,missing-values.ipynb
Imputation,my_imputer = SimpleImputer () ,missing-values.ipynb
Imputation removed column names put them back,imputed_X_train_plus.columns = X_train_plus.columns ,missing-values.ipynb
"Shape of training data num rows, num columns ",print(X_train.shape) ,missing-values.ipynb
Number of missing values in each column of training data,missing_val_count_by_column =(X_train.isnull (). sum ()) ,missing-values.ipynb
Creating Random Samples,"import random
li=random.sample(range(10, 130), 24)",ml-foundation-cross-validation-all-methods.ipynb
" HoldOut Cross validation or Train Test SplitIn this technique of cross validation, the whole dataset is randomly partitioned into a training set and validation set. Using a rule of thumb nearly 70 of the whole dataset is used as a training set and the remaining 30 is used as the validation set. Pros 1. Quick To Execute: As we have to split the dataset into training and validation set just once and the model will be built just once on the training set so gets executed quickly. Cons 1. Not Suitable for an imbalanced dataset: Suppose we have an imbalanced dataset that has class 0 and class 1 . Let s say 80 of data belongs to class 0 and the remaining 20 data to class 1 .On doing train test split with train set size as 80 and test data size as 20 of the dataset. It may happen that all 80 data of class 0 may be in the training set and all data of class 1 in the test set. So our model will not generalize well for our test data as it hasn t seen data of class 1 before.2. A large chunk of data gets deprived of training the model. In the case of a small dataset, a part will be kept aside for testing the model which may have important characteristics which our model may miss out on as it has not trained on that data. Python Code Example ","from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

iris=load_iris()
X=iris.data[li]
Y=iris.target[li]
print(""Size of Dataset {}"".format(len(X)))


logreg=LogisticRegression()

x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3,random_state=42)
logreg.fit(x_train,y_train)
predict=logreg.predict(x_test)",ml-foundation-cross-validation-all-methods.ipynb
" K Fold Cross ValidationIn this technique of K Fold cross validation, the whole dataset is partitioned into K parts of equal size. Each partition is called a Fold .So as we have K parts we call it K Folds. One Fold is used as a validation set and the remaining K 1 folds are used as the training set.The technique is repeated K times until each fold is used as a validation set and the remaining folds as the training set.The final accuracy of the model is computed by taking the mean accuracy of the k models validation data. Pros The whole dataset is used as both a training set and validation set Cons Not to be used for imbalanced datasets: As discussed in the case of HoldOut cross validation, in the case of K Fold validation too it may happen that all samples of training set will have no sample form class 1 and only of class 0 .And the validation set will have a sample of class 1 . Not suitable for Time Series data: For Time Series data the order of the samples matter. But in K Fold Cross Validation, samples are selected in random order. Python Code Example ","from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score,KFold
from sklearn.linear_model import LogisticRegression

iris=load_iris()

X=iris.data[li]
Y=iris.target[li]

logreg=LogisticRegression()
kf=KFold(n_splits=5)
score=cross_val_score(logreg,X,Y,cv=kf)
",ml-foundation-cross-validation-all-methods.ipynb
" Stratified K Fold Cross ValidationStratified K Fold is an enhanced version of K Fold cross validation which is mainly used for imbalanced datasets. Just like K fold, the whole dataset is divided into K folds of equal size.But in this technique, each fold will have the same ratio of instances of target variable as in the whole datasets Pros Works perfectly well for Imbalanced Data: Each fold in stratified cross validation will have a representation of data of all classes in the same ratio as in the whole dataset. Cons Not suitable for Time Series data: For Time Series data the order of the samples matter. But in Stratified Cross Validation, samples are selected in random order. Python Code Example ","from sklearn.model_selection import cross_val_score,StratifiedKFold
from sklearn.linear_model import LogisticRegression

iris=load_iris()
X=iris.data[li]
Y=iris.target[li]

logreg=LogisticRegression()
stratifiedkf=StratifiedKFold(n_splits=5)

score=cross_val_score(logreg,X,Y,cv=stratifiedkf)
",ml-foundation-cross-validation-all-methods.ipynb
" Leave P Out cross validationLeavePOut cross validation is an exhaustive cross validation technique, in which p samples are used as the validation set and remaining n p samples are used as the training set.Suppose we have 100 samples in the dataset. If we use p 10 then in each iteration 10 values will be used as a validation set and the remaining 90 samples as the training set.This process is repeated till the whole dataset gets divided on the validation set of p samples and n p training samples. Pros All the data samples get used as both training and validation samples. Cons High computation time: As the above technique will keep on repeating until all samples get used up as a validation set, it will have higher computational time. Not Suitable for Imbalanced dataset: Same as in K Fold Cross validation, if in the training set we have samples of only 1 class then our model will not be able to generalize for the validation set. Python Code Example ","from sklearn.model_selection import LeavePOut,cross_val_score
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier

iris=load_iris()
X=iris.data[li]
Y=iris.target[li]

lpo=LeavePOut(p=2)
lpo.get_n_splits(X)

tree=RandomForestClassifier(n_estimators=10,max_depth=5,n_jobs=-1)

score=cross_val_score(tree,X,Y,cv=lpo)",ml-foundation-cross-validation-all-methods.ipynb
 Leave One Out cross validationLeaveOneOut cross validation is an exhaustive cross validation technique in which 1 sample point is used as a validation set and the remaining n 1 samples are used as the training set.Suppose we have 100 samples in the dataset. Then in each iteration 1 value will be used as a validation set and the remaining 99 samples as the training set. Thus the process is repeated till every sample of the dataset is used as a validation point.It is the same as LeavePOut cross validation with p 1. Python Code Example ,"from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import LeaveOneOut,cross_val_score

iris=load_iris()
X=iris.data[li]
Y=iris.target[li]

loo=LeaveOneOut()
tree=RandomForestClassifier(n_estimators=10,max_depth=5,n_jobs=-1)
score=cross_val_score(tree,X,Y,cv=loo)",ml-foundation-cross-validation-all-methods.ipynb
"Pros We are free to use the size of the training and validation set. We can choose the number of repetitions and not depend on the number of folds for repetitions. Cons Few samples may not be selected for either training or validation set. Not Suitable for Imbalanced datasets: After we define the size of the training set and validation set, all the samples are randomly selected, so it may happen that the training set may don t have the class of data that is in the test set, and the model won t be able to generalize for unseen data. Python Code Example","from sklearn.model_selection import ShuffleSplit,cross_val_score
from sklearn.linear_model import LogisticRegression

logreg=LogisticRegression()

shuffle_split=ShuffleSplit(test_size=0.3,train_size=0.5,n_splits=10)

scores=cross_val_score(logreg,iris.data[li],iris.target[li],cv=shuffle_split)",ml-foundation-cross-validation-all-methods.ipynb
"Pros One of the finest techniques . Cons Not suitable for validation of other data types: As in other techniques we choose random samples as training or validation set, but in this technique order of data is very important. Python Code Example","import numpy as np
from sklearn.model_selection import TimeSeriesSplit

X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([1, 2, 3, 4, 5, 6])

time_series = TimeSeriesSplit()

print(time_series)

for train_index, test_index in time_series.split(X):
 print(""TRAIN:"", train_index, ""TEST:"", test_index)
 X_train, X_test = X[train_index], X[test_index]
 y_train, y_test = y[train_index], y[test_index]",ml-foundation-cross-validation-all-methods.ipynb
"Settings projects At first you need to install cookiecutter project, docker, enviroment and local python. Cookiecutter Instructions Poetry Docker Afther this i suggest to read this: mlops and github, more mlops tools","from IPython.display import YouTubeVideo

YouTubeVideo('l-jwZPOCChg', width=800, height=300)",mlops-how-to-be-rock-of-ml.ipynb
linear algebra,import numpy as np ,mnist-with-keras-for-beginners-99457.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,mnist-with-keras-for-beginners-99457.ipynb
for plotting,import matplotlib.pyplot as plt ,mnist-with-keras-for-beginners-99457.ipynb
loading the dataset....... Train ,"train = pd.read_csv(""../input/train.csv"") ",mnist-with-keras-for-beginners-99457.ipynb
loading the dataset....... Test ,"test = pd.read_csv(""../input/test.csv"") ",mnist-with-keras-for-beginners-99457.ipynb
all pixel values,"x_train =(train.ix[: , 1 :]. values). astype('float32') ",mnist-with-keras-for-beginners-99457.ipynb
only labels i.e targets digits,"y_train = train.ix[: , 0]. values.astype('int32') ",mnist-with-keras-for-beginners-99457.ipynb
preview the images first,"plt.figure(figsize =(12 , 10)) ",mnist-with-keras-for-beginners-99457.ipynb
Normalising The Data,"x_train = x_train/255.0
x_test = x_test/255.0",mnist-with-keras-for-beginners-99457.ipynb
Printing the shape of the Datasets,"print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')",mnist-with-keras-for-beginners-99457.ipynb
Reshape To Match The Keras s Expectations ,"X_train = x_train.reshape(x_train.shape[0], 28, 28,1)
X_test = x_test.reshape(x_test.shape[0], 28, 28,1)",mnist-with-keras-for-beginners-99457.ipynb
convert class vectors to binary class matrices One Hot Encoding,"y_train = keras.utils.to_categorical(y_train , num_classes) ",mnist-with-keras-for-beginners-99457.ipynb
Linear Model,model = Sequential () ,mnist-with-keras-for-beginners-99457.ipynb
Basic Simple Plot And Evaluation,"final_loss, final_acc = model.evaluate(X_val, Y_val, verbose=0)
print(""Final loss: {0:.6f}, final accuracy: {1:.6f}"".format(final_loss, final_acc))",mnist-with-keras-for-beginners-99457.ipynb
"Note, this code is taken straight from the SKLEARN website, an nice way of viewing confusion matrix.","print(h.history.keys())
accuracy = h.history['acc']
val_accuracy = h.history['val_acc']
loss = h.history['loss']
val_loss = h.history['val_loss']
epochs = range(len(accuracy))
plt.plot(epochs, accuracy, 'bo', label='Training accuracy')
plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.show()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()",mnist-with-keras-for-beginners-99457.ipynb
Errors are difference between predicted labels and true labels,errors =(Y_pred_classes - Y_true != 0) ,mnist-with-keras-for-beginners-99457.ipynb
Probabilities of the wrong predicted numbers,"Y_pred_errors_prob = np.max(Y_pred_errors , axis = 1) ",mnist-with-keras-for-beginners-99457.ipynb
Predicted probabilities of the true values in the error set,"true_prob_errors = np.diagonal(np.take(Y_pred_errors , Y_true_errors , axis = 1)) ",mnist-with-keras-for-beginners-99457.ipynb
Difference between the probability of the predicted label and the true label,delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors ,mnist-with-keras-for-beginners-99457.ipynb
Sorted list of the delta prob errors,sorted_dela_errors = np.argsort(delta_pred_true_errors) ,mnist-with-keras-for-beginners-99457.ipynb
Top 6 errors,most_important_errors = sorted_dela_errors[- 6 :] ,mnist-with-keras-for-beginners-99457.ipynb
Show the top 6 errors,"display_errors(most_important_errors , X_val_errors , Y_pred_classes_errors , Y_true_errors) ",mnist-with-keras-for-beginners-99457.ipynb
It looks like diversity of the similar patterns present on multiple classes effect the performance of the classifier although CNN is a robust architechture.,"test_im = X_train[154]
plt.imshow(test_im.reshape(28,28), cmap='viridis', interpolation='none')",mnist-with-keras-for-beginners-99457.ipynb
"Let s see the activation of the 2nd channel of the first layer:Had taken help from the keras docs, this answer on StackOverFlow","from keras import models
layer_outputs = [layer.output for layer in model.layers[:8]]
activation_model = models.Model(input=model.input, output=layer_outputs)
activations = activation_model.predict(test_im.reshape(1,28,28,1))

first_layer_activation = activations[0]
plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')",mnist-with-keras-for-beginners-99457.ipynb
Droping The Last Dense Layer,model.layers[: - 1] ,mnist-with-keras-for-beginners-99457.ipynb
Predict the values from the validation dataset,Y_pred = model.predict(X_val) ,mnist-with-keras-for-beginners-99457.ipynb
Convert predictions classes to one hot vectors,"Y_pred_classes = np.argmax(Y_pred , axis = 1) ",mnist-with-keras-for-beginners-99457.ipynb
Data Loading Code Hidden Here,import pandas as pd ,model-validation.ipynb
Load data,melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv' ,model-validation.ipynb
Filter rows with missing price values,filtered_melbourne_data = melbourne_data.dropna(axis = 0) ,model-validation.ipynb
Choose target and features,y = filtered_melbourne_data.Price ,model-validation.ipynb
"Once we have a model, here is how we calculate the mean absolute error:","from sklearn.metrics import mean_absolute_error

predicted_home_prices = melbourne_model.predict(X)
mean_absolute_error(y, predicted_home_prices)",model-validation.ipynb
"The Problem with In Sample ScoresThe measure we just computed can be called an in sample score. We used a single sample of houses for both building the model and evaluating it. Here s why this is bad.Imagine that, in the large real estate market, door color is unrelated to home price. However, in the sample of data you used to build the model, all homes with green doors were very expensive. The model s job is to find patterns that predict home prices, so it will see this pattern, and it will always predict high prices for homes with green doors.Since this pattern was derived from the training data, the model will appear accurate in the training data.But if this pattern doesn t hold when the model sees new data, the model would be very inaccurate when used in practice.Since models practical value come from making predictions on new data, we measure performance on data that wasn t used to build the model. The most straightforward way to do this is to exclude some data from the model building process, and then use those to test the model s accuracy on data it hasn t seen before. This data is called validation data.Coding ItThe scikit learn library has a function train test split to break up the data into two pieces. We ll use some of that data as training data to fit the model, and we ll use the other data as validation data to calculate mean absolute error.Here is the code:",from sklearn.model_selection import train_test_split ,model-validation.ipynb
run this script.,"train_X , val_X , train_y , val_y = train_test_split(X , y , random_state = 0) ",model-validation.ipynb
Define model,melbourne_model = DecisionTreeRegressor () ,model-validation.ipynb
Fit model,"melbourne_model.fit(train_X , train_y) ",model-validation.ipynb
get predicted prices on validation data,val_predictions = melbourne_model.predict(val_X) ,model-validation.ipynb
"Introduction and SetupThis notebook utilizes a CycleGAN architecture to add Monet style to photos. For this tutorial, we will be using the TFRecord dataset. Import the following packages and change the accelerator to TPU.For more information, check out TensorFlow and Keras CycleGAN documentation pages.","import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa

from kaggle_datasets import KaggleDatasets
import matplotlib.pyplot as plt
import numpy as np

try:
 tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
 print('Device:', tpu.master())
 tf.config.experimental_connect_to_cluster(tpu)
 tf.tpu.experimental.initialize_tpu_system(tpu)
 strategy = tf.distribute.experimental.TPUStrategy(tpu)
except:
 strategy = tf.distribute.get_strategy()
print('Number of replicas:', strategy.num_replicas_in_sync)

AUTOTUNE = tf.data.experimental.AUTOTUNE
 
print(tf.__version__)",monet-cyclegan-tutorial.ipynb
"Load in the dataWe want to keep our photo dataset and our Monet dataset separate. First, load in the filenames of the TFRecords.",GCS_PATH = KaggleDatasets().get_gcs_path(),monet-cyclegan-tutorial.ipynb
"All the images for the competition are already sized to 256x256. As these images are RGB images, set the channel to 3. Additionally, we need to scale the images to a 1, 1 scale. Because we are building a generative model, we don t need the labels or the image id so we ll only return the image from the TFRecord.","IMAGE_SIZE = [256, 256]

def decode_image(image):
 image = tf.image.decode_jpeg(image, channels=3)
 image = (tf.cast(image, tf.float32) / 127.5) - 1
 image = tf.reshape(image, [*IMAGE_SIZE, 3])
 return image

def read_tfrecord(example):
 tfrecord_format = {
 ""image_name"": tf.io.FixedLenFeature([], tf.string),
 ""image"": tf.io.FixedLenFeature([], tf.string),
 ""target"": tf.io.FixedLenFeature([], tf.string)
 }
 example = tf.io.parse_single_example(example, tfrecord_format)
 image = decode_image(example['image'])
 return image",monet-cyclegan-tutorial.ipynb
Define the function to extract the image from the files.,"def load_dataset(filenames, labeled=True, ordered=False):
 dataset = tf.data.TFRecordDataset(filenames)
 dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)
 return dataset",monet-cyclegan-tutorial.ipynb
Let s load in our datasets.,"monet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1)
photo_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1)",monet-cyclegan-tutorial.ipynb
Let s visualize a photo example and a Monet example.,"plt.subplot(121)
plt.title('Photo')
plt.imshow(example_photo[0] * 0.5 + 0.5)

plt.subplot(122)
plt.title('Monet')
plt.imshow(example_monet[0] * 0.5 + 0.5)",monet-cyclegan-tutorial.ipynb
"Build the generatorWe ll be using a UNET architecture for our CycleGAN. To build our generator, let s first define our downsample and upsample methods.The downsample, as the name suggests, reduces the 2D dimensions, the width and height, of the image by the stride. The stride is the length of the step the filter takes. Since the stride is 2, the filter is applied to every other pixel, hence reducing the weight and height by 2.We ll be using an instance normalization instead of batch normalization. As the instance normalization is not standard in the TensorFlow API, we ll use the layer from TensorFlow Add ons.","OUTPUT_CHANNELS = 3

def downsample(filters, size, apply_instancenorm=True):
 initializer = tf.random_normal_initializer(0., 0.02)
 gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)

 result = keras.Sequential()
 result.add(layers.Conv2D(filters, size, strides=2, padding='same',
 kernel_initializer=initializer, use_bias=False))

 if apply_instancenorm:
 result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))

 result.add(layers.LeakyReLU())

 return result",monet-cyclegan-tutorial.ipynb
Upsample does the opposite of downsample and increases the dimensions of the of the image. Conv2DTranspose does basically the opposite of a Conv2D layer.,"def upsample(filters, size, apply_dropout=False):
 initializer = tf.random_normal_initializer(0., 0.02)
 gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)

 result = keras.Sequential()
 result.add(layers.Conv2DTranspose(filters, size, strides=2,
 padding='same',
 kernel_initializer=initializer,
 use_bias=False))

 result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))

 if apply_dropout:
 result.add(layers.Dropout(0.5))

 result.add(layers.ReLU())

 return result",monet-cyclegan-tutorial.ipynb
Let s build our generator!The generator first downsamples the input image and then upsample while establishing long skip connections. Skip connections are a way to help bypass the vanishing gradient problem by concatenating the output of a layer to multiple layers instead of only one. Here we concatenate the output of the downsample layer to the upsample layer in a symmetrical fashion.,def Generator (): ,monet-cyclegan-tutorial.ipynb
"Build the discriminatorThe discriminator takes in the input image and classifies it as real or fake generated . Instead of outputing a single node, the discriminator outputs a smaller 2D image with higher pixel values indicating a real classification and lower values indicating a fake classification.",def Discriminator (): ,monet-cyclegan-tutorial.ipynb
" bs, 128, 128, 64 "," down1 = downsample(64 , 4 , False)( x) ",monet-cyclegan-tutorial.ipynb
" bs, 64, 64, 128 "," down2 = downsample(128 , 4)( down1) ",monet-cyclegan-tutorial.ipynb
" bs, 32, 32, 256 "," down3 = downsample(256 , 4)( down2) ",monet-cyclegan-tutorial.ipynb
" bs, 34, 34, 256 ", zero_pad1 = layers.ZeroPadding2D ()( down3) ,monet-cyclegan-tutorial.ipynb
transforms photos to Monet esque paintings, monet_generator = Generator () ,monet-cyclegan-tutorial.ipynb
transforms Monet paintings to be more like photos, photo_generator = Generator () ,monet-cyclegan-tutorial.ipynb
differentiates real Monet paintings and generated Monet paintings, monet_discriminator = Discriminator () ,monet-cyclegan-tutorial.ipynb
differentiates real photos and generated photos, photo_discriminator = Discriminator () ,monet-cyclegan-tutorial.ipynb
"Since our generators are not trained yet, the generated Monet esque photo does not show what is expected at this point.","to_monet = monet_generator(example_photo)

plt.subplot(1, 2, 1)
plt.title(""Original Photo"")
plt.imshow(example_photo[0] * 0.5 + 0.5)

plt.subplot(1, 2, 2)
plt.title(""Monet-esque Photo"")
plt.imshow(to_monet[0] * 0.5 + 0.5)
plt.show()",monet-cyclegan-tutorial.ipynb
"Build the CycleGAN modelWe will subclass a tf.keras.Model so that we can run fit later to train our model. During the training step, the model transforms a photo to a Monet painting and then back to a photo. The difference between the original photo and the twice transformed photo is the cycle consistency loss. We want the original photo and the twice transformed photo to be similar to one another.The losses are defined in the next section.",class CycleGan(keras.Model): ,monet-cyclegan-tutorial.ipynb
Define loss functionsThe discriminator loss function below compares real images to a matrix of 1s and fake images to a matrix of 0s. The perfect discriminator will output all 1s for real images and all 0s for fake images. The discriminator loss outputs the average of the real and generated loss.,"with strategy.scope():
 def discriminator_loss(real, generated):
 real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)

 generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)

 total_disc_loss = real_loss + generated_loss

 return total_disc_loss * 0.5",monet-cyclegan-tutorial.ipynb
"The generator wants to fool the discriminator into thinking the generated image is real. The perfect generator will have the discriminator output only 1s. Thus, it compares the generated image to a matrix of 1s to find the loss.","with strategy.scope():
 def generator_loss(generated):
 return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)",monet-cyclegan-tutorial.ipynb
"We want our original photo and the twice transformed photo to be similar to one another. Thus, we can calculate the cycle consistency loss be finding the average of their difference.","with strategy.scope():
 def calc_cycle_loss(real_image, cycled_image, LAMBDA):
 loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))

 return LAMBDA * loss1",monet-cyclegan-tutorial.ipynb
"The identity loss compares the image with its generator i.e. photo with photo generator . If given a photo as input, we want it to generate the same image as the image was originally a photo. The identity loss compares the input with the output of the generator.","with strategy.scope():
 def identity_loss(real_image, same_image, LAMBDA):
 loss = tf.reduce_mean(tf.abs(real_image - same_image))
 return LAMBDA * 0.5 * loss",monet-cyclegan-tutorial.ipynb
"Train the CycleGANLet s compile our model. Since we used tf.keras.Model to build our CycleGAN, we can just ude the fit function to train our model.","with strategy.scope():
 monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
 photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

 monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
 photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)",monet-cyclegan-tutorial.ipynb
Visualize our Monet esque photos,"_, ax = plt.subplots(5, 2, figsize=(12, 12))
for i, img in enumerate(photo_ds.take(5)):
 prediction = monet_generator(img, training=False)[0].numpy()
 prediction = (prediction * 127.5 + 127.5).astype(np.uint8)
 img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)

 ax[i, 0].imshow(img)
 ax[i, 1].imshow(prediction)
 ax[i, 0].set_title(""Input Photo"")
 ax[i, 1].set_title(""Monet-esque"")
 ax[i, 0].axis(""off"")
 ax[i, 1].axis(""off"")
plt.show()",monet-cyclegan-tutorial.ipynb
Create submission file,"import PIL
! mkdir ../images",monet-cyclegan-tutorial.ipynb
Quick navigation 1. Data overview 2. Classifier training ,"import numpy as np
import pandas as pd
import os
import math
import matplotlib.pyplot as plt
import cv2
import random

from plotly.subplots import make_subplots
import plotly.graph_objs as go

from skimage import data

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, f1_score, accuracy_score

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Input
from tensorflow.keras import backend as K
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping",monet-masterpieces-eda-and-image-classification.ipynb
Let s see number of images for every directory. We wiill work here only with .jpg images.,"MONET_JPG_PATH = '../input/gan-getting-started/monet_jpg/'
PHOTO_JPG_PATH = '../input/gan-getting-started/photo_jpg/'

print('Number of images in Monet directory: ', len(os.listdir(MONET_JPG_PATH)))
print('Number of images in Photo directory: ', len(os.listdir(PHOTO_JPG_PATH)))",monet-masterpieces-eda-and-image-classification.ipynb
Now it is time to check shapes of images.,"shapes_set = set()
image_names = os.listdir(MONET_JPG_PATH)
for img_name in image_names:
 img = cv2.imread(os.path.join(MONET_JPG_PATH, img_name))
 shapes_set.add(img.shape)

print('Number of unique image shapes inside Monet directory: ', len(shapes_set))
print('Image shape sizes: ', shapes_set.pop())

shapes_set = set()
image_names = os.listdir(PHOTO_JPG_PATH)
for img_name in image_names:
 img = cv2.imread(os.path.join(PHOTO_JPG_PATH, img_name))
 shapes_set.add(img.shape)
 
print('Number of unique image shapes inside Photo directory: ', len(shapes_set))
print('Image shape sizes: ', shapes_set.pop())",monet-masterpieces-eda-and-image-classification.ipynb
Let s visualize some images from both sets. Code for this is taken from ,"def visualize_images(path, n_images, is_random=True, figsize=(16, 16)):
 plt.figure(figsize=figsize)
 w = int(n_images ** .5)
 h = math.ceil(n_images / w)
 
 all_names = os.listdir(path)
 image_names = all_names[:n_images] 
 if is_random:
 image_names = random.sample(all_names, n_images)
 
 for ind, image_name in enumerate(image_names):
 img = cv2.imread(os.path.join(path, image_name))
 img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) 
 plt.subplot(h, w, ind + 1)
 plt.imshow(img)
 plt.xticks([])
 plt.yticks([])
 
 plt.show()",monet-masterpieces-eda-and-image-classification.ipynb
Monet pictures,"visualize_images(MONET_JPG_PATH, 9)",monet-masterpieces-eda-and-image-classification.ipynb
Photos,"visualize_images(PHOTO_JPG_PATH, 9)",monet-masterpieces-eda-and-image-classification.ipynb
Let s see image woth corresponded color s histogram,"def show_color_histogram(path):
 image_names = os.listdir(path)
 image_name = random.choice(image_names)
 img = cv2.imread(os.path.join(path, image_name))
 img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) 
 fig = make_subplots(1, 2)

 fig.add_trace(go.Image(z=img), 1, 1)
 for channel, color in enumerate(['red', 'green', 'blue']):
 fig.add_trace(
 go.Histogram(
 x=img[..., channel].ravel(), 
 opacity=0.5,
 marker_color=color, 
 name='%s channel' %color), 
 1, 
 2
 )
 fig.update_layout(height=400)
 fig.show()",monet-masterpieces-eda-and-image-classification.ipynb
Monet picture,show_color_histogram(MONET_JPG_PATH),monet-masterpieces-eda-and-image-classification.ipynb
Photos,show_color_histogram(PHOTO_JPG_PATH),monet-masterpieces-eda-and-image-classification.ipynb
Let s build simple image classifier and see how difficult to detect Monet s pictures.,"X = list()
y = list()

image_names = os.listdir(MONET_JPG_PATH)
for img_name in image_names:
 img = cv2.imread(os.path.join(MONET_JPG_PATH, img_name))
 X.append(img)
 y.append(1)

image_names = os.listdir(PHOTO_JPG_PATH)
for img_name in image_names:
 img = cv2.imread(os.path.join(PHOTO_JPG_PATH, img_name))
 X.append(img)
 y.append(0)
 
X = np.stack(X)
y = np.stack(y)",monet-masterpieces-eda-and-image-classification.ipynb
Here we drfine metrics for imbalanced data,"def recall_score(y_true, y_pred):
 true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
 possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
 recall = true_positives / (possible_positives + K.epsilon())
 return recall

def precision_score(y_true, y_pred):
 true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
 predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
 precision = true_positives / (predicted_positives + K.epsilon())
 return precision

def keras_f1_score(y_true, y_pred):
 precision = precision_score(y_true, y_pred)
 recall = recall_score(y_true, y_pred)
 return 2*((precision*recall)/(precision+recall+K.epsilon()))",monet-masterpieces-eda-and-image-classification.ipynb
To fight with imbalanced data let s try to use class weights.,"class_weight = {
 0: 1.,
 1: 20.
}",monet-masterpieces-eda-and-image-classification.ipynb
"I m Something of a Painter Myself I wanted to be a picasso like painter, but haven t got enought talent to paint. So using the GAN I am noe transforming the photo images in this competition into a monet. I am using the tfrecords to get the better out of the model in training. I tried to explain every step to the best of my knowledge! Please do support if found interesting Also I took some references from the following resources Link 1 Link 2 Link 3","import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf
import os
import re
import cv2
import math
import random",monet-using-gan.ipynb
Data is stored as tfrecords for faster training and processing. tf.io.gfile.glob returns the list of files that manages the pattern given. For eg: .tfrec ,"
monet_names = tf.io.gfile.glob(r""../input/gan-getting-started/monet_tfrec/*.tfrec"")
print(monet_names)
photo_names = tf.io.gfile.glob(r""../input/gan-getting-started/photo_tfrec/*.tfrec"")",monet-using-gan.ipynb
Loading the data read tfrecord : takes a file and parse to get required variables. prepare image : takes an image in tensor datatype and returns after reshaping and normalizing it. get dataset : reads the TFRecords file and maps with the help of read tfrecord function. data augment : as the name suggests augment the image data of tensor type for better accuracy. get gan dataset : returns our main datasets which we will use for the training purposes ,"def prepare_image(img, dim = 256): 
 img = tf.image.decode_jpeg(img, channels = 3)
 img = (tf.cast(img, tf.float32) / 255.0) - 1
 img = tf.reshape(img, [dim, dim, 3])
 return img

def read_tfrecord(example):
 tfrec_format = {
 'image' : tf.io.FixedLenFeature([], tf.string),
 'image_name' : tf.io.FixedLenFeature([], tf.string),
 ""target"": tf.io.FixedLenFeature([], tf.string)
 } 
 
 example = tf.io.parse_single_example(example, tfrec_format)
 image = prepare_image(example['image'])
 return image",monet-using-gan.ipynb
Dataset visualisation batch visualisation : shows the given number of images in given path ,"base_path = '../input/gan-getting-started/'
monet_path = os.path.join(base_path, 'monet_jpg')
photo_path = os.path.join(base_path, 'photo_jpg')",monet-using-gan.ipynb
Individual images color hist visualization : Gives the RGB bar graph of photo channels visualization : Gives the RGB channel graph of photo grayscale visualization : Converts the RGB channel to grayscale color graph : Gives the RGB histogram graph ,"rand_monet = r""../input/gan-getting-started/monet_jpg/0260d15306.jpg""
rand_photo = r""../input/gan-getting-started/photo_jpg/000ded5c41.jpg""",monet-using-gan.ipynb
loop over the image channels," for(chan , color)in zip(chans , colors): ",monet-using-gan.ipynb
channel," hist = cv2.calcHist ([chan],[0], None ,[256],[0 , 256]) ",monet-using-gan.ipynb
plot the histogram," plt.plot(hist , color = color) ",monet-using-gan.ipynb
"Training a CycleGAN In the CycleGAN s case, the architecture is complex, and as a result, we need a structure that allows us to keep accessing the original attributes and methods that we have defined. As a result, we will write out the CycleGAN as a Python class of its own with methods to build the Generator and Discriminator, and run the training.For the training to execute we will need a seperate Generator and discriminator function which we will feed to CycleGAN as methods which in turn needs the upsample and downsample of image. For downsampling we are using the Conv2D as primary layer and LeakyReLU as activation For upsampling we are using the Conv2DTranspose as primary layer and Dropout at 0.3, ReLU as secondary layers For all of these I have made a utility script as this notebook kernel size was exceeding","from shutil import copyfile
copyfile(src = ""../usr/lib/monet_using_gan_utility/monet_using_gan_utility.py"", dst = ""../working/monet.py"")
from monet import *",monet-using-gan.ipynb
"From here we are declaring generator and discriminator spaces of GANs which will work the same as encoder and decoder of autoencoders. To know more about autoencoders, click here","generator_g = Generator()
tf.keras.utils.plot_model(generator_g, dpi=48)",monet-using-gan.ipynb
transforms photos to Monet esque paintings, monet_generator = Generator () ,monet-using-gan.ipynb
transforms Monet paintings to be more like photos, photo_generator = Generator () ,monet-using-gan.ipynb
differentiates real Monet paintings and generated Monet paintings, monet_discriminator = Discriminator () ,monet-using-gan.ipynb
differentiates real photos and generated photos, photo_discriminator = Discriminator () ,monet-using-gan.ipynb
"Building the cycleGAN model Image to image translation frameworks are frequently difficult to train because of the need for perfect pairs the CycleGAN solves this by making this an unpaired domain translation.The CycleGAN has three losses: Cycle consistent, which measures the difference between the original image and an image translated into a different domain and back again Adversarial, which ensures realistic images Identity, which preserves the color space of the imagePractical applications of the CycleGAN include self driving car training and exten sions that allow us to create different styles of images during the translation process.",class CycleGan(keras.Model): ,monet-using-gan.ipynb
Training,"with strategy.scope():
 monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
 photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

 monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
 photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)",monet-using-gan.ipynb
Data Preparations,"import os
import math
import random

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import albumentations as A",monet-visualization-and-augmentation.ipynb
Show Batches Monet Paintings,"batch_visualization(MONET_PATH, 1, is_random=True, figsize=(5, 5))",monet-visualization-and-augmentation.ipynb
Show Batches Photos,"batch_visualization(PHOTO_PATH, 16, is_random=True)",monet-visualization-and-augmentation.ipynb
Color Channel Histograms,"def color_hist_visualization(image_path, figsize=(16, 4)):
 plt.figure(figsize=figsize)
 
 img = cv2.imread(image_path)
 img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) 
 plt.subplot(1, 4, 1)
 plt.imshow(img)
 plt.axis(""off"")
 
 colors = [""red"", ""green"", ""blue""]
 for i in range(len(colors)):
 plt.subplot(1, 4, i + 2)
 plt.hist(
 img[:, :, i].reshape(-1),
 bins=25,
 alpha=0.5,
 color=colors[i],
 density=True
 )
 plt.xlim(0, 255)
 plt.xticks([])
 plt.yticks([])
 
 
 plt.show()",monet-visualization-and-augmentation.ipynb
Individual Channels Visualization,"def channels_visualization(image_path, figsize=(16, 4)):
 plt.figure(figsize=figsize)
 
 img = cv2.imread(image_path)
 img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) 
 plt.subplot(1, 4, 1)
 plt.imshow(np.mean(img, axis=2), cmap=""gray"")
 plt.axis('off')
 
 for i in range(3):
 plt.subplot(1, 4, i + 2)
 tmp_img = np.full_like(img, 0)
 tmp_img[:, :, i] = img[:, :, i]
 plt.imshow(tmp_img)
 plt.xlim(0, 255)
 plt.xticks([])
 plt.yticks([])
 
 
 plt.show()",monet-visualization-and-augmentation.ipynb
Grayscale Visualization,"def grayscale_visualization(image_path, figsize=(8, 4)):
 plt.figure(figsize=figsize)
 
 img = cv2.imread(image_path)
 img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) 
 plt.subplot(1, 2, 1)
 plt.imshow(img)
 plt.axis('off')
 
 plt.subplot(1, 2, 2)
 tmp_img = np.full_like(img, 0)
 for i in range(3):
 tmp_img[:, :, i] = img.mean(axis=-1)
 plt.imshow(tmp_img)
 plt.axis('off')
 
 
 plt.show()",monet-visualization-and-augmentation.ipynb
Augmentation Blur,"def plot_simple_augmentation(image_path, transform):
 img = cv2.imread(image_path)
 img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) 
 
 plt.figure(figsize=(10, 5))
 
 plt.subplot(1, 2, 1)
 plt.imshow(img)
 plt.axis(""off"")
 
 plt.subplot(1, 2, 2)
 x = transform(image=img)[""image""]
 plt.imshow(x)
 plt.axis(""off"")

 plt.show()
 
def plot_multiple_augmentation(image_path, transform):
 img = cv2.imread(image_path)
 img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) 
 
 plt.figure(figsize=(10, 10))
 
 plt.subplot(2, 2, 1)
 plt.imshow(img)
 plt.axis(""off"")
 
 plt.subplot(2, 2, 2)
 x = transform(image=img)[""image""]
 plt.imshow(x)
 plt.axis(""off"")
 
 plt.subplot(2, 2, 3)
 x = transform(image=img)[""image""]
 plt.imshow(x)
 plt.axis(""off"")
 
 plt.subplot(2, 2, 4)
 x = transform(image=img)[""image""]
 plt.imshow(x)
 plt.axis(""off"")

 plt.show()",monet-visualization-and-augmentation.ipynb
Augmentation CLAHE,"transform = A.CLAHE(p=1.0, clip_limit=(10, 10), tile_grid_size=(3, 3))

plot_simple_augmentation(""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"", transform)",monet-visualization-and-augmentation.ipynb
Augmentation CenterCrop,"transform = A.CenterCrop(p=1.0, height=100, width=150)

plot_simple_augmentation(""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"", transform)",monet-visualization-and-augmentation.ipynb
Augmentation ChannelDropout,"transform = A.ChannelDropout(p=1.0, channel_drop_range=(1, 2), fill_value=0)

plot_multiple_augmentation(""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"", transform)",monet-visualization-and-augmentation.ipynb
Augmentation ChannelShuffle,"transform = A.ChannelShuffle(p=1.0)

plot_multiple_augmentation(""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"", transform)",monet-visualization-and-augmentation.ipynb
Augmentation Crop,"transform = A.Crop(p=1.0, x_min=0, y_min=0, x_max=150, y_max=150)

plot_simple_augmentation(""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"", transform)",monet-visualization-and-augmentation.ipynb
Augmentation RandomCrop,"transform = A.RandomCrop(p=1.0, height=100, width=100)

plot_multiple_augmentation(""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"", transform)",monet-visualization-and-augmentation.ipynb
Augmentation CutOut,"transform = A.Cutout(p=1.0, num_holes=8, max_h_size=15, max_w_size=15)

plot_multiple_augmentation(""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"", transform)",monet-visualization-and-augmentation.ipynb
Augmentation Downscale,"transform = A.Downscale(
 p=1.0, scale_min=0.01, scale_max=0.20, interpolation=0,
)

plot_multiple_augmentation(
 ""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"",
 transform,
)",monet-visualization-and-augmentation.ipynb
Augmentation Equalize,"transform = A.Equalize(
 p=1.0, mode='cv', by_channels=True,
)

plot_simple_augmentation(
 ""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"",
 transform,
)",monet-visualization-and-augmentation.ipynb
Augmentation HorizontalFlip,"transform = A.HorizontalFlip(
 p=1.0,
)

plot_simple_augmentation(
 ""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"",
 transform,
)",monet-visualization-and-augmentation.ipynb
Augmentation VerticalFlip,"transform = A.VerticalFlip(
 p=1.0,
)

plot_simple_augmentation(
 ""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"",
 transform,
)",monet-visualization-and-augmentation.ipynb
Augmentation Flip,"transform = A.Flip(
 p=1.0,
)

plot_multiple_augmentation(
 ""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"",
 transform,
)",monet-visualization-and-augmentation.ipynb
Augmentation GaussNoise,"transform = A.GaussNoise(
 p=1.0, var_limit=(500.0, 500.0),
)

plot_simple_augmentation(
 ""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"",
 transform,
)",monet-visualization-and-augmentation.ipynb
Augmentation GridDistortion,"transform = A.GridDistortion(
 p=1.0, num_steps=15, distort_limit=(-2., 2.), 
 interpolation=0, border_mode=0, value=(0, 0, 0), mask_value=None,
)

plot_multiple_augmentation(
 ""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"",
 transform,
)",monet-visualization-and-augmentation.ipynb
Augmentation HueSaturationValue,"transform = A.HueSaturationValue(
 p=1.0, 
 hue_shift_limit=(-100, 100), 
 sat_shift_limit=(-100, 100), 
 val_shift_limit=(-100, 100),
)

plot_multiple_augmentation(
 ""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"",
 transform,
)",monet-visualization-and-augmentation.ipynb
Augmentation ISONoise,"transform = A.ISONoise(
 p=1.0, intensity=(0.0, 2.0), color_shift=(0.0, 1.0)
)

plot_multiple_augmentation(
 ""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"",
 transform,
)",monet-visualization-and-augmentation.ipynb
Augmentation ImageCompression,"transform = A.ImageCompression(
 p=1.0, quality_lower=0, quality_upper=10, compression_type=0,
)

plot_multiple_augmentation(
 ""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"",
 transform,
)",monet-visualization-and-augmentation.ipynb
Augmentation InvertImg,"transform = A.InvertImg(
 p=1.0,
)

plot_simple_augmentation(
 ""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"",
 transform,
)",monet-visualization-and-augmentation.ipynb
Augmentation JpegCompression,"transform = A.JpegCompression(
 p=1.0, quality_lower=0, quality_upper=10,
)

plot_multiple_augmentation(
 ""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"",
 transform,
)",monet-visualization-and-augmentation.ipynb
Augmentation MotionBlur,"transform = A.MotionBlur(
 p=1.0, blur_limit=(3, 50),
)

plot_multiple_augmentation(
 ""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"",
 transform,
)
",monet-visualization-and-augmentation.ipynb
Augmentation MultiplicativeNoise,"transform = A.MultiplicativeNoise(
 p=1.0, multiplier=(0.1, 5.0), per_channel=True, elementwise=False,
)

plot_multiple_augmentation(
 ""../input/gan-getting-started/monet_jpg/990ed28f62.jpg"",
 transform,
)
",monet-visualization-and-augmentation.ipynb
ConnectX environment was defined in v0.1.6,! pip install 'kaggle-environments>=0.1.6' ,monte-carlo-tree-search-connectx.ipynb
Create ConnectX Environment,"env = make(""connectx"", debug=True)
configuration = env.configuration
print(configuration)",monte-carlo-tree-search-connectx.ipynb
Functions,"def MCTS_agent(observation , configuration): ",monte-carlo-tree-search-connectx.ipynb
Check agent validity,"env.reset()
try:
 del current_state
except:
 pass

env.run([MCTS_agent, MCTS_agent])
print(""Success!"" if env.state[0].status == env.state[1].status == ""DONE"" else ""Failed..."")",monte-carlo-tree-search-connectx.ipynb
Evaluate your agent,def mean_reward(rewards): ,monte-carlo-tree-search-connectx.ipynb
Run multiple episodes to estimate its performance.,"print(""My Agent vs Random Agent:"" , mean_reward(evaluate(""connectx"" ,[MCTS_agent , ""random""], num_episodes = 20))) ",monte-carlo-tree-search-connectx.ipynb
 None represents which agent you ll manually play as first or second player .,"env.play ([MCTS_agent , None], width = 500 , height = 450) ",monte-carlo-tree-search-connectx.ipynb
Write Submission File,"import inspect

submission_path = ""submission.py""
 
def write_agent_to_file(function, file):
 with open(file, ""w"") as f:
 f.write(inspect.getsource(function))
 print(function, ""written to"", file)

write_agent_to_file(MCTS_agent, submission_path)",monte-carlo-tree-search-connectx.ipynb
Note: Stdout replacement is a temporary workaround.,import sys ,monte-carlo-tree-search-connectx.ipynb
Import,import os ,more-nli-datasets-hugging-face-nlp-library.ipynb
to silence warning,"os.environ[""WANDB_API_KEY""]= ""0"" ",more-nli-datasets-hugging-face-nlp-library.ipynb
Hugging Face new library for datasets ,! pip install nlp ,more-nli-datasets-hugging-face-nlp-library.ipynb
Competition dataset,"original_train = pd.read_csv(""../input/contradictory-my-dear-watson/train.csv"")

original_train = sklearn.utils.shuffle(original_train)
original_train = sklearn.utils.shuffle(original_train)

validation_ratio = 0.2
nb_valid_examples = max(1, int(len(original_train) * validation_ratio))

original_valid = original_train[:nb_valid_examples]
original_train = original_train[nb_valid_examples:]",more-nli-datasets-hugging-face-nlp-library.ipynb
"Load a dataset The Multi Genre NLI Corpus MNLI First, let s load the The Multi Genre NLI Corpus MultiNLI, MNLI . It contains sentence pairs annotated with textual entailment information.","mnli = nlp.load_dataset(path='glue', name='mnli')",more-nli-datasets-hugging-face-nlp-library.ipynb
"check the loaded datasetLet s look some information about the MNLI dataset. The default return value of nlp.load dataset is a dictionary with split names as keys, usually they are train, validation and test, but not always. The values are nlp.arrow dataset.Dataset.","print(mnli , '\n') ",more-nli-datasets-hugging-face-nlp-library.ipynb
Get the datasets,"print(""\nmnli['train'] is "" , type(mnli['train'])) ",more-nli-datasets-hugging-face-nlp-library.ipynb
Let s use what we learned to check some training examples,"print('The number of training examples in mnli dataset:', mnli['train'].num_rows)
print('The number of validation examples in mnli dataset - part 1:', mnli['validation_matched'].num_rows)
print('The number of validation examples in mnli dataset - part 2:', mnli['validation_mismatched'].num_rows, '\n')

print('The class names in mnli dataset:', mnli['train'].features['label'].names)
print('The feature names in mnli dataset:', list(mnli['train'].features.keys()), '\n')

for elt in mnli['train']:
 
 print('premise:', elt['premise'])
 print('hypothesis:', elt['hypothesis'])
 print('label:', elt['label'])
 print('label name:', mnli['train'].features['label'].names[elt['label']])
 print('idx', elt['idx'])
 print('-' * 80)
 
 if elt['idx'] >= 10:
 break",more-nli-datasets-hugging-face-nlp-library.ipynb
convert to a dataframe and view,mnli_train_df = pd.DataFrame(mnli['train']) ,more-nli-datasets-hugging-face-nlp-library.ipynb
"The Stanford Natural Language Inference Corpus SNLI First, let s load the The Stanford Natural Language Inference Corpus SNLI . It contains sentence pairs annotated with textual entailment information.","snli = nlp.load_dataset(path='snli')

print('The number of training examples in snli dataset:', snli['train'].num_rows)
print('The number of validation examples in snli dataset:', snli['validation'].num_rows, '\n')

print('The class names in snli dataset:', snli['train'].features['label'].names)
print('The feature names in snli dataset:', list(snli['train'].features.keys()), '\n')

for idx, elt in enumerate(snli['train']):
 
 print('premise:', elt['premise'])
 print('hypothesis:', elt['hypothesis'])
 print('label:', elt['label'])
 print('label name:', snli['train'].features['label'].names[elt['label']])
 print('-' * 80)
 
 if idx >= 10:
 break",more-nli-datasets-hugging-face-nlp-library.ipynb
convert to a dataframe and view,snli_train_df = pd.DataFrame(snli['train']) ,more-nli-datasets-hugging-face-nlp-library.ipynb
"The Cross Lingual NLI Corpus XNLI The MNLI and SNLI contain only english sentences. Let s load the Cross lingual NLI Corpus XNLI dataset. It contains only validation and test dataset, not training examples.","xnli = nlp.load_dataset(path='xnli')

print('The number of validation examples in xnli dataset:', xnli['validation'].num_rows, '\n')

print('The class names in xnli dataset:', xnli['validation'].features['label'].names)
print('The feature names in xnli dataset:', list(xnli['validation'].features.keys()), '\n')

for idx, elt in enumerate(xnli['validation']):
 
 print('premise:', elt['premise'])
 print('hypothesis:', elt['hypothesis'])
 print('label:', elt['label'])
 print('label name:', xnli['validation'].features['label'].names[elt['label']])
 print('-' * 80)
 
 if idx >= 3:
 break",more-nli-datasets-hugging-face-nlp-library.ipynb
convert to a dataframe and view,xnli_valid_df.head(15 * 3),more-nli-datasets-hugging-face-nlp-library.ipynb
Import,import os ,more-nli-datasets-xmlr-large.ipynb
to silence warning,"os.environ[""WANDB_API_KEY""]= ""0"" ",more-nli-datasets-xmlr-large.ipynb
Hugging Face new library for datasets ,! pip install nlp ,more-nli-datasets-xmlr-large.ipynb
Competition dataset,"original_train = pd.read_csv(""../input/contradictory-my-dear-watson/train.csv"")

original_train = shuffle(original_train)
original_valid = original_train[:len(original_train) // 5]
original_train = original_train[len(original_train) // 5:]",more-nli-datasets-xmlr-large.ipynb
"Load a dataset The Multi Genre NLI Corpus MNLI First, let s load the The Multi Genre NLI Corpus MultiNLI, MNLI . It contains sentence pairs annotated with textual entailment information.","mnli = nlp.load_dataset(path='glue', name='mnli')",more-nli-datasets-xmlr-large.ipynb
"check the loaded datasetLet s look some information about the MNLI dataset. The default return value of nlp.load dataset is a dictionary with split names as keys, usually they are train, validation and test, but not always. The values are nlp.arrow dataset.Dataset.","print(mnli , '\n') ",more-nli-datasets-xmlr-large.ipynb
Get the datasets,"print(""\nmnli['train'] is "" , type(mnli['train'])) ",more-nli-datasets-xmlr-large.ipynb
Let s use what we learned to check some training examples,"print('The number of training examples in mnli dataset:', mnli['train'].num_rows)
print('The number of validation examples in mnli dataset - part 1:', mnli['validation_matched'].num_rows)
print('The number of validation examples in mnli dataset - part 2:', mnli['validation_mismatched'].num_rows, '\n')

print('The class names in mnli dataset:', mnli['train'].features['label'].names)
print('The feature names in mnli dataset:', list(mnli['train'].features.keys()), '\n')

for elt in mnli['train']:
 
 print('premise:', elt['premise'])
 print('hypothesis:', elt['hypothesis'])
 print('label:', elt['label'])
 print('label name:', mnli['train'].features['label'].names[elt['label']])
 print('idx', elt['idx'])
 print('-' * 80)
 
 if elt['idx'] >= 10:
 break",more-nli-datasets-xmlr-large.ipynb
"The Stanford Natural Language Inference Corpus SNLI First, let s load the The Stanford Natural Language Inference Corpus SNLI . It contains sentence pairs annotated with textual entailment information.","snli = nlp.load_dataset(path='snli')

print('The number of training examples in snli dataset:', snli['train'].num_rows)
print('The number of validation examples in snli dataset:', snli['validation'].num_rows, '\n')

print('The class names in snli dataset:', snli['train'].features['label'].names)
print('The feature names in snli dataset:', list(snli['train'].features.keys()), '\n')

for idx, elt in enumerate(snli['train']):
 
 print('premise:', elt['premise'])
 print('hypothesis:', elt['hypothesis'])
 print('label:', elt['label'])
 print('label name:', snli['train'].features['label'].names[elt['label']])
 print('-' * 80)
 
 if idx >= 10:
 break",more-nli-datasets-xmlr-large.ipynb
"The Cross Lingual NLI Corpus XNLI The MNLI and SNLI contain only english sentences. Let s load the Cross lingual NLI Corpus XNLI dataset. It contains only validation and test dataset, not training examples.","xnli = nlp.load_dataset(path='xnli')

print('The number of validation examples in xnli dataset:', xnli['validation'].num_rows, '\n')

print('The class names in xnli dataset:', xnli['validation'].features['label'].names)
print('The feature names in xnli dataset:', list(xnli['validation'].features.keys()), '\n')

for idx, elt in enumerate(xnli['validation']):
 
 print('premise:', elt['premise'])
 print('hypothesis:', elt['hypothesis'])
 print('label:', elt['label'])
 print('label name:', xnli['validation'].features['label'].names[elt['label']])
 print('-' * 80)
 
 if idx >= 3:
 break",more-nli-datasets-xmlr-large.ipynb
Make a unified format of raw datasets,def _get_features(elt): ,more-nli-datasets-xmlr-large.ipynb
sanity check,"for k in raw_ds_mapping:
 for idx, x in enumerate(get_raw_dataset(k)):
 print(x)
 if idx >= 3:
 break",more-nli-datasets-xmlr-large.ipynb
Working with tf.data.Dataset,"def get_unbatched_dataset(ds_names , model_name , max_len = 64): ",more-nli-datasets-xmlr-large.ipynb
sanity check,for k in raw_ds_mapping.keys (): ,more-nli-datasets-xmlr-large.ipynb
print x , break ,more-nli-datasets-xmlr-large.ipynb
0.5,"def keep_head_tail(lst , len_wanted , head_ratio = 0.5): ",more-nli-datasets-xmlr-large.ipynb
Tranier,class Classifier(tf.keras.Model): ,more-nli-datasets-xmlr-large.ipynb
Sequence outputs," x = self.transformer(inputs , training = training)[ 0] ",more-nli-datasets-xmlr-large.ipynb
Train,"def print_config(trainer):

 print('nb. of training examples used: {}'.format(trainer.nb_examples))
 print('nb. of valid examples used: {}'.format(trainer.nb_valid_examples))
 print('nb. of test examples used: {}'.format(trainer.nb_test_examples))
 
 print('per replica batch size for training: {}'.format(trainer.batch_size_per_replica))
 print('batch size for training: {}'.format(trainer.batch_size))

 print('per replica batch size for prediction: {}'.format(trainer.prediction_batch_size_per_replica))
 print('batch size for prediction: {}'.format(trainer.prediction_batch_size))
 
 print('steps per epoch: {}'.format(trainer.steps_per_epoch))",more-nli-datasets-xmlr-large.ipynb
15,epochs = 15 ,more-nli-datasets-xmlr-large.ipynb
model name jplu tf xlm roberta base ,model_name = 'jplu/tf-xlm-roberta-large' ,more-nli-datasets-xmlr-large.ipynb
Plot history,"def plot(history, metric):
 """"""
 metric: 'loss' or 'acc'
 """"""
 
 h = {
 f'train {metric}': [history[epoch][f'train {metric}'] for epoch in history],
 f'valid {metric}': [history[epoch][f'valid {metric}'] for epoch in history]
 }
 
 fig = px.line(
 h, x=range(1, len(history) + 1), y=[f'train {metric}', f'valid {metric}'], 
 title=f'model {metric}', labels={'x': 'Epoch', 'value': metric}
 )
 fig.show()
 
def plot_2(history1, history2, metric, desc1, desc2):
 
 h = {
 f'train {metric} - {desc1}': [history1[epoch][f'train {metric}'] for epoch in history1],
 f'valid {metric} - {desc1}': [history1[epoch][f'valid {metric}'] for epoch in history1],
 f'train {metric} - {desc2}': [history2[epoch][f'train {metric}'] for epoch in history2],
 f'valid {metric} - {desc2}': [history2[epoch][f'valid {metric}'] for epoch in history2] 
 }
 
 fig = px.line(
 h, x=range(1, len(history1) + 1), y=[f'train {metric} - {desc1}', f'valid {metric} - {desc1}', f'train {metric} - {desc2}', f'valid {metric} - {desc2}'], 
 title=f'model {metric}', labels={'x': 'Epoch', 'value': metric}
 )
 fig.show() ",more-nli-datasets-xmlr-large.ipynb
original dataset xnli mnli,"np.savez_compressed('preds',a=preds)",more-nli-datasets-xmlr-large.ipynb
"plot 2 history 2, history 3, acc , desc1 xnli , desc2 mnli xnli ","s = pd.read_csv(f'submission-{train_name}.csv')
s.to_csv(f'submission.csv', index=False)

s.head(20)",more-nli-datasets-xmlr-large.ipynb
"IntroductionFirst encountering a new dataset can sometimes feel overwhelming. You might be presented with hundreds or thousands of features without even a description to go by. Where do you even begin?A great first step is to construct a ranking with a feature utility metric, a function measuring associations between a feature and the target. Then you can choose a smaller set of the most useful features to develop initially and have more confidence that your time will be well spent.The metric we ll use is called mutual information . Mutual information is a lot like correlation in that it measures a relationship between two quantities. The advantage of mutual information is that it can detect any kind of relationship, while correlation only detects linear relationships.Mutual information is a great general purpose metric and especially useful at the start of feature development when you might not know what model you d like to use yet. It is: easy to use and interpret, computationally efficient, theoretically well founded, resistant to overfitting, and, able to detect any kind of relationshipMutual Information and What it MeasuresMutual information describes relationships in terms of uncertainty. The mutual information MI between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. If you knew the value of a feature, how much more confident would you be about the target?Here s an example from the Ames Housing data. The figure shows the relationship between the exterior quality of a house and the price it sold for. Each point represents a house. Knowing the exterior quality of a house reduces uncertainty about its sale price. From the figure, we can see that knowing the value of ExterQual should make you more certain about the corresponding SalePrice each category of ExterQual tends to concentrate SalePrice to within a certain range. The mutual information that ExterQual has with SalePrice is the average reduction of uncertainty in SalePrice taken over the four values of ExterQual. Since Fair occurs less often than Typical, for instance, Fair gets less weight in the MI score. Technical note: What we re calling uncertainty is measured using a quantity from information theory known as entropy . The entropy of a variable means roughly: how many yes or no questions you would need to describe an occurance of that variable, on average. The more questions you have to ask, the more uncertain you must be about the variable. Mutual information is how many questions you expect the feature to answer about the target. Interpreting Mutual Information ScoresThe least possible mutual information between quantities is 0.0. When MI is zero, the quantities are independent: neither can tell you anything about the other. Conversely, in theory there s no upper bound to what MI can be. In practice though values above 2.0 or so are uncommon. Mutual information is a logarithmic quantity, so it increases very slowly. The next figure will give you an idea of how MI values correspond to the kind and degree of association a feature has with the target. Left: Mutual information increases as the dependence between feature and target becomes tighter. Right: Mutual information can capture any kind of association not just linear, like correlation. Here are some things to remember when applying mutual information: MI can help you to understand the relative potential of a feature as a predictor of the target, considered by itself. It s possible for a feature to be very informative when interacting with other features, but not so informative all alone. MI can t detect interactions between features. It is a univariate metric. The actual usefulness of a feature depends on the model you use it with. A feature is only useful to the extent that its relationship with the target is one your model can learn. Just because a feature has a high MI score doesn t mean your model will be able to do anything with that information. You may need to transform the feature first to expose the association.Example 1985 AutomobilesThe Automobile dataset consists of 193 cars from the 1985 model year. The goal for this dataset is to predict a car s price the target from 23 of the car s features, such as make, body style, and horsepower. In this example, we ll rank the features with mutual information and investigate the results by data visualization.This hidden cell imports some libraries and loads the dataset.","
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

plt.style.use(""seaborn-whitegrid"")

df = pd.read_csv(""../input/fe-course-data/autos.csv"")
df.head()",mutual-information.ipynb
"The scikit learn algorithm for MI treats discrete features differently from continuous features. Consequently, you need to tell it which are which. As a rule of thumb, anything that must have a float dtype is not discrete. Categoricals object or categorial dtype can be treated as discrete by giving them a label encoding. You can review label encodings in our Categorical Variables lesson. ",X = df.copy () ,mutual-information.ipynb
Label encoding for categoricals,"for colname in X.select_dtypes(""object""): ",mutual-information.ipynb
All discrete features should now have integer dtypes double check this before using MI! ,discrete_features = X.dtypes == int ,mutual-information.ipynb
"Scikit learn has two mutual information metrics in its feature selection module: one for real valued targets mutual info regression and one for categorical targets mutual info classif . Our target, price, is real valued. The next cell computes the MI scores for our features and wraps them up in a nice dataframe.",from sklearn.feature_selection import mutual_info_regression ,mutual-information.ipynb
show a few features with their MI scores,mi_scores[: : 3] ,mutual-information.ipynb
And now a bar plot to make comparisions easier:,"def plot_mi_scores(scores):
 scores = scores.sort_values(ascending=True)
 width = np.arange(len(scores))
 ticks = list(scores.index)
 plt.barh(width, scores)
 plt.yticks(width, ticks)
 plt.title(""Mutual Information Scores"")


plt.figure(dpi=100, figsize=(8, 5))
plot_mi_scores(mi_scores)",mutual-information.ipynb
"Data visualization is a great follow up to a utility ranking. Let s take a closer look at a couple of these.As we might expect, the high scoring curb weight feature exhibits a strong relationship with price, the target.","sns.relplot(x=""curb_weight"", y=""price"", data=df);",mutual-information.ipynb
"The fuel type feature has a fairly low MI score, but as we can see from the figure, it clearly separates two price populations with different trends within the horsepower feature. This indicates that fuel type contributes an interaction effect and might not be unimportant after all. Before deciding a feature is unimportant from its MI score, it s good to investigate any possible interaction effects domain knowledge can offer a lot of guidance here.","sns.lmplot(x=""horsepower"", y=""price"", hue=""fuel_type"", data=df);",mutual-information.ipynb
"IntroductionIn the previous tutorial, you learned how to build an agent with one step lookahead. This agent performs reasonably well, but definitely still has room for improvement! For instance, consider the potential moves in the figure below. Note that we use zero based numbering for the columns, so the leftmost column corresponds to col 0, the next column corresponds to col 1, and so on. With one step lookahead, the red player picks one of column 5 or 6, each with 50 probability. But, column 5 is clearly a bad move, as it lets the opponent win the game in only one more turn. Unfortunately, the agent doesn t know this, because it can only look one move into the future. In this tutorial, you ll use the minimax algorithm to help the agent look farther into the future and make better informed decisions.MinimaxWe d like to leverage information from deeper in the game tree. For now, assume we work with a depth of 3. This way, when deciding its move, the agent considers all possible game boards that can result from 1. the agent s move, 2. the opponent s move, and 3. the agent s next move. We ll work with a visual example. For simplicity, we assume that at each turn, both the agent and opponent have only two possible moves. Each of the blue rectangles in the figure below corresponds to a different game board. We have labeled each of the leaf nodes at the bottom of the tree with the score from the heuristic. We use made up scores in the figure. In the code, we ll use the same heuristic from the previous tutorial. As before, the current game board is at the top of the figure, and the agent s goal is to end up with a score that s as high as possible. But notice that the agent no longer has complete control over its score after the agent makes its move, the opponent selects its own move. And, the opponent s selection can prove disastrous for the agent! In particular, If the agent chooses the left branch, the opponent can force a score of 1. If the agent chooses the right branch, the opponent can force a score of 10. Take the time now to check this in the figure, to make sure it makes sense to you!With this in mind, you might argue that the right branch is the better choice for the agent, since it is the less risky option. Sure, it gives up the possibility of getting the large score 40 that can only be accessed on the left branch, but it also guarantees that the agent gets at least 10 points.This is the main idea behind the minimax algorithm: the agent chooses moves to get a score that is as high as possible, and it assumes the opponent will counteract this by choosing moves to force the score to be as low as possible. That is, the agent and opponent have opposing goals, and we assume the opponent plays optimally.So, in practice, how does the agent use this assumption to select a move? We illustrate the agent s thought process in the figure below. In the example, minimax assigns the move on the left a score of 1, and the move on the right is assigned a score of 10. So, the agent will select the move on the right. CodeWe ll use several functions from the previous tutorial. These are defined in the hidden code cell below. Click on the Code button below if you d like to view them. ",import random ,n-step-lookahead.ipynb
Gets board at next step if agent drops piece in selected column,"def drop_piece(grid , col , mark , config): ",n-step-lookahead.ipynb
Helper function for get heuristic: checks if window satisfies heuristic conditions,"def check_window(window , num_discs , piece , config): ",n-step-lookahead.ipynb
Helper function for get heuristic: counts number of windows satisfying specified heuristic conditions,"def count_windows(grid , num_discs , piece , config): ",n-step-lookahead.ipynb
horizontal, for row in range(config.rows): ,n-step-lookahead.ipynb
vertical, for row in range(config.rows -(config.inarow - 1)) : ,n-step-lookahead.ipynb
positive diagonal, for row in range(config.rows -(config.inarow - 1)) : ,n-step-lookahead.ipynb
negative diagonal," for row in range(config.inarow - 1 , config.rows): ",n-step-lookahead.ipynb
Helper function for minimax: calculates value of heuristic for grid,"def get_heuristic(grid , mark , config): ",n-step-lookahead.ipynb
Uses minimax to calculate value of dropping piece in selected column,"def score_move(grid , col , mark , config , nsteps): ",n-step-lookahead.ipynb
Helper function for minimax: checks if agent or opponent has four in a row in the window,"def is_terminal_window(window , config): ",n-step-lookahead.ipynb
Helper function for minimax: checks if game has ended,"def is_terminal_node(grid , config): ",n-step-lookahead.ipynb
Check for draw," if list(grid[0 , :]).count(0)== 0 : ",n-step-lookahead.ipynb
horizontal, for row in range(config.rows): ,n-step-lookahead.ipynb
vertical, for row in range(config.rows -(config.inarow - 1)) : ,n-step-lookahead.ipynb
positive diagonal, for row in range(config.rows -(config.inarow - 1)) : ,n-step-lookahead.ipynb
negative diagonal," for row in range(config.inarow - 1 , config.rows): ",n-step-lookahead.ipynb
Minimax implementation,"def minimax(node , depth , maximizingPlayer , mark , config): ",n-step-lookahead.ipynb
How deep to make the game tree: higher values take longer to run!,N_STEPS = 3 ,n-step-lookahead.ipynb
Get list of valid moves, valid_moves =[c for c in range(config.columns)if obs.board[c]== 0] ,n-step-lookahead.ipynb
Convert the board to a 2D grid," grid = np.asarray(obs.board). reshape(config.rows , config.columns) ",n-step-lookahead.ipynb
Use the heuristic to assign a score to each possible board in the next step," scores = dict(zip(valid_moves ,[score_move(grid , col , obs.mark , config , N_STEPS)for col in valid_moves])) ",n-step-lookahead.ipynb
Get a list of columns moves that maximize the heuristic, max_cols =[key for key in scores.keys ()if scores[key]== max(scores.values ())] ,n-step-lookahead.ipynb
Select at random from the maximizing columns, return random.choice(max_cols) ,n-step-lookahead.ipynb
"In the next code cell, we see the outcome of one game round against a random agent.","from kaggle_environments import make , evaluate ",n-step-lookahead.ipynb
Create the game environment,"env = make(""connectx"") ",n-step-lookahead.ipynb
Two random agents play one game round,"env.run ([agent , ""random""]) ",n-step-lookahead.ipynb
Show the game,"env.render(mode = ""ipython"") ",n-step-lookahead.ipynb
And we check how we can expect it to perform on average.,"def get_win_percentages(agent1 , agent2 , n_rounds = 100): ",n-step-lookahead.ipynb
Use default Connect Four setup," config = { 'rows' : 6 , 'columns' : 7 , 'inarow' : 4 } ",n-step-lookahead.ipynb
Agent 1 goes first roughly half the time," outcomes = evaluate(""connectx"" ,[agent1 , agent2], config , [], n_rounds // 2) ",n-step-lookahead.ipynb
Agent 2 goes first roughly half the time," outcomes +=[[ b , a]for[a , b]in evaluate(""connectx"" ,[agent2 , agent1], config , [], n_rounds - n_rounds // 2 )] ",n-step-lookahead.ipynb
"NLP Natural Language Processing with Python Agenda Representing text as numerical data Reading a text based dataset into pandas Vectorizing our dataset Building and evaluating a model Comparing models Examining a model for further insight Practicing this workflow on another dataset Tuning the vectorizer discussion Notebook Goals In this notebook we will discuss a higher level overview of the basics of Natural Language Processing, which basically consists of combining machine learning techniques with text, and using math and statistics to get that text in a format that the machine learning algorithms can understand! ","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.set_style(""whitegrid"")
plt.style.use(""fivethirtyeight"")",natural-language-processing-nlp-for-beginners.ipynb
example text for model training SMS messages ,"simple_train =['call you tonight' , 'Call me a cab' , 'Please call me... PLEASE!'] ",natural-language-processing-nlp-for-beginners.ipynb
import and instantiate CountVectorizer with the default parameters ,from sklearn.feature_extraction.text import CountVectorizer ,natural-language-processing-nlp-for-beginners.ipynb
learn the vocabulary of the training data occurs in place ,vect.fit(simple_train) ,natural-language-processing-nlp-for-beginners.ipynb
examine the fitted vocabulary,vect.get_feature_names () ,natural-language-processing-nlp-for-beginners.ipynb
transform training data into a document term matrix ,simple_train_dtm = vect.transform(simple_train) ,natural-language-processing-nlp-for-beginners.ipynb
convert sparse matrix to a dense matrix,simple_train_dtm.toarray () ,natural-language-processing-nlp-for-beginners.ipynb
examine the vocabulary and document term matrix together,"pd.DataFrame(simple_train_dtm.toarray (), columns = vect.get_feature_names ()) ",natural-language-processing-nlp-for-beginners.ipynb
check the type of the document term matrix,type(simple_train_dtm) ,natural-language-processing-nlp-for-beginners.ipynb
examine the sparse matrix contents,print(simple_train_dtm) ,natural-language-processing-nlp-for-beginners.ipynb
example text for model testing,"simple_test =[""please don't call me""] ",natural-language-processing-nlp-for-beginners.ipynb
transform testing data into a document term matrix using existing vocabulary ,simple_test_dtm = vect.transform(simple_test) ,natural-language-processing-nlp-for-beginners.ipynb
examine the vocabulary and document term matrix together,"pd.DataFrame(simple_test_dtm.toarray (), columns = vect.get_feature_names ()) ",natural-language-processing-nlp-for-beginners.ipynb
read file into pandas using a relative path,"sms = pd.read_csv(""/kaggle/input/sms-spam-collection-dataset/spam.csv"" , encoding = 'latin-1') ",natural-language-processing-nlp-for-beginners.ipynb
Exploratory Data Analysis EDA ,sms.describe(),natural-language-processing-nlp-for-beginners.ipynb
convert label to a numerical variable,"sms['label_num']= sms.label.map({ 'ham' : 0 , 'spam' : 1 }) ",natural-language-processing-nlp-for-beginners.ipynb
"As we continue our analysis we want to start thinking about the features we are going to be using. This goes along with the general idea of feature engineering. The better your domain knowledge on the data, the better your ability to engineer more features from it. Feature engineering is a very large part of spam detection in general. ","sms['message_len'] = sms.message.apply(len)
sms.head()",natural-language-processing-nlp-for-beginners.ipynb
Very interesting! Through just basic EDA we ve been able to discover a trend that spam messages tend to have more characters. ,sms[sms.label=='ham'].describe(),natural-language-processing-nlp-for-beginners.ipynb
"Woah! 910 characters, let s use masking to find this message: ",sms[sms.message_len == 910].message.iloc[0],natural-language-processing-nlp-for-beginners.ipynb
"Text Pre processing Our main issue with our data is that it is all in text format strings . The classification algorithms that we usally use need some sort of numerical feature vector in order to perform the classification task. There are actually many methods to convert a corpus to a vector format. The simplest is the bag of words approach, where each unique word in a text will be represented by one number. In this section we ll convert the raw messages sequence of characters into vectors sequences of numbers . As a first step, let s write a function that will split a message into its individual words and return a list. We ll also remove very common words, the , a , etc.. . To do this we will take advantage of the NLTK library. It s pretty much the standard library in Python for processing text and has a lot of useful features. We ll only use some of the basic ones here. Let s create a function that will process the string in the message column, then we can just use apply in pandas do process all the text in the DataFrame. First removing punctuation. We can just take advantage of Python s built in string library to get a quick list of all the possible punctuation: ",import string ,natural-language-processing-nlp-for-beginners.ipynb
Now let s tokenize these messages. Tokenization is just the term used to describe the process of converting the normal text strings in to a list of tokens words that we actually want . ,sms['clean_msg'] = sms.message.apply(text_process),natural-language-processing-nlp-for-beginners.ipynb
how to define X and y from the SMS data for use with COUNTVECTORIZER,X = sms.clean_msg ,natural-language-processing-nlp-for-beginners.ipynb
split X and y into training and testing sets,from sklearn.model_selection import train_test_split ,natural-language-processing-nlp-for-beginners.ipynb
There are a lot of arguments and parameters that can be passed to the CountVectorizer. In this case we will just specify the analyzer to be our own previously defined function: ,from sklearn.feature_extraction.text import CountVectorizer ,natural-language-processing-nlp-for-beginners.ipynb
instantiate the vectorizer,vect = CountVectorizer () ,natural-language-processing-nlp-for-beginners.ipynb
"learn training data vocabulary, then use it to create a document term matrix",X_train_dtm = vect.transform(X_train) ,natural-language-processing-nlp-for-beginners.ipynb
equivalently: combine fit and transform into a single step,X_train_dtm = vect.fit_transform(X_train) ,natural-language-processing-nlp-for-beginners.ipynb
examine the document term matrix,X_train_dtm ,natural-language-processing-nlp-for-beginners.ipynb
transform testing data using fitted vocabulary into a document term matrix,X_test_dtm = vect.transform(X_test) ,natural-language-processing-nlp-for-beginners.ipynb
import and instantiate a Multinomial Naive Bayes model,from sklearn.naive_bayes import MultinomialNB ,natural-language-processing-nlp-for-beginners.ipynb
train the model using X train dtm timing it with an IPython magic command ,"% time nb.fit(X_train_dtm , y_train) ",natural-language-processing-nlp-for-beginners.ipynb
make class predictions for X test dtm,y_pred_class = nb.predict(X_test_dtm) ,natural-language-processing-nlp-for-beginners.ipynb
calculate accuracy of class predictions,from sklearn import metrics ,natural-language-processing-nlp-for-beginners.ipynb
print the confusion matrix,"metrics.confusion_matrix(y_test , y_pred_class) ",natural-language-processing-nlp-for-beginners.ipynb
X test y pred class 1 y test 0 ,X_test[y_pred_class > y_test] ,natural-language-processing-nlp-for-beginners.ipynb
print message text for false negatives spam incorrectly classifier ,X_test[y_pred_class < y_test] ,natural-language-processing-nlp-for-beginners.ipynb
example of false negative,X_test[4949] ,natural-language-processing-nlp-for-beginners.ipynb
calculate predicted probabilities for X test dtm poorly calibrated ,"y_pred_prob = nb.predict_proba(X_test_dtm)[ : , 1] ",natural-language-processing-nlp-for-beginners.ipynb
calculate AUC,"metrics.roc_auc_score(y_test , y_pred_prob) ",natural-language-processing-nlp-for-beginners.ipynb
import an instantiate a logistic regression model,from sklearn.linear_model import LogisticRegression ,natural-language-processing-nlp-for-beginners.ipynb
train the model using X train dtm,"% time logreg.fit(X_train_dtm , y_train) ",natural-language-processing-nlp-for-beginners.ipynb
make class predictions for X test dtm,y_pred_class = logreg.predict(X_test_dtm) ,natural-language-processing-nlp-for-beginners.ipynb
calculate predicted probabilities for X test dtm well calibrated ,"y_pred_prob = logreg.predict_proba(X_test_dtm)[ : , 1] ",natural-language-processing-nlp-for-beginners.ipynb
calculate accuracy,"metrics.accuracy_score(y_test , y_pred_class) ",natural-language-processing-nlp-for-beginners.ipynb
calculate AUC,"metrics.roc_auc_score(y_test , y_pred_prob) ",natural-language-processing-nlp-for-beginners.ipynb
show default parameters for CountVectorizer,vect ,natural-language-processing-nlp-for-beginners.ipynb
remove English stop words,vect = CountVectorizer(stop_words = 'english') ,natural-language-processing-nlp-for-beginners.ipynb
include 1 grams and 2 grams,"vect = CountVectorizer(ngram_range =(1 , 2)) ",natural-language-processing-nlp-for-beginners.ipynb
ignore terms that appear in more than 50 of the documents,vect = CountVectorizer(max_df = 0.5) ,natural-language-processing-nlp-for-beginners.ipynb
only keep terms that appear in at least 2 documents,vect = CountVectorizer(min_df = 2) ,natural-language-processing-nlp-for-beginners.ipynb
linear algebra,import numpy as np ,neural-network-version.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,neural-network-version.ipynb
"For example, running this by clicking run or pressing Shift Enter will list the files in the input directory",import os ,neural-network-version.ipynb
Any results you write to the current directory are saved as output.,"train = pd.read_csv(""../input/train.csv"", header=None)",neural-network-version.ipynb
Importing the Keras libraries and packages,import keras ,neural-network-version.ipynb
Initializing Neural Network,classifier = Sequential () ,neural-network-version.ipynb
Adding the input layer and the first hidden layer,"classifier.add(Dense(output_dim = 128 , init = 'uniform' , activation = 'relu' , input_dim = 40)) ",neural-network-version.ipynb
Adding the second hidden layer,"classifier.add(Dense(output_dim = 64 , init = 'uniform' , activation = 'relu')) ",neural-network-version.ipynb
Adding the output layer,"classifier.add(Dense(output_dim = 1 , init = 'uniform' , activation = 'sigmoid')) ",neural-network-version.ipynb
Compiling Neural Network,"classifier.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics =['accuracy']) ",neural-network-version.ipynb
Fitting our model,"classifier.fit(X , y , batch_size = 10 , nb_epoch = 500) ",neural-network-version.ipynb
Predicting the Test set results,y_pred = classifier.predict(X) ,neural-network-version.ipynb
Import Necessary Libraries,"import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf",neural-style-transfer-photo-to-monet.ipynb
Load pre trained model,model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'),neural-style-transfer-photo-to-monet.ipynb
function to load image and display,"def load_image(img_path):
 img = tf.io.read_file(img_path)
 img = tf.image.decode_image(img, channels = 3)
 img = tf.image.convert_image_dtype(img, tf.float32)
 img = img[tf.newaxis , :]
 return img",neural-style-transfer-photo-to-monet.ipynb
Plotting images,"content_image = load_image('../input/gan-getting-started/photo_jpg/00e1798585.jpg')
style_image = load_image('../input/gan-getting-started/monet_jpg/184d6c66cd.jpg')
plot(content_image,style_image)
print(""====""*38)
stylized_image = model(tf.constant(content_image), tf.constant(style_image))[0]
plot_pair(content_image,style_image,stylized_image)",neural-style-transfer-photo-to-monet.ipynb
Data Overview and EDA,"df_train = pd.read_csv('../input/contradictory-my-dear-watson/train.csv')
df_test = pd.read_csv('../input/contradictory-my-dear-watson/test.csv')",nli-beginner-eda-bert-baseline.ipynb
No null values in both train and test data,train_language_cnt = df_train['language']. value_counts(sort = False) ,nli-beginner-eda-bert-baseline.ipynb
English take the dominant place in both train and test data.,"label_cnt = df_train['label'].value_counts()
trace = go.Pie(labels = label_cnt.index, 
 values = label_cnt.values,
 hoverinfo = 'percent+value+label',
 textinfo = 'percent',
 textposition = 'inside',
 textfont = dict(size=14),
 title = 'Label',
 titlefont = dict(size=15),
 hole = 0.3,
 showlegend = True,
 marker = dict(line=dict(color='black',width=2)))
fig = go.Figure(data=[trace])
fig.update_layout(height=500, width=500)
fig.show()",nli-beginner-eda-bert-baseline.ipynb
The distribution of train labels is quite balanced on the whole and in differnt languages.,"df_train['premise_word_cnt'] = df_train['premise'].apply(lambda x: len(x.split()))
df_train['hypothesis_word_cnt'] = df_train['hypothesis'].apply(lambda x: len(x.split()))

fig,ax = plt.subplots(nrows=5,ncols=3,figsize=(15,20))
for i,lang in enumerate(list(train_language_cnt.index)):
 sns.distplot(df_train[df_train.language == lang]['premise_word_cnt'],bins=20,
 color='red',label='Premise',kde=False, ax=ax[divmod(i,3)[0],divmod(i,3)[1]])
 sns.distplot(df_train[df_train.language == lang]['hypothesis_word_cnt'],bins=20,
 color='blue',label='Hypothesis',kde=False, ax=ax[divmod(i,3)[0],divmod(i,3)[1]])
 ax[divmod(i,3)[0],divmod(i,3)[1]].set_title(lang,fontsize=13)
 ax[divmod(i,3)[0],divmod(i,3)[1]].legend()
 ax[divmod(i,3)[0],divmod(i,3)[1]].set_xlabel('')
fig.text(0.13, 0.95, 'Train: Premise & Hypothesis Word Count', fontsize=15, fontweight='bold') ",nli-beginner-eda-bert-baseline.ipynb
Thanks to the encoding method for Bert base in toturial notebook,"def encode_sentence(s):
 tokens = list(tokenizer.tokenize(s))
 tokens.append('[SEP]')
 return tokenizer.convert_tokens_to_ids(tokens)",nli-beginner-eda-bert-baseline.ipynb
Build Bert Baseline,K.clear_session(),nli-beginner-eda-bert-baseline.ipynb
sequence output clf output 0 ," output = tf.keras.layers.Dense(3 , activation = 'softmax')( sequence_output[: , 0 , :]) ",nli-beginner-eda-bert-baseline.ipynb
print layer.output shape ,filepath = 'best_weight.hdf5' ,nli-beginner-eda-bert-baseline.ipynb
Model idea from here,K.clear_session(),nli-beginner-eda-bert-baseline.ipynb
print layer.output shape ,filepath = 'LSTM_best_weight.hdf5' ,nli-beginner-eda-bert-baseline.ipynb
Thanks to the notebook as a complete guidance on TPU configuration and XLM roBerta Large fine tuning,K.clear_session(),nli-beginner-eda-bert-baseline.ipynb
"Detect hardware, return appropriate distribution strategy",try : ,nli-beginner-eda-bert-baseline.ipynb
set: this is always the case on Kaggle., tpu = tf.distribute.cluster_resolver.TPUClusterResolver () ,nli-beginner-eda-bert-baseline.ipynb
Default distribution strategy in Tensorflow. Works on CPU and single GPU., strategy = tf.distribute.get_strategy () ,nli-beginner-eda-bert-baseline.ipynb
sequence output clf output 0 ," output = tf.keras.layers.Dense(3 , activation = 'softmax')( sequence_output[: , 0 , :]) ",nli-beginner-eda-bert-baseline.ipynb
print layer.output shape , filepath = 'roberta_best_weight.hdf5' ,nli-beginner-eda-bert-baseline.ipynb
Cross Validation Score,def model_cv_score(model): ,nli-beginner-eda-bert-baseline.ipynb
subject to change when switched to roBerta," cv_train_input = bert_encode(train_premise.values , train_hypothesis.values , tokenizer) ",nli-beginner-eda-bert-baseline.ipynb
model build bert max len 55 , filepath = 'best_weight_fold%s.h5' %(fold) ,nli-beginner-eda-bert-baseline.ipynb
pred np.argmax res for res in model.predict test input ,"test_text = df_test[['premise', 'hypothesis']].values.tolist()
encoded_test_text = roberta_encode(test_text,tokenizer,max_len=80)
tf_test_dataset = (tf.data.Dataset
 .from_tensor_slices(encoded_test_text['input_ids'])
 .batch(BATCH_SIZE))
roberta_model.load_weights('./roberta_best_weight.hdf5')
pred = [np.argmax(res) for res in roberta_model.predict(tf_test_dataset)]",nli-beginner-eda-bert-baseline.ipynb
"Task:We have two sentences, there are three ways they could be related: one could entail the other entailment one could contradict the other contradiction or they could be unrelated neutral Natural Language Inferencing NLI is a popular NLP problem that involves determining how pairs of sentences consisting of a premise and a hypothesis are related. In this notebook, Will explore the Contradictory, My Dear Watson challange.",import os ,nlp-augmenter-5-fold-bert-translator.ipynb
linear algebra,import numpy as np ,nlp-augmenter-5-fold-bert-translator.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,nlp-augmenter-5-fold-bert-translator.ipynb
px.offline.init notebook mode connected True ,from IPython.core.display import HTML ,nlp-augmenter-5-fold-bert-translator.ipynb
directory,"DATA_DIR = ""/kaggle/input/contradictory-my-dear-watson"" ",nlp-augmenter-5-fold-bert-translator.ipynb
read train and test csv files,"train_df = pd.read_csv(os.path.join(DATA_DIR , ""train.csv"")) ",nlp-augmenter-5-fold-bert-translator.ipynb
print f train data train df.shape test data test df.shape ,"dataset_size = {""train set"":train_df.shape[0], ""test set"":test_df.shape[0]}
fig = px.bar(y = list(dataset_size.keys()), x = list(dataset_size.values()), 
 title=""Distribution of train and test"", text= list(dataset_size.values()))
fig.update_layout(
 xaxis_title=""No of samples"",
 yaxis_title=""Dataset"")
fig.show()",nlp-augmenter-5-fold-bert-translator.ipynb
show list of columns available,"html = '<h3>Available columns in the Training data:</h3></br><center><table style=""width:50%; border: 1px solid black; ""><tr style=""border: 1px solid black""><th style=""border: 1px solid black"">Column name</th><th style=""border: 1px solid black"">Desc</th></tr>' ",nlp-augmenter-5-fold-bert-translator.ipynb
Input text contains multiple language. This could be real challanging itself to handle Multi lingual data.,"language_distribution_class = train_df[[ ""id"" , ""language"" , ""label""]].groupby ([""language"" , ""label""])[""id""]. count (). reset_index (). rename(columns = { ""id"" : ""count"" , ""label"" : 'class' }) ",nlp-augmenter-5-fold-bert-translator.ipynb
total count of samples for each language,"language_distribution_class[""total_count""]= language_distribution_class[""language""]. map(languages_count_dict) ",nlp-augmenter-5-fold-bert-translator.ipynb
"There is 56.7 English text available in which 35 , 32 and 33 data distributed for each label 0,1, and 2 .Likewise, other language text vary from 3.39 to 2.89 distribution. This is highly unbalanced dataset with respect to each language but we would need to check with the distribution of each labels.","label_distribution = train_df[""label""].value_counts() 
label_distribution.index = [""entailment"", ""neutral"", ""contradiction""]

fig = px.bar(label_distribution, title=""Label (target variable) distribution"")
fig.update_layout(
 xaxis_title=""Labels"",
 yaxis_title=""Frequency"")
fig.show()",nlp-augmenter-5-fold-bert-translator.ipynb
Label distribution almost balanced for each class.,"stopwords = set(STOPWORDS)

url = ""https://raw.githubusercontent.com/amueller/word_cloud/master/examples/a_new_hope.png""
response = requests.get(url)
img = Image.open(BytesIO(response.content))

wordcloud = WordCloud(stopwords=stopwords, contour_width=3, contour_color='steelblue', background_color=""white"", max_words=500).generate("","".join(train_df[train_df[""language""].str.contains(""English"")][""premise""].tolist()))
mask = np.array(img)

plt.figure(figsize = (20,15))
plt.imshow(mask, cmap=plt.cm.gray, interpolation='bilinear')
plt.axis(""off"")
plt.title(""English word distribution"",fontdict={""fontsize"":20}, pad=2)
plt.show()",nlp-augmenter-5-fold-bert-translator.ipynb
instantiate translator,translator = Translator () ,nlp-augmenter-5-fold-bert-translator.ipynb
function to translate sentence,"def translate_sentence(sentence , src_lang): ",nlp-augmenter-5-fold-bert-translator.ipynb
translation for training data," for index , row in tqdm(train_df.iterrows ()) : ",nlp-augmenter-5-fold-bert-translator.ipynb
translate premise sentence train set," train_df.loc[index , ""premise_translated""]= translate_sentence(row['premise'], row[""lang_abv""]) ",nlp-augmenter-5-fold-bert-translator.ipynb
translate hypothesis sentence train set," train_df.loc[index , ""hypothesis_translated""]= translate_sentence(row['hypothesis'], row[""lang_abv""]) ",nlp-augmenter-5-fold-bert-translator.ipynb
save translated dataframe," train_df.to_csv(file_name , index = False) ",nlp-augmenter-5-fold-bert-translator.ipynb
translation for testing data," for index , row in tqdm(test_df.iterrows ()) : ",nlp-augmenter-5-fold-bert-translator.ipynb
translate premise sentence train set," test_df.loc[index , ""premise_translated""]= translate_sentence(row['premise'], row[""lang_abv""]) ",nlp-augmenter-5-fold-bert-translator.ipynb
translate hypothesis sentence train set," test_df.loc[index , ""hypothesis_translated""]= translate_sentence(row['hypothesis'], row[""lang_abv""]) ",nlp-augmenter-5-fold-bert-translator.ipynb
save translated dataframe," train_df.to_csv(file_name , index = False) ",nlp-augmenter-5-fold-bert-translator.ipynb
Stratified KFold,"FOLD = 5
k_fold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=42)
for fold, (train_idx, val_idx) in enumerate(k_fold.split(train_df, y=train_df[""label""])):
 print(fold, train_idx.shape, val_idx.shape)
 train_df.loc[val_idx,'fold'] = fold",nlp-augmenter-5-fold-bert-translator.ipynb
setting up TPU,try : ,nlp-augmenter-5-fold-bert-translator.ipynb
for CPU and single GPU, strategy = tf.distribute.get_strategy () ,nlp-augmenter-5-fold-bert-translator.ipynb
"Input tokens should be tokenized in CLS Hello, my dog is cute SEP your dog too. SEP format.","def encode_sentence(sentence):
 tokens = list(tokenizer.tokenize(sentence))
 tokens.append('[SEP]')
 return tokenizer.convert_tokens_to_ids(tokens)",nlp-augmenter-5-fold-bert-translator.ipynb
There are various ways that you can do word augument. Word embeddings TF IDF Contextual Word Embeddings SynonymWill experiment with Synonym augment. Will do substitute the word by WordNet s synonym,"!pip install nlpaug
import nlpaug.augmenter.word as naw",nlp-augmenter-5-fold-bert-translator.ipynb
augment text,"def agument_text(data_df , col_name): ",nlp-augmenter-5-fold-bert-translator.ipynb
test set input,"test_input = bert_encoder(test_df['premise_translated']. values , test_df['hypothesis_translated']. values , tokenizer) ",nlp-augmenter-5-fold-bert-translator.ipynb
storing prediction results,"test_prediction = np.zeros(( test_df.shape[0], 3)) ",nlp-augmenter-5-fold-bert-translator.ipynb
iterate each fold,for fold in range(FOLD): ,nlp-augmenter-5-fold-bert-translator.ipynb
agument 5 of training data, sample_selection = train_df_fold.sample(frac = 0.05). reset_index(drop = True) ,nlp-augmenter-5-fold-bert-translator.ipynb
predict for unseen data, predictions = model.predict(test_input) ,nlp-augmenter-5-fold-bert-translator.ipynb
take mean of the prediction,test_prediction = test_prediction / FOLD ,nlp-augmenter-5-fold-bert-translator.ipynb
take the maximum probability value,"test_prediction = np.argmax(test_prediction , axis = 1) ",nlp-augmenter-5-fold-bert-translator.ipynb
Submittion of Predictions,"test_df[""prediction""] = test_prediction",nlp-augmenter-5-fold-bert-translator.ipynb
submission file,"test_df[[ ""id"" , ""prediction""]].head () ",nlp-augmenter-5-fold-bert-translator.ipynb
Project Goal To analyze tweets classifying them into disaster and non disaster ones to extract useful information during crises,"import numpy as np
import pandas as pd
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
 for filename in filenames:
 print(os.path.join(dirname, filename))
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns
plt.style.use('ggplot')
import re
import nltk
from nltk.util import ngrams
from nltk.corpus import stopwords
stop=set(stopwords.words('english'))
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from collections import defaultdict
from collections import Counter
from sklearn.model_selection import train_test_split
import keras
from keras.models import Sequential
from keras.initializers import Constant
from keras.layers import (LSTM, 
 Embedding, 
 BatchNormalization,
 Dense, 
 TimeDistributed, 
 Dropout, 
 Bidirectional,
 Flatten, 
 GlobalMaxPool1D)
from nltk.tokenize import word_tokenize
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers.embeddings import Embedding
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from keras.optimizers import Adam
from sklearn.metrics import (
 precision_score, 
 recall_score, 
 f1_score, 
 classification_report,
 accuracy_score
)",nlp-disaster-tweets-with-glove-and-lstm.ipynb
EDA and Preprocessing,"tweet = pd.read_csv('../input/nlp-getting-started/train.csv')
test = pd.read_csv('../input/nlp-getting-started/test.csv')
submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')",nlp-disaster-tweets-with-glove-and-lstm.ipynb
Checking the class distribution,x = tweet.target.value_counts () ,nlp-disaster-tweets-with-glove-and-lstm.ipynb
Number of characters in tweets,"fig ,(ax1 , ax2)= plt.subplots(1 , 2 , figsize =(10 , 5)) ",nlp-disaster-tweets-with-glove-and-lstm.ipynb
Number of words in a tweet,"fig ,(ax1 , ax2)= plt.subplots(1 , 2 , figsize =(10 , 5)) ",nlp-disaster-tweets-with-glove-and-lstm.ipynb
Punctuations in non disaster class,"plt.figure(figsize =(10 , 5)) ",nlp-disaster-tweets-with-glove-and-lstm.ipynb
Punctuations in disaster class,"plt.figure(figsize =(10 , 5)) ",nlp-disaster-tweets-with-glove-and-lstm.ipynb
Common words,counter = Counter(corpus) ,nlp-disaster-tweets-with-glove-and-lstm.ipynb
Bigram analysis,"def get_top_tweet_bigrams(corpus , n = None): ",nlp-disaster-tweets-with-glove-and-lstm.ipynb
Renaming location names,def clean_text(text): ,nlp-disaster-tweets-with-glove-and-lstm.ipynb
Removing Emojis,def remove_emoji(text): ,nlp-disaster-tweets-with-glove-and-lstm.ipynb
We are going to use LSTM long short term memory model because it solves a vanishing gradient problem,"tweet_1 = tweet.text.values
test_1 = test.text.values
sentiments = tweet.target.values",nlp-disaster-tweets-with-glove-and-lstm.ipynb
"We need to perform tokenization the processing of segmenting text into sentences of words. In the process we throw away punctuation and extra symbols too. The benefit of tokenization is that it gets the text into a format that is easier to convert to raw numbers, which can actually be used for processing ","word_tokenizer = Tokenizer()
word_tokenizer.fit_on_texts(tweet_1)
vocab_length = len(word_tokenizer.word_index) + 1",nlp-disaster-tweets-with-glove-and-lstm.ipynb
"To obtain a vector representation for words we can use an unsupervised learning algorithm called GloVe Global Vectors for Word Representation , which focuses on words co occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together","embeddings_dictionary = dict()
embedding_dim = 100
glove_file = open('../input/glove-file/glove.6B.100d.txt')
for line in glove_file:
 records = line.split()
 word = records[0]
 vector_dimensions = np.asarray(records[1:], dtype='float32')
 embeddings_dictionary [word] = vector_dimensions
glove_file.close()",nlp-disaster-tweets-with-glove-and-lstm.ipynb
 My upgrade BERT model Back to Table of Contents ,"random_state_split = 42
Dropout_num = 0
learning_rate = 5.95e-6
valid = 0.15
epochs_num = 3
batch_size_num = 16
target_corrected = False
target_big_corrected = False",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
 Import libraries Back to Table of Contents ,"import pandas as pd
import numpy as np
import os
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns

from nltk.corpus import stopwords
from nltk.util import ngrams

from wordcloud import WordCloud

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.metrics import classification_report,confusion_matrix

from collections import defaultdict
from collections import Counter
plt.style.use('ggplot')
stop=set(stopwords.words('english'))

import re
from nltk.tokenize import word_tokenize
import gensim
import string

from tqdm import tqdm
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout
from keras.initializers import Constant
from keras.optimizers import Adam

import torch

import warnings
warnings.simplefilter('ignore')",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
 Download data Back to Table of Contents ,"tweet= pd.read_csv('../input/nlp-getting-started/train.csv')
test=pd.read_csv('../input/nlp-getting-started/test.csv')
submission = pd.read_csv(""../input/nlp-getting-started/sample_submission.csv"")",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
tweet tweet id .isin ids with target error ,"print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))
print('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
extracting the number of examples of each class,Real_len = tweet[tweet['target']== 1]. shape[0] ,nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
bar plot of the 3 classes,"plt.rcParams['figure.figsize']=(7 , 5) ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Number of characters in tweets,"def length(text): 
 '''a function which returns the length of text'''
 return len(text)",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Number of words in a tweet,"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
tweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))
ax1.hist(tweet_len,color='blue')
ax1.set_title('disaster tweets')
tweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))
ax2.hist(tweet_len,color='red')
ax2.set_title('Not disaster tweets')
fig.suptitle('Words in a tweet')
plt.show()
",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Average word length in a tweet,"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
word=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='blue')
ax1.set_title('disaster')
word=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')
ax2.set_title('Not disaster')
fig.suptitle('Average word length in each tweet')",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
First we will analyze tweets with class 0.,"corpus=create_corpus(0)

dic=defaultdict(int)
for word in corpus:
 if word in stop:
 dic[word]+=1
 
top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
displaying the stopwords,np.array(stop) ,nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
"Now,we will analyze tweets with class 1.","corpus=create_corpus(1)

dic=defaultdict(int)
for word in corpus:
 if word in stop:
 dic[word]+=1

top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] 
 

plt.rcParams['figure.figsize'] = (18.0, 6.0)
x,y=zip(*top)
plt.bar(x,y)",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
First let s check tweets indicating real disaster.,"plt.figure(figsize=(16,5))
corpus=create_corpus(1)

dic=defaultdict(int)
special = string.punctuation
for i in (corpus):
 if i in special:
 dic[i]+=1
 
x,y=zip(*dic.items())
plt.bar(x,y)",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
"Now,we will move on to class 0.","plt.figure(figsize=(16,5))
corpus=create_corpus(0)
dic=defaultdict(int)
special = string.punctuation
for i in (corpus):
 if i in special:
 dic[i]+=1
 
x,y=zip(*dic.items())
plt.bar(x,y,color='green')",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Common words,"plt.figure(figsize=(16,5))
counter=Counter(corpus)
most=counter.most_common()
x=[]
y=[]
for word,count in most[:40]:
 if (word not in stop) :
 x.append(word)
 y.append(count)",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
we will do a bigram n 2 analysis over the tweets. Let s check the most common bigrams in tweets.,"def get_top_tweet_bigrams(corpus, n=None):
 vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)
 bag_of_words = vec.transform(corpus)
 sum_words = bag_of_words.sum(axis=0) 
 words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
 words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
 return words_freq[:n]",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Thanks to ,"df=pd.concat([tweet,test])
df.shape",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Removing urls,"example=""New competition launched :https://www.kaggle.com/c/nlp-getting-started""",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Removing HTML tags,"example = """"""<div>
<h1>Real or Fake</h1>
<p>Kaggle </p>
<a href=""https://www.kaggle.com/c/nlp-getting-started"">getting started</a>
</div>""""""",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Reference : ,def remove_emoji(text): ,nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Removing punctuations,"def remove_punct(text):
 table=str.maketrans('','',string.punctuation)
 return text.translate(table)

example=""I am a #king""
print(remove_punct(example))",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Real Disaster,"corpus_new1=create_corpus_df(df,1)
len(corpus_new1)",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Generating the wordcloud with the values under the category dataframe,"plt.figure(figsize =(12 , 8)) ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Not Disaster,"corpus_new0=create_corpus_df(df,0)
len(corpus_new0)",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Generating the wordcloud with the values under the category dataframe,"plt.figure(figsize =(12 , 8)) ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Thanks to ,"def cv(data):
 count_vectorizer = CountVectorizer()

 emb = count_vectorizer.fit_transform(data)

 return emb, count_vectorizer

list_corpus = df[""text""].tolist()
list_labels = df[""target""].tolist()

X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, 
 random_state=random_state_split)

X_train_counts, count_vectorizer = cv(X_train)
X_test_counts = count_vectorizer.transform(X_test)",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Visualizing the embeddings,"def plot_LSA(test_data, test_labels, savepath=""PCA_demo.csv"", plot=True):
 lsa = TruncatedSVD(n_components=2)
 lsa.fit(test_data)
 lsa_scores = lsa.transform(test_data)
 color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}
 color_column = [color_mapper[label] for label in test_labels]
 colors = ['orange','blue']
 if plot:
 plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))
 orange_patch = mpatches.Patch(color='orange', label='Not')
 blue_patch = mpatches.Patch(color='blue', label='Real')
 plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})

fig = plt.figure(figsize=(16, 16)) 
plot_LSA(X_train_counts, y_train)
plt.show()",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
 TF IDF Back to Table of Contents ,"def tfidf(data):
 tfidf_vectorizer = TfidfVectorizer()

 train = tfidf_vectorizer.fit_transform(data)

 return train, tfidf_vectorizer

X_train_tfidf, tfidf_vectorizer = tfidf(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
"Here we will use GloVe pretrained corpus model to represent our words. It is available in 3 varieties : 50D, 100D and 200 Dimentional. We will try 100D here.","def create_corpus_new(df):
 corpus=[]
 for tweet in tqdm(df['text']):
 words=[word.lower() for word in word_tokenize(tweet)]
 corpus.append(words)
 return corpus ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Baseline Model with GloVe results,"model=Sequential()

embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),
 input_length=MAX_LEN,trainable=False)

model.add(embedding)
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))


optimzer=Adam(learning_rate=3e-4)

model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Recomended 10 20 epochs,"history = model.fit(X_train , y_train , batch_size = 4 , epochs = 10 , validation_data =(X_test , y_test), verbose = 2) ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
We will use the official tokenization script created by the Google team,! wget - - quiet https : // raw.githubusercontent.com / tensorflow / models / master / official / nlp / bert / tokenization.py ,nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Thanks to ,"def bert_encode(texts , tokenizer , max_len = 512): ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Thanks to ,"def build_model(bert_layer , max_len = 512): ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Without Dropout," out = Dense(1 , activation = 'sigmoid')( clf_output) ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
"With Dropout Dropout num , Dropout num 0", x = Dropout(Dropout_num)( clf_output) ,nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Thanks to ,def clean_tweets(tweet): ,nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Removing URLs," tweet = re.sub(r""http\S+"" , """" , tweet) ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Thanks to ,def remove_emoji(text): ,nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Thanks to ,def remove_punctuations(text): ,nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Thanks to ,def convert_abbrev(word): ,nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Thanks to ,def convert_abbrev_in_text(text): ,nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Load BERT from the Tensorflow Hub,"module_url = ""https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1"" ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Load CSV files containing training data,"train = pd.read_csv(""/kaggle/input/nlp-getting-started/train.csv"") ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
author of this kernel read tweets in training data and figure out that some of them have errors:,if target_corrected : ,nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Thanks to ,if target_big_corrected : ,nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Load tokenizer from the bert layer,vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy () ,nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
"Encode the text into tokens, masks, and segment flags","train_input = bert_encode(train.text.values , tokenizer , max_len = 160) ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Build BERT model with my tuning,"model_BERT = build_model(bert_layer , max_len = 160) ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Train BERT model with my tuning,"checkpoint = ModelCheckpoint('model_BERT.h5' , monitor = 'val_loss' , save_best_only = True) ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Prediction by BERT model with my tuning,model_BERT.load_weights('model_BERT.h5') ,nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Prediction by BERT model with my tuning for the training data for the Confusion Matrix,train_pred_BERT = model_BERT.predict(train_input) ,nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
10.1. SubmissionBack to Table of Contents,"pred = pd.DataFrame(test_pred_BERT, columns=['preds'])
pred.plot.hist()",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Showing Confusion Matrix,"def plot_cm(y_true , y_pred , title , figsize =(5 , 5)) : ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Showing Confusion Matrix for GloVe model,"plot_cm(train_pred_GloVe_int , train['target']. values , 'Confusion matrix for GloVe model' , figsize =(7 , 7)) ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
Showing Confusion Matrix for BERT model,"plot_cm(train_pred_BERT_int , train['target']. values , 'Confusion matrix for BERT model' , figsize =(7 , 7)) ",nlp-eda-bag-of-words-tf-idf-glove-bert.ipynb
linear algebra,import numpy as np ,nlp-getting-started-tutorial.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,nlp-getting-started-tutorial.ipynb
"A quick look at our dataLet s look at our data... first, an example of what is NOT a disaster tweet.","train_df[train_df[""target""] == 0][""text""].values[1]",nlp-getting-started-tutorial.ipynb
And one that is:,"train_df[train_df[""target""] == 1][""text""].values[1]",nlp-getting-started-tutorial.ipynb
"Building vectorsThe theory behind the model we ll build in this notebook is pretty simple: the words contained in each tweet are a good indicator of whether they re about a real disaster or not this is not entirely correct, but it s a great place to start .We ll use scikit learn s CountVectorizer to count the words in each tweet and turn them into data our machine learning model can process.Note: a vector is, in this context, a set of numbers that a machine learning model can work with. We ll look at one in just a second.",count_vectorizer = feature_extraction.text.CountVectorizer () ,nlp-getting-started-tutorial.ipynb
let s get counts for the first 5 tweets in the data,"example_train_vectors = count_vectorizer.fit_transform(train_df[""text""][ 0 : 5]) ",nlp-getting-started-tutorial.ipynb
we use .todense here because these vectors are sparse only non zero elements are kept to save space ,print(example_train_vectors[0]. todense (). shape) ,nlp-getting-started-tutorial.ipynb
The above tells us that: 1. There are 54 unique words or tokens in the first five tweets. 2. The first tweet contains only some of those unique tokens all of the non zero counts above are the tokens that DO exist in the first tweet.Now let s create vectors for all of our tweets.,"train_vectors = count_vectorizer.fit_transform(train_df[""text""]) ",nlp-getting-started-tutorial.ipynb
i.e. that the train and test vectors use the same set of tokens.,"test_vectors = count_vectorizer.transform(test_df[""text""]) ",nlp-getting-started-tutorial.ipynb
is a good way to do this.,clf = linear_model.RidgeClassifier () ,nlp-getting-started-tutorial.ipynb
"Let s test our model and see how well it does on the training data. For this we ll use cross validation where we train on a portion of the known data, then validate it with the rest. If we do this several times with different portions we can get a good idea for how a particular model or method performs.The metric for this competition is F1, so let s use that here.","scores = model_selection.cross_val_score(clf, train_vectors, train_df[""target""], cv=3, scoring=""f1"")
scores",nlp-getting-started-tutorial.ipynb
"The above scores aren t terrible! It looks like our assumption will score roughly 0.65 on the leaderboard. There are lots of ways to potentially improve on this TFIDF, LSA, LSTM RNNs, the list is long! give any of them a shot!In the meantime, let s do predictions on our training set and build a submission for the competition.","clf.fit(train_vectors, train_df[""target""])",nlp-getting-started-tutorial.ipynb
linear algebra,import numpy as np ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
"You can also write temporary files to kaggle temp , but they won t be saved outside of the current session","import re
import string
import numpy as np 
import random
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
from plotly import graph_objs as go
import plotly.express as px
import plotly.figure_factory as ff
from collections import Counter

from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator


import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from tqdm import tqdm
import os
import nltk
import spacy
import random
from spacy.util import compounding
from spacy.util import minibatch

from collections import defaultdict
from collections import Counter

import keras
from keras.models import Sequential
from keras.initializers import Constant
from keras.layers import (LSTM, 
 Embedding, 
 BatchNormalization,
 Dense, 
 TimeDistributed, 
 Dropout, 
 Bidirectional,
 Flatten, 
 GlobalMaxPool1D)
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers.embeddings import Embedding
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from keras.optimizers import Adam

from sklearn.metrics import (
 precision_score, 
 recall_score, 
 f1_score, 
 classification_report,
 accuracy_score
)",nlp-glove-bert-tf-idf-lstm-explained.ipynb
Defining all our palette colours.,"primary_blue = ""#496595"" ",nlp-glove-bert-tf-idf-lstm-explained.ipynb
Balanced Dataset: Let s take a simple example if in our data set we have positive values which are approximately same as negative values. Then we can say our dataset in balance.Consider Orange color as a positive values and Blue color as a Negative value. We can say that the number of positive values and negative values in approximately same.Imbalanced Dataset: If there is the very high different between the positive values and negative values. Then we can say our dataset in Imbalance Dataset.,"balance_counts = df.groupby('target')['target'].agg('count').values
balance_counts",nlp-glove-bert-tf-idf-lstm-explained.ipynb
"As we can see, the classes are imbalanced, so we can consider using some kind of resampling. We will study later. Anyway, it doesn t seem to be necessary.","ham_df = df[df['target'] == 'ham']['message_len'].value_counts().sort_index()
spam_df = df[df['target'] == 'spam']['message_len'].value_counts().sort_index()

fig = go.Figure()
fig.add_trace(go.Scatter(
 x=ham_df.index,
 y=ham_df.values,
 name='ham',
 fill='tozeroy',
 marker_color=primary_blue,
))
fig.add_trace(go.Scatter(
 x=spam_df.index,
 y=spam_df.values,
 name='spam',
 fill='tozeroy',
 marker_color=primary_grey,
))
fig.update_layout(
 title='<span style=""font-size:32px; font-family:Times New Roman"">Data Roles in Different Fields</span>'
)
fig.update_xaxes(range=[0, 70])
fig.show()",nlp-glove-bert-tf-idf-lstm-explained.ipynb
Special thanks to for this function,def clean_text(text): ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Stopwords Stopwords are commonly used words in English which have no contextual meaning in an sentence. So therefore we remove them before classification. Some examples removing stopwords are:,"stop_words = stopwords.words('english')
more_stopwords = ['u', 'im', 'c']
stop_words = stop_words + more_stopwords

def remove_stopwords(text):
 text = ' '.join(word for word in text.split(' ') if word not in stop_words)
 return text
 
df['message_clean'] = df['message_clean'].apply(remove_stopwords)
df.head()",nlp-glove-bert-tf-idf-lstm-explained.ipynb
"3.2 Stemming Stemming Lematization For grammatical reasons, documents are going to use different forms of a word, such as write, writing and writes. Additionally, there are families of derivationally related words with similar meanings. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.Stemming usually refers to a process that chops off the ends of words in the hope of achieving goal correctly most of the time and often includes the removal of derivational affixes.Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base and dictionary form of a wordAs far as the meaning of the words is not important for this study, we will focus on stemming rather than lemmatization.Stemming algorithmsThere are several stemming algorithms implemented in NLTK Python library: 1. PorterStemmer uses Suffix Stripping to produce stems. PorterStemmer is known for its simplicity and speed. Notice how the PorterStemmer is giving the root stem of the word cats by simply removing the s after cat. This is a suffix added to cat to make it plural. But if you look at trouble , troubling and troubled they are stemmed to trouble because PorterStemmer algorithm does not follow linguistics rather a set of 05 rules for different cases that are applied in phases step by step to generate stems. This is the reason why PorterStemmer does not often generate stems that are actual English words. It does not keep a lookup table for actual stems of the word but applies algorithmic rules to generate stems. It uses the rules to decide whether it is wise to strip a suffix. 2. One can generate its own set of rules for any language that is why Python nltk introduced SnowballStemmers that are used to create non English Stemmers! 3. LancasterStemmer Paice Husk stemmer is an iterative algorithm with rules saved externally. One table containing about 120 rules indexed by the last letter of a suffix. On each iteration, it tries to find an applicable rule by the last character of the word. Each rule specifies either a deletion or replacement of an ending. If there is no such rule, it terminates. It also terminates if a word starts with a vowel and there are only two letters left or if a word starts with a consonant and there are only three characters left. Otherwise, the rule is applied, and the process repeats.","stemmer = nltk.SnowballStemmer(""english"")

def stemm_text(text):
 text = ' '.join(stemmer.stem(word) for word in text.split(' '))
 return text",nlp-glove-bert-tf-idf-lstm-explained.ipynb
3.3 All together ,def preprocess_data(text): ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
"Clean puntuation, urls, and so on", text = clean_text(text) ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Remove stopwords, text = ' '.join(word for word in text.split(' ')if word not in stop_words) ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Stemm all the words in the sentence, text = ' '.join(stemmer.stem(word)for word in text.split(' ')) ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
3.4 Target encoding ,"from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
le.fit(df['target'])

df['target_encoded'] = le.transform(df['target'])
df.head()",nlp-glove-bert-tf-idf-lstm-explained.ipynb
 Tokens visualization ,"twitter_mask = np.array(Image.open('/kaggle/input/masksforwordclouds/twitter_mask3.jpg'))

wc = WordCloud(
 background_color='white', 
 max_words=200, 
 mask=twitter_mask,
)
wc.generate(' '.join(text for text in df.loc[df['target'] == 'ham', 'message_clean']))
plt.figure(figsize=(18,10))
plt.title('Top words for HAM messages', 
 fontdict={'size': 22, 'verticalalignment': 'bottom'})
plt.imshow(wc)
plt.axis(""off"")
plt.show()",nlp-glove-bert-tf-idf-lstm-explained.ipynb
how to define X and y from the SMS data for use with COUNTVECTORIZER,x = df['message_clean'] ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Split into train and test sets,from sklearn.model_selection import train_test_split ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
instantiate the vectorizer,vect = CountVectorizer () ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Use the trained to create a document term matrix from train and test sets,x_train_dtm = vect.transform(x_train) ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
"5.1 Tunning CountVectorizerCountVectorizer has a few parameters you should know. stop words: Since CountVectorizer just counts the occurrences of each word in its vocabulary, extremely common words like the , and , etc. will become very important features while they add little meaning to the text. Your model can often be improved if you don t take those words into account. Stop words are just a list of words you don t want to use as features. You can set the parameter stop words english to use a built in list. Alternatively you can set stop words equal to some custom list. This parameter defaults to None. ngram range: An n gram is just a string of n words in a row. E.g. the sentence I am Groot contains the 2 grams I am and am Groot . The sentence is itself a 3 gram. Set the parameter ngram range a,b where a is the minimum and b is the maximum size of ngrams you want to include in your features. The default ngram range is 1,1 . In a recent project where I modeled job postings online, I found that including 2 grams as features boosted my model s predictive power significantly. This makes intuitive sense many job titles such as data scientist , data engineer , and data analyst are 2 words long. min df, max df: These are the minimum and maximum document frequencies words n grams must have to be used as features. If either of these parameters are set to integers, they will be used as bounds on the number of documents each feature must be in to be considered as a feature. If either is set to a float, that number will be interpreted as a frequency rather than a numerical limit. min df defaults to 1 int and max df defaults to 1.0 float . max features: This parameter is pretty self explanatory. The CountVectorizer will choose the words features that occur most frequently to be in its vocabulary and drop everything else. You would set these parameters when initializing your CountVectorizer object as shown below.","vect_tunned = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.1, max_df=0.7, max_features=100)",nlp-glove-bert-tf-idf-lstm-explained.ipynb
"5.2 TF IDFIn information retrieval, tf idf, TF IDF, or TFIDF, short for term frequency inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf idf is one of the most popular term weighting schemes today. A survey conducted in 2015 showed that 83 of text based recommender systems in digital libraries use tf idf.","from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer()

tfidf_transformer.fit(x_train_dtm)
x_train_tfidf = tfidf_transformer.transform(x_train_dtm)

x_train_tfidf",nlp-glove-bert-tf-idf-lstm-explained.ipynb
5.3 Word Embeddings: GloVeThanks to: ,"texts = df['message_clean']
target = df['target_encoded']",nlp-glove-bert-tf-idf-lstm-explained.ipynb
Calculate the length of our vocabulary,word_tokenizer = Tokenizer () ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
"Pad sequences tf.keras.preprocessing.sequence.pad sequences sequences, maxlen None, dtype int32 , padding pre , truncating pre , value 0.0 This function transforms a list of length num samples of sequences lists of integers into a 2D Numpy array of shape num samples, num timesteps . num timesteps is either the maxlen argument if provided, or the length of the longest sequence in the list. python sequence 1 , 2, 3 , 4, 5, 6 tf.keras.preprocessing.sequence.pad sequences sequence, padding post array 1, 0, 0 , 2, 3, 0 , 4, 5, 6 , dtype int32 ","def embed(corpus): 
 return word_tokenizer.texts_to_sequences(corpus)

longest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))
length_long_sentence = len(word_tokenize(longest_train))

train_padded_sentences = pad_sequences(
 embed(texts), 
 length_long_sentence, 
 padding='post'
)

train_padded_sentences",nlp-glove-bert-tf-idf-lstm-explained.ipynb
"GloVeGloVe method is built on an important idea, You can derive semantic relationships between words from the co occurrence matrix. To obtain a vector representation for words we can use an unsupervised learning algorithm called GloVe Global Vectors for Word Representation , which focuses on words co occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together.Word embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. They have learned representations of text in an n dimensional space where words that have the same meaning have a similar representation. Meaning that two similar words are represented by almost similar vectors that are very closely placed in a vector space.Thus when using word embeddings, all individual words are represented as real valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network.",embeddings_dictionary = dict () ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Load GloVe 100D embeddings,with open('/kaggle/input/glove6b100dtxt/glove.6B.100d.txt')as fp : ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Glove dictionary. Others will be initialized to 0.,"embedding_matrix = np.zeros(( vocab_length , embedding_dim)) ",nlp-glove-bert-tf-idf-lstm-explained.ipynb
 Modeling ,import plotly.figure_factory as ff ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
change each element of z to type string for annotations, z_text =[[ str(y)for y in x]for x in z] ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
set up figure," fig = ff.create_annotated_heatmap(z , x = x , y = y , annotation_text = z_text , colorscale = 'Viridis') ",nlp-glove-bert-tf-idf-lstm-explained.ipynb
Create a Multinomial Naive Bayes model,from sklearn.naive_bayes import MultinomialNB ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Train the model,"nb.fit(x_train_dtm , y_train) ",nlp-glove-bert-tf-idf-lstm-explained.ipynb
Make class anf probability predictions,y_pred_class = nb.predict(x_test_dtm) ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
calculate accuracy of class predictions,from sklearn import metrics ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Calculate AUC,"metrics.roc_auc_score(y_test , y_pred_prob) ",nlp-glove-bert-tf-idf-lstm-explained.ipynb
6.2 Naive Bayes,"from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline

pipe = Pipeline([('bow', CountVectorizer()), 
 ('tfid', TfidfTransformer()), 
 ('model', MultinomialNB())])",nlp-glove-bert-tf-idf-lstm-explained.ipynb
Fit the pipeline with the data,"pipe.fit(x_train , y_train) ",nlp-glove-bert-tf-idf-lstm-explained.ipynb
6.3 XGBoost,import xgboost as xgb ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Fit the pipeline with the data,"pipe.fit(x_train , y_train) ",nlp-glove-bert-tf-idf-lstm-explained.ipynb
Model from ,def glove_lstm (): ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Load the model and train!!,model = glove_lstm () ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Lets see the results,"def plot_learning_curves(history, arr):
 fig, ax = plt.subplots(1, 2, figsize=(20, 5))
 for idx in range(2):
 ax[idx].plot(history.history[arr[idx][0]])
 ax[idx].plot(history.history[arr[idx][1]])
 ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)
 ax[idx].set_xlabel('A ',fontsize=16)
 ax[idx].set_ylabel('B',fontsize=16)
 ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)",nlp-glove-bert-tf-idf-lstm-explained.ipynb
install transformers,! pip install transformers ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
 NLP: Disaster Tweets ,"df = pd.read_csv(""/kaggle/input/nlp-getting-started/train.csv"", encoding=""latin-1"")
test_df = pd.read_csv(""/kaggle/input/nlp-getting-started/test.csv"", encoding=""latin-1"")

df = df.dropna(how=""any"", axis=1)
df['text_len'] = df['text'].apply(lambda x: len(x.split(' ')))

df.head()",nlp-glove-bert-tf-idf-lstm-explained.ipynb
9.1 EDA,"balance_counts = df.groupby('target')['target'].agg('count').values
balance_counts",nlp-glove-bert-tf-idf-lstm-explained.ipynb
9.2 Data preprocessing,def remove_url(text): ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Test emoji removal,"remove_emoji(""Omg another Earthquake "") ",nlp-glove-bert-tf-idf-lstm-explained.ipynb
"Clean puntuation, urls, and so on", text = clean_text(text) ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Remove stopwords and Stemm all the words in the sentence, text = ' '.join(stemmer.stem(word)for word in text.split(' ')if word not in stop_words) ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
9.3 WordCloud,"def create_corpus_df(tweet, target):
 corpus=[]
 
 for x in tweet[tweet['target']==target]['text_clean'].str.split():
 for i in x:
 corpus.append(i)
 return corpus",nlp-glove-bert-tf-idf-lstm-explained.ipynb
Real disasters,"corpus_disaster_tweets = create_corpus_df(df, 1)

dic=defaultdict(int)
for word in corpus_disaster_tweets:
 dic[word]+=1
 
top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]
top",nlp-glove-bert-tf-idf-lstm-explained.ipynb
Fake disasters,"corpus_disaster_tweets = create_corpus_df(df, 0)

dic=defaultdict(int)
for word in corpus_disaster_tweets:
 dic[word]+=1
 
top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]
top",nlp-glove-bert-tf-idf-lstm-explained.ipynb
how to define X and y from the SMS data for use with COUNTVECTORIZER,x = df['text_clean'] ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Split into train and test sets,from sklearn.model_selection import train_test_split ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
9.5 GloVe LSTMThanks to: are going to use LSTM long short term memory model .,"train_tweets = df['text_clean'].values
test_tweets = test_df['text_clean'].values
train_target = df['target'].values",nlp-glove-bert-tf-idf-lstm-explained.ipynb
Calculate the length of our vocabulary,word_tokenizer = Tokenizer () ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Pad sequences,"longest_train = max(train_tweets, key=lambda sentence: len(word_tokenize(sentence)))
length_long_sentence = len(word_tokenize(longest_train))

train_padded_sentences = pad_sequences(
 embed(train_tweets), 
 length_long_sentence, 
 padding='post'
)
test_padded_sentences = pad_sequences(
 embed(test_tweets), 
 length_long_sentence,
 padding='post'
)

train_padded_sentences",nlp-glove-bert-tf-idf-lstm-explained.ipynb
Glove dictionary. Others will be initialized to 0.,"embedding_matrix = np.zeros(( vocab_length , embedding_dim)) ",nlp-glove-bert-tf-idf-lstm-explained.ipynb
Load the model and train!!,model = glove_lstm () ,nlp-glove-bert-tf-idf-lstm-explained.ipynb
Results,"plot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])",nlp-glove-bert-tf-idf-lstm-explained.ipynb
Read and explore data Importing Main PackagesBack To Table of Contents,% time ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Libraries and packages for text pre processing,import string ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Read the Data,% time ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
read the csv file,"train_df = pd.read_csv(""/kaggle/input/nlp-getting-started/train.csv"") ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
some early explorations,"display(train_df[~ train_df[""location""]. isnull()]. head ()) ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
"Text Cleaning: Capitalization Lower case The most common approach in text cleaning is capitalization or lower case due to the diversity of capitalization to form a sentence. This technique will project all words in text and document into the same feature space. However, it would also cause problems with exceptional cases such as the USA or UK, which could be solved by replacing typos, slang, acronyms or informal abbreviations technique.Back To Table of Contents","train_df[""text_clean""] = train_df[""text""].apply(lambda x: x.lower())
display(train_df.head())",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Intall the contractions package ,! pip install contractions ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
"Noise Removal Text data could include various unnecessary characters or punctuation such as URLs, HTML tags, non ASCII characters, or other special characters symbols, emojis, and other graphic characters . Remove URLs Back To Table of Contents","def remove_URL(text):
 """"""
 Remove URLs from a sample string
 """"""
 return re.sub(r""https?://\S+|www\.\S+"", """", text)",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
remove urls from the text,"train_df[""text_clean""]= train_df[""text_clean""]. apply(lambda x : remove_URL(x)) ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
double check,"print(train_df[""text""][ 31]) ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Remove HTML tags Back To Table of Contents,"def remove_html(text):
 """"""
 Remove the html in sample text
 """"""
 html = re.compile(r""<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});"")
 return re.sub(html, """", text)",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
remove html from the text,"train_df[""text_clean""]= train_df[""text_clean""]. apply(lambda x : remove_html(x)) ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
double check,"print(train_df[""text""][ 62]) ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Remove Non ASCI: Back To Table of Contents,def remove_non_ascii(text): ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
remove non ascii characters from the text,"train_df[""text_clean""]= train_df[""text_clean""]. apply(lambda x : remove_non_ascii(x)) ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
double check,"print(train_df[""text""][ 38]) ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
"Remove special characters: The special characters could be symbols, emojis, and other graphic characters. We use the Toxic Comment Classification Challenge dataset as the Real or Not? NLP with Disaster Tweets dataset do not have any special charaters in their text.Back To Table of Contents","train_df_jtcc = pd.read_csv(""/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip"")
print(train_df_jtcc.shape)
train_df_jtcc.head()",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
remove non ascii characters from the text,"train_df_jtcc[""text_clean""]= train_df_jtcc[""comment_text""]. apply(lambda x : remove_special_characters(x)) ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
double check,"print(train_df_jtcc[""comment_text""][ 143]) ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Saving disk space,del train_df_jtcc ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Remove punctuations: Back To Table of Contents,def remove_punct(text): ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
remove punctuations from the text,"train_df[""text_clean""]= train_df[""text_clean""]. apply(lambda x : remove_punct(x)) ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
double check,"print(train_df[""text""][ 5]) ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
"Other Manual Text Cleaning Tasks:Other techniques could be considered and manually processed case by case: Replace the Unicode character with equivalent ASCII character instead of removing Replace the entity references with their actual symbols instead of removing as HTML tags Replace the Typos, slang, acronyms or informal abbreviations depend on different situations or main topics of the NLP such as finance or medical topics. List out all the hashtags usernames then replace with equivalent words Replace the emoticon emoji with equivalant word meaning such as : with smile Spelling correction Replace the Typos, slang, acronyms or informal abbreviations: Back To Table of Contents",def other_clean(text): ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
"Spelling Correction Spelling correction could also be considered an optional preprocessing task as the social media text data is often are typos or mistyped. However, the spelling correction output should be carefully double checked with the original text input as it could be a mistake.Back To Table of Contents","from textblob import TextBlob
print(""Test: "", TextBlob(""sleapy and tehre is no plaxe I'm gioong to."").correct())",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Tokenizing the tweet base texts.,from nltk.tokenize import word_tokenize ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Removing stopwords.,"nltk.download(""stopwords"") ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
"Stemming Stemming is a process of extracting a root word identifying a common stem among various forms e.g., singular and plural noun form of a word, for example, the words gardening , gardener or gardens share the same stem, garden. Stemming uproots suffixes from words to merge words with similar meanings under their standard stem.There are three major stemming algorithms in use nowadays: Porter PorterStemmer : This stemming algorithm is an older one. It s from the 1980s and its main concern is removing the common endings to words so that they can be resolved to a common form. It s not too complex and development on it is frozen. Typically, it s a nice starting basic stemmer, but it s not really advised to use it for any production complex application. Instead, it has its place in research as a nice, basic stemming algorithm that can guarantee reproducibility. It also is a very gentle stemming algorithm when compared to others. Snowball LancasterStemmer : This algorithm is also known as the Porter2 stemming algorithm. It is almost universally accepted as better than the Porter stemmer, even being acknowledged as such by the individual who created the Porter stemmer. That being said, it is also more aggressive than the Porter stemmer. A lot of the things added to the Snowball stemmer were because of issues noticed with the Porter stemmer. There is about a 5 difference in the way that Snowball stems versus Porter. Lancaster SnowballStemmer : Just for fun, the Lancaster stemming algorithm is another algorithm that you can use. This one is the most aggressive stemming algorithm of the bunch. However, if you use the stemmer in NLTK, you can add your own custom rules to this algorithm very easily. It s a good choice for that. One complaint around this stemming algorithm though is that it sometimes is overly aggressive and can really transform words into strange stems. Just make sure it does what you want it to before you go with this option! source: PorterStemmer Back To Table of Contents","from nltk.stem import PorterStemmer

def porter_stemmer(text):
 """"""
 Stem words in list of tokenized words with PorterStemmer
 """"""
 stemmer = nltk.PorterStemmer()
 stems = [stemmer.stem(i) for i in text]
 return stems",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
SnowballStemmer Back To Table of Contents,"from nltk.stem import SnowballStemmer

def snowball_stemmer(text):
 """"""
 Stem words in list of tokenized words with SnowballStemmer
 """"""
 stemmer = nltk.SnowballStemmer(""english"")
 stems = [stemmer.stem(i) for i in text]
 return stems",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
LancasterStemmer Back To Table of Contents,"from nltk.stem import LancasterStemmer

def lancaster_stemmer(text):
 """"""
 Stem words in list of tokenized words with LancasterStemmer
 """"""
 stemmer = nltk.LancasterStemmer()
 stems = [stemmer.stem(i) for i in text]
 return stems",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
"Part of Speech Tagging POS Tagging : Part of speech tagging POS tagging distinguishes the part of speech noun, verb, adjective, and etc. of each word in the text. This is the critical stage for many NLP applications since, by identifying the POS of a word, we can infer its contextual meaning. The NLTK packages offer different POS Tagging algorithms, and in this notebook, we use the combination version of them. pos tag DefaultTagger UnigramTagger BigramTagger Could also be a combination of the bigram tagger, unigram tagger, and default tagger source: Back To Table of Contents",from nltk.corpus import wordnet ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
"Lemmatization: According to the Speech and Language Processing book: Lemmatization is the task of determining that two words have the same root, despite their surface differences. The words am, are, and is have the shared lemma be the words dinner and dinners both have the lemma dinner. Lemmatizing each of these forms to the same lemma will let us nd all mentions of words in Russian like Moscow. The lemmatized form of a sentence like He is reading detective stories would thus be He be read detective story. and the book Natural Language Processing in Action: Some lemmatizers use the word s part of speech POS tag in addition to its spelling to help improve accuracy. The POS tag for a word indicates its role in the grammar of a phrase or sentence. For example, the noun POS is for words that refer to people, places, or things within a phrase. An adjective POS is for a word that modifies or describes a noun. A verb refers to an action. The POS of a word in isolation cannot be determined. The context of a word must be known for its POS to be identified. So some advanced lemmatizers can t be run on words in isolation. For example, the good , better or best is lemmatized into good and the verb gardening should be lemmatized to to garden , while the garden and gardener are both different lemmas. In this notebook, we will also explore on both lemmatize on without POS Tagging and POS Tagging examples.Back To Table of Contents","from nltk.stem import WordNetLemmatizer

def lemmatize_word(text):
 """"""
 Lemmatize the tokenized words
 """"""

 lemmatizer = WordNetLemmatizer()
 lemma = [lemmatizer.lemmatize(word, tag) for word, tag in text]
 return lemma",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Lemmatization without POS Tagging: Back To Table of Contents,% time ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Test without POS Tagging,lemmatizer = WordNetLemmatizer () ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Lemmatization with POS Tagging: Back To Table of Contents,% time ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Test with POS Tagging,lemmatizer = WordNetLemmatizer () ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
double check to remove stop words,train_df['lemmatize_word_w_pos']= train_df['lemmatize_word_w_pos']. apply(lambda x :[word for word in x if word not in stop]) ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
join back to text,"train_df['lemmatize_text']=[' '.join(map(str , l)) for l in train_df['lemmatize_word_w_pos']] ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
"Comparing the output of Lemmatization on non POS Tagging and POS Tagging output. We can see in the original text, the word happening is a verb and was corrected assigned as a verb by POS tagging stage, then Lemmatize accurately with back as happen but lemmatized without POS tagging resulted in happening is not correct. ","print(train_df[""text""][8])
print(train_df[""combined_postag_wnet""][8])
print(train_df[""lemmatize_word_wo_pos""][8])
print(train_df[""lemmatize_word_w_pos""][8])",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Comparison between original text and the lammatized text:,"display(train_df[""text""][0], train_df[""lemmatize_text""][0])
display(train_df[""text""][5], train_df[""lemmatize_text""][5])
display(train_df[""text""][10], train_df[""lemmatize_text""][10])
display(train_df[""text""][15], train_df[""lemmatize_text""][15])
display(train_df[""text""][20], train_df[""lemmatize_text""][20])",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Install the main polygot and other neccesary packages,! pip install pyicu ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
We will use the Jigsaw Multilingual Toxic Comment Classification dataset for this case as the dataset is multilingual,"train_df_jmtc = pd.read_csv(""../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv"")
print(train_df_jmtc.shape)
train_df_jmtc.head()",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Test,"display(train_df_jmtc[train_df_jmtc[""lang""]== ""de""]. head ()) ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
save disk space,del train_df_jmtc ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
"Text Features Extraction: Weighted Words Bag of Words BoW Bag of n grams: N gram is a sequence that contains n elements characters, words, etc . A single word such a apple , orange is a Uni gram hence, red apple big orange is bi gram and red ripped apple , big orange bag is tri gram. Bags of words: Vectors of word counts or frequencies Bags of n grams: Counts of word pairs bigrams , triplets trigrams , and so on The bag of words bag of n gram model is a reduced and simplied representation of a text document from selected parts of the text, based on specic criteria, such as word frequency. In a BoW, a body of text, such as a document or a sentence, is thought of like a bag of words. Lists of words are created in the BoW process. These words in a matrix are not sentences which structure sentences and grammar, and the semantic relationship between these words are ignored in their collection and construction. The words are often representative of the content of a sentence. While grammar and order of appearance are ignored, multiplicity is counted and may be used later to determine the focus points of the documents. Example: Document As the home to UVA s recognized undergraduate and graduate degree programs in systems engineering. In the UVA Department of Systems and Information Engineering, our students are exposed to a wide range of range Bag of Words BoW : As , the , home , to , UVA s , recognized , undergraduate , and , graduate , degree , program , in , systems , engineering , in , Department , Information , students , , are , exposed , wide , range Bag of Feature BoF Feature 1,1,1,3,2,1,2,1,2,3,1,1,1,2,1,1,1,1,1,1 source:Text Classification Algorithms: A Survey Frequency Vectors CountVectorizer: We will implement the Bag of Words Bag of n grams text representation via sklearn CountVectorizer function. The code will test with a sample corpus of the first five sentence of the dataset, then print out the output of uni gram, bi gram and tri gram. Finaly, we also run on the whole dataset.Back To Table of Contents","from sklearn.feature_extraction.text import CountVectorizer

def cv(data, ngram = 1, MAX_NB_WORDS = 75000):
 count_vectorizer = CountVectorizer(ngram_range = (ngram, ngram), max_features = MAX_NB_WORDS)
 emb = count_vectorizer.fit_transform(data).toarray()
 print(""count vectorize with"", str(np.array(emb).shape[1]), ""features"")
 return emb, count_vectorizer",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
implement into the whole dataset,"train_df_corpus = train_df[""lemmatize_text""]. tolist () ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
"Term Frequency Inverse Document Frequency TF IDF : The Inverse Document Frequency IDF as a method to be used in conjunction with term frequency in order to lessen the effect of implicitly common words in the corpus. IDF assigns a higher weight to words with either high or low frequencies term in the document. This combination of TF and IDF is well known as Term Frequency Inverse document frequency TF IDF . The mathematical representation of the weight of a term in a document by TF IDF is given in Equation: Here N is the number of documents and is the number of documents containing the term t in the corpus. The rst term in the equation improves the recall while the second term improves the precision of the word embedding. Although TF IDF tries to overcome the problem of common terms in the document, it still suffers from some other descriptive limitations. Namely, TF IDF cannot account for the similarity between the words in the document since each word is independently presented as an index. However, with the development of more complex models in recent years, new methods, such as word embedding, have been presented that can incorporate concepts such as similarity of words and part of speech tagging. source: Text Classification Algorithms: A Survey We also implement the TF IDF via sklearn TfidfVectorizer function, the experiments are similar to the previous Frequency Vectors CountVectorizer sectionBack To Table of Contents","from sklearn.feature_extraction.text import TfidfVectorizer

def TFIDF(data, ngram = 1, MAX_NB_WORDS = 75000):
 tfidf_x = TfidfVectorizer(ngram_range = (ngram, ngram), max_features = MAX_NB_WORDS)
 emb = tfidf_x.fit_transform(data).toarray()
 print(""tf-idf with"", str(np.array(emb).shape[1]), ""features"")
 return emb, tfidf_x",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
implement into the whole dataset,"train_df_corpus = train_df[""lemmatize_text""]. tolist () ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
"Word Embedding: Word vectors are numerical vector representations of word semantics, or meaning, including literal and implied meaning. So word vectors can capture the connotation of words, like peopleness, animalness, placeness, thingness, and even conceptness. And they combine all that into a dense vector no zeros of floating point values. This dense vector enables queries and logical reasoning. source: Natural Language Processing in Action Even though we have syntactic word representations, it does not mean that the model captures the semantics meaning of the words. On the other hand, bag of word models do not respect the semantics of the word. For example, words airplane , aeroplane , plane , and aircraft are often used in the same context. However, the vectors corresponding to these words are orthogonal in the bag of words model. This issue presents a serious problem to understanding sentences within the model. The other problem in the bag of word is that the order of words in the phrase is not respected. The n gram does not solve this problem so a similarity needs to be found for each word in the sentence. Many researchers worked on word embedding to solve this problem. The Word2Vec propose a simple single layer architecture based on the inner product between two word vectors. Word embedding is a feature learning technique in which each word or phrase from the vocabulary is mapped to a N dimension vector of real numbers. Various word embedding methods have been proposed to translate unigrams into understandable input for machine learning algorithms. This work focuses on Word2Vec, GloVe, and FastText, three of the most common methods that have been successfully used for deep learning techniques. source: Text Classification Algorithms: A Survey Basic Word Embedding Methods: Word2Vec:T. Mikolov et al. presented the Word2vec in 2013, which learns the meaning of words merely by processing a large corpus of unlabeled text. The Word2Vec approach uses shallow neural networks with two hidden layers, continuous bag of words CBOW , and the Skip gram model to create a high dimension vector for each word. This unsupervised nature of Word2vec is what makes it so powerful. The world is full of unlabeled, uncategorized, unstructured natural language text.We will implement the Word2vec via gensim libary with the pre trained word vectors on the dataset Google News corpus source: and see the embedding output on the sample sentence from the our dataset. Back To Table of Contents",% time ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
we only load 200k most common words from Google News corpus,"word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path , binary = True , limit = 200000) ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Compare the similarity between cat vs. kitten and cat vs. cats ,"print(word2vec_model.similarity('cat', 'kitten'))
print(word2vec_model.similarity('cat', 'cats'))",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
"Global Vectors for Word Representation GloVe : Another powerful word embedding technique that has been used for text classication is Global Vectors GloVe . The approach is very similar to the Word2Vec method, where each word is presented by a high dimension vector and trained based on the surrounding words over a huge corpus. The pre trained word embedding used in many works is based on 400,000 vocabularies trained over Wikipedia 2014 and Gigaword 5 as the corpus and 50 dimensions for word presentation. GloVe also provides other pre trained word vectorizations with 100, 200, 300 dimensions which are trained over even bigger corpora, including Twitter content. source: Text Classification Algorithms: A Survey We will create our GloVe s sentence embeddings via gensim libary with the pre trained word vectors on the dataset from Wikipedia 2014 Gigaword 5 source: and see the embedding output on the sample sentence from the our dataset. Back To Table of Contents",% time ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
we only load 200k most common words from Google New corpus,"glove_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file , binary = False , limit = 200000) ",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Compare the similarity between cat vs. kitten and cat vs. cats from GloVe,"print(glove_model.similarity('cat', 'kitten'))
print(glove_model.similarity('cat', 'cats'))",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
"FastText: Many other word embedding representations ignore the morphology of words by assigning a distinct vector to each word Enriching Word Vectors with Subword Information . Facebook AI Research lab released a novel technique to solve this issue by introducing a new word embedding method called FastText. Each word, w, is represented as a bag of character n gram. For example, given the word introduce and n 3, FastText will produce the following representation composed of character tri grams: in, int, ntr, tro, rod, odu, duc, uce, ce Note that the sequence , corresponding to the word here is different from the tri gram int from the word introduce. source: Text Classification Algorithms: A Survey We will create our FastText s sentence embeddings via gensim libary with the pre trained word vectors from the Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset source: and see the embedding output on the sample sentence from the our dataset. Back To Table of Contents","%time 

from gensim.models.fasttext import FastText

fasttext_path = ""../input/fasttext-wikinews/wiki-news-300d-1M.vec""
fasttext_model = gensim.models.KeyedVectors.load_word2vec_format(fasttext_path, binary=False, limit=200000)",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
Compare the similarity between cat vs. kitten and cat vs. cats from FastText,"print(fasttext_model.similarity('cat', 'kitten'))
print(fasttext_model.similarity('cat', 'cats'))",nlp-preprocessing-feature-extraction-methods-a-z.ipynb
"Advanced Word Embedding Methods Deep Contextualized Word Representations: Bidirectional Encoder Representations from Transformers BERT : BERT is a deep learning model that has given state of the art results on a wide variety of natural language processing tasks. It stands for Bidirectional Encoder Representations for Transformers. It has been pre trained on Wikipedia and BooksCorpus and requires task specific fine tuning. Lets understand BERT by breaking BERT abbreviation: Bidirectional: BERT takes whole text passage as input and reads passage in both direction to understand the meaning of each word. Transformers: BERT is based on a Deep Transformer network. Transformer network is a type of network that can process efficiently long texts by using attention. An attention is a mechanism to learn contextual relations between words or sub words in a text. Encoder Representation: Originally Transformer includes two separate mechanisms an encoder that reads the text input and a decoder that produces a prediction for the task, since BERT s goal is to generate a language model only the encoder mechanism is necessary hence encoder representation BERT is a multi layer bidirectional Transformer encoder. There are two models introduced in the paper. BERT base 12 layers transformer blocks , 12 attention heads, and 110 million parameters. BERT Large 24 layers, 16 attention heads and, 340 million parameters. How BERT performs Bidirectional training? BERT uses following two prediction models simultaneously with the goal of minimizing the combined loss function of the two strategies: Masked Language Model: Before feeding word sequences into BERT, 15 of the words in each sequence are replaced with a MASK token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non masked, words in the sequence. Next Sentence Prediction: The model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50 of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50 a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence. Resources and further reading on BERT s explanation could be found in the great Kaggle notebooks and Blogs here: will create our sentence embeddings by BERT s pre trained word vectors Uncased via Tensorflow source: and see the embedding output on the sample sentence from the our dataset. Noted that we will use the BERT isself tonkenizer. Back To Table of Contents",% time ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
download the tonkenizer,! wget - - quiet https : // raw.githubusercontent.com / tensorflow / models / master / official / nlp / bert / tokenization.py ,nlp-preprocessing-feature-extraction-methods-a-z.ipynb
" Introduction and References I decided to write this kernel when I first started learning about NLP. It is basically the things I learned documented in Kaggle Notebook format. It can be helpful for you if you are looking for data analysis on competition data, feature engineering ideas for NLP, cleaning and text processing ideas, baseline BERT model or test set with labels. If you have any idea that might improve this kernel, please be sure to comment, or fork and experiment as you like. If you don t understand any part, feel free to ask in the comment section.This kernel includes codes and ideas from kernels below. If this kernel helps you, please upvote their work as well. Simple Exploration Notebook QIQC by How to: Preprocessing when using embeddings by Improve your Score with some Text Preprocessing by A Real Disaster Leaked Label by Disaster NLP: Keras BERT using TFHub by ",!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
"1.1 Missing Values Both training and test set have same ratio of missing values in keyword and location. 0.8 of keyword is missing in both training and test set 33 of location is missing in both training and test setSince missing value ratios between training and test set are too close, they are most probably taken from the same sample. Missing values in those features are filled with no keyword and no location respectively.","missing_cols = ['keyword', 'location']

fig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)

sns.barplot(x=df_train[missing_cols].isnull().sum().index, y=df_train[missing_cols].isnull().sum().values, ax=axes[0])
sns.barplot(x=df_test[missing_cols].isnull().sum().index, y=df_test[missing_cols].isnull().sum().values, ax=axes[1])

axes[0].set_ylabel('Missing Value Count', size=15, labelpad=20)
axes[0].tick_params(axis='x', labelsize=15)
axes[0].tick_params(axis='y', labelsize=15)
axes[1].tick_params(axis='x', labelsize=15)
axes[1].tick_params(axis='y', labelsize=15)

axes[0].set_title('Training Set', fontsize=13)
axes[1].set_title('Test Set', fontsize=13)

plt.show()

for df in [df_train, df_test]:
 for col in ['keyword', 'location']:
 df[col] = df[col].fillna(f'no_{col}')",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
"1.2 Cardinality and Target Distribution Locations are not automatically generated, they are user inputs. That s why location is very dirty and there are too many unique values in it. It shouldn t be used as a feature.Fortunately, there is signal in keyword because some of those words can only be used in one context. Keywords have very different tweet counts and target means. keyword can be used as a feature by itself or as a word added to the text. Every single keyword in training set exists in test set. If training and test set are from the same sample, it is also possible to use target encoding on keyword.","print(f'Number of unique values in keyword = {df_train[""keyword""].nunique()} (Training) - {df_test[""keyword""].nunique()} (Test)')
print(f'Number of unique values in location = {df_train[""location""].nunique()} (Training) - {df_test[""location""].nunique()} (Test)')",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
word count,df_train['word_count']= df_train['text']. apply(lambda x : len(str(x). split ())) ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
unique word count,df_train['unique_word_count']= df_train['text']. apply(lambda x : len(set(str(x). split ()))) ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
stop word count,df_train['stop_word_count']= df_train['text']. apply(lambda x : len ([w for w in str(x). lower (). split ()if w in STOPWORDS])) ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
url count,df_train['url_count']= df_train['text']. apply(lambda x : len ([w for w in str(x). lower (). split ()if 'http' in w or 'https' in w])) ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
mean word length,df_train['mean_word_length']= df_train['text']. apply(lambda x : np.mean ([len(w)for w in str(x). split()])) ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
char count,df_train['char_count']= df_train['text']. apply(lambda x : len(str(x))) ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
punctuation count,df_train['punctuation_count']= df_train['text']. apply(lambda x : len ([c for c in str(x)if c in string.punctuation])) ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
hashtag count,df_train['hashtag_count']= df_train['text']. apply(lambda x : len ([c for c in str(x)if c == '#'])) ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
mention count,df_train['mention_count']= df_train['text']. apply(lambda x : len ([c for c in str(x)if c == '@'])) ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
"All of the meta features have very similar distributions in training and test set which also proves that training and test set are taken from the same sample.All of the meta features have information about target as well, but some of them are not good enough such as url count, hashtag count and mention count.On the other hand, word count, unique word count, stop word count, mean word length, char count, punctuation count have very different distributions for disaster and non disaster tweets. Those features might be useful in models.","METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',
 'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']
DISASTER_TWEETS = df_train['target'] == 1

fig, axes = plt.subplots(ncols=2, nrows=len(METAFEATURES), figsize=(20, 50), dpi=100)

for i, feature in enumerate(METAFEATURES):
 sns.distplot(df_train.loc[~DISASTER_TWEETS][feature], label='Not Disaster', ax=axes[i][0], color='green')
 sns.distplot(df_train.loc[DISASTER_TWEETS][feature], label='Disaster', ax=axes[i][0], color='red')

 sns.distplot(df_train[feature], label='Training', ax=axes[i][1])
 sns.distplot(df_test[feature], label='Test', ax=axes[i][1])
 
 for j in range(2):
 axes[i][j].set_xlabel('')
 axes[i][j].tick_params(axis='x', labelsize=12)
 axes[i][j].tick_params(axis='y', labelsize=12)
 axes[i][j].legend()
 
 axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)
 axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)

plt.show()",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
3.1 Target Class distributions are 57 for 0 Not Disaster and 43 for 1 Disaster . Classes are almost equally separated so they don t require any stratification by target in cross validation.,"fig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)
plt.tight_layout()

df_train.groupby('target').count()['id'].plot(kind='pie', ax=axes[0], labels=['Not Disaster (57%)', 'Disaster (43%)'])
sns.countplot(x=df_train['target'], hue=df_train['target'], ax=axes[1])

axes[0].set_ylabel('')
axes[1].set_ylabel('')
axes[1].set_xticklabels(['Not Disaster (4342)', 'Disaster (3271)'])
axes[0].tick_params(axis='x', labelsize=15)
axes[0].tick_params(axis='y', labelsize=15)
axes[1].tick_params(axis='x', labelsize=15)
axes[1].tick_params(axis='y', labelsize=15)

axes[0].set_title('Target Distribution in Training Set', fontsize=13)
axes[1].set_title('Target Count in Training Set', fontsize=13)

plt.show()",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Unigrams,disaster_unigrams = defaultdict(int) ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Bigrams,disaster_bigrams = defaultdict(int) ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Trigrams,disaster_trigrams = defaultdict(int) ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
"3.2 Unigrams Most common unigrams exist in both classes are mostly punctuations, stop words or numbers. It is better to clean them before modelling since they don t give much information about target.Most common unigrams in disaster tweets are already giving information about disasters. It is very hard to use some of those words in other contexts.Most common unigrams in non disaster tweets are verbs. This makes sense because most of those sentences have informal active structure since they are coming from individual users.","fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)
plt.tight_layout()

sns.barplot(y=df_disaster_unigrams[0].values[:N], x=df_disaster_unigrams[1].values[:N], ax=axes[0], color='red')
sns.barplot(y=df_nondisaster_unigrams[0].values[:N], x=df_nondisaster_unigrams[1].values[:N], ax=axes[1], color='green')

for i in range(2):
 axes[i].spines['right'].set_visible(False)
 axes[i].set_xlabel('')
 axes[i].set_ylabel('')
 axes[i].tick_params(axis='x', labelsize=13)
 axes[i].tick_params(axis='y', labelsize=13)

axes[0].set_title(f'Top {N} most common unigrams in Disaster Tweets', fontsize=15)
axes[1].set_title(f'Top {N} most common unigrams in Non-disaster Tweets', fontsize=15)

plt.show()",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
"3.3 Bigrams There are no common bigrams exist in both classes because the context is clearer.Most common bigrams in disaster tweets are giving more information about the disasters than unigrams, but punctuations have to be stripped from words.Most common bigrams in non disaster tweets are mostly about reddit or youtube, and they contain lots of punctuations. Those punctuations have to be cleaned out of words as well.","fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)
plt.tight_layout()

sns.barplot(y=df_disaster_bigrams[0].values[:N], x=df_disaster_bigrams[1].values[:N], ax=axes[0], color='red')
sns.barplot(y=df_nondisaster_bigrams[0].values[:N], x=df_nondisaster_bigrams[1].values[:N], ax=axes[1], color='green')

for i in range(2):
 axes[i].spines['right'].set_visible(False)
 axes[i].set_xlabel('')
 axes[i].set_ylabel('')
 axes[i].tick_params(axis='x', labelsize=13)
 axes[i].tick_params(axis='y', labelsize=13)

axes[0].set_title(f'Top {N} most common bigrams in Disaster Tweets', fontsize=15)
axes[1].set_title(f'Top {N} most common bigrams in Non-disaster Tweets', fontsize=15)

plt.show()",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
"3.4 Trigrams There are no common trigrams exist in both classes because the context is clearer.Most common trigrams in disaster tweets are very similar to bigrams. They give lots of information about disasters, but they may not provide any additional information along with bigrams.Most common trigrams in non disaster tweets are also very similar to bigrams, and they contain even more punctuations.","fig, axes = plt.subplots(ncols=2, figsize=(20, 50), dpi=100)

sns.barplot(y=df_disaster_trigrams[0].values[:N], x=df_disaster_trigrams[1].values[:N], ax=axes[0], color='red')
sns.barplot(y=df_nondisaster_trigrams[0].values[:N], x=df_nondisaster_trigrams[1].values[:N], ax=axes[1], color='green')

for i in range(2):
 axes[i].spines['right'].set_visible(False)
 axes[i].set_xlabel('')
 axes[i].set_ylabel('')
 axes[i].tick_params(axis='x', labelsize=13)
 axes[i].tick_params(axis='y', labelsize=11)

axes[0].set_title(f'Top {N} most common trigrams in Disaster Tweets', fontsize=15)
axes[1].set_title(f'Top {N} most common trigrams in Non-disaster Tweets', fontsize=15)

plt.show()",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
"4.1 Embeddings Coverage When you have pre trained embeddings, doing standard preprocessing steps might not be a good idea because some of the valuable information can be lost. It is better to get vocabulary as close to embeddings as possible. In order to do that, train vocab and test vocab are created by counting the words in tweets.Text cleaning is based on the embeddings below: GloVe 300d 840B FastText Crawl 300d 2M","%%time

glove_embeddings = np.load('../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', allow_pickle=True)
fasttext_embeddings = np.load('../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl', allow_pickle=True)",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Words in the intersection of vocab and embeddings are stored in covered along with their counts. Words in vocab that don t exist in embeddings are stored in oov along with their counts. n covered and n oov are total number of counts and they are used for calculating coverage percentages.Both GloVe and FastText embeddings have more than 50 vocabulary and 80 text coverage without cleaning. GloVe and FastText coverage are very close but GloVe has slightly higher coverage.,"def build_vocab(X):
 
 tweets = X.apply(lambda s: s.split()).values 
 vocab = {}
 
 for tweet in tweets:
 for word in tweet:
 try:
 vocab[word] += 1
 except KeyError:
 vocab[word] = 1 
 return vocab


def check_embeddings_coverage(X, embeddings):
 
 vocab = build_vocab(X) 
 
 covered = {}
 oov = {} 
 n_covered = 0
 n_oov = 0
 
 for word in vocab:
 try:
 covered[word] = embeddings[word]
 n_covered += vocab[word]
 except:
 oov[word] = vocab[word]
 n_oov += vocab[word]
 
 vocab_coverage = len(covered) / len(vocab)
 text_coverage = (n_covered / (n_covered + n_oov))
 
 sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]
 return sorted_oov, vocab_coverage, text_coverage

train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(df_train['text'], glove_embeddings)
test_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(df_test['text'], glove_embeddings)
print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))
print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))

train_fasttext_oov, train_fasttext_vocab_coverage, train_fasttext_text_coverage = check_embeddings_coverage(df_train['text'], fasttext_embeddings)
test_fasttext_oov, test_fasttext_vocab_coverage, test_fasttext_text_coverage = check_embeddings_coverage(df_test['text'], fasttext_embeddings)
print('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_fasttext_vocab_coverage, train_fasttext_text_coverage))
print('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_fasttext_vocab_coverage, test_fasttext_text_coverage))",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
"4.2 Text Cleaning Tweets require lots of cleaning but it is inefficient to clean every single tweet because that would consume too much time. A general approach must be implemented for cleaning. The most common type of words that require cleaning in oov have punctuations at the start or end. Those words doesn t have embeddings because of the trailing punctuations. Punctuations , , !, ?, , , , , , , , , , , , , , , , , , ,..., ,.,:, are separated from words Special characters that are attached to words are removed completely Contractions are expanded Urls are removed Character entity references are replaced with their actual symbols Typos and slang are corrected, and informal abbreviations are written in their long forms Some words are replaced with their acronyms and some words are grouped into one Finally, hashtags and usernames contain lots of information about the context but they are written without spaces in between words so they don t have embeddings. Informational usernames and hashtags should be expanded but there are too many of them. I expanded as many as I could, but it takes too much time to run clean function after adding those replace calls. ",% % time ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Special characters," tweet = re.sub(r""\x89_"" , """" , tweet) ",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Contractions," tweet = re.sub(r""he's"" , ""he is"" , tweet) ",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Character entity references," tweet = re.sub(r""&gt;"" , "">"" , tweet) ",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
"Typos, slang and informal abbreviations"," tweet = re.sub(r""w/e"" , ""whatever"" , tweet) ",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Hashtags and usernames," tweet = re.sub(r""IranDeal"" , ""Iran Deal"" , tweet) ",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Urls," tweet = re.sub(r""https?:\/\/t.co\/[A-Za-z0-9]+"" , """" , tweet) ",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Words with punctuations and special characters," punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + ""'`"" ",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
... and ..," tweet = tweet.replace('...' , ' ... ') ",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Acronyms," tweet = re.sub(r""MH370"" , ""Malaysia Airlines Flight 370"" , tweet) ",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Grouping same words without embeddings," tweet = re.sub(r""Bestnaijamade"" , ""bestnaijamade"" , tweet) ",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
"After cleaning the tweets, glove embeddings and fasttext embeddings are deleted and garbage collected because they consume too much memory 10 gigabytes .","del glove_embeddings, fasttext_embeddings, train_glove_oov, test_glove_oov, train_fasttext_oov, test_fasttext_oov
gc.collect()",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
 Mislabeled Samples There are 18 unique tweets in training set which are labeled differently in their duplicates. Those tweets are probably labeled by different people and they interpreted the meaning differently because some of them are not very clear. Tweets with two unique target values are relabeled since they can affect the training score. ,"df_mislabeled = df_train.groupby(['text']).nunique().sort_values(by='target', ascending=False)
df_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']
df_mislabeled.index.tolist()",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
" Cross validation First of all, when the training test sets are concatenated, and tweet counts by keyword are computed, it can be seen that training and test set are split inside keyword groups. We can also come to that conclusion by looking at id feature. This means every keyword are stratified while creating training and test set. We can replicate the same split for cross validation.Tweets from every keyword group exist in both training and test set and they are from the same sample. In order to replicate the same split technique, StratifiedKFold is used and keyword is passed as y, so stratification is done based on the keyword feature. shuffle is set to True for extra training diversity. Both folds have tweets from every keyword group in training and validation sets which can be seen from below. ","K = 2
skf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)

DISASTER = df_train['target'] == 1
print('Whole Training Set Shape = {}'.format(df_train.shape))
print('Whole Training Set Unique keyword Count = {}'.format(df_train['keyword'].nunique()))
print('Whole Training Set Target Rate (Disaster) {}/{} (Not Disaster)'.format(df_train[DISASTER]['target_relabeled'].count(), df_train[~DISASTER]['target_relabeled'].count()))

for fold, (trn_idx, val_idx) in enumerate(skf.split(df_train['text_cleaned'], df_train['target']), 1):
 print('\nFold {} Training Set Shape = {} - Validation Set Shape = {}'.format(fold, df_train.loc[trn_idx, 'text_cleaned'].shape, df_train.loc[val_idx, 'text_cleaned'].shape))
 print('Fold {} Training Set Unique keyword Count = {} - Validation Set Unique keyword Count = {}'.format(fold, df_train.loc[trn_idx, 'keyword'].nunique(), df_train.loc[val_idx, 'keyword'].nunique())) ",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
"7.1 Metric The leaderboard is based on Mean F Score which can be implemented with Macro Average F1 Score. However, it won t be very informative without Accuracy, Precision and Recall because classes are almost balanced and it is hard to tell which class is harder to predict. Accuracy measures the fraction of the total sample that is correctly identified Precision measures that out of all the examples predicted as positive, how many are actually positive Recall measures that out of all the actual positives, how many examples were correctly classified as positive by the model F1 Score is the harmonic mean of the Precision and Recall Keras has accuracy in its metrics module, but doesn t have rest of the metrics stated above. Another crucial point is Precision, Recall and F1 Score are global metrics so they should be calculated on whole training or validation set. Computing them on every batch would be both misleading and ineffective in terms of execution time. ClassificationReport which is similar to sklearn.metrics.classification report, computes those metrics after every epoch for the given training and validation set.","class ClassificationReport(Callback):
 
 def __init__(self, train_data=(), validation_data=()):
 super(Callback, self).__init__()
 
 self.X_train, self.y_train = train_data
 self.train_precision_scores = []
 self.train_recall_scores = []
 self.train_f1_scores = []
 
 self.X_val, self.y_val = validation_data
 self.val_precision_scores = []
 self.val_recall_scores = []
 self.val_f1_scores = [] 
 
 def on_epoch_end(self, epoch, logs={}):
 train_predictions = np.round(self.model.predict(self.X_train, verbose=0)) 
 train_precision = precision_score(self.y_train, train_predictions, average='macro')
 train_recall = recall_score(self.y_train, train_predictions, average='macro')
 train_f1 = f1_score(self.y_train, train_predictions, average='macro')
 self.train_precision_scores.append(train_precision) 
 self.train_recall_scores.append(train_recall)
 self.train_f1_scores.append(train_f1)
 
 val_predictions = np.round(self.model.predict(self.X_val, verbose=0))
 val_precision = precision_score(self.y_val, val_predictions, average='macro')
 val_recall = recall_score(self.y_val, val_predictions, average='macro')
 val_f1 = f1_score(self.y_val, val_predictions, average='macro')
 self.val_precision_scores.append(val_precision) 
 self.val_recall_scores.append(val_recall) 
 self.val_f1_scores.append(val_f1)
 
 print('\nEpoch: {} - Training Precision: {:.6} - Training Recall: {:.6} - Training F1: {:.6}'.format(epoch + 1, train_precision, train_recall, train_f1))
 print('Epoch: {} - Validation Precision: {:.6} - Validation Recall: {:.6} - Validation F1: {:.6}'.format(epoch + 1, val_precision, val_recall, val_f1)) ",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
"7.2 BERT LayerThis model uses the implementation of BERT from the TensorFlow Models repository on GitHub at tensorflow models official nlp bert. It uses L 12 hidden layers Transformer blocks , a hidden size of H 768, and A 12 attention heads.This model has been pre trained for English on the Wikipedia and BooksCorpus. Inputs have been uncased , meaning that the text has been lower cased before tokenization into word pieces, and any accent markers have been stripped. In order to download this model, Internet must be activated on the kernel.","%%time

bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', trainable=True)",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
"7.3 Architecture DisasterDetector is a wrapper that incorporates the cross validation and metrics stated above. The tokenization of input text is performed with the FullTokenizer class from tensorflow models official nlp bert tokenization.py. max seq length parameter can be used for tuning the sequence length of text.Parameters such as lr, epochs and batch size can be used for controlling the learning process. There are no dense or pooling layers added after last layer of BERT. SGD is used as optimizer since others have hard time while converging.plot learning curve plots Accuracy, Precision, Recall and F1 Score for validation set stored after every epoch alongside with training validation loss curve. This helps to see which metric fluctuates most while training.",class DisasterDetector : ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
BERT and Tokenization params, self.bert_layer = bert_layer ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Learning control params, self.lr = lr ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Callbacks," metrics = ClassificationReport(train_data =(X_trn_encoded , y_trn), validation_data =(X_val_encoded , y_val)) ",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Model, model = self.build_model () ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
"7.4 Training, Evaluation and Prediction","clf = DisasterDetector(bert_layer, max_seq_length=128, lr=0.0001, epochs=10, batch_size=32)

clf.train(df_train)",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
" Test Set Labels Test set labels can be found on this website. Dataset is named Disasters on social media. This is how people are submitting perfect scores. Other Getting Started competitions also have their test labels available. The main point of Getting Started competitions is learning and sharing, and perfect score doesn t mean anything. Phil Culliton wrote: For the AutoML prize, any use of the label set will result in disqualification. According to from Kaggle Team, competitors who use test set labels in any way are not eligible to win AutoML prize. There are no other penalties for using them. ","df_leak = pd.read_csv('../input/nlp-with-disaster-tweets-test-set-with-labels/socialmedia-disaster-tweets-DFE.csv' , encoding = 'ISO-8859-1')[['choose_one' , 'text']] ",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Creating target and id,df_leak['target']=(df_leak['choose_one']== 'Relevant'). astype(np.int8) ,nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
Merging target to test set,"df_test = df_test.merge(df_leak , on =['id'], how = 'left') ",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
" Preprocessed Datasets Preprocessed datasets are saved in pickle format for people who don t want to wait for preprocessing. Instead of forking and waiting for all preprocessing operations, it is easier to use this kernel as a data source in your own kernel. It can be done by searching and selecting this kernel after clicking Add Data button. ","TRAIN_FEATURES = ['id', 'keyword', 'location', 'text', 'target', 'text_cleaned', 'target_relabeled']
TEST_FEATURES = ['id', 'keyword', 'location', 'text', 'target', 'text_cleaned']

df_train[TRAIN_FEATURES].to_pickle('train.pkl')
df_test[TEST_FEATURES].to_pickle('test.pkl')

print('Training Set Shape = {}'.format(df_train[TRAIN_FEATURES].shape))
print('Training Set Memory Usage = {:.2f} MB'.format(df_train[TRAIN_FEATURES].memory_usage().sum() / 1024**2))
print('Test Set Shape = {}'.format(df_test[TEST_FEATURES].shape))
print('Test Set Memory Usage = {:.2f} MB'.format(df_test[TEST_FEATURES].memory_usage().sum() / 1024**2))",nlp-with-disaster-tweets-eda-cleaning-and-bert.ipynb
"In this Notebook, I will be classifying reviews as positive or negative using 2 different concepts of Natural Language Procession 1. The first one involves creating the classical bag of words model 2. The second one involves involves creating a bag of vectors model i.e. each word is converted into a vector Word to Vec The advantage of converting words to vector is that semantically similar words are placed near to each other and words opposite in meaning are placed further apart. In the project, the accuracy in predicting the sentiment of the reviews remains almost the same with both the approaches but it is found that when the amount of training data is increased, the word2vec approach performs better. P.S. Google Search utlises the word2vec approach.","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline",nlp-word2vec.ipynb
removing HTML Markups and Tags like ,from bs4 import BeautifulSoup ,nlp-word2vec.ipynb
creating a function to clean the reviews,def review_to_words(raw_review): ,nlp-word2vec.ipynb
1. Remove HTML, review_text = BeautifulSoup(raw_review). get_text () ,nlp-word2vec.ipynb
2. Remove non letters," letters_only = re.sub(""[^a-zA-Z]"" , "" "" , review_text) ",nlp-word2vec.ipynb
"3. Convert to lower case, split into individual words", words = letters_only.lower (). split () ,nlp-word2vec.ipynb
"a list, so convert the stop words to a set"," stops = set(stopwords.words(""english"")) ",nlp-word2vec.ipynb
5. Remove stop words, meaningful_words =[w for w in words if not w in stops] ,nlp-word2vec.ipynb
6.Lemmatization, for word in meaningful_words : ,nlp-word2vec.ipynb
and return the result.," return("" "".join(meaningful_words)) ",nlp-word2vec.ipynb
"If the index is evenly divisible by 1000, print a message", if(( i + 1)% 1000 == 0): ,nlp-word2vec.ipynb
"Initialize the CountVectorizer object, which is scikit learn s bag of words tool.",train_data_features = vectorizer.fit_transform(clean_train_reviews),nlp-word2vec.ipynb
"Numpy arrays are easy to work with, so convert the result to an array",train_data_features = train_data_features.toarray () ,nlp-word2vec.ipynb
to limit the size of dictionary of unique words to 5000 ,train_data_features.shape ,nlp-word2vec.ipynb
Take a look at the words in the vocabulary,vocab = vectorizer.get_feature_names () ,nlp-word2vec.ipynb
Using RandomForest Classifier to classify reviews,from sklearn.ensemble import RandomForestClassifier ,nlp-word2vec.ipynb
"Get a bag of words for the test set, and convert to a numpy array",test_data_features = vectorizer.transform(clean_test_reviews) ,nlp-word2vec.ipynb
Use the random forest to make sentiment label predictions,result = RF.predict(test_data_features) ,nlp-word2vec.ipynb
a sentiment column,"output = pd.DataFrame(data = { ""id"" : test_data[""id""], ""sentiment"" : result }) ",nlp-word2vec.ipynb
Use pandas to write the comma separated output file,"output.to_csv(""Bag_of_Words_model.csv"" , index = False , quoting = 3) ",nlp-word2vec.ipynb
I tried to use Neural Network to see if there is any improvement in accuracy,import tensorflow as tf,nlp-word2vec.ipynb
building the model and compiling it,"model.fit(X_train,y_train,epochs=10)",nlp-word2vec.ipynb
to convert the output into binary True False form so that I can be compared with y test,predictions_ANN =(predictions_ANN > 0.9) ,nlp-word2vec.ipynb
a sentiment column,"output = pd.DataFrame(data = { ""id"" : test_data[""id""], ""sentiment"" : result_ANN }) ",nlp-word2vec.ipynb
Use pandas to write the comma separated output file,"output.to_csv(""Bag_of_Words_model_ANN.csv"" , index = False , quoting = 3) ",nlp-word2vec.ipynb
"Now using the 2nd Approach i.e Word to Vec. It is an unsupervised approach i.e. it doesnt involve using labels, it just places the similar words together and dissimilar words far apart.","unlabelled_data = pd.read_csv('../input/word2vec-nlp-tutorial/unlabeledTrainData.tsv',delimiter='\t',quoting=3,header=0)",nlp-word2vec.ipynb
To train Word2Vec it is better not to remove stop words or numbers because the algorithm relies on the broader context of the sentence in order to produce high quality word vectors.,"def review_to_wordlist(review , remove_stopwords = False): ",nlp-word2vec.ipynb
1. Remove HTML, review_text = BeautifulSoup(review). get_text () ,nlp-word2vec.ipynb
2. Remove non letters," review_text = re.sub(""[^a-zA-Z]"" , "" "" , review_text) ",nlp-word2vec.ipynb
3. Convert words to lower case and split them, words = review_text.lower (). split () ,nlp-word2vec.ipynb
4. Optionally remove stop words false by default , if remove_stopwords : ,nlp-word2vec.ipynb
5. Return a list of words, return(words) ,nlp-word2vec.ipynb
Download the punkt tokenizer for sentence splitting,import nltk.data ,nlp-word2vec.ipynb
Load the punkt tokenizer,tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') ,nlp-word2vec.ipynb
Define a function to split a review into parsed sentences,"def review_to_sentences(review , tokenizer , remove_stopwords = False): ",nlp-word2vec.ipynb
1. Use the NLTK tokenizer to split the paragraph into sentences, raw_sentences = tokenizer.tokenize(review.strip ()) ,nlp-word2vec.ipynb
2. Loop over each sentence, sentences = [] ,nlp-word2vec.ipynb
"If a sentence is empty, skip it", if len(raw_sentence)> 0 : ,nlp-word2vec.ipynb
Initialize an empty list of sentences,sentences = [] ,nlp-word2vec.ipynb
Import the built in logging module and configure it so that Word2Vec creates nice output messages,import logging ,nlp-word2vec.ipynb
Word vector dimensionality,num_features = 300 ,nlp-word2vec.ipynb
Minimum word count,min_word_count = 40 ,nlp-word2vec.ipynb
Number of threads to run in parallel,num_workers = 4 ,nlp-word2vec.ipynb
Context window size,context = 10 ,nlp-word2vec.ipynb
Downsample setting for frequent words,downsampling = 1e-3 ,nlp-word2vec.ipynb
Initialize and train the model this will take some time ,from gensim.models import word2vec ,nlp-word2vec.ipynb
Load the model that we created,from gensim.models import Word2Vec ,nlp-word2vec.ipynb
"The number of rows in syn0 is the number of words in the model s vocabulary, and the number of columns corresponds to the size of the feature vector.Setting the minimum word count to 40 gave us a total vocabulary of 16,492 words with 300 features apiece. Individual word vectors can be accessed in the following way:",model['great'],nlp-word2vec.ipynb
"One challenge with the IMDB dataset is the variable length reviews. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review.Since each word is a vector in 300 dimensional space, we can use vector operations to combine the words in each review. One method we tried was to simply average the word vectors in a given review for this purpose, we removed stop words, which would just add noise .The following code averages the feature vectors","def makeFeatureVec(words , model , num_features): ",nlp-word2vec.ipynb
Pre initialize an empty numpy array for speed ," featureVec = np.zeros(( num_features ,), dtype = ""float32"") ",nlp-word2vec.ipynb
"the model s vocabulary. Convert it to a set, for speed", index2word_set = set(model.wv.index2word) ,nlp-word2vec.ipynb
"vocaublary, add its feature vector to the total", for word in words : ,nlp-word2vec.ipynb
here we are adding the feature vectors of all the words that were in the review," featureVec = np.add(featureVec , model[word]) ",nlp-word2vec.ipynb
Divide the result by the number of words to get the average," featureVec = np.divide(featureVec , nwords) ",nlp-word2vec.ipynb
Initialize a counter, counter = 0 ,nlp-word2vec.ipynb
"Preallocate a 2D numpy array, for speed"," reviewFeatureVecs = np.zeros(( len(reviews), num_features), dtype = ""float32"") ",nlp-word2vec.ipynb
Loop through the reviews, for review in reviews : ,nlp-word2vec.ipynb
Print a status message every 1000th review, if counter % 1000 == 0 : ,nlp-word2vec.ipynb
Call the function defined above that makes average feature vectors," reviewFeatureVecs[counter]= makeFeatureVec(review , model , num_features) ",nlp-word2vec.ipynb
Increment the counter, counter = counter + 1 ,nlp-word2vec.ipynb
removal.,clean_train_reviews = [] ,nlp-word2vec.ipynb
Test extract results,result = forest_train.predict(X_test_vec) ,nlp-word2vec.ipynb
"Fit a random forest to the training data, using 100 trees",forest = RandomForestClassifier(n_estimators = 100) ,nlp-word2vec.ipynb
Test extract results,result_to_be_submitted = forest.predict(testDataVecs) ,nlp-word2vec.ipynb
Write the test results,"output = pd.DataFrame(data = { ""id"" : test_data[""id""], ""sentiment"" : result_to_be_submitted }) ",nlp-word2vec.ipynb
"Word2Vec creates clusters of semantically related words, so another possible approach is to exploit the similarity of words within a cluster. Grouping vectors in this way is known as vector quantization. To accomplish this, we first need to find the centers of the word clusters, which we can do by using a clustering algorithm such as K Means.",from sklearn.cluster import KMeans ,nlp-word2vec.ipynb
Start time,start = time.time () ,nlp-word2vec.ipynb
average of 5 words per cluster,word_vectors = model.wv.syn0 ,nlp-word2vec.ipynb
Initalize a k means object and use it to extract centroids,kmeans_clustering = KMeans(n_clusters = num_clusters) ,nlp-word2vec.ipynb
Get the end time and print how long the process took,end = time.time () ,nlp-word2vec.ipynb
"Create a Word Index dictionary, mapping each vocabulary word to a cluster label","word_centroid_map = dict(zip(model.wv.index2word , idx)) ",nlp-word2vec.ipynb
For the first 10 clusters,"for cluster in range(0 , 10): ",nlp-word2vec.ipynb
Print the cluster number," print(""\nCluster %d"" % cluster) ",nlp-word2vec.ipynb
"Find all of the words for that cluster number, and print them out", words = [] ,nlp-word2vec.ipynb
in the word centroid map, num_centroids = max(word_centroid_map.values ()) + 1 ,nlp-word2vec.ipynb
Pre allocate the bag of centroids vector for speed ," bag_of_centroids = np.zeros(num_centroids , dtype = ""float32"") ",nlp-word2vec.ipynb
by one, for word in wordlist : ,nlp-word2vec.ipynb
Return the bag of centroids , return bag_of_centroids ,nlp-word2vec.ipynb
Fit a random forest and extract predictions,forest = RandomForestClassifier(n_estimators = 100) ,nlp-word2vec.ipynb
Fitting the forest may take a few minutes,"print(""Fitting a random forest to labeled training data..."") ",nlp-word2vec.ipynb
Write the test results,"output = pd.DataFrame(data = { ""id"" : test_data[""id""], ""sentiment"" : result }) ",nlp-word2vec.ipynb
"IntroductionEven if you re new to Connect Four, you ve likely developed several game playing strategies. In this tutorial, you ll learn to use a heuristic to share your knowledge with the agent. Game treesAs a human player, how do you think about how to play the game? How do you weigh alternative moves?You likely do a bit of forecasting. For each potential move, you predict what your opponent is likely to do in response, along with how you d then respond, and what the opponent is likely to do then, and so on. Then, you choose the move that you think is most likely to result in a win.We can formalize this idea and represent all possible outcomes in a complete game tree. The game tree represents each possible move by agent and opponent , starting with an empty board. The first row shows all possible moves the agent red player can make. Next, we record each move the opponent yellow player can make in response, and so on, until each branch reaches the end of the game. The game tree for Connect Four is quite large, so we show only a small preview in the image above. Once we can see every way the game can possibly end, it can help us to pick the move where we are most likely to win.HeuristicsThe complete game tree for Connect Four has over 4 trillion different boards! So in practice, our agent only works with a small subset when planning a move. To make sure the incomplete tree is still useful to the agent, we will use a heuristic or heuristic function . The heuristic assigns scores to different game boards, where we estimate that boards with higher scores are more likely to result in the agent winning the game. You will design the heuristic based on your knowledge of the game.For instance, one heuristic that might work reasonably well for Connect Four looks at each group of four adjacent locations in a horizontal, vertical, or diagonal line and assigns: 1000000 1e6 points if the agent has four discs in a row the agent won , 1 point if the agent filled three spots, and the remaining spot is empty the agent wins if it fills in the empty spot , and 100 points if the opponent filled three spots, and the remaining spot is empty the opponent wins by filling in the empty spot .This is also represented in the image below. And how exactly will the agent use the heuristic? Consider it s the agent s turn, and it s trying to plan a move for the game board shown at the top of the figure below. There are seven possible moves one for each column . For each move, we record the resulting game board. Then we use the heuristic to assign a score to each board. To do this, we search the grid and look for all occurrences of the pattern in the heuristic, similar to a word search puzzle. Each occurrence modifies the score. For instance, The first board where the agent plays in column 0 gets a score of 2. This is because the board contains two distinct patterns that each add one point to the score where both are circled in the image above . The second board is assigned a score of 1. The third board where the agent plays in column 2 gets a score of 0. This is because none of the patterns from the heuristic appear in the board.The first board receives the highest score, and so the agent will select this move. It s also the best outcome for the agent, since it has a guaranteed win in just one more move. Check this in figure now, to make sure it makes sense to you! The heuristic works really well for this specific example, since it matches the best move with the highest score. It is just one of many heuristics that works reasonably well for creating a Connect Four agent, and you may find that you can design a heuristic that works much better!In general, if you re not sure how to design your heuristic i.e., how to score different game states, or which scores to assign to different conditions , often the best thing to do is to simply take an initial guess and then play against your agent. This will let you identify specific cases when your agent makes bad moves, which you can then fix by modifying the heuristic.CodeOur one step lookahead agent will: use the heuristic to assign a score to each possible valid move, and select the move that gets the highest score. If multiple moves get the high score, we select one at random. One step lookahead refers to the fact that the agent looks only one step or move into the future, instead of deeper in the game tree. To define this agent, we will use the functions in the code cell below. These functions will make more sense when we use them to specify the agent.","
import random
import numpy as np",one-step-lookahead.ipynb
Calculates score if agent drops piece in selected column,"def score_move(grid , col , mark , config): ",one-step-lookahead.ipynb
Helper function for score move: gets board at next step if agent drops piece in selected column,"def drop_piece(grid , col , mark , config): ",one-step-lookahead.ipynb
Helper function for score move: calculates value of heuristic for grid,"def get_heuristic(grid , mark , config): ",one-step-lookahead.ipynb
Helper function for get heuristic: checks if window satisfies heuristic conditions,"def check_window(window , num_discs , piece , config): ",one-step-lookahead.ipynb
Helper function for get heuristic: counts number of windows satisfying specified heuristic conditions,"def count_windows(grid , num_discs , piece , config): ",one-step-lookahead.ipynb
horizontal, for row in range(config.rows): ,one-step-lookahead.ipynb
vertical, for row in range(config.rows -(config.inarow - 1)) : ,one-step-lookahead.ipynb
positive diagonal, for row in range(config.rows -(config.inarow - 1)) : ,one-step-lookahead.ipynb
negative diagonal," for row in range(config.inarow - 1 , config.rows): ",one-step-lookahead.ipynb
The agent is always implemented as a Python function that accepts two arguments: obs and config,"def agent(obs , config): ",one-step-lookahead.ipynb
Get list of valid moves, valid_moves =[c for c in range(config.columns)if obs.board[c]== 0] ,one-step-lookahead.ipynb
Convert the board to a 2D grid," grid = np.asarray(obs.board). reshape(config.rows , config.columns) ",one-step-lookahead.ipynb
Use the heuristic to assign a score to each possible board in the next turn," scores = dict(zip(valid_moves ,[score_move(grid , col , obs.mark , config)for col in valid_moves])) ",one-step-lookahead.ipynb
Get a list of columns moves that maximize the heuristic, max_cols =[key for key in scores.keys ()if scores[key]== max(scores.values ())] ,one-step-lookahead.ipynb
Select at random from the maximizing columns, return random.choice(max_cols) ,one-step-lookahead.ipynb
"In the code for the agent, we begin by getting a list of valid moves. This is the same line of code we used in the previous tutorial!Next, we convert the game board to a 2D numpy array. For Connect Four, grid is an array with 6 rows and 7 columns.Then, the score move function calculates the value of the heuristic for each valid move. It uses a couple of helper functions: drop piece returns the grid that results when the player drops its disc in the selected column. get heuristic calculates the value of the heuristic for the supplied board grid , where mark is the mark of the agent. This function uses the count windows function, which counts the number of windows of four adjacent locations in a row, column, or diagonal that satisfy specific conditions from the heuristic. Specifically, count windows grid, num discs, piece, config yields the number of windows in the game board grid that contain num discs pieces from the player agent or opponent with mark piece, and where the remaining locations in the window are empty. For instance, setting num discs 4 and piece obs.mark counts the number of times the agent got four discs in a row. setting num discs 3 and piece obs.mark 2 1 counts the number of windows where the opponent has three discs, and the remaining location is empty the opponent wins by filling in the empty spot .Finally, we get the list of columns that maximize the heuristic and select one uniformly at random. Note: For this course, we decided to provide relatively slower code that was easier to follow. After you ve taken the time to understand the code above, can you see how to re write it, to make it run much faster? As a hint, note that the count windows function is used several times to loop over the locations in the game board. In the next code cell, we see the outcome of one game round against a random agent.","from kaggle_environments import make , evaluate ",one-step-lookahead.ipynb
Create the game environment,"env = make(""connectx"") ",one-step-lookahead.ipynb
Two random agents play one game round,"env.run ([agent , ""random""]) ",one-step-lookahead.ipynb
Show the game,"env.render(mode = ""ipython"") ",one-step-lookahead.ipynb
We use the get win percentage function from the previous tutorial to check how we can expect it to perform on average.,"def get_win_percentages(agent1 , agent2 , n_rounds = 100): ",one-step-lookahead.ipynb
Use default Connect Four setup," config = { 'rows' : 6 , 'columns' : 7 , 'inarow' : 4 } ",one-step-lookahead.ipynb
Agent 1 goes first roughly half the time," outcomes = evaluate(""connectx"" ,[agent1 , agent2], config , [], n_rounds // 2) ",one-step-lookahead.ipynb
Agent 2 goes first roughly half the time," outcomes +=[[ b , a]for[a , b]in evaluate(""connectx"" ,[agent2 , agent1], config , [], n_rounds - n_rounds // 2 )] ",one-step-lookahead.ipynb
"Outlier Analysis What is an Outlier? Outlier is an observation that is numerically distant from the rest of the data or in a simple word it is the value which is out of the range.let s take an example to check what happens to a data set with and data set without outliers. Data without outlier Data with outlier Data 1,2,3,3,4,5,4 1,2,3,3,4,5,400 Mean 3.142 59.714 Median 3 3 Standard Deviation 1.345185 150.057 As you can see, data set with outliers has significantly different mean and standard deviation. In the first scenario, we will say that average is 3.14. But with the outlier, average soars to 59.71. This would change the estimate completely. The above meme makes you better understanding of outlier. Lets take a real world example. In a company of 50 employees, 45 people having monthly salary of Rs.6,000, 5 senior employees having monthly salary of Rs.100000 each. If you calculate the average monthly salary of employees in the company is Rs.14,500, which will give you the wrong conclusion majority of employees have lesser than 14.5k salary . But if you take median salary, it is Rs.6000 which is more sense than the average.For this reason median is appropriate measure than mean. Here you can see the effect of outlier. Outlier is a commonly used terminology by analysts and data scientists as it needs close attention else it can result in wildly wrong estimations. Simply speaking, Outlier is an observation that appears far away and diverges from an overall pattern in a sample. Cause for outliers Data Entry Errors: Human errors such as errors caused during data collection, recording, or entry can cause outliers in data. Data Entry Errors: Human errors such as errors caused during data collection, recording, or entry can cause outliers in data. Measurement Error: It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty. Natural Outlier: When an outlier is not artificial due to error , it is a natural outlier. Most of real world data belong to this category. Outlier Detection Outlier can be of two types: Univariate and Multivariate. Above, we have discussed the example of univariate outlier. These outliers can be found when we look at distribution of a single variable. Multi variate outliers are outliers in an n dimensional space. Different outlier detection technique. 1. Hypothesis Testing 2. Z score method 3. Robust Z score 4. I.Q.R method 5. Winsorization method Percentile Capping 6. DBSCAN Clustering 7. Isolation Forest 8. Visualizing the data 1. Hypothesis Testing grubbs test begin array l text If the calculated value is greater than critical, you can reject the null hypothesis and conclude that one of the values is an outlier end array ","import numpy as np
import scipy.stats as stats
x = np.array([12,13,14,19,21,23])
y = np.array([12,13,14,19,21,23,45])
def grubbs_test(x):
 n = len(x)
 mean_x = np.mean(x)
 sd_x = np.std(x)
 numerator = max(abs(x-mean_x))
 g_calculated = numerator/sd_x
 print(""Grubbs Calculated Value:"",g_calculated)
 t_value = stats.t.ppf(1 - 0.05 / (2 * n), n - 2)
 g_critical = ((n - 1) * np.sqrt(np.square(t_value))) / (np.sqrt(n) * np.sqrt(n - 2 + np.square(t_value)))
 print(""Grubbs Critical Value:"",g_critical)
 if g_critical > g_calculated:
 print(""From grubbs_test we observe that calculated value is lesser than critical value, Accept null hypothesis and conclude that there is no outliers\n"")
 else:
 print(""From grubbs_test we observe that calculated value is greater than critical value, Reject null hypothesis and conclude that there is an outliers\n"")
grubbs_test(x)
grubbs_test(y)",outlier-the-silent-killer.ipynb
" Z score method Using Z score method,we can find out how many standard deviations value away from the mean. Figure in the left shows area under normal curve and how much area that standard deviation covers. 68 of the data points lie between or 1 standard deviation. 95 of the data points lie between or 2 standard deviation 99.7 of the data points lie between or 3 standard deviation Z score formula begin array l Z score frac X Mean Standard Deviation end array If the z score of a data point is more than 3 because it cover 99.7 of area , it indicates that the data value is quite different from the other values. It is taken as outliers. ","import pandas as pd
import numpy as np
train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')
out=[]
def Zscore_outlier(df):
 m = np.mean(df)
 sd = np.std(df)
 for i in df: 
 z = (i-m)/sd
 if np.abs(z) > 3: 
 out.append(i)
 print(""Outliers:"",out)
Zscore_outlier(train['LotArea'])",outlier-the-silent-killer.ipynb
" Robust Z score It is also called as Median absolute deviation method. It is similar to Z score method with some changes in parameters. Since mean and standard deviations are heavily influenced by outliers, alter to this parameters we use median and absolute deviation from median. Robust Z score formula begin array l R.Z.score frac 0.6745 X i Median MAD end array Where MAD median X median Suppose x follows a standard normal distribution. The MAD will converge to the median of the half normal distribution, which is the 75 percentile of a normal distribution, and N 0.75 0.6745. ","import pandas as pd
import numpy as np
train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')
out=[]
def ZRscore_outlier(df):
 med = np.median(df)
 ma = stats.median_absolute_deviation(df)
 for i in df: 
 z = (0.6745*(i-med))/ (np.median(ma))
 if np.abs(z) > 3: 
 out.append(i)
 print(""Outliers:"",out)
ZRscore_outlier(train['LotArea'])",outlier-the-silent-killer.ipynb
" IQR method In this method by using Inter Quartile Range IQR , we detect outliers. IQR tells us the variation in the data set.Any value, which is beyond the range of 1.5 x IQR to 1.5 x IQR treated as outliers Q1 represents the 1st quartile 25th percentile of the data. Q2 represents the 2nd quartile median 50th percentile of the data. Q3 represents the 3rd quartile 75th percentile of the data. Q1 1.5 IQR represent the smallest value in the data set and Q3 1.5 IQR represnt the largest value in the data set. ","import pandas as pd
import numpy as np
train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')
out=[]
def iqr_outliers(df):
 q1 = df.quantile(0.25)
 q3 = df.quantile(0.75)
 iqr = q3-q1
 Lower_tail = q1 - 1.5 * iqr
 Upper_tail = q3 + 1.5 * iqr
 for i in df:
 if i > Upper_tail or i < Lower_tail:
 out.append(i)
 print(""Outliers:"",out)
iqr_outliers(train['LotArea'])",outlier-the-silent-killer.ipynb
 Winsorization Method Percentile Capping This method is similar to IQR method. If a value exceeds the value of the 99th percentile and below the 1st percentile of given values are treated as outliers. ,"import pandas as pd
import numpy as np
train = pd.read_csv('../input/titanic/train.csv')
out=[]
def Winsorization_outliers(df):
 q1 = np.percentile(df , 1)
 q3 = np.percentile(df , 99)
 for i in df:
 if i > q3 or i < q1:
 out.append(i)
 print(""Outliers:"",out)
Winsorization_outliers(train['Fare'])",outlier-the-silent-killer.ipynb
" DBSCAN Density Based Spatial Clustering of Applications with Noise DBSCAN is a density based clustering algorithm that divides a dataset into subgroups of high density regions and identifies high density regions cluster as outliers. Here cluster 1 indicates that the cluster contains outlier and rest of clusters have no outliers. This approch is similar to the K mean clustering. There are two parameters required for DBSCAN. DBSCAN give best result for multivariate outlier detection. 1. epsilon: a distance parameter that defines the radius to search for nearby neighbors. 2. minimum amount of points required to form a cluster. Using epsilon and minPts, we can classify each data point as: Core point a point that has at least a minimum number of other points minPts within its radius. Border point a point is within the radius of a core point but has less than the minimum number of other points minPts within its own radius. Noise point a point that is neither a core point or a border point ","import pandas as pd
from sklearn.cluster import DBSCAN
train = pd.read_csv('../input/titanic/train.csv')
def DB_outliers(df):
 outlier_detection = DBSCAN(eps = 2, metric='euclidean', min_samples = 5)
 clusters = outlier_detection.fit_predict(df.values.reshape(-1,1))
 data = pd.DataFrame()
 data['cluster'] = clusters
 print(data['cluster'].value_counts().sort_values(ascending=False))
DB_outliers(train['Fare']) ",outlier-the-silent-killer.ipynb
" Isolation Forest It is an clustering algorithm that belongs to the ensemble decision trees family and is similar in principle to Random Forest. 1. It classify the data point to outlier and not outliers and works great with very high dimensional data. 2. It works based on decision tree and it isolate the outliers. 3. If the result is 1, it means that this specific data point is an outlier. If the result is 1, then it means that the data point is not an outlier. ","from sklearn.ensemble import IsolationForest
import numpy as np
import pandas as pd
train = pd.read_csv('../input/titanic/train.csv')
train['Fare'].fillna(train[train.Pclass==3]['Fare'].median(),inplace=True)
def Iso_outliers(df):
 iso = IsolationForest( behaviour = 'new', random_state = 1, contamination= 'auto')
 preds = iso.fit_predict(df.values.reshape(-1,1))
 data = pd.DataFrame()
 data['cluster'] = preds
 print(data['cluster'].value_counts().sort_values(ascending=False))
Iso_outliers(train['Fare']) ",outlier-the-silent-killer.ipynb
" Visualizing the data Data visualization is useful for data cleaning, exploring data, detecting outliers and unusual groups, identifying trends and clusters etc. Here the list of data visualization plots to spot the outliers. 1. Box and whisker plot box plot . 2. Scatter plot. 3. Histogram. 4. Distribution Plot. 5. QQ plot. ","import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from statsmodels.graphics.gofplots import qqplot
train = pd.read_csv('../input/titanic/train.csv')
def Box_plots(df):
 plt.figure(figsize=(10, 4))
 plt.title(""Box Plot"")
 sns.boxplot(df)
 plt.show()
Box_plots(train['Age'])

def hist_plots(df):
 plt.figure(figsize=(10, 4))
 plt.hist(df)
 plt.title(""Histogram Plot"")
 plt.show()
hist_plots(train['Age'])

def scatter_plots(df1,df2):
 fig, ax = plt.subplots(figsize=(10,4))
 ax.scatter(df1,df2)
 ax.set_xlabel('Age')
 ax.set_ylabel('Fare')
 plt.title(""Scatter Plot"")
 plt.show()
scatter_plots(train['Age'],train['Fare'])

def dist_plots(df):
 plt.figure(figsize=(10, 4))
 sns.distplot(df)
 plt.title(""Distribution plot"")
 sns.despine()
 plt.show()
dist_plots(train['Fare'])

def qq_plots(df):
 plt.figure(figsize=(10, 4))
 qqplot(df,line='s')
 plt.title(""Normal QQPlot"")
 plt.show()
qq_plots(train['Fare'])

",outlier-the-silent-killer.ipynb
"What Next?? After detecting the outlier we should remove treat the outlier because it is a silent killer!! yes. Outliers badly affect mean and standard deviation of the dataset. These may statistically give erroneous results. It increases the error variance and reduces the power of statistical tests. If the outliers are non randomly distributed, they can decrease normality. Most machine learning algorithms do not work well in the presence of outlier. So it is desirable to detect and remove outliers. They can also impact the basic assumption of Regression, ANOVA and other statistical model assumptions. With all these reasons we must be careful about outlier and treat them before build a statistical machine learning model. There are some techniques used to deal with outliers. 1. Deleting observations. 2. Transforming values. 3. Imputation. 4. Separately treating Deleting observations: We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers. But deleting the observation is not a good idea when we have small dataset.","import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
train = pd.read_csv('../input/cost-of-living/cost-of-living-2018.csv')
sns.boxplot(train['Cost of Living Index'])
plt.title(""Box Plot before outlier removing"")
plt.show()
def drop_outliers(df, field_name):
 iqr = 1.5 * (np.percentile(df[field_name], 75) - np.percentile(df[field_name], 25))
 df.drop(df[df[field_name] > (iqr + np.percentile(df[field_name], 75))].index, inplace=True)
 df.drop(df[df[field_name] < (np.percentile(df[field_name], 25) - iqr)].index, inplace=True)
drop_outliers(train, 'Cost of Living Index')
sns.boxplot(train['Cost of Living Index'])
plt.title(""Box Plot after outlier removing"")
plt.show()",outlier-the-silent-killer.ipynb
Scalling,import pandas as pd ,outlier-the-silent-killer.ipynb
Log Transformation,import pandas as pd ,outlier-the-silent-killer.ipynb
cube root Transformation,import pandas as pd ,outlier-the-silent-killer.ipynb
Box transformation,import pandas as pd ,outlier-the-silent-killer.ipynb
mean imputation,import pandas as pd ,outlier-the-silent-killer.ipynb
median imputation,import pandas as pd ,outlier-the-silent-killer.ipynb
Zero value imputation,import pandas as pd ,outlier-the-silent-killer.ipynb
"IntroductionRecall from the example in the previous lesson that Keras will keep a history of the training and validation loss over the epochs that it is training the model. In this lesson, we re going to learn how to interpret these learning curves and how we can use them to guide model development. In particular, we ll examine at the learning curves for evidence of underfitting and overfitting and look at a couple of strategies for correcting it.Interpreting the Learning CurvesYou might think about the information in the training data as being of two kinds: signal and noise. The signal is the part that generalizes, the part that can help our model make predictions from new data. The noise is that part that is only true of the training data the noise is all of the random fluctuation that comes from data in the real world or all of the incidental, non informative patterns that can t actually help the model make predictions. The noise is the part might look useful but really isn t.We train a model by choosing weights or parameters that minimize the loss on a training set. You might know, however, that to accurately assess a model s performance, we need to evaluate it on a new set of data, the validation data. You could see our lesson on model validation in Introduction to Machine Learning for a review. When we train a model we ve been plotting the loss on the training set epoch by epoch. To this we ll add a plot the validation data too. These plots we call the learning curves. To train deep learning models effectively, we need to be able to interpret them. The validation loss gives an estimate of the expected error on unseen data. Now, the training loss will go down either when the model learns signal or when it learns noise. But the validation loss will go down only when the model learns signal. Whatever noise the model learned from the training set won t generalize to new data. So, when a model learns signal both curves go down, but when it learns noise a gap is created in the curves. The size of the gap tells you how much noise the model has learned.Ideally, we would create models that learn all of the signal and none of the noise. This will practically never happen. Instead we make a trade. We can get the model to learn more signal at the cost of learning more noise. So long as the trade is in our favor, the validation loss will continue to decrease. After a certain point, however, the trade can turn against us, the cost exceeds the benefit, and the validation loss begins to rise. Underfitting and overfitting. This trade off indicates that there can be two problems that occur when training a model: not enough signal or too much noise. Underfitting the training set is when the loss is not as low as it could be because the model hasn t learned enough signal. Overfitting the training set is when the loss is not as low as it could be because the model learned too much noise. The trick to training deep learning models is finding the best balance between the two.We ll look at a couple ways of getting more signal out of the training data while reducing the amount of noise.CapacityA model s capacity refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity.You can increase the capacity of a network either by making it wider more units to existing layers or by making it deeper adding more layers . Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset. model keras.Sequential layers.Dense 16, activation relu , layers.Dense 1 , wider keras.Sequential layers.Dense 32, activation relu , layers.Dense 1 , deeper keras.Sequential layers.Dense 16, activation relu , layers.Dense 16, activation relu , layers.Dense 1 , You ll explore how the capacity of a network can affect its performance in the exercise.Early StoppingWe mentioned that when a model is too eagerly learning noise, the validation loss may start to increase during training. To prevent this, we can simply stop the training whenever it seems the validation loss isn t decreasing anymore. Interrupting the training this way is called early stopping. We keep the model where the validation loss is at a minimum. Once we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured. This ensures that the model won t continue to learn noise and overfit the data.Training with early stopping also means we re in less danger of stopping the training too early, before the network has finished learning signal. So besides preventing overfitting from training too long, early stopping can also prevent underfitting from not training long enough. Just set your training epochs to some large number more than you ll need , and early stopping will take care of the rest.Adding Early StoppingIn Keras, we include early stopping in our training through a callback. A callback is just a function you want run every so often while the network trains. The early stopping callback will run after every epoch. Keras has a variety of useful callbacks pre defined, but you can define your own, too. ",from tensorflow.keras.callbacks import EarlyStopping ,overfitting-and-underfitting.ipynb
"These parameters say: If there hasn t been at least an improvement of 0.001 in the validation loss over the previous 20 epochs, then stop the training and keep the best model you found. It can sometimes be hard to tell if the validation loss is rising due to overfitting or just due to random batch variation. The parameters allow us to set some allowances around when to stop.As we ll see in our example, we ll pass this callback to the fit method along with the loss and optimizer.Example Train a Model with Early StoppingLet s continue developing the model from the example in the last tutorial. We ll increase the capacity of that network but also add an early stopping callback to prevent overfitting.Here s the data prep again.",import pandas as pd ,overfitting-and-underfitting.ipynb
Create training and validation splits,"df_train = red_wine.sample(frac = 0.7 , random_state = 0) ",overfitting-and-underfitting.ipynb
"Scale to 0, 1 ",max_ = df_train.max(axis = 0) ,overfitting-and-underfitting.ipynb
Split features and target,"X_train = df_train.drop('quality' , axis = 1) ",overfitting-and-underfitting.ipynb
"Now let s increase the capacity of the network. We ll go for a fairly large network, but rely on the callback to halt the training once the validation loss shows signs of increasing.",from tensorflow import keras ,overfitting-and-underfitting.ipynb
"Partial Dependence PlotsWhile feature importance shows what variables most affect predictions, partial dependence plots show how a feature affects predictions.This is useful to answer questions like: Controlling for all other house features, what impact do longitude and latitude have on home prices? To restate this, how would similarly sized houses be priced in different areas? Are predicted health differences between two groups due to differences in their diets, or due to some other factor? If you are familiar with linear or logistic regression models, partial dependence plots can be interpreted similarly to the coefficients in those models. Though, partial dependence plots on sophisticated models can capture more complex patterns than coefficients from simple models. If you aren t familiar with linear or logistic regressions, don t worry about this comparison.We will show a couple examples, explain the interpretation of these plots, and then review the code to create these plots.How it WorksLike permutation importance, partial dependence plots are calculated after a model has been fit. The model is fit on real data that has not been artificially manipulated in any way. In our soccer example, teams may differ in many ways. How many passes they made, shots they took, goals they scored, etc. At first glance, it seems difficult to disentangle the effect of these features.To see how partial plots separate out the effect of each feature, we start by considering a single row of data. For example, that row of data might represent a team that had the ball 50 of the time, made 100 passes, took 10 shots and scored 1 goal.We will use the fitted model to predict our outcome probability their player won man of the match . But we repeatedly alter the value for one variable to make a series of predictions. We could predict the outcome if the team had the ball only 40 of the time. We then predict with them having the ball 50 of the time. Then predict again for 60 . And so on. We trace out predicted outcomes on the vertical axis as we move from small values of ball possession to large values on the horizontal axis .In this description, we used only a single row of data. Interactions between features may cause the plot for a single row to be atypical. So, we repeat that mental experiment with multiple rows from the original dataset, and we plot the average predicted outcome on the vertical axis. Code ExampleModel building isn t our focus, so we won t focus on the data exploration or model building code.",import numpy as np ,partial-plots.ipynb
Convert from string Yes No to binary,"y =(data['Man of the Match']== ""Yes"") ",partial-plots.ipynb
"Our first example uses a decision tree, which you can see below. In practice, you ll use more sophistated models for real world applications.","from sklearn import tree
import graphviz

tree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=feature_names)
graphviz.Source(tree_graph)",partial-plots.ipynb
Here is the code to create the Partial Dependence Plot using the PDPBox library.,from matplotlib import pyplot as plt ,partial-plots.ipynb
Create the data that we will plot,"pdp_goals = pdp.pdp_isolate(model = tree_model , dataset = val_X , model_features = feature_names , feature = 'Goal Scored') ",partial-plots.ipynb
plot it,"pdp.pdp_plot(pdp_goals , 'Goal Scored') ",partial-plots.ipynb
"A few items are worth pointing out as you interpret this plot The y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value. A blue shaded area indicates level of confidenceFrom this particular graph, we see that scoring a goal substantially increases your chances of winning Man of The Match. But extra goals beyond that appear to have little impact on predictions.Here is another example plot:","feature_to_plot = 'Distance Covered (Kms)'
pdp_dist = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)

pdp.pdp_plot(pdp_dist, feature_to_plot)
plt.show()",partial-plots.ipynb
Build Random Forest model,"rf_model = RandomForestClassifier(random_state = 0). fit(train_X , train_y) ",partial-plots.ipynb
Similar to previous PDP plot except we use pdp interact instead of pdp isolate and pdp interact plot instead of pdp isolate plot,"features_to_plot =['Goal Scored' , 'Distance Covered (Kms)'] ",partial-plots.ipynb
"In this tutorial, you will learn how to use pipelines to clean up your modeling code.IntroductionPipelines are a simple way to keep your data preprocessing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.Many data scientists hack together models without pipelines, but pipelines have some important benefits. Those include: 1. Cleaner Code: Accounting for data at each step of preprocessing can get messy. With a pipeline, you won t need to manually keep track of your training and validation data at each step. 2. Fewer Bugs: There are fewer opportunities to misapply a step or forget a preprocessing step. 3. Easier to Productionize: It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won t go into the many related concerns here, but pipelines can help. 4. More Options for Model Validation: You will see an example in the next tutorial, which covers cross validation.ExampleAs in the previous tutorial, we will work with the Melbourne Housing dataset. We won t focus on the data loading step. Instead, you can imagine you are at a point where you already have the training and validation data in X train, X valid, y train, and y valid. ",import pandas as pd ,pipelines.ipynb
Read the data,data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv') ,pipelines.ipynb
Separate target from predictors,y = data.Price ,pipelines.ipynb
"We take a peek at the training data with the head method below. Notice that the data contains both categorical data and columns with missing values. With a pipeline, it s easy to deal with both!",X_train.head(),pipelines.ipynb
"We construct the full pipeline in three steps.Step 1: Define Preprocessing StepsSimilar to how a pipeline bundles together preprocessing and modeling steps, we use the ColumnTransformer class to bundle together different preprocessing steps. The code below: imputes missing values in numerical data, and imputes missing values and applies a one hot encoding to categorical data.",from sklearn.compose import ColumnTransformer ,pipelines.ipynb
Preprocessing for numerical data,numerical_transformer = SimpleImputer(strategy = 'constant') ,pipelines.ipynb
"Step 2: Define the ModelNext, we define a random forest model with the familiar RandomForestRegressor class.","from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=100, random_state=0)",pipelines.ipynb
"Step 3: Create and Evaluate the PipelineFinally, we use the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps. There are a few important things to notice: With the pipeline, we preprocess the training data and fit the model in a single line of code. In contrast, without a pipeline, we have to do imputation, one hot encoding, and model training in separate steps. This becomes especially messy if we have to deal with both numerical and categorical variables! With the pipeline, we supply the unprocessed features in X valid to the predict command, and the pipeline automatically preprocesses the features before generating predictions. However, without a pipeline, we have to remember to preprocess the validation data before making predictions. ",from sklearn.metrics import mean_absolute_error ,pipelines.ipynb
"IntroductionConnect Four is a game where two players alternate turns dropping colored discs into a vertical grid. Each player uses a different color usually red or yellow , and the objective of the game is to be the first player to get four discs in a row. In this course, you will build your own intelligent agents to play the game. In the first lesson, you ll learn how to set up the game environment and create your first agent. The next two lessons focus on traditional methods for building game AI. These agents will be smart enough to defeat many novice players! In the final lesson, you ll experiment with cutting edge algorithms from the field of reinforcement learning. The agents that you build will come up with gameplay strategies much like humans do: gradually, and with experience. Join the competitionThroughout the course, you ll test your agents performance by competing against agents that other users have created. To join the competition, open a new window with the competition page, and click on the Join Competition button. If you see a Submit Agent button instead of a Join Competition button, you have already joined the competition, and don t need to do so again. This takes you to the rules acceptance page. You must accept the competition rules in order to participate. These rules govern how many submissions you can make per day, the maximum team size, and other competition specific details. Then, click on I Understand and Accept to indicate that you will abide by the competition rules.Getting startedThe game environment comes equipped with agents that have already been implemented for you. To see a list of these default agents, run:","from kaggle_environments import make , evaluate ",play-the-game.ipynb
Set debug True to see the errors if your agent refuses to run,"env = make(""connectx"" , debug = True) ",play-the-game.ipynb
List of available default agents,print(list(env.agents)) ,play-the-game.ipynb
Two random agents play one game round,"env.run ([""random"" , ""random""]) ",play-the-game.ipynb
Show the game,"env.render(mode = ""ipython"") ",play-the-game.ipynb
"You can use the player above to view the game in detail: every move is captured and can be replayed. Try this now!As you ll soon see, this information will prove incredibly useful for brainstorming ways to improve our agents.Defining agentsTo participate in the competition, you ll create your own agents. Your agent should be implemented as a Python function that accepts two arguments: obs and config. It returns an integer with the selected column, where indexing starts at zero. So, the returned value is one of 0 6, inclusive.We ll start with a few examples, to provide some context. In the code cell below: The first agent behaves identically to the random agent above. The second agent always selects the middle column, whether it s valid or not! Note that if any agent selects an invalid move, it loses the game. The third agent selects the leftmost valid column.","
import random
import numpy as np",play-the-game.ipynb
Selects random valid column,"def agent_random(obs , config): ",play-the-game.ipynb
Selects middle column,"def agent_middle(obs , config): ",play-the-game.ipynb
Selects leftmost valid column,"def agent_leftmost(obs , config): ",play-the-game.ipynb
Agents play one game round,"env.run ([agent_leftmost , agent_random]) ",play-the-game.ipynb
Show the game,"env.render(mode = ""ipython"") ",play-the-game.ipynb
"The outcome of a single game is usually not enough information to figure out how well our agents are likely to perform. To get a better idea, we ll calculate the win percentages for each agent, averaged over multiple games. For fairness, each agent goes first half of the time.To do this, we ll use the get win percentages function defined in a hidden code cell . To view the details of this function, click on the Code button below.","def get_win_percentages(agent1 , agent2 , n_rounds = 100): ",play-the-game.ipynb
Use default Connect Four setup," config = { 'rows' : 6 , 'columns' : 7 , 'inarow' : 4 } ",play-the-game.ipynb
Agent 1 goes first roughly half the time," outcomes = evaluate(""connectx"" ,[agent1 , agent2], config , [], n_rounds // 2) ",play-the-game.ipynb
Agent 2 goes first roughly half the time," outcomes +=[[ b , a]for[a , b]in evaluate(""connectx"" ,[agent2 , agent1], config , [], n_rounds - n_rounds // 2 )] ",play-the-game.ipynb
"Which agent do you think performs better against the random agent: the agent that always plays in the middle agent middle , or the agent that chooses the leftmost valid column agent leftmost ? Let s find out!","get_win_percentages(agent1=agent_middle, agent2=agent_random)",play-the-game.ipynb
linear algebra,import numpy as np ,popcorn-rnn-model.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,popcorn-rnn-model.ipynb
Any results you write to the current directory are saved as output.,"train = pd.read_csv(""../input/labeledTrainData.tsv"", header = 0, delimiter = '\t')
test = pd.read_csv(""../input/testData.tsv"", header = 0, delimiter = '\t')",popcorn-rnn-model.ipynb
"Extract out the labels from the test data, due to the data leakage issue as pointed in the post . The label from the test data will be used for evaluation purpose.","test[""sentiment""] = test[""id""].map(lambda x: 1 if int(x.strip('""').split(""_"")[1]) >= 5 else 0)
y_test = test[""sentiment""]",popcorn-rnn-model.ipynb
Define some pre processing functions,def html_to_text(review): ,popcorn-rnn-model.ipynb
ignore xc3 etc.," review_text = review_text.encode('ascii' , 'ignore'). decode('ascii') ",popcorn-rnn-model.ipynb
It is probably worth experimenting with milder prepreocessing eg just removing punctuation ," return re.sub(""[^a-zA-Z]"" , "" "" , text) ",popcorn-rnn-model.ipynb
Propcessed the data,"x_train, y_train, x_val, y_val = get_train_val_data(rnn_tokenizer_review_preprocess)
x_test = test[""review""].map(rnn_tokenizer_review_preprocess)
y_test = test[""sentiment""]",popcorn-rnn-model.ipynb
Generate the text sequence for RNN model,"from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences",popcorn-rnn-model.ipynb
all review list train review list val review list,train_review_list = x_train.tolist () ,popcorn-rnn-model.ipynb
Architecture the RNN Model,"from keras.layers import Input, Embedding, Dropout, Conv1D, MaxPool1D, GRU, LSTM, Dense
from keras.models import Model",popcorn-rnn-model.ipynb
GRU Model,gru_model = rnn_model(use_lstm=False),popcorn-rnn-model.ipynb
LSTM Model,"lstm_model = rnn_model(use_lstm=True)
lstm_model.summary()",popcorn-rnn-model.ipynb
Evaluate the Model Performance on Test Data,"from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
%matplotlib inline",popcorn-rnn-model.ipynb
"Linear Regression with Python Linear Regression is the simplest algorithm in machine learning, it can be trained in different ways. In this notebook we will cover the following linear algorithms: Linear Regression Robust Regression Ridge Regression LASSO Regression Elastic Net Polynomial Regression Stochastic Gradient Descent Artificial Neaural Networks Data We are going to use the USA Housing dataset. Since house price is a continues variable, this is a regression problem. The data contains the following columns: Avg. Area Income : Avg. Income of residents of the city house is located in. Avg. Area House Age : Avg Age of Houses in same city Avg. Area Number of Rooms : Avg Number of Rooms for Houses in same city Avg. Area Number of Bedrooms : Avg Number of Bedrooms for Houses in same city Area Population : Population of city hou se is located in Price : Price that the house sold at Address : Address for the house ",!pip install -q hvplot,practical-introduction-to-10-regression-algorithm.ipynb
Import Libraries,import pandas as pd ,practical-introduction-to-10-regression-algorithm.ipynb
Check out the Data,"USAhousing = pd.read_csv('/kaggle/input/usa-housing/USA_Housing.csv')
USAhousing.head()",practical-introduction-to-10-regression-algorithm.ipynb
Exploratory Data Analysis EDA Let s create some simple plots to check out the data!,sns.pairplot(USAhousing),practical-introduction-to-10-regression-algorithm.ipynb
"Training a Linear Regression Model Let s now begin to train out regression model! We will need to first split up our data into an X array that contains the features to train on, and a y array with the target variable, in this case the Price column. We will toss out the Address column because it only has text info that the linear regression model can t use. X and y arrays","X = USAhousing[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',
 'Avg. Area Number of Bedrooms', 'Area Population']]
y = USAhousing['Price']",practical-introduction-to-10-regression-algorithm.ipynb
Train Test SplitNow let s split the data into a training set and a testing set. We will train out model on the training set and then use the test set to evaluate the model.,"from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",practical-introduction-to-10-regression-algorithm.ipynb
"Preparing Data For Linear Regression Linear regression is been studied at great length, and there is a lot of literature on how your data must be structured to make best use of the model. As such, there is a lot of sophistication when talking about these requirements and expectations which can be intimidating. In practice, you can uses these rules more as rules of thumb when using Ordinary Least Squares Regression, the most common implementation of linear regression. Try different preparations of your data using these heuristics and see what works best for your problem. Linear Assumption. Linear regression assumes that the relationship between your input and output is linear. It does not support anything else. This may be obvious, but it is good to remember when you have a lot of attributes. You may need to transform data to make the relationship linear e.g. log transform for an exponential relationship . Remove Noise. Linear regression assumes that your input and output variables are not noisy. Consider using data cleaning operations that let you better expose and clarify the signal in your data. This is most important for the output variable and you want to remove outliers in the output variable y if possible. Remove Collinearity. Linear regression will over fit your data when you have highly correlated input variables. Consider calculating pairwise correlations for your input data and removing the most correlated. Gaussian Distributions. Linear regression will make more reliable predictions if your input and output variables have a Gaussian distribution. You may get some benefit using transforms e.g. log or BoxCox on you variables to make their distribution more Gaussian looking. Rescale Inputs: Linear regression will often make more reliable predictions if you rescale input variables using standardization or normalization. ","from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
 ('std_scalar', StandardScaler())
])

X_train = pipeline.fit_transform(X_train)
X_test = pipeline.transform(X_test)",practical-introduction-to-10-regression-algorithm.ipynb
Linear Regression,"from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression(normalize=True)
lin_reg.fit(X_train,y_train)",practical-introduction-to-10-regression-algorithm.ipynb
print the intercept,print(lin_reg.intercept_) ,practical-introduction-to-10-regression-algorithm.ipynb
Predictions from our ModelLet s grab predictions off our test set and see how well it did!,pred = lin_reg.predict(X_test),practical-introduction-to-10-regression-algorithm.ipynb
Residual Histogram,pd.DataFrame({'Error Values': (y_test - pred)}).hvplot.kde(),practical-introduction-to-10-regression-algorithm.ipynb
"Regression Evaluation MetricsHere are three common evaluation metrics for regression problems: Mean Absolute Error MAE is the mean of the absolute value of the errors: Mean Squared Error MSE is the mean of the squared errors: Root Mean Squared Error RMSE is the square root of the mean of the squared errors: Comparing these metrics: MAE is the easiest to understand, because it s the average error. MSE is more popular than MAE, because MSE punishes larger errors, which tends to be useful in the real world. RMSE is even more popular than MSE, because RMSE is interpretable in the y units. All of these are loss functions, because we want to minimize them. ","test_pred = lin_reg.predict(X_test)
train_pred = lin_reg.predict(X_train)

print('Test set evaluation:\n_____________________________________')
print_evaluate(y_test, test_pred)
print('Train set evaluation:\n_____________________________________')
print_evaluate(y_train, train_pred)

results_df = pd.DataFrame(data=[[""Linear Regression"", *evaluate(y_test, test_pred) , cross_val(LinearRegression())]], 
 columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', ""Cross Validation""])",practical-introduction-to-10-regression-algorithm.ipynb
"Random Sample Consensus RANSAC Random sample consensus RANSAC is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method. A basic assumption is that the data consists of inliers , i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and outliers which are data that do not fit the model. The outliers can come, for example, from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a usually small set of inliers, there exists a procedure which can estimate the parameters of a model that optimally explains or fits this data. ","from sklearn.linear_model import RANSACRegressor

model = RANSACRegressor(base_estimator=LinearRegression(), max_trials=100)
model.fit(X_train, y_train)

test_pred = model.predict(X_test)
train_pred = model.predict(X_train)

print('Test set evaluation:\n_____________________________________')
print_evaluate(y_test, test_pred)
print('====================================')
print('Train set evaluation:\n_____________________________________')
print_evaluate(y_train, train_pred)

results_df_2 = pd.DataFrame(data=[[""Robust Regression"", *evaluate(y_test, test_pred) , cross_val(RANSACRegressor())]], 
 columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', ""Cross Validation""])
results_df = results_df.append(results_df_2, ignore_index=True)",practical-introduction-to-10-regression-algorithm.ipynb
"Ridge Regression Source: scikit learn Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares, is a complexity parameter that controls the amount of shrinkage: the larger the value of , the greater the amount of shrinkage and thus the coefficients become more robust to collinearity. Ridge regression is an L2 penalized model. Add the squared sum of the weights to the least squares cost function. ","from sklearn.linear_model import Ridge

model = Ridge(alpha=100, solver='cholesky', tol=0.0001, random_state=42)
model.fit(X_train, y_train)
pred = model.predict(X_test)

test_pred = model.predict(X_test)
train_pred = model.predict(X_train)

print('Test set evaluation:\n_____________________________________')
print_evaluate(y_test, test_pred)
print('====================================')
print('Train set evaluation:\n_____________________________________')
print_evaluate(y_train, train_pred)

results_df_2 = pd.DataFrame(data=[[""Ridge Regression"", *evaluate(y_test, test_pred) , cross_val(Ridge())]], 
 columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', ""Cross Validation""])
results_df = results_df.append(results_df_2, ignore_index=True)",practical-introduction-to-10-regression-algorithm.ipynb
"LASSO Regression A linear model that estimates sparse coefficients. Mathematically, it consists of a linear model trained with prior as regularizer. The objective function to minimize is: The lasso estimate thus solves the minimization of the least squares penalty with added, where is a constant and is the of the parameter vector. ",from sklearn.linear_model import Lasso ,practical-introduction-to-10-regression-algorithm.ipynb
"Elastic Net A linear regression model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non zero like Lasso, while still maintaining the regularization properties of Ridge. Elastic net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic net is likely to pick both. A practical advantage of trading off between Lasso and Ridge is it allows Elastic Net to inherit some of Ridge s stability under rotation. The objective function to minimize is in this case ","from sklearn.linear_model import ElasticNet

model = ElasticNet(alpha=0.1, l1_ratio=0.9, selection='random', random_state=42)
model.fit(X_train, y_train)

test_pred = model.predict(X_test)
train_pred = model.predict(X_train)

print('Test set evaluation:\n_____________________________________')
print_evaluate(y_test, test_pred)
print('====================================')
print('Train set evaluation:\n_____________________________________')
print_evaluate(y_train, train_pred)

results_df_2 = pd.DataFrame(data=[[""Elastic Net Regression"", *evaluate(y_test, test_pred) , cross_val(ElasticNet())]], 
 columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', ""Cross Validation""])
results_df = results_df.append(results_df_2, ignore_index=True)",practical-introduction-to-10-regression-algorithm.ipynb
"Polynomial Regression Source: scikit learn One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data. For example, a simple linear regression can be extended by constructing polynomial features from the coefficients. In the standard linear regression case, you might have a model that looks like this for two dimensional data: If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second order polynomials, so that the model looks like this: The sometimes surprising observation is that this is still a linear model: to see this, imagine creating a new variable With this re labeling of the data, our problem can be written We see that the resulting polynomial regression is in the same class of linear models we d considered above i.e. the model is linear in w and can be solved by the same techniques. By considering linear fits within a higher dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data. ","from sklearn.preprocessing import PolynomialFeatures

poly_reg = PolynomialFeatures(degree=2)

X_train_2_d = poly_reg.fit_transform(X_train)
X_test_2_d = poly_reg.transform(X_test)

lin_reg = LinearRegression(normalize=True)
lin_reg.fit(X_train_2_d,y_train)

test_pred = lin_reg.predict(X_test_2_d)
train_pred = lin_reg.predict(X_train_2_d)

print('Test set evaluation:\n_____________________________________')
print_evaluate(y_test, test_pred)
print('====================================')
print('Train set evaluation:\n_____________________________________')
print_evaluate(y_train, train_pred)

results_df_2 = pd.DataFrame(data=[[""Polynomail Regression"", *evaluate(y_test, test_pred), 0]], 
 columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])
results_df = results_df.append(results_df_2, ignore_index=True)",practical-introduction-to-10-regression-algorithm.ipynb
"Stochastic Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Sescent is to tweak parameters iteratively in order to minimize a cost function. Gradient Descent measures the local gradient of the error function with regards to the parameters vector, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum. ","from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor(n_iter_no_change=250, penalty=None, eta0=0.0001, max_iter=100000)
sgd_reg.fit(X_train, y_train)

test_pred = sgd_reg.predict(X_test)
train_pred = sgd_reg.predict(X_train)

print('Test set evaluation:\n_____________________________________')
print_evaluate(y_test, test_pred)
print('====================================')
print('Train set evaluation:\n_____________________________________')
print_evaluate(y_train, train_pred)

results_df_2 = pd.DataFrame(data=[[""Stochastic Gradient Descent"", *evaluate(y_test, test_pred), 0]], 
 columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])
results_df = results_df.append(results_df_2, ignore_index=True)",practical-introduction-to-10-regression-algorithm.ipynb
Artficial Neural Network,from tensorflow.keras.models import Sequential ,practical-introduction-to-10-regression-algorithm.ipynb
model.add Dropout 0.2 ,"model.add(Dense(64 , activation = 'relu')) ",practical-introduction-to-10-regression-algorithm.ipynb
model.add Dropout 0.2 ,"model.add(Dense(128 , activation = 'relu')) ",practical-introduction-to-10-regression-algorithm.ipynb
model.add Dropout 0.2 ,"model.add(Dense(512 , activation = 'relu')) ",practical-introduction-to-10-regression-algorithm.ipynb
Random Forest Regressor,"from sklearn.ensemble import RandomForestRegressor

rf_reg = RandomForestRegressor(n_estimators=1000)
rf_reg.fit(X_train, y_train)

test_pred = rf_reg.predict(X_test)
train_pred = rf_reg.predict(X_train)

print('Test set evaluation:\n_____________________________________')
print_evaluate(y_test, test_pred)

print('Train set evaluation:\n_____________________________________')
print_evaluate(y_train, train_pred)

results_df_2 = pd.DataFrame(data=[[""Random Forest Regressor"", *evaluate(y_test, test_pred), 0]], 
 columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])
results_df = results_df.append(results_df_2, ignore_index=True)",practical-introduction-to-10-regression-algorithm.ipynb
Support Vector Machine,"from sklearn.svm import SVR

svm_reg = SVR(kernel='rbf', C=1000000, epsilon=0.001)
svm_reg.fit(X_train, y_train)

test_pred = svm_reg.predict(X_test)
train_pred = svm_reg.predict(X_train)

print('Test set evaluation:\n_____________________________________')
print_evaluate(y_test, test_pred)

print('Train set evaluation:\n_____________________________________')
print_evaluate(y_train, train_pred)

results_df_2 = pd.DataFrame(data=[[""SVM Regressor"", *evaluate(y_test, test_pred), 0]], 
 columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])
results_df = results_df.append(results_df_2, ignore_index=True)",practical-introduction-to-10-regression-algorithm.ipynb
Models Comparison,"results_df.set_index('Model', inplace=True)
results_df['R2 Square'].plot(kind='barh', figsize=(12, 8))",practical-introduction-to-10-regression-algorithm.ipynb
linear algebra,import numpy as np ,practicing-sentimental-analysis.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,practicing-sentimental-analysis.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,practicing-sentimental-analysis.ipynb
"You can also write temporary files to kaggle temp , but they won t be saved outside of the current session","import pandas as pd

review_df = pd.read_csv('/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip',header=0,sep='\t',quoting=3)",practicing-sentimental-analysis.ipynb
 Libraries ,"import random, re, math
import numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
import tensorflow as tf, tensorflow.keras.backend as K
from kaggle_datasets import KaggleDatasets
from IPython.display import Image
from tensorflow.keras.utils import plot_model
print('Tensorflow version ' + tf.__version__)
from sklearn.model_selection import KFold
import gc
from scipy import stats
import gc
from collections import Counter
import time
import seaborn as sns
from sklearn.metrics import confusion_matrix
import shap
import matplotlib.image as mpimg

!pip install -q efficientnet

from tensorflow.keras.applications import Xception
from tensorflow.keras.applications import VGG16
from tensorflow.keras.applications import DenseNet201
from tensorflow.keras.applications import InceptionV3
from efficientnet.tfkeras import EfficientNetB7
from tensorflow.keras.applications import ResNet152V2
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications import InceptionResNetV2",pretrained-cnn-epic-fight.ipynb
"Detect hardware, return appropriate distribution strategy",try : ,pretrained-cnn-epic-fight.ipynb
TPU detection. No parameters necessary if TPU NAME environment variable is set. On Kaggle this is always the case., tpu = tf.distribute.cluster_resolver.TPUClusterResolver () ,pretrained-cnn-epic-fight.ipynb
default distribution strategy in Tensorflow. Works on CPU and single GPU., strategy = tf.distribute.get_strategy () ,pretrained-cnn-epic-fight.ipynb
Configuration,"IMAGE_SIZE =[224 , 224] ",pretrained-cnn-epic-fight.ipynb
Data access,GCS_DS_PATH = KaggleDatasets (). get_gcs_path('tpu-getting-started') ,pretrained-cnn-epic-fight.ipynb
available image sizes,def decode_image(image_data): ,pretrained-cnn-epic-fight.ipynb
"convert image to floats in 0, 1 range"," image = tf.cast(image , tf.float32)/ 255.0 ",pretrained-cnn-epic-fight.ipynb
explicit size needed for TPU," image = tf.reshape(image ,[* IMAGE_SIZE , 3]) ",pretrained-cnn-epic-fight.ipynb
 Data Augmentation Big Thanks to Chris Deotte for thisRotation Augmentation GPU TPU 0.96 ,"def get_mat(rotation , shear , height_zoom , width_zoom , height_shift , width_shift): ",pretrained-cnn-epic-fight.ipynb
CONVERT DEGREES TO RADIANS, rotation = math.pi * rotation / 180. ,pretrained-cnn-epic-fight.ipynb
ROTATION MATRIX, c1 = tf.math.cos(rotation) ,pretrained-cnn-epic-fight.ipynb
SHEAR MATRIX, c2 = tf.math.cos(shear) ,pretrained-cnn-epic-fight.ipynb
ZOOM MATRIX," zoom_matrix = tf.reshape(tf.concat ([one / height_zoom , zero , zero , zero , one / width_zoom , zero , zero , zero , one], axis = 0),[3 , 3]) ",pretrained-cnn-epic-fight.ipynb
SHIFT MATRIX," shift_matrix = tf.reshape(tf.concat ([one , zero , height_shift , zero , one , width_shift , zero , zero , one], axis = 0),[3 , 3]) ",pretrained-cnn-epic-fight.ipynb
"output image randomly rotated, sheared, zoomed, and shifted", DIM = IMAGE_SIZE[0] ,pretrained-cnn-epic-fight.ipynb
fix for size 331, XDIM = DIM % 2 ,pretrained-cnn-epic-fight.ipynb
GET TRANSFORMATION MATRIX," m = get_mat(rot , shr , h_zoom , w_zoom , h_shift , w_shift) ",pretrained-cnn-epic-fight.ipynb
LIST DESTINATION PIXEL INDICES," x = tf.repeat(tf.range(DIM // 2 , - DIM // 2 , - 1), DIM) ",pretrained-cnn-epic-fight.ipynb
ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS," idx2 = K.dot(m , tf.cast(idx , dtype = 'float32')) ",pretrained-cnn-epic-fight.ipynb
FIND ORIGIN PIXEL VALUES," idx3 = tf.stack ([DIM // 2 - idx2[0 ,], DIM // 2 - 1 + idx2[1 ,]]) ",pretrained-cnn-epic-fight.ipynb
"Create Test, TRain and validation Data","train_dataset_all = load_dataset(list(pd.DataFrame({ 'TRAINING_FILENAMES' : TRAINING_FILENAMES })[ 'TRAINING_FILENAMES']) , labeled = True) ",pretrained-cnn-epic-fight.ipynb
Both validation and train concatenated for final fitting,train_all = get_training_dataset(train_dataset_all) ,pretrained-cnn-epic-fight.ipynb
 Transfer Learning ,"for name , model_ in models.items (): ",pretrained-cnn-epic-fight.ipynb
"interpretation of Inception modules in convolutional neural networks as being an intermediate step in between regular convolution and the depthwise separable convolution operation a depthwise convolution followed by a pointwise convolution . In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset which Inception V3 was designed for , and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.Xception: Deep Learning with Depthwise Separable Convolutions","def plot_perf(name) :
 
 print('{} Training took {}'.format(name, times[name]))
 
 history = historys[name]

 fig = plt.figure(figsize = [30,5])

 plt.subplot(1, 3, 1)

 img=mpimg.imread(name+'.png')
 plt.title(name + ' Architecture')
 plt.imshow(img)

 plt.subplot(1, 3, 2)

 plt.plot(history.history['sparse_categorical_accuracy'])
 plt.plot(history.history['val_sparse_categorical_accuracy'])
 plt.title(name + ' accuracy')
 plt.ylabel('accuracy')
 plt.xlabel('epoch')
 plt.legend(['train', 'test'], loc='upper left')

 plt.subplot(1, 3, 3)

 plt.plot(history.history['loss'])
 plt.plot(history.history['val_loss'])
 plt.title(name + ' loss')
 plt.ylabel('loss')
 plt.xlabel('epoch')
 plt.legend(['train', 'test'], loc='upper left')
 ",pretrained-cnn-epic-fight.ipynb
tf.data.Dataset.from tensor slices list val ,"def display_confusion_matrix(cmat , score , precision , recall , name): ",pretrained-cnn-epic-fight.ipynb
cm probabilities model1.predict images ds ,cm_predictions = predictions_val['Xception'] ,pretrained-cnn-epic-fight.ipynb
"VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper Very Deep Convolutional Networks for Large Scale Image Recognition . The model achieves 92.7 top 5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It was one of the famous model submitted to ILSVRC 2014. It makes the improvement over AlexNet by replacing large kernel sized filters 11 and 5 in the first and second convolutional layer, respectively with multiple 3 3 kernel sized filters one after another. VGG16 was trained for weeks and was using NVIDIA Titan Black GPU s.Very Deep Convolutional Networks for Large Scale Image Recognition",plot_perf('VGG16'),pretrained-cnn-epic-fight.ipynb
cm probabilities model1.predict images ds ,cm_predictions = predictions_val['VGG16'] ,pretrained-cnn-epic-fight.ipynb
"Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network DenseNet , which connects each layer to every other layer in a feed forward fashion. Whereas traditional convolutional networks with L layers have L connections one between each layer and its subsequent layer our network has L L 1 2 direct connections. For each layer, the feature maps of all preceding layers are used as inputs, and its own feature maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks CIFAR 10, CIFAR 100, SVHN, and ImageNet . DenseNets obtain significant improvements over the state of the art on most of them, whilst requiring less computation to achieve high performance. Code and pre trained models are available at this https URL .Densely Connected Convolutional Networks",plot_perf('DenseNet201'),pretrained-cnn-epic-fight.ipynb
cm probabilities model1.predict images ds ,cm_predictions = predictions_val['DenseNet201'] ,pretrained-cnn-epic-fight.ipynb
"Convolutional networks are at the core of most state of the art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks as long as enough labeled data is provided for training , computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2 top 1 and 5.6 top 5 error for single frame evaluation using a network with a computational cost of 5 billion multiply adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi crop evaluation, we report 3.5 top 5 error on the validation set 3.6 error on the test set and 17.3 top 1 error on the validation set.Rethinking the Inception Architecture for Computer Vision",plot_perf('InceptionV3'),pretrained-cnn-epic-fight.ipynb
cm probabilities model1.predict images ds ,cm_predictions = predictions_val['InceptionV3'] ,pretrained-cnn-epic-fight.ipynb
"Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state of the art performance in the 2015 ILSVRC challenge its performance was similar to the latest generation Inception v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non residual Inception networks. These variations improve the single frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception v4, we achieve 3.08 percent top 5 error on the test set of the ImageNet classification CLS challenge.Inception v4, Inception ResNet and the Impact of Residual Connections on Learning",plot_perf('InceptionResNetV2'),pretrained-cnn-epic-fight.ipynb
cm probabilities model1.predict images ds ,cm_predictions = predictions_val['InceptionResNetV2'] ,pretrained-cnn-epic-fight.ipynb
 Assembling and Submission ,"df = pd.DataFrame(predictions)
pred = []
for i in range(0, 7382) :
 if df.loc[i,:].unique().shape[0] < MODELS_NUMBER :
 pred.append(stats.mode(df.loc[i,:].values)[0][0])
 else :
 pred.append(df.loc[i,'InceptionResNetV2'])
 
df = pd.DataFrame(predictions_val)
pred_val = []
for i in range(0, 3712) :
 if df.loc[i,:].unique().shape[0] < MODELS_NUMBER :
 pred_val.append(stats.mode(df.loc[i,:].values)[0][0])
 else :
 pred_val.append(df.loc[i,'InceptionResNetV2'])

avg_prob = predictions_prob['Xception'] + predictions_prob['VGG16'] + predictions_prob['DenseNet201'] + predictions_prob['InceptionV3'] + predictions_prob['InceptionResNetV2']
pred_avg = pd.DataFrame(np.argmax(avg_prob, axis=-1))",pretrained-cnn-epic-fight.ipynb
all in one batch,test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))). numpy (). astype('U') ,pretrained-cnn-epic-fight.ipynb
"This table of loadings is telling us that in the Size component, Height and Diameter vary in the same direction same sign , but in the Shape component they vary in opposite directions opposite sign . In each component, the loadings are all of the same magnitude and so the features contribute equally in both.PCA also tells us the amount of variation in each component. We can see from the figures that there is more variation in the data along the Size component than along the Shape component. PCA makes this precise through each component s percent of explained variance. Size accounts for about 96 and the Shape for about 4 of the variance between Height and Diameter. The Size component captures the majority of the variation between Height and Diameter. It s important to remember, however, that the amount of variance in a component doesn t necessarily correspond to how good it is as a predictor: it depends on what you re trying to predict.PCA for Feature EngineeringThere are two ways you could use PCA for feature engineering.The first way is to use it as a descriptive technique. Since the components tell you about the variation, you could compute the MI scores for the components and see what kind of variation is most predictive of your target. That could give you ideas for kinds of features to create a product of Height and Diameter if Size is important, say, or a ratio of Height and Diameter if Shape is important. You could even try clustering on one or more of the high scoring components.The second way is to use the components themselves as features. Because the components expose the variational structure of the data directly, they can often be more informative than the original features. Here are some use cases: Dimensionality reduction: When your features are highly redundant multicollinear, specifically , PCA will partition out the redundancy into one or more near zero variance components, which you can then drop since they will contain little or no information. Anomaly detection: Unusual variation, not apparent from the original features, will often show up in the low variance components. These components could be highly informative in an anomaly or outlier detection task. Noise reduction: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the informative signal into a smaller number of features while leaving the noise alone, thus boosting the signal to noise ratio. Decorrelation: Some ML algorithms struggle with highly correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.PCA basically gives you direct access to the correlational structure of your data. You ll no doubt come up with applications of your own! PCA Best Practices There are a few things to keep in mind when applying PCA: PCA only works with numeric features, like continuous quantities or counts. PCA is sensitive to scale. It s good practice to standardize your data before applying PCA, unless you know you have good reason not to. Consider removing or constraining outliers, since they can have an undue influence on the results. Example 1985 AutomobilesIn this example, we ll return to our Automobile dataset and apply PCA, using it as a descriptive technique to discover features. We ll look at other use cases in the exercise.This hidden cell loads the data and defines the functions plot variance and make mi scores.",import matplotlib.pyplot as plt ,principal-component-analysis.ipynb
"We ve selected four features that cover a range of properties. Each of these features also has a high MI score with the target, price. We ll standardize the data since these features aren t naturally on the same scale.","features =[""highway_mpg"" , ""engine_size"" , ""horsepower"" , ""curb_weight""] ",principal-component-analysis.ipynb
Standardize,X_scaled =(X - X.mean(axis = 0)) / X.std(axis = 0) ,principal-component-analysis.ipynb
Now we can fit scikit learn s PCA estimator and create the principal components. You can see here the first few rows of the transformed dataset.,from sklearn.decomposition import PCA ,principal-component-analysis.ipynb
Create principal components,pca = PCA () ,principal-component-analysis.ipynb
Convert to dataframe,"component_names =[f""PC{i+1}"" for i in range(X_pca.shape[1])] ",principal-component-analysis.ipynb
Look at explained variance,plot_variance(pca); ,principal-component-analysis.ipynb
"Let s also look at the MI scores of the components. Not surprisingly, PC1 is highly informative, though the remaining components, despite their small variance, still have a significant relationship with price. Examining those components could be worthwhile to find relationships not captured by the main Luxury Economy axis.","mi_scores = make_mi_scores(X_pca, y, discrete_features=False)
mi_scores",principal-component-analysis.ipynb
Show dataframe sorted by PC3,"idx = X_pca[""PC3""]. sort_values(ascending = False). index ",principal-component-analysis.ipynb
"To express this contrast, let s create a new ratio feature:","df[""sports_or_wagon""] = X.curb_weight / X.horsepower
sns.regplot(x=""sports_or_wagon"", y='price', data=df, order=2);",principal-component-analysis.ipynb
Modules,"!pip install pycaret
from pycaret.classification import *",pycaret-visualization-optimization-0-81.ipynb
User Modules,"def multi_table(table_list):
 return HTML(
 f""<table><tr> {''.join(['<td>' + table._repr_html_() + '</td>' for table in table_list])} </tr></table>"")",pycaret-visualization-optimization-0-81.ipynb
Read Data,"train = pd.read_csv(""../input/spaceship-titanic/train.csv"")
test = pd.read_csv(""../input/spaceship-titanic/test.csv"")
submission = pd.read_csv(""../input/spaceship-titanic/sample_submission.csv"")

all_data = pd.concat([train, test], axis=0)",pycaret-visualization-optimization-0-81.ipynb
 Check Data ,all_data.head(10).style.background_gradient(),pycaret-visualization-optimization-0-81.ipynb
"Variable Definition PassengerId A unique Id for each passenger. Each Id takes the form gggg pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always. HomePlanet The planet the passenger departed from, typically their planet of permanent residence. CryoSleep Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.HomePlanet Cabin The cabin number where the passenger is staying. Takes the form deck num side, where side can be either P for Port or S for Starboard. Destination The planet the passenger will be debarking to. Age The age of the passenger. VIP Whether the passenger has paid for special VIP service during the voyage. RoomService, FoodCourt, ShoppingMall, Spa, VRDeck Amount the passenger has billed at each of the Spaceship Titanic s many luxury amenities. Name The first and last names of the passenger. Transported Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict. ",all_data.columns,pycaret-visualization-optimization-0-81.ipynb
"If you look at it so far, you can see exactly which variables are nominal and which are continuous. Additional basic statistics can be found for numeric variables. I checked the statistics of the training data, the test data, and the total data.","nominal_vars = ['HomePlanet', 'CryoSleep', 'Cabin', 'Desination', 'VIP', 'Name']
continuous_vars = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']
target = 'Transported'",pycaret-visualization-optimization-0-81.ipynb
 Missing Value Most variables had missing values. These are subject to preprocessing. ,"import missingno as msno
msno.matrix(all_data)",pycaret-visualization-optimization-0-81.ipynb
"Transported Dependent, Nominal The target value is a binary label consisting of True and False. Therefore, it is necessary to make sure that it is balanced. I checked the number and percentage of each category using countplot. When I checked, I found that True was 50.36 and False was 49.64 .","plt.subplots(figsize=(25, 10))
plt.pie(train.Transported.value_counts(), shadow=True, explode=[.03,.03], autopct='%1.1f%%', textprops={'fontsize': 20, 'color': 'white'})
plt.title('Transported Distribution', size=20)
plt.legend(['False', 'True'], loc='best', fontsize=12)
plt.show()",pycaret-visualization-optimization-0-81.ipynb
"HomePlanet Nominal These variables are nominal variables, so we decided to check the distribution with countplot. To see if this variable can explain the dependent variable well, we checked the distribution of the dependent variable again. I confirmed the following results.1 Among the Homeplanets, 54.19 of the earth s share is the highest. 2 The ratio of Europa to Mars is almost the same. 3 Among Europa, the percentage of Transported is certainly high. 4 The false percentage of Transported in Earth is certainly high. 5 There is not much difference in Transported among Mars. More than half of people belong to Earth. In addition, the difference between Earth and Europa s transported ratio is certain, so the sorting algorithm can work well. Therefore, you will be able to use the variable well to solve this problem.","cat_dist(train, var='HomePlanet', hue='Transported')
train.pivot_table(index=""HomePlanet"", values=""Transported"", aggfunc=['count', 'sum', 'mean']).style.background_gradient(vmin=0)",pycaret-visualization-optimization-0-81.ipynb
"CryoSleep Nominal CryoSleep is a nominal variable. Therefore, you can view the distribution in Countplot. It can be seen that CryoSleep has a high false percentage of 64.17 . You should try to see how the CryoSleep value affects Transported.If CryoSleep is False, the Transported false ratio is approximately 68 . When True, the Transported ratio is approximately 80 . Verified that the value of CryoSleep clearly distinguishes Transported. Therefore, just looking at a person s CryoSleep gives you a 50 chance of matching Transported to True or False. Classification algorithms will also take a big hint from looking at this variable.","cat_dist(train, var='CryoSleep', hue='Transported')
train.pivot_table(index=""CryoSleep"", values=""Transported"", aggfunc=['count', 'sum', 'mean']).style.background_gradient(vmin=0)",pycaret-visualization-optimization-0-81.ipynb
"Cabin Nominal There are so many categories of Cabin that I thought it would be meaningless to check them individually. I was able to find something in common with Cabin s name, but I thought the first word and the last word were Cabin s type. That s why the distribution was confirmed based on them.","tmp = train.copy()
tmp['Deck'] = train.Cabin.apply(lambda x:str(x)[:1])
tmp['side'] = train.Cabin.apply(lambda x:str(x)[-1:])

cat_dist(tmp, var='Deck', hue='Transported')",pycaret-visualization-optimization-0-81.ipynb
Destination Nominal ,"cat_dist(train, var='Destination', hue='Transported')",pycaret-visualization-optimization-0-81.ipynb
VIP Nominal ,"cat_dist(train, var='VIP', hue='Transported')",pycaret-visualization-optimization-0-81.ipynb
"Age continuous Age is a continuous value, so we checked the distribution in four flows. I understood the following.1 Transported is not significantly affected by age. 2 However, it has been confirmed that the proportion of transported in young sections is high, and that this is not the case in people in their 20s. 3 Therefore, I thought that categorizing data into categorical data by age would create more meaningful variables.","continuous_dist(train, 'Transported', 'Age')",pycaret-visualization-optimization-0-81.ipynb
Other Continuous Variables They have a one sided distribution. Most people didn t pay for this service. These can be extracted from variables such as total consumption.,"f, ax = plt.subplots(1, 5, figsize=(40, 7))
sns.distplot(train.RoomService, ax=ax[0])
sns.distplot(train.FoodCourt, ax=ax[1])
sns.distplot(train.ShoppingMall, ax=ax[2])
sns.distplot(train.Spa, ax=ax[3])
sns.distplot(train.VRDeck, ax=ax[4])
plt.show()",pycaret-visualization-optimization-0-81.ipynb
Heatmap can visualize continuous values or binary variables in categories and categories.,"plt.subplots(figsize =(10 , 5)) ",pycaret-visualization-optimization-0-81.ipynb
"conclusion: Passengers in suspended sleep are generally likely to be transmitted. Especially in Europa and Mars, most passengers during housekeeping sleep were forwarded. ","plt.subplots(figsize=(10, 5))
g = sns.heatmap(train.pivot_table(index='HomePlanet', columns='Destination', values='Transported'), annot=True, cmap=""YlGnBu"")
g.set_title('Transported ratio by HomePlanet and Destination', weight='bold', size=15)
g.set_xlabel('Destination', weight='bold', size=13)
g.set_ylabel('HomePlanet', weight='bold', size=13)
plt.show()

pd.crosstab([train.Destination, train.Transported], train.HomePlanet,margins=True).style.background_gradient()",pycaret-visualization-optimization-0-81.ipynb
"Replace categorical variables with specific values False, None or freeest values.","all_data['CryoSleep']. fillna(False , inplace = True) ",pycaret-visualization-optimization-0-81.ipynb
Replace continuous variables with specific values 0 or averages.,"all_data['Age']. fillna(all_data.Age.mean (), inplace = True) ",pycaret-visualization-optimization-0-81.ipynb
"As mentioned earlier, create a new variable by decomposing strings in Cabin and PassengerId.",all_data['Deck']= all_data.Cabin.apply(lambda x : str(x)[ : 1]) ,pycaret-visualization-optimization-0-81.ipynb
Generate new variables based on the amount of money used for various services.,all_data['TotalSpend']= all_data['RoomService']+ all_data['FoodCourt']+ all_data['ShoppingMall']+ all_data['Spa']+ all_data['VRDeck'] ,pycaret-visualization-optimization-0-81.ipynb
Create new variables by dividing age groups.,all_data['AgeBin']= 7 ,pycaret-visualization-optimization-0-81.ipynb
Replaces the missing value that occurred when generating the derived variable.,"fill_cols =['PctRoomService' , 'PctFoodCourt' , 'PctShoppingMall' , 'PctSpa' , 'PctVRDeck'] ",pycaret-visualization-optimization-0-81.ipynb
Remove unnecessary variables.,"all_data.drop (['PassengerId' , 'Name' , 'Cabin'], axis = 1 , inplace = True) ",pycaret-visualization-optimization-0-81.ipynb
"Encoding Typically, categorical variable encoding is divided into one hot encoding and label encoding. The anchor is one hot encoding for nominal variables and label encoding for ordered variables. However, it uses a tree based boost algorithm to perform label encoding simply. ","for col in all_data.columns[all_data.dtypes == object]:
 if col != 'Transported':
 le = LabelEncoder()
 all_data[col] = le.fit_transform(all_data[col])
 
all_data['CryoSleep'] = all_data['CryoSleep'].astype('int')
all_data['VIP'] = all_data['VIP'].astype('int')",pycaret-visualization-optimization-0-81.ipynb
Split Train Test,"train, X_test = all_data.iloc[:train.shape[0]], all_data.iloc[train.shape[0]:].drop(['Transported'], axis=1)
X_train, y_train = train.drop(['Transported'], axis=1), train['Transported']",pycaret-visualization-optimization-0-81.ipynb
"Modeling and Optimizing Previously, data was verified and preprocessed. Finally, I made the final data. I used pycaret to proceed with the test method and optimization of models suitable for this data. The first thing you need to do is set up your environment. You can use the setup function to proceed with the configuration of your environment. The most important thing is to pass the data you create and the target variable. Other options are optional.","s = setup(data=train,
 session_id=7010,
 target='Transported',
 train_size=0.99,
 fold_strategy='stratifiedkfold',
 fold=5,
 fold_shuffle=True,
 silent=True,
 ignore_low_variance=True,
 remove_multicollinearity = True,
 normalize = True,
 normalize_method = 'robust',)",pycaret-visualization-optimization-0-81.ipynb
Compare model allows you to compare the results of learning given data by model. I checked the top four models.,top4 = compare_models(n_select=4),pycaret-visualization-optimization-0-81.ipynb
"The best model is Catboost. I have been optimizing Catboost models in various ways, and I have also evaluated the performance of models with the top four models as ensemble.","!pip install scikit-optimize
!pip install tune-sklearn ray[tune]
import optuna
!pip install hpbandster ConfigSpace",pycaret-visualization-optimization-0-81.ipynb
"The following model is the best model I obtained while tuning myself using the annotation code below. I hope you can refer to it. If you derive results after learning in this model, you can get a score of about 0.8097.","catboost_best = create_model('catboost', nan_mode= 'Min',
 eval_metric='Logloss',
 iterations=1000,
 sampling_frequency='PerTree',
 leaf_estimation_method='Newton',
 grow_policy='SymmetricTree',
 penalties_coefficient=1,
 boosting_type='Plain',
 model_shrink_mode='Constant',
 feature_border_type='GreedyLogSum', 
 l2_leaf_reg=3,
 random_strength=1, 
 rsm=1, 
 boost_from_average=False,
 model_size_reg=0.5, 
 subsample=0.800000011920929, 
 use_best_model=False, 
 class_names=[0, 1],
 depth=6, 
 posterior_sampling=False, 
 border_count=254, 
 classes_count=0, 
 auto_class_weights='None',
 sparse_features_conflict_fraction=0, 
 leaf_estimation_backtracking='AnyImprovement',
 best_model_min_trees=1, 
 model_shrink_rate=0, 
 min_data_in_leaf=1, 
 loss_function='Logloss',
 learning_rate=0.02582800015807152,
 score_function='Cosine',
 task_type='CPU',
 leaf_estimation_iterations=10, 
 bootstrap_type='MVS',
 max_leaves=64)",pycaret-visualization-optimization-0-81.ipynb
blender top4 blend models estimator list tuned top4 ,"df_pred = predict_model(catboost_best, X_test)
y_pred = df_pred.loc[:, ['Label']]",pycaret-visualization-optimization-0-81.ipynb
Interpreting Model Pycaret provides SHAP. This explains why the model derived the results in this way. The following figure visualizes the shape value of each variable based on the size of the value.,interpret_model(catboost_best),pycaret-visualization-optimization-0-81.ipynb
PyTorch XLAThe PyTorch TPU project was born out of a collaborative effort between the Facebook PyTorch and Google TPU teams and was officially launched at the 2019 PyTorch Developer Conference. This new integration enables PyTorch users to run and scale up their models on Cloud TPUs. PyTorch XLA package lets PyTorch connect to Cloud TPUs and use TPU cores as devices,"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev
!pip install timm",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
PyTorch XLA adds a new xla device type to PyTorch. This device type works just like other PyTorch device types.,"import os
import pandas as pd
from scipy import stats
import numpy as np
import glob
import tensorflow as tf
import timm
import random
import time
import copy
from operator import itemgetter

from collections import OrderedDict, namedtuple
import joblib

import logging
import sys

from PIL import Image
import cv2
import albumentations
import io
import IPython.display as display

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import lr_scheduler
import torch.optim as optim
import torch_xla
import torch_xla.core.xla_model as xm
import torch_xla.debug.metrics as met
import torch_xla.distributed.parallel_loader as pl
import torch_xla.distributed.xla_multiprocessing as xmp
import torch_xla.utils.utils as xu
import torchvision
from torchvision import datasets, transforms
from torch.utils.data import Dataset, DataLoader, ConcatDataset
import torchvision.transforms as transforms
from torchvision.utils import make_grid

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn import metrics, model_selection

import warnings
warnings.filterwarnings(""ignore"");",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
Paths,"train_files = glob.glob('../input/tpu-getting-started/*/train/*.tfrec')
val_files = glob.glob('../input/tpu-getting-started/*/val/*.tfrec')
test_files = glob.glob('../input/tpu-getting-started/*/test/*.tfrec')",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
Here we read tfrecords files in PyTorch. I recommend ,"test_feature_description = {
 'id': tf.io.FixedLenFeature([], tf.string),
 'image': tf.io.FixedLenFeature([], tf.string),
}

def _parse_image_function_test(example_proto):
 return tf.io.parse_single_example(example_proto, test_feature_description)

test_ids = []
test_images = []
for i in test_files:
 test_image_dataset = tf.data.TFRecordDataset(i)
 
 test_image_dataset = test_image_dataset.map(_parse_image_function_test)

 ids = [str(id_features['id'].numpy())[2:-1] for id_features in test_image_dataset]
 test_ids = test_ids + ids

 images = [image_features['image'].numpy() for image_features in test_image_dataset]
 test_images = test_images + images",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
"Let s take a quick look at the data. I don t know about you, but the first thing I always want to do is look at what our data looks like : ","import IPython.display as display

display.display(display.Image(data=val_images[1]))",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
Let s write our dataset,"class MyDataset():
 def __init__(self, ids, cls, imgs, transforms, is_test=False):
 self.ids = ids
 if not is_test:
 self.cls = cls
 self.imgs = imgs
 self.transforms = transforms
 self.is_test = is_test
 
 def __len__(self):
 return len(self.ids)

 def __getitem__(self, idx):
 img = self.imgs[idx]
 img = Image.open(io.BytesIO(img))
 img = self.transforms(img)
 if self.is_test:
 return img, -1, self.ids[idx]
 return img, int(self.cls[idx]), self.ids[idx]",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
Let s write augmentation and normalization right away,"train_transforms = transforms.Compose([
 transforms.RandomResizedCrop(224),
 transforms.RandomHorizontalFlip(),
 transforms.RandomVerticalFlip(),
 transforms.ToTensor(),
 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
 transforms.RandomErasing()
 ])

test_transforms = transforms.Compose([
 transforms.CenterCrop(224),
 transforms.Resize(224),
 transforms.ToTensor(),
 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
 ])",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
"We have already normalized the data, but at this stage I would like to dwell in more detail, because this is very important.In datasets, we have three channel images, that is, we need to normalize for each channel separately !!! . Because of the unnormalized data, problems may appear, for example, regularization during training can work to the detriment, but we do not want this at all. The task of normalization is to make the mean as close to zero as possible, and the standard deviation around 1.How each channel looks separately can be seen below:","transforms_example = transforms.Compose([
 transforms.CenterCrop(224),
 transforms.ToTensor(),])

exampleset = MyDataset(train_ids, train_class, train_images, transforms_example)

x, y, _ = next(iter(DataLoader(exampleset)))

channels = ['Red', 'Green', 'Blue']
cmaps = [plt.cm.Reds_r, plt.cm.Greens_r, plt.cm.Blues_r]

fig, ax = plt.subplots(1, 4, figsize=(15, 10))

for i, axs in enumerate(fig.axes[:3]):
 axs.imshow(x[0][i,:,:], cmap=cmaps[i])
 axs.set_title(f'{channels[i]} Channel')
 axs.set_xticks([])
 axs.set_yticks([])
 
ax[3].imshow(x[0].permute(1,2,0))
ax[3].set_title('Three Channels')
ax[3].set_xticks([])
ax[3].set_yticks([]);",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
"Now let s check how well we managed to normalize the data for each channel for the test, training and validation datasets:",channels = 3 ,pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
Let s take a look at the pixel distribution after normalization,"x, y, _ = next(iter(exampleset))

def plotHist(img):
 plt.figure(figsize=(10,5))
 plt.subplot(1,2,1)
 plt.imshow(x.permute(1,2,0))
 plt.axis('off')
 histo = plt.subplot(1,2,2)
 histo.set_ylabel('Count')
 histo.set_xlabel('Pixel Intensity')
 plt.hist(img.numpy().flatten(), bins=10, lw=0, alpha=0.5, color='r')

plotHist(x)",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
Let s take a batch from the training dataset and see its mean and standard deviation:,"x.mean(), x.std()",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
".permute 1, 2, 0 "," ax.imshow(norm_out(make_grid(images[: 60], nrow = 10))) ",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
Training for one epoch,"def train(loader , epoch , model , optimizer , criterion): ",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
tracker xm.RateTracker , model.train () ,pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
Append outputs," _ , pred = output.max(dim = 1) ",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
compute gradient and do SGD step, optimizer.zero_grad () ,pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
optimizer.step , xm.optimizer_step(optimizer) ,pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
Val for one epoch,"def test(loader, model, criterion):
 with torch.no_grad():
 model.eval()
 running_loss = 0.
 running_corrects = 0.
 tot = 0
 for i, (ip, tgt, _) in enumerate(loader):
 ip, tgt = ip.to(device), tgt.to(device)
 output = model(ip)
 loss = criterion(output, tgt)
 tot += ip.shape[0]
 _, pred = output.max(dim=1)
 running_corrects += torch.sum(pred == tgt.data)
 running_loss += loss.item()*ip.size(0)

 return running_corrects, running_loss",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
Function for predictions on a test set,"def predict(model, loader, device):
 with torch.no_grad():
 torch.cuda.empty_cache()
 model.eval()
 preds = dict()
 for i, (ip, _, ids) in enumerate(loader):
 ip = ip.to(device)
 output = model(ip)
 _, pred = output.max(dim=1)
 for i, j in zip(ids, pred.cpu().detach()):
 preds[i] = j
 
 return preds",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
"This is where we will record the history of learning, so that we can make visualization later. We need visualization to evaluate learning, for example, overfitting or underfitting. Of course, we can analyze with numbers, but it is much easier to perceive information visually","losses = {'train':[], 'val':[]}
accuracies = {'train':[], 'val':[]}",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
In a typical XLA:TPU training scenario we re training on multiple TPU cores in parallel a single Cloud TPU device includes 8 TPU cores . So we need to ensure that all the gradients are exchanged between the data parallel replicas by consolidating the gradients and taking an optimizer step. For this we provide the xm.optimizer step optimizer which does the gradient consolidation and step taking,"def fit(seed , epochs , model): ",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
Train and valid dataloaders, xm.master_print('Creating a model {}...'.format(seed)) ,pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
"scheduler torch.optim.lr scheduler.ReduceLROnPlateau optimizer, mode max , factor 0.7, patience 3, verbose True "," scheduler = torch.optim.lr_scheduler.StepLR(optimizer , 4 , gamma = 0.1) ",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
train," xm.master_print('Epoch: {}/{}'.format(epoch + 1 , epochs)) ",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
val," para_loader = pl.ParallelLoader(val_loader ,[device]) ",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
epoch end," xm.master_print('Time: {}m {}s'.format(( time.time ()- since)// 60 ,(time.time ()- since)% 60)) ",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
"scheduler torch.optim.lr scheduler.ReduceLROnPlateau optimizer, mode max , factor 0.7, patience 3, verbose True "," scheduler = torch.optim.lr_scheduler.StepLR(optimizer , 4 , gamma = 0.1) ",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
train," xm.master_print('Epoch: {}/{}'.format(epoch + 1 , epochs)) ",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
val," para_loader = pl.ParallelLoader(val_loader ,[device]) ",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
epoch end," xm.master_print('Time: {}m {}s'.format(( time.time ()- since)// 60 ,(time.time ()- since)% 60)) ",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
 DenseNet ,"densenet121 = torchvision.models.densenet121(pretrained=True)
for param in densenet121.parameters():
 param.requires_grad=False

densenet121.classifier = nn.Linear(in_features=densenet121.classifier.in_features, out_features=104, bias=True)",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
 ViT ,"ViT = timm.create_model(""vit_base_patch16_224"", pretrained=True)
for param in ViT.parameters():
 param.requires_grad=False

ViT.head = nn.Linear(ViT.head.in_features, 104)",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
 GoogLeNet ,"googlenet = torchvision.models.googlenet(pretrained=True)
for param in googlenet.parameters():
 param.grad_requires = False

googlenet.fc = nn.Linear(in_features=googlenet.fc.in_features, out_features=104, bias=True)",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
 ResNet ,"resnet101 = torchvision.models.resnet101(pretrained=True)
for param in resnet101.parameters():
 param.grad_requires = False

resnet101.fc = nn.Linear(in_features=resnet101.fc.in_features, out_features=104, bias=True)",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
 VGG19 ,"vgg19_bn = torchvision.models.vgg19_bn(pretrained=True)
for param in vgg19_bn.parameters():
 param.grad_requires = False

vgg19_bn.classifier[6] = nn.Linear(4096, 104, bias=True)",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
Launching training,"test_transforms = transforms.Compose([
 transforms.CenterCrop(224),
 transforms.Resize(224),
 transforms.ToTensor(),
 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
 ])

test_ds = MyDataset(test_ids, [], test_images, test_transforms, True)
testloader = DataLoader(test_ds, 128, num_workers=4, pin_memory=True, shuffle=False)

submit_df = pd.read_csv('../input/tpu-getting-started/sample_submission.csv')
ensemble_df = submit_df.copy()

num_models = 5
num_epochs = 10

models = [densenet121, ViT, googlenet, resnet101, vgg19_bn]

for seed in range(num_models):
 preds = fit(seed=seed, epochs=num_epochs, model=models[seed])",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
 Submit Preparing ,ensemble_df.head(10),pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
Final prediction,"final_pred = ensemble_df.iloc[: , 2 :]. mode(axis = 1). iloc[: , 0] ",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
Create a submission file,"submit_df.to_csv('submission11062021.csv' , index = False) ",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
"As you can see, the idea of defrosting feature extractor worked and we see a sharp increase in accuracy","fig, ax = plt.subplots(5, 2, figsize=(15, 15))
modelname = ['DenseNet', 'ViT', 'GoogLeNet', 'ResNet101', 'VGG16 with BN']

epochs=10

i=0

for row in range(5):

 epoch_list = list(range(1,epochs*2+1))

 ax[row][0].plot(epoch_list, accuracies['train'][i:20+i], '-o', label='Train Accuracy')
 ax[row][0].plot(epoch_list, accuracies['val'][i:20+i], '-o', label='Validation Accuracy')
 ax[row][0].plot([epochs for x in range(20)], np.linspace(min(accuracies['train'][i:20+i]).cpu(), max(accuracies['train'][i:20+i]).cpu(), 20), color='r', label='Unfreeze net')
 ax[row][0].set_xticks(np.arange(0, epochs*2+1, 5))
 ax[row][0].set_ylabel('Accuracy Value')
 ax[row][0].set_xlabel('Epoch')
 ax[row][0].set_title('Accuracy {}'.format(modelname[row]))
 ax[row][0].legend(loc=""best"")

 ax[row][1].plot(epoch_list, losses['train'][i:20+i], '-o', label='Train Loss')
 ax[row][1].plot(epoch_list, losses['val'][i:20+i], '-o',label='Validation Loss')
 ax[row][1].plot([epochs for x in range(20)], np.linspace(min(losses['train'][i:20+i]), max(losses['train'][i:20+i]), 20), color='r', label='Unfreeze net')
 ax[row][1].set_xticks(np.arange(0, epochs*2+1, 5))
 ax[row][1].set_ylabel('Loss Value')
 ax[row][1].set_xlabel('Epoch')
 ax[row][1].set_title('Loss {}'.format(modelname[row]))
 ax[row][1].legend(loc=""best"")
 fig.tight_layout()
 fig.subplots_adjust(top=1.5, wspace=0.3)

 i+=20",pytorch-tpu-baseline-flowers-tranlearning-ensemble.ipynb
linear algebra,import numpy as np ,pytorch-tutorial-for-deep-learning-lovers.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,pytorch-tutorial-for-deep-learning-lovers.ipynb
"For example, running this by clicking run or pressing Shift Enter will list the files in the input directory",import os ,pytorch-tutorial-for-deep-learning-lovers.ipynb
import numpy library,import numpy as np ,pytorch-tutorial-for-deep-learning-lovers.ipynb
numpy array,"array =[[ 1 , 2 , 3],[4 , 5 , 6]] ",pytorch-tutorial-for-deep-learning-lovers.ipynb
2x3 array,first_array = np.array(array) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
type,"print(""Array Type: {}"".format(type(first_array))) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
shape,"print(""Array Shape: {}"".format(np.shape(first_array))) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
import pytorch library,import torch ,pytorch-tutorial-for-deep-learning-lovers.ipynb
pytorch array,tensor = torch.Tensor(array) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
type,"print(""Array Type: {}"".format(tensor.type)) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
shape,"print(""Array Shape: {}"".format(tensor.shape)) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
numpy ones,"print(""Numpy {}\n"".format(np.ones(( 2 , 3)))) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
pytorch ones,"print(torch.ones(( 2 , 3))) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
numpy random,"print(""Numpy {}\n"".format(np.random.rand(2 , 3))) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
pytorch random,"print(torch.rand(2 , 3)) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
random numpy array,"array = np.random.rand(2 , 2) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
from numpy to tensor,from_numpy_to_tensor = torch.from_numpy(array) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
from tensor to numpy,tensor = from_numpy_to_tensor ,pytorch-tutorial-for-deep-learning-lovers.ipynb
create tensor,"tensor = torch.ones(3 , 3) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Resize,"print(""{}{}\n"".format(tensor.view(9). shape , tensor.view(9))) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Addition,"print(""Addition: {}\n"".format(torch.add(tensor , tensor))) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Subtraction,"print(""Subtraction: {}\n"".format(tensor.sub(tensor))) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Element wise multiplication,"print(""Element wise multiplication: {}\n"".format(torch.mul(tensor , tensor))) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Element wise division,"print(""Element wise division: {}\n"".format(torch.div(tensor , tensor))) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Mean,"tensor = torch.Tensor ([1 , 2 , 3 , 4 , 5]) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Standart deviation std ,"print(""std: {}"".format(tensor.std ())) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
import variable from pytorch library,from torch.autograd import Variable ,pytorch-tutorial-for-deep-learning-lovers.ipynb
define variable,"var = Variable(torch.ones(3), requires_grad = True) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
we have an equation that is y x 2,"array =[2 , 4] ",pytorch-tutorial-for-deep-learning-lovers.ipynb
recap o equation o 1 2 sum y ,o =(1 / 2)* sum(y) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
calculates gradients,o.backward () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Lets look at gradients with x.grad,"print(""gradients: "" , x.grad) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
lets define car prices,"car_prices_array =[3 , 4 , 5 , 6 , 7 , 8 , 9] ",pytorch-tutorial-for-deep-learning-lovers.ipynb
lets define number of car sell,"number_of_car_sell_array =[7.5 , 7 , 6.5 , 6.0 , 5.5 , 5.0 , 4.5] ",pytorch-tutorial-for-deep-learning-lovers.ipynb
lets visualize our data,import matplotlib.pyplot as plt ,pytorch-tutorial-for-deep-learning-lovers.ipynb
libraries,import torch ,pytorch-tutorial-for-deep-learning-lovers.ipynb
create class,class LinearRegression(nn.Module): ,pytorch-tutorial-for-deep-learning-lovers.ipynb
super function. It inherits from nn.Module and we can access everythink in nn.Module," super(LinearRegression , self). __init__ () ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Linear function.," self.linear = nn.Linear(input_dim , output_dim) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
define model,input_dim = 1 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
input and output size are 1,"model = LinearRegression(input_dim , output_dim) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
MSE,mse = nn.MSELoss () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
how fast we reach best parameters,learning_rate = 0.02 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
train model,loss_list = [] ,pytorch-tutorial-for-deep-learning-lovers.ipynb
optimization, optimizer.zero_grad () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Forward to get output, results = model(car_price_tensor) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Calculate Loss," loss = mse(results , number_of_car_sell_tensor) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
backward propagation, loss.backward () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Updating parameters, optimizer.step () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
store loss, loss_list.append(loss.data) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
print loss, if(iteration % 50 == 0): ,pytorch-tutorial-for-deep-learning-lovers.ipynb
predict our car price,predicted = model(car_price_tensor). data.numpy () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
"plt.scatter 10,predicted 10.data,label car price 10 ,color green ",plt.legend () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Import Libraries,import torch ,pytorch-tutorial-for-deep-learning-lovers.ipynb
load data,"train = pd.read_csv(r""../input/train.csv"" , dtype = np.float32) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
split data into features pixels and labels numbers from 0 to 9 ,targets_numpy = train.label.values ,pytorch-tutorial-for-deep-learning-lovers.ipynb
normalization,"features_numpy = train.loc[: , train.columns != ""label""]. values / 255 ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Create Logistic Regression Model,class LogisticRegressionModel(nn.Module): ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Linear part," self.linear = nn.Linear(input_dim , output_dim) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
"So actually we do not forget to put it, it is only at next parts"," def forward(self , x): ",pytorch-tutorial-for-deep-learning-lovers.ipynb
size of image px px,input_dim = 28 * 28 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
"labels 0,1,2,3,4,5,6,7,8,9",output_dim = 10 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
create logistic regression model,"model = LogisticRegressionModel(input_dim , output_dim) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Cross Entropy Loss,error = nn.CrossEntropyLoss () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
SGD Optimizer,learning_rate = 0.001 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Traning the Model,count = 0 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Define variables," train = Variable(images.view(- 1 , 28 * 28)) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Clear gradients, optimizer.zero_grad () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Forward propagation, outputs = model(train) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Calculate softmax and cross entropy loss," loss = error(outputs , labels) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Calculate gradients, loss.backward () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Update parameters, optimizer.step () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Prediction, if count % 50 == 0 : ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Calculate Accuracy, correct = 0 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Predict test dataset," for images , labels in test_loader : ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Forward propagation, outputs = model(test) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Get predictions from the maximum value," predicted = torch.max(outputs.data , 1)[ 1] ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Total number of labels, total += len(labels) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Total correct predictions, correct +=(predicted == labels). sum () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
store loss and iteration, loss_list.append(loss.data) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Print Loss," print('Iteration: {} Loss: {} Accuracy: {}%'.format(count , loss.data , accuracy)) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
visualization,"plt.plot(iteration_list , loss_list) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Import Libraries,import torch ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Create ANN Model,class ANNModel(nn.Module): ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Linear function 1: 784 150," self.fc1 = nn.Linear(input_dim , hidden_dim) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Non linearity 1, self.relu1 = nn.ReLU () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Linear function 2: 150 150," self.fc2 = nn.Linear(hidden_dim , hidden_dim) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Non linearity 2, self.tanh2 = nn.Tanh () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Linear function 3: 150 150," self.fc3 = nn.Linear(hidden_dim , hidden_dim) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Non linearity 3, self.elu3 = nn.ELU () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Linear function 4 readout : 150 10," self.fc4 = nn.Linear(hidden_dim , output_dim) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Linear function 1, out = self.fc1(x) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Non linearity 1, out = self.relu1(out) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Linear function 2, out = self.fc2(out) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Non linearity 2, out = self.tanh2(out) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Linear function 2, out = self.fc3(out) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Non linearity 2, out = self.elu3(out) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Linear function 4 readout , out = self.fc4(out) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
instantiate ANN,input_dim = 28 * 28 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
hidden layer dim is one of the hyper parameter and it should be chosen and tuned. For now I only say 150 there is no reason.,hidden_dim = 150 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Create ANN,"model = ANNModel(input_dim , hidden_dim , output_dim) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Cross Entropy Loss,error = nn.CrossEntropyLoss () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
SGD Optimizer,learning_rate = 0.02 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
ANN model training,count = 0 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Clear gradients, optimizer.zero_grad () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Forward propagation, outputs = model(train) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Calculate softmax and ross entropy loss," loss = error(outputs , labels) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Calculating gradients, loss.backward () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Update parameters, optimizer.step () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Calculate Accuracy, correct = 0 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Predict test dataset," for images , labels in test_loader : ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Forward propagation, outputs = model(test) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Get predictions from the maximum value," predicted = torch.max(outputs.data , 1)[ 1] ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Total number of labels, total += len(labels) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Total correct predictions, correct +=(predicted == labels). sum () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
store loss and iteration, loss_list.append(loss.data) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Print Loss," print('Iteration: {} Loss: {} Accuracy: {} %'.format(count , loss.data , accuracy)) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
visualization loss,"plt.plot(iteration_list , loss_list) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
visualization accuracy,"plt.plot(iteration_list , accuracy_list , color = ""red"") ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Import Libraries,import torch ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Create CNN Model,class CNNModel(nn.Module): ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Convolution 1," self.cnn1 = nn.Conv2d(in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 0) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Max pool 1, self.maxpool1 = nn.MaxPool2d(kernel_size = 2) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Convolution 2," self.cnn2 = nn.Conv2d(in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 0) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Max pool 2, self.maxpool2 = nn.MaxPool2d(kernel_size = 2) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Fully connected 1," self.fc1 = nn.Linear(32 * 4 * 4 , 10) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Convolution 1, out = self.cnn1(x) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Max pool 1, out = self.maxpool1(out) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Convolution 2, out = self.cnn2(out) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Max pool 2, out = self.maxpool2(out) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
flatten," out = out.view(out.size(0), - 1) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Linear function readout , out = self.fc1(out) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
"batch size, epoch and iteration",batch_size = 100 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Pytorch train and test sets,"train = torch.utils.data.TensorDataset(featuresTrain , targetsTrain) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
data loader,"train_loader = torch.utils.data.DataLoader(train , batch_size = batch_size , shuffle = False) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Create CNN,model = CNNModel () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Cross Entropy Loss,error = nn.CrossEntropyLoss () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
SGD Optimizer,learning_rate = 0.1 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
CNN model training,count = 0 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Clear gradients, optimizer.zero_grad () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Forward propagation, outputs = model(train) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Calculate softmax and ross entropy loss," loss = error(outputs , labels) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Calculating gradients, loss.backward () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Update parameters, optimizer.step () ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Calculate Accuracy, correct = 0 ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Iterate through test dataset," for images , labels in test_loader : ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Forward propagation, outputs = model(test) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Get predictions from the maximum value," predicted = torch.max(outputs.data , 1)[ 1] ",pytorch-tutorial-for-deep-learning-lovers.ipynb
Total number of labels, total += len(labels) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
store loss and iteration, loss_list.append(loss.data) ,pytorch-tutorial-for-deep-learning-lovers.ipynb
Print Loss," print('Iteration: {} Loss: {} Accuracy: {} %'.format(count , loss.data , accuracy)) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
visualization loss,"plt.plot(iteration_list , loss_list) ",pytorch-tutorial-for-deep-learning-lovers.ipynb
visualization accuracy,"plt.plot(iteration_list , accuracy_list , color = ""red"") ",pytorch-tutorial-for-deep-learning-lovers.ipynb
"IntroductionDecision trees leave you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.Even today s most sophisticated modeling techniques face this tension between underfitting and overfitting. But, many models have clever ideas that can lead to better performance. We ll look at the random forest as an example.The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters. ExampleYou ve already seen the code to load the data a few times. At the end of data loading, we have the following variables: train X val X train y val y",import pandas as pd ,random-forests.ipynb
Load data,melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv' ,random-forests.ipynb
Filter rows with missing values,melbourne_data = melbourne_data.dropna(axis = 0) ,random-forests.ipynb
Choose target and features,y = melbourne_data.Price ,random-forests.ipynb
We build a random forest model similarly to how we built a decision tree in scikit learn this time using the RandomForestRegressor class instead of DecisionTreeRegressor.,"from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))",random-forests.ipynb
linear algebra,import numpy as np ,real-time-cnn-architecture.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,real-time-cnn-architecture.ipynb
"For example, running this by clicking run or pressing Shift Enter will list the files in the input directory",import os ,real-time-cnn-architecture.ipynb
Any results you write to the current directory are saved as output.,"
train_data.dropna(inplace=True)
train_data.info()",real-time-cnn-architecture.ipynb
for train set,"X_train_d = np.zeros ([len(X_train), 96 , 96]) ",real-time-cnn-architecture.ipynb
for test set,"X_test_d = np.zeros ([len(X_test), 96 , 96]) ",real-time-cnn-architecture.ipynb
For Output,"Y_train = train_data.iloc[: , : - 1] ",real-time-cnn-architecture.ipynb
test train split,from sklearn.cross_validation import train_test_split ,real-time-cnn-architecture.ipynb
plotting the images,arr = [] ,real-time-cnn-architecture.ipynb
plotting the images with transformation,"fig , axis = plt.subplots(nrows = 5 , ncols = 5 , figsize =[10 , 10]) ",real-time-cnn-architecture.ipynb
"print Y train.values np.nonzero Y train.values :,0 float 68 ,0 ","print(np.nonzero(b_train.values[: , 0]> 71)[ 0]. shape) ",real-time-cnn-architecture.ipynb
plotting image with T points,arr = [] ,real-time-cnn-architecture.ipynb
augmented data creation,A_train = [] ,real-time-cnn-architecture.ipynb
model,"from tensorflow.keras.layers import Activation , Convolution2D , Dropout , Conv2D , Dense ",real-time-cnn-architecture.ipynb
base, img_input = Input(input_shape) ,real-time-cnn-architecture.ipynb
training the model,batch_size = 32 ,real-time-cnn-architecture.ipynb
saving the predictions,import pickle ,real-time-cnn-architecture.ipynb
plotting the test images with predicted points,"fig , axis = plt.subplots(nrows = 15 , ncols = 5 , figsize =[50 , 50]) ",real-time-cnn-architecture.ipynb
creating the submission file,"idlook = pd.read_csv(""../data/competitions/facial-keypoints-detection/IdLookupTable.csv"") ",real-time-cnn-architecture.ipynb
linear algebra,import numpy as np ,recurrent-neural-network-with-pytorch.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,recurrent-neural-network-with-pytorch.ipynb
"For example, running this by clicking run or pressing Shift Enter will list the files in the input directory",import os ,recurrent-neural-network-with-pytorch.ipynb
Import Libraries,import torch ,recurrent-neural-network-with-pytorch.ipynb
load data,"train = pd.read_csv(r""../input/train.csv"" , dtype = np.float32) ",recurrent-neural-network-with-pytorch.ipynb
split data into features pixels and labels numbers from 0 to 9 ,targets_numpy = train.label.values ,recurrent-neural-network-with-pytorch.ipynb
normalization,"features_numpy = train.loc[: , train.columns != ""label""]. values / 255 ",recurrent-neural-network-with-pytorch.ipynb
Create RNN Model,class RNNModel(nn.Module): ,recurrent-neural-network-with-pytorch.ipynb
Number of hidden dimensions, self.hidden_dim = hidden_dim ,recurrent-neural-network-with-pytorch.ipynb
Number of hidden layers, self.layer_dim = layer_dim ,recurrent-neural-network-with-pytorch.ipynb
RNN," self.rnn = nn.RNN(input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'relu') ",recurrent-neural-network-with-pytorch.ipynb
Readout layer," self.fc = nn.Linear(hidden_dim , output_dim) ",recurrent-neural-network-with-pytorch.ipynb
Initialize hidden state with zeros," h0 = Variable(torch.zeros(self.layer_dim , x.size(0), self.hidden_dim)) ",recurrent-neural-network-with-pytorch.ipynb
One time step," out , hn = self.rnn(x , h0) ",recurrent-neural-network-with-pytorch.ipynb
"batch size, epoch and iteration",batch_size = 100 ,recurrent-neural-network-with-pytorch.ipynb
Pytorch train and test sets,"train = TensorDataset(featuresTrain , targetsTrain) ",recurrent-neural-network-with-pytorch.ipynb
data loader,"train_loader = DataLoader(train , batch_size = batch_size , shuffle = False) ",recurrent-neural-network-with-pytorch.ipynb
input dimension,input_dim = 28 ,recurrent-neural-network-with-pytorch.ipynb
hidden layer dimension,hidden_dim = 100 ,recurrent-neural-network-with-pytorch.ipynb
number of hidden layers,layer_dim = 1 ,recurrent-neural-network-with-pytorch.ipynb
output dimension,output_dim = 10 ,recurrent-neural-network-with-pytorch.ipynb
Cross Entropy Loss,error = nn.CrossEntropyLoss () ,recurrent-neural-network-with-pytorch.ipynb
SGD Optimizer,learning_rate = 0.05 ,recurrent-neural-network-with-pytorch.ipynb
Clear gradients, optimizer.zero_grad () ,recurrent-neural-network-with-pytorch.ipynb
Forward propagation, outputs = model(train) ,recurrent-neural-network-with-pytorch.ipynb
Calculate softmax and ross entropy loss," loss = error(outputs , labels) ",recurrent-neural-network-with-pytorch.ipynb
Calculating gradients, loss.backward () ,recurrent-neural-network-with-pytorch.ipynb
Update parameters, optimizer.step () ,recurrent-neural-network-with-pytorch.ipynb
Calculate Accuracy, correct = 0 ,recurrent-neural-network-with-pytorch.ipynb
Iterate through test dataset," for images , labels in test_loader : ",recurrent-neural-network-with-pytorch.ipynb
Forward propagation, outputs = model(images) ,recurrent-neural-network-with-pytorch.ipynb
Get predictions from the maximum value," predicted = torch.max(outputs.data , 1)[ 1] ",recurrent-neural-network-with-pytorch.ipynb
Total number of labels, total += labels.size(0) ,recurrent-neural-network-with-pytorch.ipynb
store loss and iteration, loss_list.append(loss.data) ,recurrent-neural-network-with-pytorch.ipynb
Print Loss," print('Iteration: {} Loss: {} Accuracy: {} %'.format(count , loss.data[0], accuracy)) ",recurrent-neural-network-with-pytorch.ipynb
visualization loss,"plt.plot(iteration_list , loss_list) ",recurrent-neural-network-with-pytorch.ipynb
visualization accuracy,"plt.plot(iteration_list , accuracy_list , color = ""red"") ",recurrent-neural-network-with-pytorch.ipynb
"Trying out a linear model:Author: Alexandru Papiu apapiu, GitHub If you use parts of this notebook in your own scripts, please give some sort of credit for example link back to this . Thanks!There have been a few great scripts on xgboost already so I d figured I d try something simpler: a regularized linear regression model. Surprisingly it does really well with very little feature engineering. The key point is to to log transform the numeric variables since most of them are skewed.",import pandas as pd ,regularized-linear-models.ipynb
set png here when working on notebook,% config InlineBackend.figure_format = 'retina' ,regularized-linear-models.ipynb
Data preprocessing: We re not going to do anything fancy here: First I ll transform the skewed numeric features by taking log feature 1 this will make the features more normal Create Dummy variables for the categorical features Replace the numeric missing values NaN s with the mean of their respective columns ,"matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)
prices = pd.DataFrame({""price"":train[""SalePrice""], ""log(price + 1)"":np.log1p(train[""SalePrice""])})
prices.hist()",regularized-linear-models.ipynb
log transform the target:,"train[""SalePrice""]= np.log1p(train[""SalePrice""]) ",regularized-linear-models.ipynb
log transform skewed numeric features:,"numeric_feats = all_data.dtypes[all_data.dtypes != ""object""]. index ",regularized-linear-models.ipynb
compute skewness,skewed_feats = train[numeric_feats]. apply(lambda x : skew(x.dropna ())) ,regularized-linear-models.ipynb
filling NA s with the mean of the column:,all_data = all_data.fillna(all_data.mean ()) ,regularized-linear-models.ipynb
creating matrices for sklearn:,X_train = all_data[: train.shape[0]] ,regularized-linear-models.ipynb
ModelsNow we are going to use regularized linear regression models from the scikit learn module. I m going to try both l 1 Lasso and l 2 Ridge regularization. I ll also define a function that returns the cross validation rmse error so we can evaluate our models and pick the best tuning par,"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV
from sklearn.model_selection import cross_val_score

def rmse_cv(model):
 rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=""neg_mean_squared_error"", cv = 5))
 return(rmse)",regularized-linear-models.ipynb
The main tuning parameter for the Ridge model is alpha a regularization parameter that measures how flexible our model is. The higher the regularization the less prone our model will be to overfit. However it will also lose flexibility and might not capture all of the signal in the data.,"alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]
cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() 
 for alpha in alphas]",regularized-linear-models.ipynb
Note the U ish shaped curve above. When alpha is too large the regularization is too strong and the model cannot capture all the complexities in the data. If however we let the model be too flexible alpha small the model begins to overfit. A value of alpha 10 is about right based on the plot above.,cv_ridge.min(),regularized-linear-models.ipynb
So for the Ridge regression we get a rmsle of about 0.127Let try out the Lasso model. We will do a slightly different approach here and use the built in Lasso CV to figure out the best alpha for us. For some reason the alphas in Lasso CV are really the inverse or the alphas in Ridge.,"model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)",regularized-linear-models.ipynb
Nice! The lasso performs even better so we ll just use this one to predict on the test set. Another neat thing about the Lasso is that it does feature selection for you setting coefficients of features it deems unimportant to zero. Let s take a look at the coefficients:,"coef = pd.Series(model_lasso.coef_, index = X_train.columns)",regularized-linear-models.ipynb
We can also take a look directly at what the most important coefficients are:,"imp_coef = pd.concat([coef.sort_values().head(10),
 coef.sort_values().tail(10)])",regularized-linear-models.ipynb
let s look at the residuals as well:,"matplotlib.rcParams['figure.figsize']=(6.0 , 6.0) ",regularized-linear-models.ipynb
Let s add an xgboost model to our linear model to see if we can improve our score:,import xgboost as xgb,regularized-linear-models.ipynb
the params were tuned using xgb.cv,"model_xgb = xgb.XGBRegressor(n_estimators = 360 , max_depth = 2 , learning_rate = 0.1) ",regularized-linear-models.ipynb
Many times it makes sense to take a weighted average of uncorrelated results this usually imporoves the score although in this case it doesn t help that much.,preds = 0.7*lasso_preds + 0.3*xgb_preds,regularized-linear-models.ipynb
Trying out keras?Feedforward Neural Nets doesn t seem to work well at all...I wonder why.,"from keras.layers import Dense
from keras.models import Sequential
from keras.regularizers import l1
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split",regularized-linear-models.ipynb
"model.add Dense 256, activation relu , input dim X train.shape 1 ","model.add(Dense(1 , input_dim = X_train.shape[1], W_regularizer = l1(0.001))) ",regularized-linear-models.ipynb
linear algebra,import numpy as np ,reinforcement-learning-chess-1-policy-iteration.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,reinforcement-learning-chess-1-policy-iteration.ipynb
Python Chess is the Python Chess Package that handles the chess environment,! pip install python - chess ,reinforcement-learning-chess-1-policy-iteration.ipynb
RLC is the Reinforcement Learning package,! pip install - - upgrade git + https : // github.com / arjangroen / RLC.git ,reinforcement-learning-chess-1-policy-iteration.ipynb
"The state space is a 8 by 8 grid The starting state S is the top left square 0,0 The terminal state F is square 5,7 . Every move from state to state gives a reward of minus 1 Naturally the best policy for this evironment is to move from S to F in the lowest amount of moves possible. ","env = Board()
env.render()
env.visual_board",reinforcement-learning-chess-1-policy-iteration.ipynb
"The agent The agent is a chess Piece king, queen, rook, knight or bishop The agent has a behavior policy determining what the agent does in what state ",p = Piece(piece='king'),reinforcement-learning-chess-1-policy-iteration.ipynb
Reinforce The reinforce object contains the algorithms for solving move chess The agent and the environment are attributes of the Reinforce object ,"r = Reinforce(p,env)",reinforcement-learning-chess-1-policy-iteration.ipynb
Python Implementation,print(inspect.getsource(r.evaluate_state)),reinforcement-learning-chess-1-policy-iteration.ipynb
"Demonstration The initial value function assigns value 0 to each state The initial policy gives an equal probability to each action We evaluate state 0,0 ",r.agent.value_function.astype(int),reinforcement-learning-chess-1-policy-iteration.ipynb
1.2 Policy Evaluation Policy evaluation is the act of doe state evaluation for each state in the statespace As you can see in my implementatin I simply iterate over all state and update the value function This is the algorithm provided by Sutton and Barto: ,print(inspect.getsource(r.evaluate_policy)),reinforcement-learning-chess-1-policy-iteration.ipynb
We end up with the following value of 1 for all states except the terminal state. ,r.agent.value_function.astype(int),reinforcement-learning-chess-1-policy-iteration.ipynb
Demonstration,"eps=0.1
k_max = 1000
value_delta_max = 0
gamma = 1
synchronous=True
value_delta_max = 0
for k in range(k_max):
 r.evaluate_policy(gamma=gamma,synchronous=synchronous)
 value_delta = np.max(np.abs(r.agent.value_function_prev - r.agent.value_function))
 value_delta_max = value_delta
 if value_delta_max < eps:
 print('converged at iter',k)
 break",reinforcement-learning-chess-1-policy-iteration.ipynb
"This value function below shows the expected discounted future reward from state 0,0 185",r.agent.value_function.astype(int),reinforcement-learning-chess-1-policy-iteration.ipynb
"Now that we know what the values of the states are, we want to improve our Policy so that we the behavior is guided towards the state with the highest value. Policy Improvement is simply the act of making the policy greedy with respect to the value function. In my implementation, we do this by setting the value of the action that leads to the most valuable state to 1 while the rest remains 0 ",print(inspect.getsource(r.improve_policy)),reinforcement-learning-chess-1-policy-iteration.ipynb
Python implementation,print(inspect.getsource(r.policy_iteration)),reinforcement-learning-chess-1-policy-iteration.ipynb
Demonstration,r.policy_iteration(),reinforcement-learning-chess-1-policy-iteration.ipynb
Demonstration,"agent = Piece(piece='king')
r = Reinforce(agent,env)",reinforcement-learning-chess-1-policy-iteration.ipynb
Let s pick a rook for a change.,agent = Piece(piece = 'rook') ,reinforcement-learning-chess-1-policy-iteration.ipynb
The only difference here is that we set k max to 1.,"r.policy_iteration(k = 1 , gamma = 1) ",reinforcement-learning-chess-1-policy-iteration.ipynb
"Data Augmentation using GPU TPU for Maximum Speed! This notebook shows how perform rotation, shear, zoom, and shift data augmentation for the GPU TPU with TensorFlow.data.Dataset. Data augmentation is a technique to increase model accuracy and using GPU TPU achieves this goal quicker. GPUs and TPUs can consume 200 or more images sized 512x512x3 in one second while training DenseNet201 ! That s incredible. If we perform data augmentation beforehand, we need to make sure we are preparing at least 200 images per second. Otherwise we will slow down our GPU TPU training.This is the advantage of tensorflow.data.Dataset. After writing augmentation in TensorFlow language, your program will optimize these operations for GPU TPU. Similarily, you can use libraries like Nvidia DALI here 1 for GPU image preprocess and or Nvidia RAPIDS here 2 for GPU tabular preprocess.","import random, re, math
import numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
import tensorflow as tf, tensorflow.keras.backend as K
from kaggle_datasets import KaggleDatasets
print('Tensorflow version ' + tf.__version__)
from sklearn.model_selection import KFold",rotation-augmentation-gpu-tpu-0-96.ipynb
"Detect hardware, return appropriate distribution strategy",try : ,rotation-augmentation-gpu-tpu-0-96.ipynb
TPU detection. No parameters necessary if TPU NAME environment variable is set. On Kaggle this is always the case., tpu = tf.distribute.cluster_resolver.TPUClusterResolver () ,rotation-augmentation-gpu-tpu-0-96.ipynb
default distribution strategy in Tensorflow. Works on CPU and single GPU., strategy = tf.distribute.get_strategy () ,rotation-augmentation-gpu-tpu-0-96.ipynb
Configuration,"IMAGE_SIZE =[224 , 224] ",rotation-augmentation-gpu-tpu-0-96.ipynb
Mixed Precision and or XLA The following booleans can enable mixed precision and or XLA on GPU TPU. By default TPU already uses some mixed precision but we can add more. These allow the GPU TPU memory to handle larger batch sizes and can speed up the training process. The Nvidia V100 GPU has special Tensor Cores which get utilized when mixed precision is enabled. Unfortunately Kaggle s Nvidia P100 GPU does not have Tensor Cores to receive speed up.,"MIXED_PRECISION = False
XLA_ACCELERATE = False

if MIXED_PRECISION:
 from tensorflow.keras.mixed_precision import experimental as mixed_precision
 if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')
 else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')
 mixed_precision.set_policy(policy)
 print('Mixed precision enabled')

if XLA_ACCELERATE:
 tf.config.optimizer.set_jit(True)
 print('Accelerated Linear Algebra enabled')",rotation-augmentation-gpu-tpu-0-96.ipynb
Data access,GCS_DS_PATH = KaggleDatasets (). get_gcs_path('tpu-getting-started') ,rotation-augmentation-gpu-tpu-0-96.ipynb
Starting with a high LR would break the pre trained weights.,LR_START = 0.00001 ,rotation-augmentation-gpu-tpu-0-96.ipynb
Dataset Functions From starter kernel 1 ,def decode_image(image_data): ,rotation-augmentation-gpu-tpu-0-96.ipynb
"convert image to floats in 0, 1 range"," image = tf.cast(image , tf.float32)/ 255.0 ",rotation-augmentation-gpu-tpu-0-96.ipynb
explicit size needed for TPU," image = tf.reshape(image ,[* IMAGE_SIZE , 3]) ",rotation-augmentation-gpu-tpu-0-96.ipynb
"Data Augmentation The following code does random rotations, shear, zoom, and shift using the GPU TPU. When an image gets moved away from an edge revealing blank space, the blank space is filled by stretching the colors on the original edge. Change the variables in function transform below to control the desired amount of augmentation. Here s a diagram illustrating the mathematics.","def get_mat(rotation , shear , height_zoom , width_zoom , height_shift , width_shift): ",rotation-augmentation-gpu-tpu-0-96.ipynb
CONVERT DEGREES TO RADIANS, rotation = math.pi * rotation / 180. ,rotation-augmentation-gpu-tpu-0-96.ipynb
ROTATION MATRIX, c1 = tf.math.cos(rotation) ,rotation-augmentation-gpu-tpu-0-96.ipynb
SHEAR MATRIX, c2 = tf.math.cos(shear) ,rotation-augmentation-gpu-tpu-0-96.ipynb
ZOOM MATRIX," zoom_matrix = tf.reshape(tf.concat ([one / height_zoom , zero , zero , zero , one / width_zoom , zero , zero , zero , one], axis = 0),[3 , 3]) ",rotation-augmentation-gpu-tpu-0-96.ipynb
SHIFT MATRIX," shift_matrix = tf.reshape(tf.concat ([one , zero , height_shift , zero , one , width_shift , zero , zero , one], axis = 0),[3 , 3]) ",rotation-augmentation-gpu-tpu-0-96.ipynb
"output image randomly rotated, sheared, zoomed, and shifted", DIM = IMAGE_SIZE[0] ,rotation-augmentation-gpu-tpu-0-96.ipynb
fix for size 331, XDIM = DIM % 2 ,rotation-augmentation-gpu-tpu-0-96.ipynb
GET TRANSFORMATION MATRIX," m = get_mat(rot , shr , h_zoom , w_zoom , h_shift , w_shift) ",rotation-augmentation-gpu-tpu-0-96.ipynb
LIST DESTINATION PIXEL INDICES," x = tf.repeat(tf.range(DIM // 2 , - DIM // 2 , - 1), DIM) ",rotation-augmentation-gpu-tpu-0-96.ipynb
ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS," idx2 = K.dot(m , tf.cast(idx , dtype = 'float32')) ",rotation-augmentation-gpu-tpu-0-96.ipynb
FIND ORIGIN PIXEL VALUES," idx3 = tf.stack ([DIM // 2 - idx2[0 ,], DIM // 2 - 1 + idx2[1 ,]]) ",rotation-augmentation-gpu-tpu-0-96.ipynb
Display Example Augmentation Below are examples of 3 training images where each is randomly augmented 12 different times.,"row = 3; col = 4;
all_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()
one_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )
augmented_element = one_element.repeat().map(transform).batch(row*col)

for (img,label) in augmented_element:
 plt.figure(figsize=(15,int(15*row/col)))
 for j in range(row*col):
 plt.subplot(row,col,j+1)
 plt.axis('off')
 plt.imshow(img[j,])
 plt.show()
 break",rotation-augmentation-gpu-tpu-0-96.ipynb
"Build, Train, Infer Model This is the 5 Fold workflow copied from Ragnar s notebook here 1 . Now we add data augmentation to the training images on the fly! Notice how his notebook completes epochs in 70 seconds using TPU. This notebook also completes epochs in 70 seconds when we turn on TPU and we are augmentating every image! Augmenting a single image requires 5,000,000 calculations a batch requires 600,000,000 calculations! We see that our augmentation is occuring as fast as the GPU TPU training! We are augmenting 200 images per second. In other words we are performing 1,000,000,000 calculations per second in addition to normal training computation! Wow!",from tensorflow.keras.applications import DenseNet201 ,rotation-augmentation-gpu-tpu-0-96.ipynb
Confusion Matrix and Validation Score Try forking and modifying this notebook to maximize validation score below. Tune the data augmentation and or train for more epochs to increase accuracy. Good luck! Code below is from starter kernel 1 .,"def display_confusion_matrix(cmat, score, precision, recall):
 plt.figure(figsize=(15,15))
 ax = plt.gca()
 ax.matshow(cmat, cmap='Reds')
 ax.set_xticks(range(len(CLASSES)))
 ax.set_xticklabels(CLASSES, fontdict={'fontsize': 7})
 plt.setp(ax.get_xticklabels(), rotation=45, ha=""left"", rotation_mode=""anchor"")
 ax.set_yticks(range(len(CLASSES)))
 ax.set_yticklabels(CLASSES, fontdict={'fontsize': 7})
 plt.setp(ax.get_yticklabels(), rotation=45, ha=""right"", rotation_mode=""anchor"")
 titlestring = """"
 if score is not None:
 titlestring += 'f1 = {:.3f} '.format(score)
 if precision is not None:
 titlestring += '\nprecision = {:.3f} '.format(precision)
 if recall is not None:
 titlestring += '\nrecall = {:.3f} '.format(recall)
 if len(titlestring) > 0:
 ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})
 plt.show()",rotation-augmentation-gpu-tpu-0-96.ipynb
get everything as one batch, all_labels.append(next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))). numpy ()) ,rotation-augmentation-gpu-tpu-0-96.ipynb
 IMPORTATIONS ,"import glob
import random
import os
from PIL import Image
import torch
from torch import nn
from tqdm.auto import tqdm
from torchvision import transforms
from torchvision.utils import make_grid
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
torch.manual_seed(0)",sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
"PyTorch supports two classes, which are torch.utils.data.Dataset and torch.utils.data.DataLoader, to facilitate loading dataset and to make mini batch without large effort. Let s use them now as below in order to load our Monet and Real Photos ","class ImageDataset(Dataset):
 def __init__(self,MONET_FILENAMES,PHOTO_FILENAMES, transform=None): 
 self.transform = transform
 self.PHOTO_FILENAMES = PHOTO_FILENAMES
 self.MONET_FILENAMES = MONET_FILENAMES
 if len(self.MONET_FILENAMES) > len (self.PHOTO_FILENAMES):
 self.MONET_FILENAMES, self.PHOTO_FILENAMES = self.PHOTO_FILENAMES, self.MONET_FILENAMES
 self.new_perm()

 def new_perm(self):
 self.randperm = torch.randperm(len(self.PHOTO_FILENAMES))[:len(self.MONET_FILENAMES)]
 def __getitem__(self, index): 
 item_MONET_FILENAMES = self.transform(Image.open(self.MONET_FILENAMES[index % len(self.MONET_FILENAMES)]))
 item_PHOTO_FILENAMES = self.transform(Image.open(self.PHOTO_FILENAMES[self.randperm[index]]))
 if item_MONET_FILENAMES.shape[0] != 3: 
 item_MONET_FILENAMES = item_MONET_FILENAMES.repeat(3, 1, 1)
 if item_PHOTO_FILENAMES.shape[0] != 3: 
 item_PHOTO_FILENAMES = item_PHOTO_FILENAMES.repeat(3, 1, 1)
 if index == len(self) - 1:
 self.new_perm()
 return (item_MONET_FILENAMES - 0.5) * 2, (item_PHOTO_FILENAMES - 0.5) * 2

 def __len__(self):
 return min(len(self.MONET_FILENAMES), len(self.PHOTO_FILENAMES))",sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
Here we the building blocks of our models:ContractingBlock Encoder block: Apply convolutional filters while also reducing data resolution and increasing features.Expanding Decoder block: Apply convolutional filters while also increasing data resolution and decreasing features.Residual block: Apply convolutional filters to find relevant data patterns and keeps features constant. We will use here some skip connections in order to get Residual blocksLet s go to the implementation .... Feel free to ask about any line of code,"class ResidualBlock(nn.Module): 
 def __init__(self, input_channels):
 super(ResidualBlock, self).__init__()
 self.conv1 = nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1, padding_mode='reflect')
 self.conv2 = nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1, padding_mode='reflect')
 self.instancenorm = nn.InstanceNorm2d(input_channels)
 self.activation = nn.ReLU()

 def forward(self, x):
 original_x = x.clone()
 x = self.conv1(x)
 x = self.instancenorm(x)
 x = self.activation(x)
 x = self.conv2(x)
 x = self.instancenorm(x)
 return original_x + x",sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
This layer will be on the top and in the final layers of the models .. It s role is to map the input matrix to desired output channels,"class FeatureMapBlock(nn.Module):
 def __init__(self, input_channels, output_channels):
 super(FeatureMapBlock, self).__init__()
 self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=7, padding=3, padding_mode='reflect')

 def forward(self, x):
 x = self.conv(x)
 return x",sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
4 . Full Generator,"class Generator(nn.Module):
 def __init__(self, input_channels, output_channels, hidden_channels=64):
 super(Generator, self).__init__()
 self.upfeature = FeatureMapBlock(input_channels, hidden_channels)
 self.contract1 = ContractingBlock(hidden_channels)
 self.contract2 = ContractingBlock(hidden_channels * 2)
 res_mult = 4
 self.res0 = ResidualBlock(hidden_channels * res_mult)
 self.res1 = ResidualBlock(hidden_channels * res_mult)
 self.res2 = ResidualBlock(hidden_channels * res_mult)
 self.res3 = ResidualBlock(hidden_channels * res_mult)
 self.res4 = ResidualBlock(hidden_channels * res_mult)
 self.res5 = ResidualBlock(hidden_channels * res_mult)
 self.res6 = ResidualBlock(hidden_channels * res_mult)
 self.res7 = ResidualBlock(hidden_channels * res_mult)
 self.res8 = ResidualBlock(hidden_channels * res_mult)
 self.expand2 = ExpandingBlock(hidden_channels * 4)
 self.expand3 = ExpandingBlock(hidden_channels * 2)
 self.downfeature = FeatureMapBlock(hidden_channels, output_channels)
 self.tanh = torch.nn.Tanh()

 def forward(self, x):
 x0 = self.upfeature(x)
 x1 = self.contract1(x0)
 x2 = self.contract2(x1)
 x3 = self.res0(x2)
 x4 = self.res1(x3)
 x5 = self.res2(x4)
 x6 = self.res3(x5)
 x7 = self.res4(x6)
 x8 = self.res5(x7)
 x9 = self.res6(x8)
 x10 = self.res7(x9)
 x11 = self.res8(x10)
 x12 = self.expand2(x11)
 x13 = self.expand3(x12)
 xn = self.downfeature(x13)
 return self.tanh(xn)",sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
6 . Discriminator Code,"class Discriminator(nn.Module):
 def __init__(self, input_channels, hidden_channels=64):
 super(Discriminator, self).__init__()
 self.upfeature = FeatureMapBlock(input_channels, hidden_channels)
 self.contract1 = ContractingBlock(hidden_channels, use_bn=False, kernel_size=4, activation='lrelu')
 self.contract2 = ContractingBlock(hidden_channels * 2, kernel_size=4, activation='lrelu')
 self.contract3 = ContractingBlock(hidden_channels * 4, kernel_size=4, activation='lrelu')
 self.final = nn.Conv2d(hidden_channels * 8, 1, kernel_size=1)

 def forward(self, x):
 x0 = self.upfeature(x)
 x1 = self.contract1(x0)
 x2 = self.contract2(x1)
 x3 = self.contract3(x2)
 xn = self.final(x3)
 return xn",sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
"First, we will going to be implementing the discriminator loss ... Like a classique discriminator loss in a Simple GAN","def get_disc_loss(real_X , fake_X , disc_X , adv_criterion): ",sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
Detach generator in order the fix it s params while training the discriminator, disc_fake_X_hat = disc_X(fake_X.detach ()) ,sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
The first component of the generator s loss I m going to implement is its adversarial loss ,"def get_gen_adversarial_loss(real_X, disc_Y, gen_XY, adv_criterion):
 fake_Y = gen_XY(real_X)
 disc_fake_Y_hat = disc_Y(fake_Y)
 adversarial_loss = adv_criterion(disc_fake_Y_hat, torch.ones_like(disc_fake_Y_hat))
 return adversarial_loss, fake_Y",sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
"We ll want to measure the change in an image when you pass the generator an example from the target domain instead of the input domain it s expecting. The output should be the same as the input since it is already of the target domain class. For example, if you put a Monet Photo through a PHOTO MONET generator, We ll expect the output to be the same Monet PHOTO because nothing needed to be transformed. It s already a MONET PHOTO! You don t want your generator to be transforming it into any other thing, so you want to encourage this behavior. In encouraging this identity mapping, this will help also to properly preserve the colors of an image, even when the expected input here, a MONET PHOTO was put in. ","def get_identity_loss(real_X, gen_YX, identity_criterion):
 identity_X = gen_YX(real_X)
 identity_loss = identity_criterion(identity_X, real_X)
 return identity_loss, identity_X",sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
"This is used to ensure that when you put an image through one generator, that if it is then transformed back into the input class using the opposite generator, the image is the same as the original input image.Since I ve already generated a fake image for the adversarial part,I can now pass that fake image back to produce a full cycle this loss will encourage the cycle to preserve as much information as possible.","def get_cycle_consistency_loss(real_X, fake_Y, gen_YX, cycle_criterion):
 cycle_X = gen_YX(fake_Y)
 cycle_loss = cycle_criterion(cycle_X, real_X)
 return cycle_loss, cycle_X",sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
"Finally, I can put it all together!","def get_gen_loss(real_P, real_M, gen_PM, gen_MP, disc_P, disc_M, adv_criterion, identity_criterion, cycle_criterion, lambda_identity=0.1, lambda_cycle=10):
 
 adv_loss_MP, fake_P = get_gen_adversarial_loss(real_M, disc_P, gen_MP, adv_criterion)
 adv_loss_PM, fake_M = get_gen_adversarial_loss(real_P, disc_M, gen_PM, adv_criterion)
 gen_adversarial_loss = adv_loss_MP + adv_loss_PM

 identity_loss_P, identity_P = get_identity_loss(real_P, gen_MP, identity_criterion)
 identity_loss_M, identity_M = get_identity_loss(real_M, gen_PM, identity_criterion)
 gen_identity_loss = identity_loss_M + identity_loss_P

 cycle_loss_MP, cycle_P = get_cycle_consistency_loss(real_P, fake_M, gen_MP, cycle_criterion)
 cycle_loss_PM, cycle_M = get_cycle_consistency_loss(real_M, fake_P, gen_PM, cycle_criterion)
 gen_cycle_loss = cycle_loss_PM + cycle_loss_PM

 gen_loss = lambda_identity * gen_identity_loss + lambda_cycle * gen_cycle_loss + gen_adversarial_loss

 return gen_loss, fake_P, fake_M",sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
In order to have a better train performance I ll introduce a learning rate scheduler the ReduceLROnPlateau scheduler. ,from skimage import color ,sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
10.1. Loading Data,import os ,sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
a function to prepare the path names of MONET Real photos,def prepare_Paths(BASE_PATH): ,sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
A function to prepare two lists of MONET PHOTO names,"def Prepare_list_names(MONET_PATH , PHOTO_PATH): ",sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
The set of transformations to do in order to make some data augmentation to help the model avoiding the overfitting,target_shape = 256 ,sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
10.2. Visuals,import cv2 ,sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
a function to make some stats on the folder of images,def print_folder_statistics(path): ,sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
a function to visualize the batch of images ... Is random param to choose weather to plot a random image or not, plt.figure(figsize = figsize) ,sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
a function to plot an image with the histogram of it s RGB colors, plt.figure(figsize = figsize) ,sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
10.3.1 Hyperparams Tunning,import torch.nn.functional as F ,sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
10.3.2 Creating the Generator and Discriminator and initialization,"gen_PM = Generator(dim_PHOTO , dim_MONET) ",sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
a function to initialise the weights of our model,def weights_init(m): ,sbh-advanced-cycle-gan-implementation-and-tricks.ipynb
 ,"print(""\n \n\n"")
from IPython.display import Image
from IPython.core.display import HTML 
Image(url= ""https://i.imgur.com/6FQ3BZA.png"")",scikit-learn-data-science-lon-william.ipynb
 ,import pandas as pd ,scikit-learn-data-science-lon-william.ipynb
test data pd.read csv r C: Users user Desktop  Data Science London test.csv ,"train_data_feature = pd.read_csv(""../input/data-science-london-scikit-learn/train.csv"" , header = None) ",scikit-learn-data-science-lon-william.ipynb
 ,train_data_feature.info () ,scikit-learn-data-science-lon-william.ipynb
 ,train_data_label.info () ,scikit-learn-data-science-lon-william.ipynb
 ,test_data.info () ,scikit-learn-data-science-lon-william.ipynb
 ,from sklearn.model_selection import train_test_split ,scikit-learn-data-science-lon-william.ipynb
train data  ,"print(""(Support Vector Machines)():"" , svm_model.score(train_feature , train_label)) ",scikit-learn-data-science-lon-william.ipynb
 ,from sklearn.model_selection import train_test_split ,scikit-learn-data-science-lon-william.ipynb
train data  ,"print(""(Nearest Neighbors)()"" , KNeighbors_model.score(train_feature , train_label)) ",scikit-learn-data-science-lon-william.ipynb
 ,from sklearn.model_selection import train_test_split ,scikit-learn-data-science-lon-william.ipynb
train data  ,"print(""(Decision Trees)()"" , DecisionTree_model.score(train_feature , train_label)) ",scikit-learn-data-science-lon-william.ipynb
 ,from sklearn.model_selection import train_test_split ,scikit-learn-data-science-lon-william.ipynb
train data  ,"print(""(Forests of randomized trees)()"" , RandomForest_model.score(train_feature , train_label)) ",scikit-learn-data-science-lon-william.ipynb
 ,from sklearn.model_selection import train_test_split ,scikit-learn-data-science-lon-william.ipynb
 ,from sklearn.model_selection import train_test_split ,scikit-learn-data-science-lon-william.ipynb
train data  ,"print(""(GaussianProcess)()"" , GaussianProcess_model.score(train_feature , train_label)) ",scikit-learn-data-science-lon-william.ipynb
 ,"models = pd.DataFrame({
 'Model': ['(Support Vector Machines)', 
 '(Nearest Neighbors)', 
 '(Decision Trees)',
 '(Forests of randomized trees)', 
 '(Neural Network models)',
 '(GaussianProcess)'
 ],
 'Score': [svm_model_acc,
 KNeighbors_model_acc,
 DecisionTree_model_acc,
 RandomForest_model_model_acc,
 MLP_model_acc,
 GaussianProcess_model_acc, 
 ]
 })
models.sort_values(by='Score', ascending=False)",scikit-learn-data-science-lon-william.ipynb
  ,from sklearn.model_selection import train_test_split ,scikit-learn-data-science-lon-william.ipynb
  ,test_data_predict = DecisionTree_model.predict(test_data) ,scikit-learn-data-science-lon-william.ipynb
 ,"output = pd.DataFrame({ 'Id' : Id , 'Solution' : test_data_predict }) ",scikit-learn-data-science-lon-william.ipynb
"For example, here s several helpful packages to load","import os , zipfile ",second-attempt-2.ipynb
linear algebra,import numpy as np ,second-attempt-2.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,second-attempt-2.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory","for dirname , _ , filenames in os.walk('/kaggle/input'): ",second-attempt-2.ipynb
plt.title LABELS 213 ,train_data_img_list = [] ,second-attempt-2.ipynb
"This is part of Kaggle s Learn Machine Learning series.Selecting and Filtering Data Your dataset had too many variables to wrap your head around, or even to print out nicely. How can you pare down this overwhelming amount of data to something you can understand?To show you the techniques, we ll start by picking a few variables using our intuition. Later tutorials will show you statistical techniques to automatically prioritize variables.Before we can choose variables columns, it is helpful to see a list of all columns in the dataset. That is done with the columns property of the DataFrame the bottom line of code below .","import pandas as pd

melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path) 
print(melbourne_data.columns)",selecting-and-filtering-in-pandas.ipynb
store the series of prices separately as melbourne price data.,melbourne_price_data = melbourne_data.Price ,selecting-and-filtering-in-pandas.ipynb
the head command returns the top few lines of data.,print(melbourne_price_data.head ()) ,selecting-and-filtering-in-pandas.ipynb
"Selecting Multiple Columns You can select multiple columns from a DataFrame by providing a list of column names inside brackets. Remember, each item in that list should be a string with quotes .","columns_of_interest = ['Landsize', 'BuildingArea']
two_columns_of_data = melbourne_data[columns_of_interest]",selecting-and-filtering-in-pandas.ipynb
We can verify that we got the columns we need with the describe command.,two_columns_of_data.describe(),selecting-and-filtering-in-pandas.ipynb
"Importing the required libraries to read,visualize and model the givn dataset files",import pandas as pd ,sentiment-analysis-101-using-word2vec.ipynb
Reading the data,"df_train = pd.read_csv('../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip', delimiter=""\t"")
df_train.head()",sentiment-analysis-101-using-word2vec.ipynb
we will be removing the id column bc it is useless,"df_train = df_train.drop(['id'], axis=1)
df_train.head()",sentiment-analysis-101-using-word2vec.ipynb
Class distribution lets see the class distibution for 0 and 1,"print('Sentiment of 0 is {} % of total'.format(round(df_train['sentiment'].value_counts()[0]/len(df_train['sentiment'])*100)))
print('Sentiment of 1 is {} % of total'.format(round(df_train['sentiment'].value_counts()[1]/len(df_train['sentiment'])*100)))
x=df_train.sentiment.value_counts()
sns.barplot(x.index,x)
plt.gca().set_ylabel('samples')",sentiment-analysis-101-using-word2vec.ipynb
"Data cleaning Before starting any NLP project, text data needs to be pre processed to convert it into in a consistent format.Text will be cleaned, tokneized and converted into a matrix.Some of the basic text pre processing techniques includes:Make text all lower or uppercase Algorithms does not treat the same word different in different cases.Removing Noise Everything in the text that isn t a standard number or letter i.e. Punctuation, Numerical values,etc.Tokenization Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e. words.Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.Stopword Removal Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop wordsStemming Stemming is the process of reducing inflected or sometimes derived words to their stem, base or root form generally a written word form. Example if we were to stem the following words: Stems , Stemming , Stemmed , and Stemtization , the result would be a single word stem .Lemmatization A slight variant of stemming is lemmatization. The major difference between these is, that, stemming can often create non existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. Examples of Lemmatization are that run is a base form for words like running or ran or that the word better and good are in the same lemma so they are considered the same.",import re ,sentiment-analysis-101-using-word2vec.ipynb
Create a function to clean the text,def clean_text(text): ,sentiment-analysis-101-using-word2vec.ipynb
Lets apply the clean text function to both test and training datasets copies,df_train1 = df_train.copy () ,sentiment-analysis-101-using-word2vec.ipynb
Lets look cleaned text data,"def text_after_preprocess(before_text , after_text): ",sentiment-analysis-101-using-word2vec.ipynb
"Tokenization Tokenization is a process which splits an input text into tokens and the tokens can be a word, sentence, paragraph etc.Following code will show how tokenization of text works:",import nltk ,sentiment-analysis-101-using-word2vec.ipynb
Example how tokenization of text works,"text = ""Heard about #earthquake is different cities, stay safe everyone."" ",sentiment-analysis-101-using-word2vec.ipynb
Lets Tokenize the training and the test dataset copies with RegEx tokenizer,tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+') ,sentiment-analysis-101-using-word2vec.ipynb
Lets checy tokenized text,df_train1['review']. head () ,sentiment-analysis-101-using-word2vec.ipynb
"Stopwords Removal Now, let s get rid of the stopwords i.e words which occur very frequently and have possible value like a, an, the, are etc.",df_train1['review'].head(),sentiment-analysis-101-using-word2vec.ipynb
Stemming and Lemmatization examples,"text = ""ran deduced dogs talking studies"" ",sentiment-analysis-101-using-word2vec.ipynb
Stemmer,stemmer = nltk.stem.PorterStemmer () ,sentiment-analysis-101-using-word2vec.ipynb
Lemmatizer,lemmatizer = nltk.stem.WordNetLemmatizer () ,sentiment-analysis-101-using-word2vec.ipynb
Lets combine text after processing it,def combine_text(text): ,sentiment-analysis-101-using-word2vec.ipynb
Download pretrained word2vec model from Google This tool provides an efficient implementation of the continuous bag of words and skip gram architectures for computing vector representations of words. These representations can be subsequently used in many natural language processing,"!wget -c ""https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz""
!gunzip GoogleNews-vectors-negative300.bin.gz",sentiment-analysis-101-using-word2vec.ipynb
print sentence , vectors = [] ,sentiment-analysis-101-using-word2vec.ipynb
meaningless, i = False ,sentiment-analysis-101-using-word2vec.ipynb
print s not found x , vectors = np.array(vectors) ,sentiment-analysis-101-using-word2vec.ipynb
but inner product might be a more widely accepted solution," meanw2v = np.mean(vectors , axis = 0) ",sentiment-analysis-101-using-word2vec.ipynb
linear algebra,import numpy as np ,sentiment-analysis-part-i.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,sentiment-analysis-part-i.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,sentiment-analysis-part-i.ipynb
"Data Set The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating 5 results in a sentiment score of 0, and rating 7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.File descriptions labeledTrainData The labeled training set. The file is tab delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review. testData The test set. The tab delimited file has a header row followed by 25,000 rows containing an id and text for each review. Your task is to predict the sentiment for each one. unlabeledTrainData An extra training set with no labels. The tab delimited file has a header row followed by 50,000 rows containing an id and text for each review. sampleSubmission A comma delimited sample submission file in the correct format. Data fields id Unique ID of each review sentiment Sentiment of the review 1 for positive reviews and 0 for negative reviews review Text of the review ",import pandas as pd ,sentiment-analysis-part-i.ipynb
plotly,import plotly.offline as py ,sentiment-analysis-part-i.ipynb
Reading a zip file with help of Pandas,"train=pd.read_csv('../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip',compression='zip',
 header=0,delimiter='\t',quoting=0, doublequote=False, escapechar='\\')
train.head()",sentiment-analysis-part-i.ipynb
first view of the database," print(""No of columns of database"" , df.shape[1]) ",sentiment-analysis-part-i.ipynb
plot in barplot," sns.barplot(x = missing_stats.index , y = missing_stats.values , alpha = 0.8) ",sentiment-analysis-part-i.ipynb
to draw the KDE plot," plt.figure(figsize =(10 , 10)) ",sentiment-analysis-part-i.ipynb
plot," trace = go.Pie(labels = grouped[col], values = grouped['count'], pull =[0.05 , 0], marker = dict(colors =[""#6ad49b"" , ""#a678de""])) ",sentiment-analysis-part-i.ipynb
1.1 Lower Case,"train['review_lower']=train['review'].str.lower()
train.head()",sentiment-analysis-part-i.ipynb
Removal of punctuation,"punc_to_remove=string.punctuation

def remove_punctuation(text):
 return text.translate(str.maketrans('','',punc_to_remove))

train[""text_to_punc""]=train['review_lower'].apply(lambda text: remove_punctuation(text))
",sentiment-analysis-part-i.ipynb
Stop Words,"from nltk.corpus import stopwords
"","".join(stopwords.words(""english""))",sentiment-analysis-part-i.ipynb
Removal of Frequent words,"from collections import Counter
cnt=Counter()

for text in train['text_to_stop'].values:
 for word in text.split():
 cnt[word]+=1
 
cnt.most_common(10)",sentiment-analysis-part-i.ipynb
Snow ball stemmer,"from nltk.stem.snowball import SnowballStemmer
SnowballStemmer.languages",sentiment-analysis-part-i.ipynb
Lemmatization,"from nltk.stem import WordNetLemmatizer

lematizer=WordNetLemmatizer()

def lemmatizer_words(text):
 return "" "".join([lematizer.lemmatize(word) for word in text.split()])

train['lemma_text']=train['review'].apply(lambda text: lemmatizer_words(text))
train.head()",sentiment-analysis-part-i.ipynb
Remove HTML tag from the text,"import re

TAG_RE = re.compile(r'<[^>]+>')

def remove_tags(text):
 return TAG_RE.sub('', text)

train['text_to_html']=train['review'].apply(lambda text: remove_tags(text))
train.head()",sentiment-analysis-part-i.ipynb
Removal of URLs,"def remove_urld(text):
 url_pattern=re.compile(r'https?://\S+|www\.\S+')
 return url_pattern.sub(r'', text)

train['no_url']=train['review'].apply(lambda text: remove_urld(text)) 
train.head()",sentiment-analysis-part-i.ipynb
Decontracted Expanding the chat words like i ve I have ,"c_re = re.compile('(%s)' % '|'.join(cList.keys()))
def expandContractions(text, c_re=c_re):
 def replace(match):
 return cList[match.group(0)]
 return c_re.sub(replace, text)",sentiment-analysis-part-i.ipynb
Word Cloud,from wordcloud import WordCloud ,sentiment-analysis-part-i.ipynb
Thanks : ,"def plot_wordcloud(text, mask=None, max_words=400, max_font_size=120, figure_size=(24.0,16.0), 
 title = None, title_size=40, image_color=False):
 stopwords = set(STOPWORDS)
 more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}
 stopwords = stopwords.union(more_stopwords)

 wordcloud = WordCloud(background_color='white',
 stopwords = stopwords,
 max_words = max_words,
 max_font_size = max_font_size, 
 random_state = 42,
 mask = mask)
 wordcloud.generate(text)
 
 plt.figure(figsize=figure_size)
 if image_color:
 image_colors = ImageColorGenerator(mask);
 plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=""bilinear"");
 plt.title(title, fontdict={'size': title_size, 
 'verticalalignment': 'bottom'})
 else:
 plt.imshow(wordcloud);
 plt.title(title, fontdict={'size': title_size, 'color': 'green', 
 'verticalalignment': 'bottom'})
 plt.axis('off');
 plt.tight_layout() 
 
d = '../input/masks/masks-wordclouds/'",sentiment-analysis-part-i.ipynb
Conclusion More Update awill be done Soon This is the firdt step of text cleaning 2nd part will come soon Till then keep liking this ,"from IPython.display import Image
Image(""../input/thank-you/download.jpg"", width=1000, height=1000)",sentiment-analysis-part-i.ipynb
linear algebra,import numpy as np ,sentiment-analysis-part-ii.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,sentiment-analysis-part-ii.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,sentiment-analysis-part-ii.ipynb
"You can also write temporary files to kaggle temp , but they won t be saved outside of the current session",import pandas as pd ,sentiment-analysis-part-ii.ipynb
plotly,import plotly.offline as py ,sentiment-analysis-part-ii.ipynb
Text Preprocesing,"from nltk.corpus import stopwords
"","".join(stopwords.words(""english""))",sentiment-analysis-part-ii.ipynb
Text cleanings Lower case Remove Puncuation Remove Frequents Words Remove Rare Words Stemmer Lemmatization Removing stop words Remove URLS Expanding the deontracted words ,"def clean_text(df , col): ",sentiment-analysis-part-ii.ipynb
lowering the case," def lower_case(df , col): ",sentiment-analysis-part-ii.ipynb
remove punctuation, punc_to_remove = string.punctuation ,sentiment-analysis-part-ii.ipynb
remove frequent words," FREQWORDS = set ([w for(w , wc)in cnt.most_common(10 )]) ",sentiment-analysis-part-ii.ipynb
remove rare words, n_rare_word = 10 ,sentiment-analysis-part-ii.ipynb
Porter Stemming, stemmer = PorterStemmer () ,sentiment-analysis-part-ii.ipynb
Lemmatization, lematizer = WordNetLemmatizer () ,sentiment-analysis-part-ii.ipynb
Remove HTML tag from the text, TAG_RE = re.compile(r'<[^>]+>') ,sentiment-analysis-part-ii.ipynb
remove urls, def remove_urld(text): ,sentiment-analysis-part-ii.ipynb
expamnds the decontracted words, c_re = re.compile('(%s)' % '|'.join(cList.keys ())) ,sentiment-analysis-part-ii.ipynb
counting the most common words,"cnt=Counter()

for text in train['review'].values:
 for word in text.split():
 cnt[word]+=1
 
cnt.most_common(50)",sentiment-analysis-part-ii.ipynb
Visualize the Most Common words in review,"fig , ax = plt.subplots(figsize =(8 , 8)) ",sentiment-analysis-part-ii.ipynb
Plot horizontal bar graph,"fig = px.bar(most_common_word, x=""words"", y=""count"", title=""Bar Chart(including the Stop words)"", color=""words"")
fig.show()",sentiment-analysis-part-ii.ipynb
linear algebra,import numpy as np ,sentiment-analysis-with-word2vec.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,sentiment-analysis-with-word2vec.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,sentiment-analysis-with-word2vec.ipynb
Any results you write to the current directory are saved as output.,import pandas as pd ,sentiment-analysis-with-word2vec.ipynb
Import various modules for string cleaning,from bs4 import BeautifulSoup ,sentiment-analysis-with-word2vec.ipynb
1. Remove HTML, review_text = BeautifulSoup(review). get_text () ,sentiment-analysis-with-word2vec.ipynb
2. Remove non letters," review_text = re.sub(""[^a-zA-Z]"" , "" "" , review_text) ",sentiment-analysis-with-word2vec.ipynb
3. Convert words to lower case and split them, words = review_text.lower (). split () ,sentiment-analysis-with-word2vec.ipynb
4. Optionally remove stop words false by default , if remove_stopwords : ,sentiment-analysis-with-word2vec.ipynb
5. Return a list of words, return(words) ,sentiment-analysis-with-word2vec.ipynb
Download the punkt tokenizer for sentence splitting,import nltk.data ,sentiment-analysis-with-word2vec.ipynb
Load the punkt tokenizer,tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') ,sentiment-analysis-with-word2vec.ipynb
Define a function to split a review into parsed sentences,"def review_to_sentences(review , tokenizer , remove_stopwords = False): ",sentiment-analysis-with-word2vec.ipynb
1. Use the NLTK tokenizer to split the paragraph into sentences, raw_sentences = tokenizer.tokenize(review.strip ()) ,sentiment-analysis-with-word2vec.ipynb
2. Loop over each sentence, sentences = [] ,sentiment-analysis-with-word2vec.ipynb
"If a sentence is empty, skip it", if len(raw_sentence)> 0 : ,sentiment-analysis-with-word2vec.ipynb
"Otherwise, call review to wordlist to get a list of words"," sentences.append(review_to_wordlist(raw_sentence , remove_stopwords)) ",sentiment-analysis-with-word2vec.ipynb
"Return the list of sentences each sentence is a list of words,so this returns a list of lists", return sentences ,sentiment-analysis-with-word2vec.ipynb
Initialize an empty list of sentences,sentences = [] ,sentiment-analysis-with-word2vec.ipynb
creates nice output messages,import logging ,sentiment-analysis-with-word2vec.ipynb
Make sure that numpy is imported,import numpy as np ,sentiment-analysis-with-word2vec.ipynb
Pre initialize an empty numpy array for speed ," featureVec = np.zeros(( num_features ,), dtype = ""float32"") ",sentiment-analysis-with-word2vec.ipynb
"the model s vocabulary. Convert it to a set, for speed", index2word_set = set(model.wv.index2word) ,sentiment-analysis-with-word2vec.ipynb
"vocaublary, add its feature vector to the total", for word in words : ,sentiment-analysis-with-word2vec.ipynb
Divide the result by the number of words to get the average," featureVec = np.divide(featureVec , nwords) ",sentiment-analysis-with-word2vec.ipynb
Initialize a counter, counter = 0 ,sentiment-analysis-with-word2vec.ipynb
"Preallocate a 2D numpy array, for speed"," reviewFeatureVecs = np.zeros(( len(reviews), num_features), dtype = ""float32"") ",sentiment-analysis-with-word2vec.ipynb
Loop through the reviews, for review in reviews : ,sentiment-analysis-with-word2vec.ipynb
Print a status message every 1000th review, if counter % 1000 == 0 : ,sentiment-analysis-with-word2vec.ipynb
Call the function defined above that makes average feature vectors," reviewFeatureVecs[counter]= makeFeatureVec(review , model , num_features) ",sentiment-analysis-with-word2vec.ipynb
Increment the counter, counter = counter + 1 ,sentiment-analysis-with-word2vec.ipynb
removal.,clean_train_reviews = [] ,sentiment-analysis-with-word2vec.ipynb
"Fit a random forest to the training data, using 100 trees",from sklearn.ensemble import RandomForestClassifier ,sentiment-analysis-with-word2vec.ipynb
Test extract results,result = forest.predict(testDataVecs) ,sentiment-analysis-with-word2vec.ipynb
Write the test results,"output = pd.DataFrame(data = { ""id"" : test[""id""], ""sentiment"" : result }) ",sentiment-analysis-with-word2vec.ipynb
"From Words to Paragraphs, Attempt 2: Clustering ",from sklearn.cluster import KMeans ,sentiment-analysis-with-word2vec.ipynb
Start time,start = time.time () ,sentiment-analysis-with-word2vec.ipynb
average of 5 words per cluster,word_vectors = model.wv.syn0 ,sentiment-analysis-with-word2vec.ipynb
Initalize a k means object and use it to extract centroids,kmeans_clustering = KMeans(n_clusters = num_clusters) ,sentiment-analysis-with-word2vec.ipynb
Get the end time and print how long the process took,end = time.time () ,sentiment-analysis-with-word2vec.ipynb
a cluster number,"word_centroid_map = dict(zip(model.wv.index2word , idx)) ",sentiment-analysis-with-word2vec.ipynb
For the first 10 clusters,"for cluster in range(0 , 10): ",sentiment-analysis-with-word2vec.ipynb
Print the cluster number," print(""\nCluster %d"" % cluster) ",sentiment-analysis-with-word2vec.ipynb
"Find all of the words for that cluster number, and print them out", words = [] ,sentiment-analysis-with-word2vec.ipynb
print val , if(val == cluster): ,sentiment-analysis-with-word2vec.ipynb
in the word centroid map, num_centroids = max(word_centroid_map.values ()) + 1 ,sentiment-analysis-with-word2vec.ipynb
Pre allocate the bag of centroids vector for speed ," bag_of_centroids = np.zeros(num_centroids , dtype = ""float32"") ",sentiment-analysis-with-word2vec.ipynb
by one, for word in wordlist : ,sentiment-analysis-with-word2vec.ipynb
Return the bag of centroids , return bag_of_centroids ,sentiment-analysis-with-word2vec.ipynb
Pre allocate an array for the training set bags of centroids for speed ,"train_centroids = np.zeros(( train[""review""]. size , num_clusters), dtype = ""float32"") ",sentiment-analysis-with-word2vec.ipynb
Transform the training set reviews into bags of centroids,counter = 0 ,sentiment-analysis-with-word2vec.ipynb
Repeat for test reviews,"test_centroids = np.zeros(( test[""review""]. size , num_clusters), dtype = ""float32"") ",sentiment-analysis-with-word2vec.ipynb
Fit a random forest and extract predictions,forest = RandomForestClassifier(n_estimators = 100) ,sentiment-analysis-with-word2vec.ipynb
Fitting the forest may take a few minutes,"print(""Fitting a random forest to labeled training data..."") ",sentiment-analysis-with-word2vec.ipynb
Write the test results,"output = pd.DataFrame(data = { ""id"" : test[""id""], ""sentiment"" : result }) ",sentiment-analysis-with-word2vec.ipynb
"How do you interpret this?We predicted 0.7, whereas the base value is 0.4979. Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature s effect. Feature values decreasing the prediction are in blue. The biggest impact comes from Goal Scored being 2. Though the ball possession value has a meaningful effect decreasing the prediction.If you subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output.There is some complexity to the technique, to ensure that the baseline plus the sum of individual effects adds up to the prediction which isn t as straightforward as it sounds . We won t go into that detail here, since it isn t critical for using the technique. This blog post has a longer theoretical explanation.Code to Calculate SHAP Values We calculate SHAP values using the wonderful Shap library.For this example, we ll reuse the model you ve already seen with the Soccer data.",import numpy as np ,shap-values.ipynb
Convert from string Yes No to binary,"y =(data['Man of the Match']== ""Yes"") ",shap-values.ipynb
"We will look at SHAP values for a single row of the dataset we arbitrarily chose row 5 . For context, we ll look at the raw predictions before looking at the SHAP values.",row_to_show = 5 ,shap-values.ipynb
use 1 row of data here. Could use multiple rows if desired,data_for_prediction = val_X.iloc[row_to_show] ,shap-values.ipynb
package used to calculate Shap values,import shap ,shap-values.ipynb
Create object that can calculate shap values,explainer = shap.TreeExplainer(my_model) ,shap-values.ipynb
Calculate Shap values,shap_values = explainer.shap_values(data_for_prediction) ,shap-values.ipynb
"The shap values object above is a list with two arrays. The first array is the SHAP values for a negative outcome don t win the award , and the second array is the list of SHAP values for the positive outcome wins the award . We typically think about predictions in terms of the prediction of a positive outcome, so we ll pull out SHAP values for positive outcomes pulling out shap values 1 .It s cumbersome to review raw arrays, but the shap package has a nice way to visualize the results. ","shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)",shap-values.ipynb
use Kernel SHAP to explain test set predictions,"k_explainer = shap.KernelExplainer(my_model.predict_proba , train_X) ",shap-values.ipynb
DataSet,"import numpy as np
import pandas as pd
import lightgbm as lgb
from pandasql import sqldf

from pandas_profiling import ProfileReport",simple-gaussian-lgbm-data-science-london.ipynb
Gaussian Model,"x_all = pd.concat([train_df, test_df])",simple-gaussian-lgbm-data-science-london.ipynb
Final Model,"from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(x_train, train_labels_df['label'], test_size=0.3, random_state=42)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)",simple-gaussian-lgbm-data-science-london.ipynb
Submit Results,pred_result = model.predict(x_test),simple-gaussian-lgbm-data-science-london.ipynb
"many reference image from matplotlib cheatsheet This is a notebook which organizes various tips and contents of matplotlib which we browse every day.I am a developer who loves visualization.So far I ve built a kernel to share the tips I ve gained from doing a lot of visualizations.matplotlib is the most basic visualization tool, and even if you use it well, sometimes you don t need to use the rest of the visualization tools.Table of Contents Setting dpi figsize title Alignments subplots, tight layout subplot2grid add axes add gridspec Colormap diverging qualitative sequential scientific Text Annotate Patch parameter text example patches example Details Example font weight, color, size, etc Horizontal and Vertical barplot Border edge color and thickness Main Color Sub Color Transparency Span MEME xkcd style ",import numpy as np ,simple-matplotlib-visualization-tips.ipynb
Alignments,import matplotlib.gridspec as gridspec ,simple-matplotlib-visualization-tips.ipynb
theme dataset,import seaborn as sns ,simple-matplotlib-visualization-tips.ipynb
for visualization samples,import pandas as pd ,simple-matplotlib-visualization-tips.ipynb
" Alignments The first nine graph plots 3 by 3 are a combination of matplotlib layout and design. subplots subplot2grid add axes gridspec, add subplot inset axes make axes locatable Two or more graphs are much more visually and semantically better than just one.The easiest way to do this is to place the rectangles of the same shape.Usually you can start with the initial size with subplots. ","fig, axes = plt.subplots(2, 3, figsize=(8, 5))
plt.show()",simple-matplotlib-visualization-tips.ipynb
"The first of the plt.subplot parameters specifies the number of rows and the second the number of columns. The graph looks a bit frustrating. In this case, you can use plt.tight layout to solve the frustration.","fig, axes = plt.subplots(2, 3, figsize=(8, 5))
plt.tight_layout()
plt.show()",simple-matplotlib-visualization-tips.ipynb
initialize figure,"fig = plt.figure(figsize =(8 , 5)) ",simple-matplotlib-visualization-tips.ipynb
list to save many ax for setting parameter in each,ax =[None for _ in range(6 )] ,simple-matplotlib-visualization-tips.ipynb
make ax title for distinguish: , ax[ix]. set_title('ax[{}]'.format(ix)) ,simple-matplotlib-visualization-tips.ipynb
to remove x ticks, ax[ix]. set_xticks([]) ,simple-matplotlib-visualization-tips.ipynb
to remove y ticks, ax[ix]. set_yticks([]) ,simple-matplotlib-visualization-tips.ipynb
"Alternatively, you can use plt.add axes to create an ax where you want.","fig = plt.figure(figsize =(8 , 5)) ",simple-matplotlib-visualization-tips.ipynb
"x, y, dx, dy","ax[0]= fig.add_axes ([0.1 , 0.1 , 0.8 , 0.4]) ",simple-matplotlib-visualization-tips.ipynb
"Another way is to use gridspec. This allows you to use add subplot together, similar to subplots to grid.This approach allows you to take advantage of the concept of list to use a developer friendly grid.","fig = plt.figure(figsize =(8 , 5)) ",simple-matplotlib-visualization-tips.ipynb
"make 3 by 3 grid row, col ","gs = fig.add_gridspec(3 , 3) ",simple-matplotlib-visualization-tips.ipynb
"Here you can change the color of ax or plt itself, such as facecolor, to make it look more dashboard like.","fig, ax = plt.subplots()
axin1 = ax.inset_axes([0.8, 0.1, 0.15, 0.15])
plt.show()",simple-matplotlib-visualization-tips.ipynb
"Diverging ColormapThis colormap is usually used in visualizations where the median is obvious.It is usually visualized on a white background, white in the center, and darker in color toward both ends. In other words, the lighter the value, the closer to the center, the darker, the closer to the end.Useful for expressing bias, such as correlation and political disposition.Currently it is a continuous colormap, but you can also use discrete colorpalette depending on the interval.matplotlib loads the library s palette with that element in the cmap parameter. You can, of course, make it custom.","def cmap_plot(cmap_list, ctype):
 cmaps = cmap_list

 n = len(cmaps)

 fig = plt.figure(figsize=(8.25, n*.20), dpi=200)
 ax = plt.subplot(1, 1, 1, frameon=False, xlim=[0,10], xticks=[], yticks=[])
 fig.subplots_adjust(top=0.99, bottom=0.01, left=0.18, right=0.99)

 y, dy, pad = 0, 0.3, 0.08

 ticks, labels = [], []

 for cmap in cmaps[::-1]:
 Z = np.linspace(0,1,512).reshape(1,512)
 plt.imshow(Z, extent=[0,10,y,y+dy], cmap=plt.get_cmap(cmap))
 ticks.append(y+dy/2)
 labels.append(cmap)
 y = y + dy + pad

 ax.set_ylim(-pad,y)
 ax.set_yticks(ticks)
 ax.set_yticklabels(labels)

 ax.tick_params(axis='y', which='both', length=0, labelsize=5)
 plt.title(f'{ctype} Colormap', fontweight='bold', fontsize=8)
 plt.show()
",simple-matplotlib-visualization-tips.ipynb
 r mean reverse,"diverge_cmap =('PRGn' , 'PiYG' , 'RdYlGn' , 'BrBG' , 'RdGy' , 'PuOr' , 'RdBu' , 'RdYlBu' , 'Spectral' , 'coolwarm_r' , 'bwr_r' , 'seismic_r') ",simple-matplotlib-visualization-tips.ipynb
"Qualitative ColormapA palette of independent colors, often used for categorical variables. It is recommended to organize up to 10 colors, and to group more and smaller categories with other. Repeating colors can be confusing, so try to avoid overlapping as much as possible. It s a good idea to change color to color rather than saturation and brightness.Personally, I like Set2 palette.","qualitative_cmap = ('tab10', 'tab20', 'tab20b', 'tab20c',
 'Pastel1', 'Pastel2', 'Paired',
 'Set1', 'Set2', 'Set3', 'Accent', 'Dark2' )

cmap_plot(qualitative_cmap, 'Qualitative')",simple-matplotlib-visualization-tips.ipynb
"Sequential ColormapThis palette is appropriate for variables with numbers or sorted values. Used a lot in comparison of figures. Especially effective for expressing density. Take advantage of map graphs for better visualization.Similar to diverging, but with a slightly different part because each endpoint is a color criterion, not the median. It usually indicates that light values are dark on dark backgrounds and dark values on light backgrounds. It is recommended to use a single hue for the color.Like diverging, it can also be used in discrete form. r means reverse . ","sequential_cmap = ('Greys', 'Reds', 'Oranges', 
 'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',
 'Purples', 'YlGnBu', 'Blues', 'PuBu', 'GnBu', 'PuBuGn', 'BuGn',
 'Greens', 'YlGn','bone', 'gray', 'pink', 'afmhot', 'hot', 'gist_heat', 'copper', 
 'Wistia', 'autumn_r', 'summer_r', 'spring_r', 'cool', 'winter_r') 

cmap_plot(sequential_cmap, 'Sequential')",simple-matplotlib-visualization-tips.ipynb
It can be used to give a gradual feeling or to check the emphasis of a particular part.The following visualizations are possible:,"netflix_date = netflix_titles[['date_added']].dropna()
netflix_date['year'] = netflix_date['date_added'].apply(lambda x : x.split(', ')[-1])
netflix_date['month'] = netflix_date['date_added'].apply(lambda x : x.lstrip().split(' ')[0])

month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'][::-1]
df = netflix_date.groupby('year')['month'].value_counts().unstack().fillna(0)[month_order].T",simple-matplotlib-visualization-tips.ipynb
heatmap,"plt.pcolor(df , cmap = 'gist_heat_r' , edgecolors = 'white' , linewidths = 2) ",simple-matplotlib-visualization-tips.ipynb
"Scientific ColormapI don t know why, but it s used a lot in scientific visualization. It is used as a basic colormap of plotly and has high utilization and relatively professional feel.I ve seen it often used in signal data like spectrograms when dealing with voice data.","scientific_cmap = ('viridis', 'plasma', 'inferno', 'magma')
cmap_plot(scientific_cmap, 'Scientific')",simple-matplotlib-visualization-tips.ipynb
" Text Annotate PatchMany people often end up with just a picture in the graph, but the detail of the graph comes from the description. Just putting text on a specific part can change the feel of the graph.ax.text and ax.annotate are almost similar, but each has a different purpose. In ax.text, The first two numbers represent the ratio coordinates in the graph. In ax.annotate, xy represent the coordinates in the graph. va, ha is a parameter that determines whether the current coordinate is the center of the text or the left right of the text. color stands for color, and you can enter a custom color or rgb value directly. bbox sets an element for the box that wraps the text. Internal color facecolor and edge color edgecolor can be set separately. You can adjust the space by setting padding like in html. You can use the boxstyle to adjust the end of the rectangle. ","fig , ax = plt.subplots(figsize =(5 , 5), dpi = 100) ",simple-matplotlib-visualization-tips.ipynb
Gray Box,"ax.text(0.1 , 0.9 , 'Test' , color = 'gray' , va = ""center"" , ha = ""center"") ",simple-matplotlib-visualization-tips.ipynb
"Using a patch with text is more effective.Except for path patches, they are provided by default, so you can use them well.Arrow is especially effective.",import matplotlib.path as mpath ,simple-matplotlib-visualization-tips.ipynb
shift y value for label so that it s below the artist, y = xy[1]- 0.15 ,simple-matplotlib-visualization-tips.ipynb
create 3x3 grid to plot the artists,"grid = np.mgrid[0.2 : 0.8 : 3j , 0.2 : 0.8 : 3j]. reshape(2 , - 1). T ",simple-matplotlib-visualization-tips.ipynb
add a circle,"circle = mpatches.Circle(grid[0], 0.1 , ec = ""none"") ",simple-matplotlib-visualization-tips.ipynb
add a rectangle,"rect = mpatches.Rectangle(grid[1]-[0.025 , 0.05], 0.05 , 0.1 , ec = ""none"") ",simple-matplotlib-visualization-tips.ipynb
add a wedge,"wedge = mpatches.Wedge(grid[2], 0.1 , 30 , 270 , ec = ""none"") ",simple-matplotlib-visualization-tips.ipynb
add a Polygon,"polygon = mpatches.RegularPolygon(grid[3], 5 , 0.1) ",simple-matplotlib-visualization-tips.ipynb
add an ellipse,"ellipse = mpatches.Ellipse(grid[4], 0.2 , 0.1) ",simple-matplotlib-visualization-tips.ipynb
"Main Color Sub ColorIf you draw a plot of seaborn, it is displayed in various colors by default. You can draw various colorful graphs while changing the palette.But basically, the visualization should focus on information, so it s not always nice to have a colorful plot.Rather, think about what data you want to focus on. It is better to make a difference between the main color with strong color and the sub color of achromatic color system.Alternatively, it is a good idea to choose the palette according to the application mentioned above.In the case of a colormap, you can select a palette or pass it to a list, so it is convenient to pass it to a list when highlighting a specific part.",from matplotlib.ticker import FuncFormatter ,simple-matplotlib-visualization-tips.ipynb
ax1,"ax[0]. bar(titanic_age['Survival rate']. index , titanic_age['Survival rate'], color = 'gray') ",simple-matplotlib-visualization-tips.ipynb
ax2,color_map =['gray' for _ in range(9 )] ,simple-matplotlib-visualization-tips.ipynb
"TransparencyI told you to use transparency above, but transparency is a great tool.Scatter plots also contain important points with many overlapping points. That s why it s important to know the overlapping data by adjusting transparency.When comparing lineplots or barplots, placing two or more plots together using transparency allows you to hold comparison information.If you add transparency to the graph, you can complete the graph with refined colors. alpha : Parameter name of normal transparency setting ",import seaborn as sns ,simple-matplotlib-visualization-tips.ipynb
You can use sns.scatterplot hue parameter,"ax[1]. scatter(x = 'math score' , y = 'reading score' , data = exam_data[exam_data['gender']== 'male'], color = 'skyblue' , alpha = 0.5 , label = 'Male' , s = 70) ",simple-matplotlib-visualization-tips.ipynb
upper right border remove,plt.gca (). spines['top']. set_visible(False) ,simple-matplotlib-visualization-tips.ipynb
plot with grid,sns.set_style('whitegrid') ,simple-matplotlib-visualization-tips.ipynb
original code : ,"def rating_barplot(data , title , height , h_lim = None): ",simple-matplotlib-visualization-tips.ipynb
"MEME : xkcd theme xkcd : Webcomic for Geeks Depending on the current kaggle and version, the font is broken, but you can draw a graph like this: ",import matplotlib ,simple-matplotlib-visualization-tips.ipynb
Based on Stove Ownership from XKCD by Randall Munroe, fig = plt.figure () ,simple-matplotlib-visualization-tips.ipynb
Based on The Data So Far from XKCD by Randall Munroe, fig = plt.figure () ,simple-matplotlib-visualization-tips.ipynb
"Simple MNIST NN from scratchIn this notebook, I implemented a simple two layer neural network and trained it on the MNIST digit recognizer dataset. It s meant to be an instructional example, through which you can understand the underlying math of neural networks better.Here s a video I made explaining all the math and showing my progress as I coded the network: ","import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

data = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')",simple-mnist-nn-from-scratch-numpy-no-tf-keras.ipynb
shuffle before splitting into dev and training sets,np.random.shuffle(data) ,simple-mnist-nn-from-scratch-numpy-no-tf-keras.ipynb
"Our NN will have a simple two layer architecture. Input layer will have 784 units corresponding to the 784 pixels in each 28x28 input image. A hidden layer will have 10 units with ReLU activation, and finally our output layer will have 10 units corresponding to the ten digit classes with softmax activation.Forward propagationBackward propagationParameter updatesVars and shapesForward prop : 784 x m : 10 x m : 10 x 784 as : 10 x 1 : 10 x m : 10 x 10 as : 10 x 1 Backprop : 10 x m : 10 x 10 : 10 x 1 : 10 x m : 10 x 10 : 10 x 1 ","def init_params():
 W1 = np.random.rand(10, 784) - 0.5
 b1 = np.random.rand(10, 1) - 0.5
 W2 = np.random.rand(10, 10) - 0.5
 b2 = np.random.rand(10, 1) - 0.5
 return W1, b1, W2, b2

def ReLU(Z):
 return np.maximum(Z, 0)

def softmax(Z):
 A = np.exp(Z) / sum(np.exp(Z))
 return A
 
def forward_prop(W1, b1, W2, b2, X):
 Z1 = W1.dot(X) + b1
 A1 = ReLU(Z1)
 Z2 = W2.dot(A1) + b2
 A2 = softmax(Z2)
 return Z1, A1, Z2, A2

def ReLU_deriv(Z):
 return Z > 0

def one_hot(Y):
 one_hot_Y = np.zeros((Y.size, Y.max() + 1))
 one_hot_Y[np.arange(Y.size), Y] = 1
 one_hot_Y = one_hot_Y.T
 return one_hot_Y

def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):
 one_hot_Y = one_hot(Y)
 dZ2 = A2 - one_hot_Y
 dW2 = 1 / m * dZ2.dot(A1.T)
 db2 = 1 / m * np.sum(dZ2)
 dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)
 dW1 = 1 / m * dZ1.dot(X.T)
 db1 = 1 / m * np.sum(dZ1)
 return dW1, db1, dW2, db2

def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):
 W1 = W1 - alpha * dW1
 b1 = b1 - alpha * db1 
 W2 = W2 - alpha * dW2 
 b2 = b2 - alpha * db2 
 return W1, b1, W2, b2",simple-mnist-nn-from-scratch-numpy-no-tf-keras.ipynb
85 accuracy on training set.,"def make_predictions(X, W1, b1, W2, b2):
 _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)
 predictions = get_predictions(A2)
 return predictions

def test_prediction(index, W1, b1, W2, b2):
 current_image = X_train[:, index, None]
 prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)
 label = Y_train[index]
 print(""Prediction: "", prediction)
 print(""Label: "", label)
 
 current_image = current_image.reshape((28, 28)) * 255
 plt.gray()
 plt.imshow(current_image, interpolation='nearest')
 plt.show()",simple-mnist-nn-from-scratch-numpy-no-tf-keras.ipynb
Let s look at a couple of examples:,"test_prediction(0, W1, b1, W2, b2)
test_prediction(1, W1, b1, W2, b2)
test_prediction(2, W1, b1, W2, b2)
test_prediction(3, W1, b1, W2, b2)",simple-mnist-nn-from-scratch-numpy-no-tf-keras.ipynb
"Finally, let s find the accuracy on the dev set:","dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)
get_accuracy(dev_predictions, Y_dev)",simple-mnist-nn-from-scratch-numpy-no-tf-keras.ipynb
"SPACESHIP TITANIC Intergalactic flight is a hypothetical travel between galaxies with or without a crew. Because of the vast distances between our own galaxy, the Milky Way, and even its closest neighbors hundreds of thousands and millions of light years any such undertaking would be far more technologically complex than even interstellar travel. Intergalactic distances are about a hundred thousand times five orders of magnitude greater than their interstellar counterparts.The technology needed to travel between galaxies is far beyond humanity s current capabilities and is currently only the subject of speculation, conjecture and science fiction. However, there is nothing in the theory to definitively indicate that intergalactic travel is impossible. There are several proposed methods for carrying out such travel, and to date, several scientists have seriously studied intergalactic travel. TABLE OF CONTENTS 1. IMPORTING LIBRARIES AND LOADING DATA2. DATA INFORMATION3. EXPLORATORY DATA ANALYSIS 4. FEATURE ENGENEERING 5. MACHINE LEARNING6. WHAT NOW? IMPORTING LIBRARIES AND LOADING DATA","import pandas as pd
import numpy as np
from scipy import stats

import seaborn as sns
import matplotlib.pyplot as plt
from plotly.subplots import make_subplots

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OrdinalEncoder
from sklearn import preprocessing
from sklearn.metrics import accuracy_score
import missingno as msno
import warnings
warnings.filterwarnings('ignore')
import plotly
import plotly.graph_objs as go
import plotly.express as px
from plotly.subplots import make_subplots
from plotly.offline import iplot, init_notebook_mode",spaceship-eda-catboost-with-optuna.ipynb
Let s see what the data is.,train,spaceship-eda-catboost-with-optuna.ipynb
The training dataset is represented by 8693 records across 14 features.,test,spaceship-eda-catboost-with-optuna.ipynb
Let s look at the data types in the training dataset.,train.info(),spaceship-eda-catboost-with-optuna.ipynb
Let s look at the dataset statistics.,train.describe(),spaceship-eda-catboost-with-optuna.ipynb
Let s look at the missing values in the training and test dataset.,"msno.bar(train, figsize = (16,5),color = ""#32CD32"")
plt.show()",spaceship-eda-catboost-with-optuna.ipynb
Let s look at the number of unique values in each of the variables.,"for column_name in train.columns:
 unique_values = len(train[column_name].unique())
 print(""Feature '{column_name}' has '{unique_values}' unique values"".format(column_name = column_name,
 unique_values=unique_values))",spaceship-eda-catboost-with-optuna.ipynb
"Let s analyze continuous numerical variables, they include Age , RoomService , FoodCourt , ShoppingMall , Spa .","numeric_features=['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa']",spaceship-eda-catboost-with-optuna.ipynb
Let s build boxplots of these variables for both datasets.,"for column_name in numeric_features:
 fig = go.Figure()
 fig.add_trace(go.Box(y=train[column_name], name=column_name))
 fig.update_layout(title=column_name,
 yaxis_title=""Value"")
 fig.show()",spaceship-eda-catboost-with-optuna.ipynb
Let s analyze the categorical variables in the context of the target variable.,"categorical_features=['HomePlanet','CryoSleep','Destination','VIP','Transported']",spaceship-eda-catboost-with-optuna.ipynb
FEATURE ENGENEERING,train.shape,spaceship-eda-catboost-with-optuna.ipynb
"Let s merge the two tables and remove the variables PassengerId, Cabin, Name.","WITH=pd.concat([train,test])",spaceship-eda-catboost-with-optuna.ipynb
"because the variables HomePlanet,CryoSleep,Destination,VIP are categorical and must be encoded. Because there are missing values in the dataset, we will initially encode them manually, in the future we will apply the OneHot technique.",WITH.Destination.unique(),spaceship-eda-catboost-with-optuna.ipynb
Let s fill in the gaps of categorical variables. Let s look at the number of unique values for each variable again.,"for column_name in WITH.columns:
 unique_values = len(WITH[column_name].unique())
 print(""Feature '{column_name}' has '{unique_values}' unique values"".format(column_name = column_name,
 unique_values=unique_values))",spaceship-eda-catboost-with-optuna.ipynb
"Let s proceed directly to filling in the gaps of information. Because KNNImputer fills in the average between neighbors, then we set the number of neighbors 1.","imputer = KNNImputer(n_neighbors=1, weights=""uniform"")",spaceship-eda-catboost-with-optuna.ipynb
Filling in gaps completed successfully. Let s encode the categorical features using the OneHot method.,WITH1,spaceship-eda-catboost-with-optuna.ipynb
Divide datasets into training and test.,"train1=WITH1[:8693]
test1=WITH1[8693:]",spaceship-eda-catboost-with-optuna.ipynb
Separation of datasets is done. Combine with the target variable for outlier processing.,y,spaceship-eda-catboost-with-optuna.ipynb
"Let s process the emissions. Because the number of objects in the dataset 8.5k, then it is allowed to delete approximately 100 150 values without losing information. In this process, it is important to understand that individual patterns of information are masked as outliers, so the LocalOutlierFactor method must be applied. Because we are dealing with outliers on signs of purchases, the amount can be increased up to 10 .","clf = LocalOutlierFactor(n_neighbors=50, contamination='auto')
y_pred = clf.fit_predict(train1) ",spaceship-eda-catboost-with-optuna.ipynb
Figure size,"plt.figure(figsize =(6 , 6)) ",spaceship-eda-catboost-with-optuna.ipynb
Pie plot,"train2['Transported']. value_counts (). plot.pie(explode =[0.1 , 0.1], autopct = '%1.1f%%' , shadow = True , textprops = { 'fontsize' : 16 }). set_title(""Target distribution"") ",spaceship-eda-catboost-with-optuna.ipynb
Let s eliminate the slight imbalance of classes by using the SMOTE method.,from imblearn.over_sampling import SMOTE ,spaceship-eda-catboost-with-optuna.ipynb
MACHINE LEARNING,"X_train, X_test, y_train, y_test=train_test_split(features,labels,test_size=0.3,random_state=42)",spaceship-eda-catboost-with-optuna.ipynb
"For further tuning of the model, we will choose the Catboost algorithm.","def objective(trial):
 data, target = features,labels
 train_x, valid_x, train_y, valid_y = train_test_split(features,labels, test_size=0.3)

 param = {
 ""objective"": trial.suggest_categorical(""objective"", [""Logloss"", ""CrossEntropy""]),
 ""colsample_bylevel"": trial.suggest_float(""colsample_bylevel"", 0.01, 0.1),
 ""depth"": trial.suggest_int(""depth"", 1, 12),
 ""boosting_type"": trial.suggest_categorical(""boosting_type"", [""Ordered"", ""Plain""]),
 ""bootstrap_type"": trial.suggest_categorical(
 ""bootstrap_type"", [""Bayesian"", ""Bernoulli"", ""MVS""]
 ),
 ""used_ram_limit"": ""3gb"",
 }

 if param[""bootstrap_type""] == ""Bayesian"":
 param[""bagging_temperature""] = trial.suggest_float(""bagging_temperature"", 0, 10)
 elif param[""bootstrap_type""] == ""Bernoulli"":
 param[""subsample""] = trial.suggest_float(""subsample"", 0.1, 1)

 gbm = CatBoostClassifier(**param)

 gbm.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], verbose=0, early_stopping_rounds=100)

 preds = gbm.predict(valid_x)
 pred_labels = np.rint(preds)
 accuracy = accuracy_score(valid_y, pred_labels)
 return accuracy


if __name__ == ""__main__"":
 study = optuna.create_study(direction=""maximize"")
 study.optimize(objective, n_trials=100, timeout=600)

 print(""Number of finished trials: {}"".format(len(study.trials)))

 print(""Best trial:"")
 trial = study.best_trial

 print("" Value: {}"".format(trial.value))

 print("" Params: "")
 for key, value in trial.params.items():
 print("" {}: {}"".format(key, value))",spaceship-eda-catboost-with-optuna.ipynb
Let s visualize the best solution.,"clf=CatBoostClassifier()
parametres={'objective': ['Logloss'], 'colsample_bylevel': [0.09694924732790938], 'depth': [8], 'boosting_type': ['Plain'], 'bootstrap_type': ['Bernoulli'], 'subsample': [0.7169320988219898]}
grid_search_cv_clf=GridSearchCV(clf,parametres,cv=5)
grid_search_cv_clf.fit(X_train,y_train)
best_clf2=grid_search_cv_clf.best_estimator_
y_pred2=best_clf2.predict(X_test)
print(f'Accuracy_score: {accuracy_score(y_test,y_pred2)}')",spaceship-eda-catboost-with-optuna.ipynb
Setting UP,"import numpy as np
import pandas as pd

import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import rcParams

import warnings
warnings.filterwarnings(action='ignore')

import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go

from sklearn.preprocessing import PowerTransformer

sns.set(style=""ticks"", context=""talk"",font_scale = 1)
plt.style.use(""dark_background"")",spaceship-sos-decision-is-always-difficult.ipynb
"Anomaly DetectionPicture Credit: proceeding with EDA in earnest, anomaly detection is performed with a dataset. And, through this, we want to check which cases are judged as outliers and gain insight into EDA.",from pycaret.anomaly import *,spaceship-sos-decision-is-always-difficult.ipynb
Extracting top 5 outliers,"knn_df = pycaret.anomaly.assign_model(knn)
abnormal_data = knn_df[knn_df.Anomaly == 1].sort_values(by='Anomaly_Score', ascending=False)
print(""the size of anomaly = "",len(abnormal_data))
abnormal_data.head().style.set_properties(**{'background-color': 'black',
 'color': 'white',
 'border-color': 'white'})",spaceship-sos-decision-is-always-difficult.ipynb
"Observation: There are a total of 435 outliers. In cases where HomePlanet is Europa and Destination is 55 Cancri e, there are many cases where anomaly was decided.","plt.style.use(""dark_background"")
plot_model(knn,plot='umap')",spaceship-sos-decision-is-always-difficult.ipynb
EDA,"space_df.head().T.style.set_properties(**{'background-color': 'black',
 'color': 'white',
 'border-color': 'white'})",spaceship-sos-decision-is-always-difficult.ipynb
Checking Missing ValuesPicture Credit: ,"import missingno as msno
msno.matrix(space_df,color=(0, 0, 0))",spaceship-sos-decision-is-always-difficult.ipynb
Checking Data Type,"plt.figure(figsize = (10,8))
with plt.rc_context({'figure.facecolor':'black'}):
 sns.set(style=""ticks"", context=""talk"",font_scale = 1)
 plt.style.use(""dark_background"")
 ax = space_df.dtypes.value_counts().plot(kind='bar',fontsize=20)
 for p in ax.patches:
 height = p.get_height()
 ax.text(p.get_x()+ p.get_width() / 2., height + 0.1, height, ha = 'center', size = 25)
 sns.despine()",spaceship-sos-decision-is-always-difficult.ipynb
Checking Target Balance,"colors = ['gold', 'mediumturquoise']
labels = ['Not-Transported','Transported']
values = space_df['Transported'].value_counts()/space_df['Transported'].shape[0]

fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])
fig.update_traces(hoverinfo='label+percent', textinfo='percent', textfont_size=20,
 marker=dict(colors=colors, line=dict(color='white', width=0.1)))
fig.update_layout(
 title_text=""Target Blance"",
 title_font_color=""white"",
 legend_title_font_color=""yellow"",
 paper_bgcolor=""black"",
 plot_bgcolor='black',
 font_color=""white"",
)
fig.show()",spaceship-sos-decision-is-always-difficult.ipynb
Let s decide the name as a unique value and drop it.,"space_df.drop(['Name'],axis=1,inplace=True,errors='ignore')",spaceship-sos-decision-is-always-difficult.ipynb
Let s make a group feature.,"def extract_group(s):
 return s.split('_')[1]

space_df['Group'] = space_df['PassengerId'].apply(extract_group).astype(int)",spaceship-sos-decision-is-always-difficult.ipynb
Observation: People in Group 1 were relatively untransported.,"space_df.drop(['PassengerId'],axis=1,inplace=True,errors='ignore')",spaceship-sos-decision-is-always-difficult.ipynb
"HomePlanetThe planet the passenger departed from, typically their planet of permanent residence.",space_df['Has_HomePlanet'] = space_df['HomePlanet'].isnull().astype(int),spaceship-sos-decision-is-always-difficult.ipynb
CryoSleepPicture Credit: whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.,space_df['Has_CryoSleep'] = space_df['CryoSleep'].isnull().astype(int),spaceship-sos-decision-is-always-difficult.ipynb
DestinationThe planet the passenger will be debarking to.,space_df['Has_Destination'] = space_df['Destination'].isnull().astype(int),spaceship-sos-decision-is-always-difficult.ipynb
"CabinThe cabin number where the passenger is staying. Takes the form deck num side, where side can be either P for Port or S for Starboard.","plt.figure(figsize=(15,8))
ax = space_df['Cabin'].value_counts().sort_values(ascending=False)[:10].plot(kind='bar',
 grid = False,
 fontsize=20)
plt.legend(loc = 'upper right')
for p in ax.patches:
 height = p.get_height()
 ax.text(p.get_x()+ p.get_width() / 2., height + 0.1, height, ha = 'center', size = 20)
sns.despine()",spaceship-sos-decision-is-always-difficult.ipynb
Deck,"plt.figure(figsize=(20,8))
sns.set(style=""ticks"", context=""talk"",font_scale = 1)
plt.style.use(""dark_background"")
ax = sns.countplot(x=""Deck"",
 hue=""Transported"", 
 data=space_df[tr_idx])
ax.set_title('Deck/Rate')
for p in ax.patches:
 x, height, width = p.get_x(), p.get_height(), p.get_width()
 ax.text(x + width / 2, height + 40, f'{height / total_cnt * 100:2.1f}%', va='center', ha='center', size=15)
sns.despine()",spaceship-sos-decision-is-always-difficult.ipynb
Side,"plt.figure(figsize=(15,6))
sns.set(style=""ticks"", context=""talk"",font_scale = 1)
plt.style.use(""dark_background"")
ax = sns.countplot(x=""Side"",
 hue=""Transported"", 
 data=space_df[tr_idx])
ax.set_title('Side/Rate')
for p in ax.patches:
 x, height, width = p.get_x(), p.get_height(), p.get_width()
 ax.text(x + width / 2, height + 80, f'{height} / {height / total_cnt * 100:2.1f}%', va='center', ha='center', size=15)
sns.despine()",spaceship-sos-decision-is-always-difficult.ipynb
Observation: There are many cases where people on the S side are transported.,"cat_cols = ['Deck','Num','Side']
space_df[cat_cols].nunique()",spaceship-sos-decision-is-always-difficult.ipynb
Observation: The number of Num Deck levels is large. It seems better to do label encoding than one hot encoding.,"for c in cat_cols:
 from sklearn.preprocessing import LabelEncoder
 le = LabelEncoder()
 space_df[c]= le.fit_transform(space_df[c])",spaceship-sos-decision-is-always-difficult.ipynb
VIPWhether the passenger has paid for special VIP service during the voyage.,space_df['Has_VIP'] = space_df['VIP'].isnull().astype(int),spaceship-sos-decision-is-always-difficult.ipynb
"Numerical FeaturesPicture Credit: Numeric variables have values that describe a measurable quantity as a number, like how many or how much . Therefore numeric variables are quantitative variables. Numeric variables may be further described as either continuous or discrete: A continuous variable is a numeric variable. Observations can take any value between a certain set of real numbers. The value given to an observation for a continuous variable can include values as small as the instrument of measurement allows. Examples of continuous variables include height, time, age, and temperature. A discrete variable is a numeric variable. Observations can take a value based on a count from a set of distinct whole values. A discrete variable cannot take the value of a fraction between one value and the next closest value. Examples of discrete variables include the number of registered cars, number of business locations, and number of children in a family, all of of which measured as whole units i.e. 1, 2, 3 cars . Ref: ","def display_stat(df,feature):
 mean = df[feature].mean()
 std = df[feature].std()
 skew = df[feature].skew()
 kurtosis = df[feature].kurtosis()
 print('mean: {0:.4f}, std: {1:.4f}, skew: {2:.4f}, kurtosis: {3:.4f} '.format(mean, std, skew, kurtosis))",spaceship-sos-decision-is-always-difficult.ipynb
"First, let s check the correlation between Age and other features.","corr=space_df.corr()
sns.set(style=""ticks"", context=""talk"",font_scale = 1)
plt.style.use(""dark_background"")
plt.figure(figsize=(10, 8))
abs(corr['Age']).sort_values()[:-1].plot.barh()
plt.title('Correlation with Age',fontsize=20)
sns.despine()",spaceship-sos-decision-is-always-difficult.ipynb
Observation: The correlation between Age and Deck is higher than other features. ,"plt.figure(figsize=(20,6))
sns.set(style=""ticks"", context=""talk"",font_scale = 1)
plt.style.use(""dark_background"")
plt.subplots_adjust(wspace=0.3)
plt.subplot(1,2,1)
sns.boxenplot(data=space_df, x='Deck',y='Age')
plt.subplot(1,2,2)
sns.regplot(data=space_df, x='Deck',y='Age')",spaceship-sos-decision-is-always-difficult.ipynb
Observation: The people on deck 6 appear to be younger than the people on the other decks.,"plot_histgram(space_df[tr_idx],'Age')
display_stat(space_df[tr_idx],'Age')",spaceship-sos-decision-is-always-difficult.ipynb
Fill in the missing values with the median value for age for each deck.,space_df['Age'] = space_df['Age'].fillna(space_df.groupby('Deck')['Age'].transform('median')),spaceship-sos-decision-is-always-difficult.ipynb
RoomService,space_df['Has_RoomService'] = space_df['RoomService'].isnull().astype(int),spaceship-sos-decision-is-always-difficult.ipynb
Observation: This feature is skewed. It seems necessary to do a non linear transformation.,"pt = PowerTransformer(method='yeo-johnson')
space_df[['RoomService_pt']] = pt.fit_transform(space_df[['RoomService']])",spaceship-sos-decision-is-always-difficult.ipynb
FoodCourt,space_df['Has_FoodCourt'] = space_df['FoodCourt'].isnull().astype(int),spaceship-sos-decision-is-always-difficult.ipynb
Observation: This feature is skewed. It seems necessary to do a non linear transformation.,space_df[['FoodCourt_pt']] = pt.fit_transform(space_df[['FoodCourt']]),spaceship-sos-decision-is-always-difficult.ipynb
ShoppingMall,space_df['Has_ShoppingMall'] = space_df['ShoppingMall'].isnull().astype(int),spaceship-sos-decision-is-always-difficult.ipynb
Observation: This feature is skewed. It seems necessary to do a non linear transformation.,space_df[['ShoppingMall_pt']] = pt.fit_transform(space_df[['ShoppingMall']]),spaceship-sos-decision-is-always-difficult.ipynb
Spa,space_df['Has_Spa'] = space_df['Spa'].isnull().astype(int),spaceship-sos-decision-is-always-difficult.ipynb
Observation: This feature is skewed. It seems necessary to do a non linear transformation.,space_df[['Spa_pt']] = pt.fit_transform(space_df[['Spa']]),spaceship-sos-decision-is-always-difficult.ipynb
VRDeck,space_df['Has_VRDeck'] = space_df['VRDeck'].isnull().astype(int),spaceship-sos-decision-is-always-difficult.ipynb
Observation: This feature is skewed. It seems necessary to do a non linear transformation.,space_df[['VRDeck_pt']] = pt.fit_transform(space_df[['VRDeck']]),spaceship-sos-decision-is-always-difficult.ipynb
Non Linear Transformation,"transform_features = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','TotalSpend']
space_df[transform_features] = pt.fit_transform(space_df[transform_features])",spaceship-sos-decision-is-always-difficult.ipynb
Machine LearningPicture Credit: ,"num_cols = space_df.select_dtypes(exclude = ['object', 'bool']).columns.tolist()",spaceship-sos-decision-is-always-difficult.ipynb
Making Pipeline before TrainingPicture Credit: ,"_ = setup(data = space_df[tr_idx], 
 target = 'Transported',
 numeric_features = num_cols,
 silent = True,
 remove_multicollinearity = True,
 ignore_low_variance = True,
 imputation_type = 'simple',
 categorical_imputation = 'mode',
 numeric_imputation = 'median' )",spaceship-sos-decision-is-always-difficult.ipynb
Comparing Models,"top3 = compare_models(sort='Accuracy',n_select = 3
 ,exclude = ['knn', 'svm','ridge','nb','dummy','qda','xgboost'] )",spaceship-sos-decision-is-always-difficult.ipynb
"Tuning HyperparamtersRef: In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters typically node weights are learned. The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data. The objective function takes a tuple of hyperparameters and returns the associated loss. Cross validation is often used to estimate this generalization performance. Ref: ","tuned_lightgbm = tune_model(lightgbm, 
 optimize = 'Accuracy',
 search_library=""tune-sklearn"",
 search_algorithm=""optuna"",
 early_stopping = True,
 n_iter = 40)",spaceship-sos-decision-is-always-difficult.ipynb
Interpreting Models,"with plt.rc_context({'figure.facecolor':'lightgrey'}):
 interpret_model(catboost)",spaceship-sos-decision-is-always-difficult.ipynb
"Observation: Spa, VRDeck, and RoomService features were judged to be important features.","interpret_model(catboost,plot='pdp',feature='Spa')",spaceship-sos-decision-is-always-difficult.ipynb
"Ensemble Soft Voting Picture Credit: Ensemble learning is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem. Ensemble learning is primarily used to improve the classification, prediction, function approximation, etc. performance of a model, or reduce the likelihood of an unfortunate selection of a poor one. Other applications of ensemble learning include assigning a confidence to the decision made by the model, selecting optimal or near optimal features, data fusion, incremental learning, nonstationary learning and error correcting. This article focuses on classification related applications of ensemble learning, however, all principle ideas described below can be easily generalized to function approximation or prediction type problems as well. Ref: ","blend_soft = blend_models(estimator_list = [catboost,lightgbm], optimize = 'Accuracy',method = 'soft')",spaceship-sos-decision-is-always-difficult.ipynb
"Observation: Boundary decision is an important thing to do with models. Looking at the picture above, it can be seen that our model does its best to determine the boundary. Areas where the model cannot determine the boundary at all are observed. ","plt.figure(figsize=(8, 8))
plot_model(final_model, plot='confusion_matrix')",spaceship-sos-decision-is-always-difficult.ipynb
Core,import numpy as np ,spaceship-titanic-a-complete-guide.ipynb
Sklearn,"from sklearn.model_selection import train_test_split , GridSearchCV , RandomizedSearchCV , StratifiedKFold ",spaceship-titanic-a-complete-guide.ipynb
Models,"from sklearn.linear_model import LinearRegression , LogisticRegression ",spaceship-titanic-a-complete-guide.ipynb
Save to df,train = pd.read_csv('../input/spaceship-titanic/train.csv') ,spaceship-titanic-a-complete-guide.ipynb
Shape and preview,"print('Train set shape:' , train.shape) ",spaceship-titanic-a-complete-guide.ipynb
Missing values,"print('TRAIN SET MISSING VALUES:')
print(train.isna().sum())
print('')
print('TEST SET MISSING VALUES:')
print(test.isna().sum())",spaceship-titanic-a-complete-guide.ipynb
Duplicates,"print(f'Duplicates in train set: {train.duplicated().sum()}, ({np.round(100*train.duplicated().sum()/len(train),1)}%)')
print('')
print(f'Duplicates in test set: {test.duplicated().sum()}, ({np.round(100*test.duplicated().sum()/len(test),1)}%)')",spaceship-titanic-a-complete-guide.ipynb
"There are 6 continuous features, 4 categorical features excluding the target and 3 descriptive qualitative features.",train.nunique(),spaceship-titanic-a-complete-guide.ipynb
Data types,train.dtypes,spaceship-titanic-a-complete-guide.ipynb
Figure size,"plt.figure(figsize =(6 , 6)) ",spaceship-titanic-a-complete-guide.ipynb
Pie plot,"train['Transported']. value_counts (). plot.pie(explode =[0.1 , 0.1], autopct = '%1.1f%%' , shadow = True , textprops = { 'fontsize' : 16 }). set_title(""Target distribution"") ",spaceship-titanic-a-complete-guide.ipynb
Figure size,"plt.figure(figsize =(10 , 4)) ",spaceship-titanic-a-complete-guide.ipynb
Histogram,"sns.histplot(data = train , x = 'Age' , hue = 'Transported' , binwidth = 1 , kde = True) ",spaceship-titanic-a-complete-guide.ipynb
Aesthetics,plt.title('Age distribution') ,spaceship-titanic-a-complete-guide.ipynb
Expenditure features,"exp_feats =['RoomService' , 'FoodCourt' , 'ShoppingMall' , 'Spa' , 'VRDeck'] ",spaceship-titanic-a-complete-guide.ipynb
Plot expenditure features,"fig = plt.figure(figsize =(10 , 20)) ",spaceship-titanic-a-complete-guide.ipynb
Left plot," ax = fig.add_subplot(5 , 2 , 2 * i + 1) ",spaceship-titanic-a-complete-guide.ipynb
Right plot truncated ," ax = fig.add_subplot(5 , 2 , 2 * i + 2) ",spaceship-titanic-a-complete-guide.ipynb
Improves appearance a bit,fig.tight_layout () ,spaceship-titanic-a-complete-guide.ipynb
Categorical features,"cat_feats =['HomePlanet' , 'CryoSleep' , 'Destination' , 'VIP'] ",spaceship-titanic-a-complete-guide.ipynb
Plot categorical features,"fig = plt.figure(figsize =(10 , 16)) ",spaceship-titanic-a-complete-guide.ipynb
Improves appearance a bit,fig.tight_layout () ,spaceship-titanic-a-complete-guide.ipynb
Qualitative features,"qual_feats =['PassengerId' , 'Cabin' , 'Name'] ",spaceship-titanic-a-complete-guide.ipynb
Preview qualitative features,train[qual_feats]. head () ,spaceship-titanic-a-complete-guide.ipynb
New features training set,train['Age_group']= np.nan ,spaceship-titanic-a-complete-guide.ipynb
New features test set,test['Age_group']= np.nan ,spaceship-titanic-a-complete-guide.ipynb
Plot distribution of new features,"plt.figure(figsize =(10 , 4)) ",spaceship-titanic-a-complete-guide.ipynb
New features training set,train['Expenditure']= train[exp_feats]. sum(axis = 1) ,spaceship-titanic-a-complete-guide.ipynb
New features test set,test['Expenditure']= test[exp_feats]. sum(axis = 1) ,spaceship-titanic-a-complete-guide.ipynb
Plot distribution of new features,"fig = plt.figure(figsize =(12 , 4)) ",spaceship-titanic-a-complete-guide.ipynb
New feature Group,train['Group']= train['PassengerId']. apply(lambda x : x.split('_')[ 0]).astype(int) ,spaceship-titanic-a-complete-guide.ipynb
New feature Group size,"train['Group_size']= train['Group']. map(lambda x : pd.concat ([train['Group'], test['Group']]). value_counts ()[ x]) ",spaceship-titanic-a-complete-guide.ipynb
Plot distribution of new features,"plt.figure(figsize =(20 , 4)) ",spaceship-titanic-a-complete-guide.ipynb
New feature,train['Solo']=(train['Group_size']== 1). astype(int) ,spaceship-titanic-a-complete-guide.ipynb
New feature distribution,"plt.figure(figsize =(10 , 4)) ",spaceship-titanic-a-complete-guide.ipynb
Replace NaN s with outliers for now so we can split feature ,"train['Cabin']. fillna('Z/9999/Z' , inplace = True) ",spaceship-titanic-a-complete-guide.ipynb
New features training set,train['Cabin_deck']= train['Cabin']. apply(lambda x : x.split('/')[ 0]) ,spaceship-titanic-a-complete-guide.ipynb
New features test set,test['Cabin_deck']= test['Cabin']. apply(lambda x : x.split('/')[ 0]) ,spaceship-titanic-a-complete-guide.ipynb
Put Nan s back in we will fill these later ,"train.loc[train['Cabin_deck']== 'Z' , 'Cabin_deck']= np.nan ",spaceship-titanic-a-complete-guide.ipynb
Drop Cabin we don t need it anymore ,"train.drop('Cabin' , axis = 1 , inplace = True) ",spaceship-titanic-a-complete-guide.ipynb
Plot distribution of new features,"fig = plt.figure(figsize =(10 , 12)) ",spaceship-titanic-a-complete-guide.ipynb
one hot encoding,train['Cabin_region1']=(train['Cabin_number']< 300). astype(int) ,spaceship-titanic-a-complete-guide.ipynb
one hot encoding,test['Cabin_region1']=(test['Cabin_number']< 300). astype(int) ,spaceship-titanic-a-complete-guide.ipynb
Plot distribution of new features,"plt.figure(figsize =(10 , 4)) ",spaceship-titanic-a-complete-guide.ipynb
Replace NaN s with outliers for now so we can split feature ,"train['Name']. fillna('Unknown Unknown' , inplace = True) ",spaceship-titanic-a-complete-guide.ipynb
New feature Surname,train['Surname']= train['Name']. str.split (). str[- 1] ,spaceship-titanic-a-complete-guide.ipynb
New feature Family size,"train['Family_size']= train['Surname']. map(lambda x : pd.concat ([train['Surname'], test['Surname']]). value_counts ()[ x]) ",spaceship-titanic-a-complete-guide.ipynb
Put Nan s back in we will fill these later ,"train.loc[train['Surname']== 'Unknown' , 'Surname']= np.nan ",spaceship-titanic-a-complete-guide.ipynb
Drop name we don t need it anymore ,"train.drop('Name' , axis = 1 , inplace = True) ",spaceship-titanic-a-complete-guide.ipynb
New feature distribution,"plt.figure(figsize =(12 , 4)) ",spaceship-titanic-a-complete-guide.ipynb
Labels and features,y = train['Transported']. copy (). astype(int) ,spaceship-titanic-a-complete-guide.ipynb
Concatenate dataframes,"data = pd.concat ([X , test], axis = 0). reset_index(drop = True) ",spaceship-titanic-a-complete-guide.ipynb
Columns with missing values,na_cols = data.columns[data.isna (). any()]. tolist () ,spaceship-titanic-a-complete-guide.ipynb
Missing values summary,"mv = pd.DataFrame(data[na_cols]. isna (). sum (), columns =['Number_missing']) ",spaceship-titanic-a-complete-guide.ipynb
Heatmap of missing values,"plt.figure(figsize =(12 , 6)) ",spaceship-titanic-a-complete-guide.ipynb
Countplot of number of missing values by passenger,train['na_count']= train.isna (). sum(axis = 1) ,spaceship-titanic-a-complete-guide.ipynb
Joint distribution of Group and HomePlanet,"GHP_gb = data.groupby (['Group' , 'HomePlanet'])['HomePlanet']. size (). unstack (). fillna(0) ",spaceship-titanic-a-complete-guide.ipynb
Countplot of unique values,sns.countplot(( GHP_gb > 0). sum(axis = 1)) ,spaceship-titanic-a-complete-guide.ipynb
Missing values before,HP_bef = data['HomePlanet']. isna (). sum () ,spaceship-titanic-a-complete-guide.ipynb
Passengers with missing HomePlanet and in a group with known HomePlanet,GHP_index = data[data['HomePlanet']. isna()][(data[data['HomePlanet']. isna()][ 'Group']).isin(GHP_gb.index )]. index ,spaceship-titanic-a-complete-guide.ipynb
Fill corresponding missing values,"data.loc[GHP_index , 'HomePlanet']= data.iloc[GHP_index , :][ 'Group']. map(lambda x : GHP_gb.idxmax(axis = 1)[ x]) ",spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#HomePlanet missing values before:' , HP_bef) ",spaceship-titanic-a-complete-guide.ipynb
Joint distribution of CabinDeck and HomePlanet,"CDHP_gb = data.groupby (['Cabin_deck' , 'HomePlanet'])['HomePlanet']. size (). unstack (). fillna(0) ",spaceship-titanic-a-complete-guide.ipynb
Heatmap of missing values,"plt.figure(figsize =(10 , 4)) ",spaceship-titanic-a-complete-guide.ipynb
Missing values before,HP_bef = data['HomePlanet']. isna (). sum () ,spaceship-titanic-a-complete-guide.ipynb
"Decks A, B, C or T came from Europa","data.loc[( data['HomePlanet']. isna ()) &(data['Cabin_deck']. isin (['A' , 'B' , 'C' , 'T'])), 'HomePlanet']= 'Europa' ",spaceship-titanic-a-complete-guide.ipynb
Deck G came from Earth,"data.loc[( data['HomePlanet']. isna ()) &(data['Cabin_deck']== 'G'), 'HomePlanet']= 'Earth' ",spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#HomePlanet missing values before:' , HP_bef) ",spaceship-titanic-a-complete-guide.ipynb
Joint distribution of Surname and HomePlanet,"SHP_gb = data.groupby (['Surname' , 'HomePlanet'])['HomePlanet']. size (). unstack (). fillna(0) ",spaceship-titanic-a-complete-guide.ipynb
Countplot of unique values,"plt.figure(figsize =(10 , 4)) ",spaceship-titanic-a-complete-guide.ipynb
Missing values before,HP_bef = data['HomePlanet']. isna (). sum () ,spaceship-titanic-a-complete-guide.ipynb
Passengers with missing HomePlanet and in a family with known HomePlanet,SHP_index = data[data['HomePlanet']. isna()][(data[data['HomePlanet']. isna()][ 'Surname']).isin(SHP_gb.index )]. index ,spaceship-titanic-a-complete-guide.ipynb
Fill corresponding missing values,"data.loc[SHP_index , 'HomePlanet']= data.iloc[SHP_index , :][ 'Surname']. map(lambda x : SHP_gb.idxmax(axis = 1)[ x]) ",spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#HomePlanet missing values before:' , HP_bef) ",spaceship-titanic-a-complete-guide.ipynb
Only 10 HomePlanet missing values left let s look at them,"data[data['HomePlanet']. isna()][['PassengerId' , 'HomePlanet' , 'Destination']] ",spaceship-titanic-a-complete-guide.ipynb
Joint distribution of HomePlanet and Destination,"HPD_gb = data.groupby (['HomePlanet' , 'Destination'])['Destination']. size (). unstack (). fillna(0) ",spaceship-titanic-a-complete-guide.ipynb
Heatmap of missing values,"plt.figure(figsize =(10 , 4)) ",spaceship-titanic-a-complete-guide.ipynb
Missing values before,HP_bef = data['HomePlanet']. isna (). sum () ,spaceship-titanic-a-complete-guide.ipynb
Fill remaining HomePlanet missing values with Earth if not on deck D or Mars if on Deck D ,"data.loc[( data['HomePlanet']. isna ()) & ~(data['Cabin_deck']== 'D'), 'HomePlanet']= 'Earth' ",spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#HomePlanet missing values before:' , HP_bef) ",spaceship-titanic-a-complete-guide.ipynb
Missing values before,D_bef = data['Destination']. isna (). sum () ,spaceship-titanic-a-complete-guide.ipynb
Fill missing Destination values with mode,"data.loc[( data['Destination']. isna ()) , 'Destination']= 'TRAPPIST-1e' ",spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#Destination missing values before:' , D_bef) ",spaceship-titanic-a-complete-guide.ipynb
Joint distribution of Group and Surname,"GSN_gb = data[data['Group_size']> 1]. groupby (['Group' , 'Surname'])['Surname']. size (). unstack (). fillna(0) ",spaceship-titanic-a-complete-guide.ipynb
Countplot of unique values,"plt.figure(figsize =(10 , 4)) ",spaceship-titanic-a-complete-guide.ipynb
Missing values before,SN_bef = data['Surname']. isna (). sum () ,spaceship-titanic-a-complete-guide.ipynb
Passengers with missing Surname and in a group with known majority Surname,GSN_index = data[data['Surname']. isna()][(data[data['Surname']. isna()][ 'Group']).isin(GSN_gb.index )]. index ,spaceship-titanic-a-complete-guide.ipynb
Fill corresponding missing values,"data.loc[GSN_index , 'Surname']= data.iloc[GSN_index , :][ 'Group']. map(lambda x : GSN_gb.idxmax(axis = 1)[ x]) ",spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#Surname missing values before:' , SN_bef) ",spaceship-titanic-a-complete-guide.ipynb
Replace NaN s with outliers so we can use map ,"data['Surname']. fillna('Unknown' , inplace = True) ",spaceship-titanic-a-complete-guide.ipynb
Update family size feature,data['Family_size']= data['Surname']. map(lambda x : data['Surname']. value_counts ()[ x]) ,spaceship-titanic-a-complete-guide.ipynb
Put NaN s back in place of outliers,"data.loc[data['Surname']== 'Unknown' , 'Surname']= np.nan ",spaceship-titanic-a-complete-guide.ipynb
Say unknown surname means no family,"data.loc[data['Family_size']> 100 , 'Family_size']= 0 ",spaceship-titanic-a-complete-guide.ipynb
Joint distribution of Group and Cabin features,"GCD_gb = data[data['Group_size']> 1]. groupby (['Group' , 'Cabin_deck'])['Cabin_deck']. size (). unstack (). fillna(0) ",spaceship-titanic-a-complete-guide.ipynb
Countplots,"fig = plt.figure(figsize =(16 , 4)) ",spaceship-titanic-a-complete-guide.ipynb
Missing values before,CS_bef = data['Cabin_side']. isna (). sum () ,spaceship-titanic-a-complete-guide.ipynb
Passengers with missing Cabin side and in a group with known Cabin side,GCS_index = data[data['Cabin_side']. isna()][(data[data['Cabin_side']. isna()][ 'Group']).isin(GCS_gb.index )]. index ,spaceship-titanic-a-complete-guide.ipynb
Fill corresponding missing values,"data.loc[GCS_index , 'Cabin_side']= data.iloc[GCS_index , :][ 'Group']. map(lambda x : GCS_gb.idxmax(axis = 1)[ x]) ",spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#Cabin_side missing values before:' , CS_bef) ",spaceship-titanic-a-complete-guide.ipynb
Joint distribution of Surname and Cabin side,"SCS_gb = data[data['Group_size']> 1]. groupby (['Surname' , 'Cabin_side'])['Cabin_side']. size (). unstack (). fillna(0) ",spaceship-titanic-a-complete-guide.ipynb
Ratio of sides,SCS_gb['Ratio']= SCS_gb['P']/(SCS_gb['P']+ SCS_gb['S']) ,spaceship-titanic-a-complete-guide.ipynb
Histogram of ratio,"plt.figure(figsize =(10 , 4)) ",spaceship-titanic-a-complete-guide.ipynb
Print proportion,"print('Percentage of families all on the same cabin side:' , 100 * np.round(( SCS_gb['Ratio']. isin ([0 , 1])). sum ()/ len(SCS_gb), 3), '%') ",spaceship-titanic-a-complete-guide.ipynb
Another view of the same information,SCS_gb.head () ,spaceship-titanic-a-complete-guide.ipynb
Missing values before,CS_bef = data['Cabin_side']. isna (). sum () ,spaceship-titanic-a-complete-guide.ipynb
Drop ratio column,"SCS_gb.drop('Ratio' , axis = 1 , inplace = True) ",spaceship-titanic-a-complete-guide.ipynb
Passengers with missing Cabin side and in a family with known Cabin side,SCS_index = data[data['Cabin_side']. isna()][(data[data['Cabin_side']. isna()][ 'Surname']).isin(SCS_gb.index )]. index ,spaceship-titanic-a-complete-guide.ipynb
Fill corresponding missing values,"data.loc[SCS_index , 'Cabin_side']= data.iloc[SCS_index , :][ 'Surname']. map(lambda x : SCS_gb.idxmax(axis = 1)[ x]) ",spaceship-titanic-a-complete-guide.ipynb
Drop surname we don t need it anymore ,"data.drop('Surname' , axis = 1 , inplace = True) ",spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#Cabin_side missing values before:' , CS_bef) ",spaceship-titanic-a-complete-guide.ipynb
Value counts,data['Cabin_side']. value_counts () ,spaceship-titanic-a-complete-guide.ipynb
Missing values before,CS_bef = data['Cabin_side']. isna (). sum () ,spaceship-titanic-a-complete-guide.ipynb
Fill remaining missing values with outlier,"data.loc[data['Cabin_side']. isna (), 'Cabin_side']= 'Z' ",spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#Cabin_side missing values before:' , CS_bef) ",spaceship-titanic-a-complete-guide.ipynb
Missing values before,CD_bef = data['Cabin_deck']. isna (). sum () ,spaceship-titanic-a-complete-guide.ipynb
Passengers with missing Cabin deck and in a group with known majority Cabin deck,GCD_index = data[data['Cabin_deck']. isna()][(data[data['Cabin_deck']. isna()][ 'Group']).isin(GCD_gb.index )]. index ,spaceship-titanic-a-complete-guide.ipynb
Fill corresponding missing values,"data.loc[GCD_index , 'Cabin_deck']= data.iloc[GCD_index , :][ 'Group']. map(lambda x : GCD_gb.idxmax(axis = 1)[ x]) ",spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#Cabin_deck missing values before:' , CD_bef) ",spaceship-titanic-a-complete-guide.ipynb
Joint distribution,"data.groupby (['HomePlanet' , 'Destination' , 'Solo' , 'Cabin_deck'])['Cabin_deck']. size (). unstack (). fillna(0) ",spaceship-titanic-a-complete-guide.ipynb
Missing values before,CD_bef = data['Cabin_deck']. isna (). sum () ,spaceship-titanic-a-complete-guide.ipynb
Fill missing values using the mode,"na_rows_CD = data.loc[data['Cabin_deck']. isna (), 'Cabin_deck']. index ",spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#Cabin_deck missing values before:' , CD_bef) ",spaceship-titanic-a-complete-guide.ipynb
Scatterplot,"plt.figure(figsize =(10 , 4)) ",spaceship-titanic-a-complete-guide.ipynb
Missing values before,CN_bef = data['Cabin_number']. isna (). sum () ,spaceship-titanic-a-complete-guide.ipynb
Extrapolate linear relationship on a deck by deck basis,"for deck in['A' , 'B' , 'C' , 'D' , 'E' , 'F' , 'G']: ",spaceship-titanic-a-complete-guide.ipynb
Features and labels," X_CN = data.loc[~(data['Cabin_number']. isna ()) &(data['Cabin_deck']== deck), 'Group'] ",spaceship-titanic-a-complete-guide.ipynb
Linear regression, model_CN = LinearRegression () ,spaceship-titanic-a-complete-guide.ipynb
Fill missing values with predictions," data.loc[( data['Cabin_number']. isna ()) &(data['Cabin_deck']== deck), 'Cabin_number']= preds_CN.astype(int) ",spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#Cabin_number missing values before:' , CN_bef) ",spaceship-titanic-a-complete-guide.ipynb
One hot encode cabin regions,data['Cabin_region1']=(data['Cabin_number']< 300). astype(int) ,spaceship-titanic-a-complete-guide.ipynb
VIP is a highly unbalanced binary feature so we will just impute the mode.,data['VIP'].value_counts(),spaceship-titanic-a-complete-guide.ipynb
Missing values before,V_bef = data['VIP']. isna (). sum () ,spaceship-titanic-a-complete-guide.ipynb
Fill missing values with mode,"data.loc[data['VIP']. isna (), 'VIP']= False ",spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#VIP missing values before:' , V_bef) ",spaceship-titanic-a-complete-guide.ipynb
Joint distribution,"data.groupby (['HomePlanet' , 'No_spending' , 'Solo' , 'Cabin_deck'])['Age']. median (). unstack (). fillna(0) ",spaceship-titanic-a-complete-guide.ipynb
Missing values before,A_bef = data[exp_feats]. isna (). sum (). sum () ,spaceship-titanic-a-complete-guide.ipynb
Fill missing values using the median,"na_rows_A = data.loc[data['Age']. isna (), 'Age']. index ",spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#Age missing values before:' , A_bef) ",spaceship-titanic-a-complete-guide.ipynb
Update age group feature,"data.loc[data['Age']<= 12 , 'Age_group']= 'Age_0-12' ",spaceship-titanic-a-complete-guide.ipynb
Joint distribution,"data.groupby (['No_spending' , 'CryoSleep'])['CryoSleep']. size (). unstack (). fillna(0) ",spaceship-titanic-a-complete-guide.ipynb
Missing values before,CSL_bef = data['CryoSleep']. isna (). sum () ,spaceship-titanic-a-complete-guide.ipynb
Fill missing values using the mode,"na_rows_CSL = data.loc[data['CryoSleep']. isna (), 'CryoSleep']. index ",spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#CryoSleep missing values before:' , CSL_bef) ",spaceship-titanic-a-complete-guide.ipynb
This one makes a lot of sense. We don t expect people in CryoSleep to be able to spend anything.,"print('Maximum expenditure of passengers in CryoSleep:',data.loc[data['CryoSleep']==True,exp_feats].sum(axis=1).max())",spaceship-titanic-a-complete-guide.ipynb
Missing values before,E_bef = data[exp_feats]. isna (). sum (). sum () ,spaceship-titanic-a-complete-guide.ipynb
CryoSleep has no expenditure,for col in exp_feats : ,spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#Expenditure missing values before:' , E_bef) ",spaceship-titanic-a-complete-guide.ipynb
Joint distribution,"data.groupby (['HomePlanet' , 'Solo' , 'Age_group'])['Expenditure']. mean (). unstack (). fillna(0) ",spaceship-titanic-a-complete-guide.ipynb
Missing values before,E_bef = data[exp_feats]. isna (). sum (). sum () ,spaceship-titanic-a-complete-guide.ipynb
Fill remaining missing values using the median,for col in exp_feats : ,spaceship-titanic-a-complete-guide.ipynb
Print number of missing values left,"print('#Expenditure missing values before:' , E_bef) ",spaceship-titanic-a-complete-guide.ipynb
Update expenditure and no spending,data['Expenditure']= data[exp_feats]. sum(axis = 1) ,spaceship-titanic-a-complete-guide.ipynb
Train and test,X = data[data['PassengerId']. isin(train['PassengerId']. values )]. copy () ,spaceship-titanic-a-complete-guide.ipynb
Drop qualitative redundant collinear high cardinality features,"X.drop (['PassengerId' , 'Group' , 'Group_size' , 'Age_group' , 'Cabin_number'], axis = 1 , inplace = True) ",spaceship-titanic-a-complete-guide.ipynb
Plot log transform results,"fig = plt.figure(figsize =(12 , 20)) ",spaceship-titanic-a-complete-guide.ipynb
Apply log transform,"for col in['RoomService' , 'FoodCourt' , 'ShoppingMall' , 'Spa' , 'VRDeck' , 'Expenditure']: ",spaceship-titanic-a-complete-guide.ipynb
Indentify numerical and categorical columns,"numerical_cols =[cname for cname in X.columns if X[cname]. dtype in['int64' , 'float64']] ",spaceship-titanic-a-complete-guide.ipynb
Scale numerical data to have mean 0 and variance 1,"numerical_transformer = Pipeline(steps =[( 'scaler' , StandardScaler ())]) ",spaceship-titanic-a-complete-guide.ipynb
One hot encode categorical data,"categorical_transformer = Pipeline(steps =[( 'onehot' , OneHotEncoder(drop = 'if_binary' , handle_unknown = 'ignore' , sparse = False))]) ",spaceship-titanic-a-complete-guide.ipynb
"Just for fun, let s look at the transformed data in PCA space. This gives a low dimensional representation of the data, which preserves local and global structure.","pca = PCA(n_components=3)
components = pca.fit_transform(X)

total_var = pca.explained_variance_ratio_.sum() * 100

fig = px.scatter_3d(
 components, x=0, y=1, z=2, color=y, size=0.1*np.ones(len(X)), opacity = 1,
 title=f'Total Explained Variance: {total_var:.2f}%',
 labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'},
 width=800, height=500
)
fig.show()",spaceship-titanic-a-complete-guide.ipynb
Explained variance how important each additional principal component is ,pca = PCA (). fit(X) ,spaceship-titanic-a-complete-guide.ipynb
Aesthetics,"plt.ylim(0.0 , 1.1) ",spaceship-titanic-a-complete-guide.ipynb
Train validation split,"X_train , X_valid , y_train , y_valid = train_test_split(X , y , stratify = y , train_size = 0.8 , test_size = 0.2 , random_state = 0) ",spaceship-titanic-a-complete-guide.ipynb
Train models with grid search but no cross validation so it doesn t take too long to get a rough idea of which are the best models for this dataset.,i = 0 ,spaceship-titanic-a-complete-guide.ipynb
Train and score," clf.fit(X_train , y_train) ",spaceship-titanic-a-complete-guide.ipynb
Save trained model, clf_best_params[key]= clf.best_params_ ,spaceship-titanic-a-complete-guide.ipynb
Print iteration and training time, stop = time.time () ,spaceship-titanic-a-complete-guide.ipynb
Show results,valid_scores ,spaceship-titanic-a-complete-guide.ipynb
Show best parameters from grid search,clf_best_params ,spaceship-titanic-a-complete-guide.ipynb
Number of folds in cross validation,FOLDS = 10 ,spaceship-titanic-a-complete-guide.ipynb
10 fold cross validation," cv = StratifiedKFold(n_splits = FOLDS , shuffle = True , random_state = 0) ",spaceship-titanic-a-complete-guide.ipynb
Get training and validation sets," X_train , X_valid = X[train_idx], X[val_idx] ",spaceship-titanic-a-complete-guide.ipynb
Train model, clf = classifier ,spaceship-titanic-a-complete-guide.ipynb
Make predictions and measure accuracy," preds += clf.predict_proba(X_test)[ : , 1] ",spaceship-titanic-a-complete-guide.ipynb
Average accuracy, score = score / FOLDS ,spaceship-titanic-a-complete-guide.ipynb
Stop timer, stop = time.time () ,spaceship-titanic-a-complete-guide.ipynb
Print accuracy and time," print('Model:' , key) ",spaceship-titanic-a-complete-guide.ipynb
Ensemble predictions,preds = preds /(FOLDS * len(best_classifiers)) ,spaceship-titanic-a-complete-guide.ipynb
Let s look at the distribution of the predicted probabilities.,"plt.figure(figsize=(10,4))
sns.histplot(preds, binwidth=0.01, kde=True)
plt.title('Predicted probabilities')
plt.xlabel('Probability')",spaceship-titanic-a-complete-guide.ipynb
Proportion in test set we get from rounding,"print(np.round(100 * np.round(preds). sum ()/ len(preds), 2)) ",spaceship-titanic-a-complete-guide.ipynb
Proportion of predicted positive transported classes,"def preds_prop(preds_arr , thresh): ",spaceship-titanic-a-complete-guide.ipynb
Plot proportions across a range of thresholds,def plot_preds_prop(preds_arr): ,spaceship-titanic-a-complete-guide.ipynb
Array of thresholds," T_array = np.arange(0 , 1 , 0.001) ",spaceship-titanic-a-complete-guide.ipynb
Calculate proportions, prop = np.zeros(len(T_array)) ,spaceship-titanic-a-complete-guide.ipynb
Plot proportions," plt.figure(figsize =(10 , 4)) ",spaceship-titanic-a-complete-guide.ipynb
Experiment with this value, target_prop = 0.519 ,spaceship-titanic-a-complete-guide.ipynb
Find optimal threshold the one that leads to the proportion being closest to target prop , T_opt = T_array[np.abs(prop - target_prop). argmin()] ,spaceship-titanic-a-complete-guide.ipynb
Classify test set using optimal threshold,preds_tuned =(preds >= T_opt). astype(int) ,spaceship-titanic-a-complete-guide.ipynb
Sample submission to get right format ,sub = pd.read_csv('../input/spaceship-titanic/sample_submission.csv') ,spaceship-titanic-a-complete-guide.ipynb
Add predictions,sub['Transported']= preds_tuned ,spaceship-titanic-a-complete-guide.ipynb
Replace 0 to False and 1 to True,"sub = sub.replace({ 0 : False , 1 : True }) ",spaceship-titanic-a-complete-guide.ipynb
Prediction distribution,"plt.figure(figsize =(6 , 6)) ",spaceship-titanic-a-complete-guide.ipynb
Output to csv,"sub.to_csv('submission.csv' , index = False) ",spaceship-titanic-a-complete-guide.ipynb
Check the offical Documentation of LazyPredict here : ,from IPython.display import clear_output ,spaceship-titanic-eda-27-different-models.ipynb
Upgrading pandas,! pip3 install - U pandas ,spaceship-titanic-eda-27-different-models.ipynb
Data Loading and Preparation ,"train = pd.read_csv(""../input/spaceship-titanic/train.csv"")
test = pd.read_csv(""../input/spaceship-titanic/test.csv"")
submission = pd.read_csv(""../input/spaceship-titanic/sample_submission.csv"")

RANDOM_STATE = 12 
FOLDS = 5
STRATEGY = 'median'",spaceship-titanic-eda-27-different-models.ipynb
Below are the first 5 rows of train dataset:,train.head(),spaceship-titanic-eda-27-different-models.ipynb
Column Wise missing values : ,"print(f'\033[94m')
print(train.isna().sum().sort_values(ascending = False))",spaceship-titanic-eda-27-different-models.ipynb
"Below is the basic statistics for each variables which contain information on count, mean, standard deviation, minimum, 1st quartile, median, 3rd quartile and maximum.",train.describe(),spaceship-titanic-eda-27-different-models.ipynb
Quick view of Test Data ,test.head(),spaceship-titanic-eda-27-different-models.ipynb
Column Wise missing values ,"print(f'\033[94m')
print((test.isna().sum().sort_values(ascending = False)))",spaceship-titanic-eda-27-different-models.ipynb
"Below is the basic statistics for each variables which contain information on count, mean, standard deviation, minimum, 1st quartile, median, 3rd quartile and maximum.",test.describe(),spaceship-titanic-eda-27-different-models.ipynb
Quick view of Submission File ,submission.head(),spaceship-titanic-eda-27-different-models.ipynb
Overview of Data ,"train.drop([""PassengerId""] , axis = 1 , inplace = True)
test.drop([""PassengerId""] , axis = 1 , inplace = True)
TARGET = 'Transported'
FEATURES = [col for col in train.columns if col != TARGET]
RANDOM_STATE = 12 ",spaceship-titanic-eda-27-different-models.ipynb
Column wise Null Value Distribution ,"test_null = pd.DataFrame(test.isna().sum())
test_null = test_null.sort_values(by = 0 ,ascending = False)
train_null = pd.DataFrame(train.isna().sum())
train_null = train_null.sort_values(by = 0 ,ascending = False)[:-1]


fig = make_subplots(rows=1, 
 cols=2,
 column_titles = [""Train Data"", ""Test Data""] ,
 x_title=""Missing Values"")

fig.add_trace(go.Bar(x=train_null[0],
 y=train_null.index,
 orientation=""h"",
 marker=dict(color=[n for n in range(12)], 
 line_color='rgb(0,0,0)' , 
 line_width = 2,
 coloraxis=""coloraxis"")),
 1, 1)
fig.add_trace(go.Bar(x=test_null[0], 
 y=test_null.index,
 orientation=""h"",
 marker=dict(color=[n for n in range(12)], 
 line_color='rgb(0,0,0)', 
 line_width = 2,
 coloraxis=""coloraxis"")),
 1, 2)

fig.update_layout(showlegend=False, title_text=""Column wise Null Value Distribution"", title_x=0.5)",spaceship-titanic-eda-27-different-models.ipynb
Row wise Null Value Distribution ,"missing_train_row = train.isna().sum(axis=1)
missing_train_row = pd.DataFrame(missing_train_row.value_counts()/train.shape[0]).reset_index()
missing_test_row = test.isna().sum(axis=1)
missing_test_row = pd.DataFrame(missing_test_row.value_counts()/test.shape[0]).reset_index()
missing_train_row.columns = ['no', 'count']
missing_test_row.columns = ['no', 'count']
missing_train_row[""count""] = missing_train_row[""count""]*100
missing_test_row[""count""] = missing_test_row[""count""]*100


fig = make_subplots(rows=1, 
 cols=2,
 column_titles = [""Train Data"", ""Test Data""] ,
 x_title=""Missing Values"",)

fig.add_trace(go.Bar(x=missing_train_row[""no""], 
 y=missing_train_row[""count""] ,
 marker=dict(color=[n for n in range(4)], 
 line_color='rgb(0,0,0)' ,
 line_width = 3
 ,coloraxis=""coloraxis"")),
 1, 1)
fig.add_trace(go.Bar(x= missing_test_row[""no""], 
 y=missing_test_row[""count""],
 marker=dict(color=[n for n in range(4)], 
 line_color='rgb(0,0,0)',
 line_width = 3 ,
 coloraxis=""coloraxis"")),
 1, 2)
fig.update_layout(showlegend=False, title_text=""Row wise Null Value Distribution"", title_x=0.5)",spaceship-titanic-eda-27-different-models.ipynb
"Observations in Null Value Distribution : Out of 12 features 6 features are continous, 2 features are text data and 4 features are categorical. HomePlanet and Destination have 3 differnt unique values. CryoSleep and VIP are bool features ","df = pd.concat([train[FEATURES], test[FEATURES]], axis=0)
text_features = [""Cabin"", ""Name""]
cat_features = [col for col in FEATURES if df[col].nunique() < 25 and col not in text_features ]
cont_features = [col for col in FEATURES if df[col].nunique() >= 25 and col not in text_features ]

del df
print(f'\033[94mTotal number of features: {len(FEATURES)}')
print(f'\033[94mNumber of categorical features: {len(cat_features)}')
print(f'\033[94mNumber of continuos features: {len(cont_features)}')
print(f'\033[94mNumber of text features: {len(text_features)}')

labels=['Categorical', 'Continuos', ""Text""]
values= [len(cat_features), len(cont_features), len(text_features)]
colors = ['#DE3163', '#58D68D']

fig = go.Figure(data=[go.Pie(
 labels=labels, 
 values=values, pull=[0.1, 0, 0 ],
 marker=dict(colors=colors, 
 line=dict(color='#000000', 
 width=2))
)])
fig.show()",spaceship-titanic-eda-27-different-models.ipynb
Distribution of Age ,"train_age = train.copy()
test_age = test.copy()
train_age[""type""] = ""Train""
test_age[""type""] = ""Test""
ageDf = pd.concat([train_age, test_age])
fig = px.histogram(data_frame = ageDf, 
 x=""Age"",
 color= ""type"",
 color_discrete_sequence = ['#58D68D','#DE3163'],
 marginal=""box"",
 nbins= 100,
 template=""plotly_white""
 )
fig.update_layout(title = ""Distribution of Age"" , title_x = 0.5)
fig.show()",spaceship-titanic-eda-27-different-models.ipynb
Feature Distribution of Categorical Features ,"if len(cat_features) == 0 :
 print(""No Categorical features"")
else:
 ncols = 2
 nrows = 2

 fig, axes = plt.subplots(nrows, ncols, figsize=(18, 10))
 for r in range(nrows):
 for c in range(ncols):
 col = cat_features[r*ncols+c]
 sns.countplot(train[col],ax = axes[r,c] ,palette = ""viridis"", label='Train data')
 sns.countplot(test[col],ax = axes[r,c] ,palette = ""magma"", label='Test data')
 axes[r,c].legend()
 axes[r,c].set_ylabel('')
 axes[r,c].set_xlabel(col, fontsize=20)
 axes[r,c].tick_params(labelsize=10, width=0.5)
 axes[r,c].xaxis.offsetText.set_fontsize(4)
 axes[r,c].yaxis.offsetText.set_fontsize(4)
 plt.show()",spaceship-titanic-eda-27-different-models.ipynb
Observations in Null Value Distribution : There are two target values 0 and 1. Both the target values are almost equally distributed. ,"target_df = pd.DataFrame(train[TARGET].value_counts()).reset_index()
target_df.columns = [TARGET, 'count']
fig = px.bar(data_frame =target_df, 
 x = TARGET,
 y = 'count'
 )
fig.update_traces(marker_color =['#58D68D','#DE3163'], 
 marker_line_color='rgb(0,0,0)',
 marker_line_width=2,)
fig.update_layout(title = ""Target Distribution"",
 template = ""plotly_white"",
 title_x = 0.5)
print(""\033[94mPercentage of Transported = 0: {:.2f} %"".format(target_df[""count""][0] *100 / train.shape[0]))
print(""\033[94mPercentage of Transported = 1: {:.2f} %"".format(target_df[""count""][1]* 100 / train.shape[0]))
fig.show()",spaceship-titanic-eda-27-different-models.ipynb
Correlation matrix ,"fig = px.imshow(train.corr() ,text_auto=True, aspect=""auto"" , color_continuous_scale = ""viridis"")
fig.show()",spaceship-titanic-eda-27-different-models.ipynb
Imputing Missing Values ,"imputer_cols = [""Age"", ""FoodCourt"", ""ShoppingMall"", ""Spa"", ""VRDeck"" ,""RoomService""]
imputer = SimpleImputer(strategy=STRATEGY )
imputer.fit(train[imputer_cols])
train[imputer_cols] = imputer.transform(train[imputer_cols])
test[imputer_cols] = imputer.transform(test[imputer_cols])
train[""HomePlanet""].fillna('Z', inplace=True)
test[""HomePlanet""].fillna('Z', inplace=True)",spaceship-titanic-eda-27-different-models.ipynb
Encoding Categorical Faatures ,"label_cols = [""HomePlanet"", ""CryoSleep"",""Cabin"", ""Destination"" ,""VIP""]
def label_encoder(train,test,columns):
 for col in columns:
 train[col] = train[col].astype(str)
 test[col] = test[col].astype(str)
 train[col] = LabelEncoder().fit_transform(train[col])
 test[col] = LabelEncoder().fit_transform(test[col])
 return train, test

train ,test = label_encoder(train,test ,label_cols)",spaceship-titanic-eda-27-different-models.ipynb
27 Different Classifiers LAZY PREDICT : ,"clf = LazyClassifier(verbose=0,
 ignore_warnings=True,
 custom_metric=None,
 predictions=False,
 random_state=12,
 classifiers='all')

models, predictions = clf.fit(X_train , X_test , y_train , y_test)
clear_output()",spaceship-titanic-eda-27-different-models.ipynb
TOP 15 Models ,models[:15],spaceship-titanic-eda-27-different-models.ipynb
Visualizing Results ,"line = px.line(data_frame= models ,y =[""Accuracy""] , markers = True)
line.update_xaxes(title=""Model"",
 rangeslider_visible = False)
line.update_yaxes(title = ""Accuracy"")
line.update_traces(line_color=""red"")
line.update_layout(showlegend = True,
 title = {
 'text': 'Accuracy vs Model',
 'y':0.94,
 'x':0.5,
 'xanchor': 'center',
 'yanchor': 'top'})

line.show()",spaceship-titanic-eda-27-different-models.ipynb
LGBM Classifier 5 FOLDS : ,"lgb_params = {
 'objective' : 'binary',
 'n_estimators' :50,
 'learning_rate' : 0.08
}

lgb_predictions = 0
lgb_scores = []
lgb_fimp = []
LGBM_FEATURES = list(train.columns)[:-1]
skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)
for fold, (train_idx, valid_idx) in enumerate(skf.split(train[LGBM_FEATURES], train[TARGET])):
 print(f'\033[94m')
 print(10*""="", f""Fold={fold+1}"", 10*""="")
 start_time = time.time()
 
 X_train, X_valid = train.iloc[train_idx][LGBM_FEATURES], train.iloc[valid_idx][LGBM_FEATURES]
 y_train , y_valid = train[TARGET].iloc[train_idx] , train[TARGET].iloc[valid_idx]
 
 model = LGBMClassifier(**lgb_params)
 model.fit(X_train, y_train,verbose=0)
 
 preds_valid = model.predict(X_valid)
 acc = accuracy_score(y_valid, preds_valid)
 lgb_scores.append(acc)
 run_time = time.time() - start_time
 
 fim = pd.DataFrame(index=LGBM_FEATURES,
 data=model.feature_importances_,
 columns=[f'{fold}_importance'])
 lgb_fimp.append(fim)
 
 print(f""Fold={fold+1}, Accuracy score: {acc:.2f}%, Run Time: {run_time:.2f}s"")
 test_preds = model.predict(test[LGBM_FEATURES]) 
 lgb_predictions += test_preds/FOLDS
print("""")
print(""Mean Accuracy :"", np.mean(lgb_scores))",spaceship-titanic-eda-27-different-models.ipynb
Feature Importance ,"lgbm_fis_df = pd.concat(lgb_fimp, axis=1).head(15)
lgbm_fis_df.sort_values('1_importance').plot(kind='barh', figsize=(15, 10),
 title='Feature Importance Across Folds')
plt.show()",spaceship-titanic-eda-27-different-models.ipynb
LGBM Classifier Submission : ,"submission[TARGET] = lgb_predictions.astype(""bool"")
submission.to_csv(""submission.csv"",index=False)
submission.head()",spaceship-titanic-eda-27-different-models.ipynb
Importing Libraries ,"import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn.metrics as metrics
import missingno as msno
import shap
import gc

import wandb
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import confusion_matrix, classification_report
from tqdm.notebook import tqdm

from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold,cross_val_score
from sklearn.metrics import accuracy_score, roc_curve,auc, confusion_matrix,precision_recall_curve,precision_recall_curve,plot_precision_recall_curve

import warnings
warnings.simplefilter('ignore')",spaceship-titanic-eda-pytorch-baseline-w-b.ipynb
"I will be integrating W B for visualizations and logging artifacts!Spaceship Titanic Project on W B Dashboard To get the API key, an account is to be created on the website first. Next, use secrets to use API Keys more securely ","from kaggle_secrets import UserSecretsClient
user_secrets = UserSecretsClient()
api_key = user_secrets.get_secret(""api_key"")

CONFIG = {'competition': 'spaceship', '_wandb_kernel': 'ruch'}

os.environ[""WANDB_SILENT""] = ""true""",spaceship-titanic-eda-pytorch-baseline-w-b.ipynb
Some utility functions,def wandb_log(** kwargs): ,spaceship-titanic-eda-pytorch-baseline-w-b.ipynb
What does the data look like? ,"train_data = pd.read_csv(""../input/spaceship-titanic/train.csv"")
print(train_data.shape)
train_data.head()",spaceship-titanic-eda-pytorch-baseline-w-b.ipynb
Missing values ,"msno.bar(train_data,color=space[2], sort=""ascending"", figsize=(10,5), fontsize=12)
plt.show()",spaceship-titanic-eda-pytorch-baseline-w-b.ipynb
 Function to plot wandb bar chart ,"def plot_wb_bar(df , col1 , col2 , title): ",spaceship-titanic-eda-pytorch-baseline-w-b.ipynb
 Function to create a dataframe of value counts ,"def count_values(df , col , top = False): ",spaceship-titanic-eda-pytorch-baseline-w-b.ipynb
normalizing the values to get a range of colours," norm = plt.Normalize(values.min (), values.max ()) ",spaceship-titanic-eda-pytorch-baseline-w-b.ipynb
range of colours from colourmap rainbow, colors = plt.cm.rainbow(norm(values)) ,spaceship-titanic-eda-pytorch-baseline-w-b.ipynb
set colour for each patch," for patch , color in zip(ax.patches , colors): ",spaceship-titanic-eda-pytorch-baseline-w-b.ipynb
Data pre processing ,"train_data['is_train'] = True
test_data['is_train'] = False

df = pd.concat([train_data, test_data])",spaceship-titanic-eda-pytorch-baseline-w-b.ipynb
Start W B logging,"train_data = train_data.dropna()
train_data.drop([""PassengerId"",""Cabin"",""Name""], axis=1, inplace=True)
test_data.drop([""PassengerId"",""Cabin"",""Name""], axis=1, inplace=True)
train_data['Transported'] = train_data['Transported'].map({True: 1, False: 0})",spaceship-titanic-eda-pytorch-baseline-w-b.ipynb
Model Architecture," self.fc1 = nn.Linear(self.input_size , 1024) ",spaceship-titanic-eda-pytorch-baseline-w-b.ipynb
Finish the logging run,run.finish () ,spaceship-titanic-eda-pytorch-baseline-w-b.ipynb
Submission time! ,"passenger_IDs = pd.read_csv(""../input/spaceship-titanic/sample_submission.csv"")[[""PassengerId""]].values

df = {'PassengerId': passenger_IDs.ravel(), 'Transported': preds}
df_predictions = pd.DataFrame(df).set_index(['PassengerId'])
df_predictions.head(10)",spaceship-titanic-eda-pytorch-baseline-w-b.ipynb
2 IMPORT NECESSARY LIBRARIES,"import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats
import matplotlib
import matplotlib.pyplot as plt
import missingno as msno
%matplotlib inline

import random
from nltk.corpus import names
import nltk
nltk.download(""names"")
from nltk import NaiveBayesClassifier

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.metrics import roc_auc_score,roc_curve
from sklearn.metrics import plot_confusion_matrix
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from collections import Counter


from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier

matplotlib.rc(""xtick"", labelsize = 14) 
matplotlib.rc(""ytick"", labelsize = 14)

from sklearn import set_config
set_config(print_changed_only = False)

import warnings
warnings.filterwarnings('ignore')",spaceship-titanic-eda-xgboost-80.ipynb
3 LOAD DATASETS,"train_data = pd.read_csv(""../input/spaceship-titanic/train.csv"")
test_data = pd.read_csv(""../input/spaceship-titanic/test.csv"")",spaceship-titanic-eda-xgboost-80.ipynb
4.1 Look at train dataset,"train_data.head().style.background_gradient(cmap = ""magma"")",spaceship-titanic-eda-xgboost-80.ipynb
4.2 Look at test dataset,"test_data.head().style.background_gradient(cmap = ""rocket"")",spaceship-titanic-eda-xgboost-80.ipynb
4.3 Change column names of train and test datasets,"datasets = [train_data, test_data]
for data in datasets:
 data.rename(columns = {""PassengerId"" : ""id"", ""HomePlanet"" : ""home_planet"", 
 ""CryoSleep"" : ""cryo_sleep"", ""Cabin"" : ""cabin"", 
 ""Destination"" : ""destination"", ""Age"" : ""age"",
 ""VIP"" : ""vip"", ""RoomService"" : ""room_service"",
 ""FoodCourt"" : ""food_court"", ""ShoppingMall"" : ""shopping_mall"",
 ""Spa"" : ""spa"", ""VRDeck"" : ""vr_deck"", ""Name"" : ""name"",
 ""Transported"" : ""transported""}, inplace = True)",spaceship-titanic-eda-xgboost-80.ipynb
4.4 Change values of some variables,"train_data[""transported""] = train_data[""transported""].replace(to_replace = [False, True], 
 value = [""No"", ""Yes""])
train_data[""vip""] = train_data[""vip""].replace(to_replace = [False, True], 
 value = [""No"", ""Yes""]).astype(""object"")
train_data[""cryo_sleep""] = train_data[""cryo_sleep""].replace(to_replace = [False, True], 
 value = [""No"", ""Yes""]).astype(""object"")",spaceship-titanic-eda-xgboost-80.ipynb
4.5 Get the number of rows and columns of train and test datasets,"print(""The number of rows in train data is {0}, and the number of columns in train data is {1}"".
 format(train_data.shape[0], train_data.shape[1]))
 
print(""The number of rows in test data is {0}, and the number of columns in test data is {1}"".
 format(test_data.shape[0], test_data.shape[1]))",spaceship-titanic-eda-xgboost-80.ipynb
4.6 Get common information about train dataset,train_data.info(),spaceship-titanic-eda-xgboost-80.ipynb
4.7 Get descriptive statistics of numeric variables of train dataset,"train_data.describe().T.style.background_gradient(cmap = ""spring"")",spaceship-titanic-eda-xgboost-80.ipynb
4.8 How many passengers have not spent any money?,"not_billed_data = train_data[(train_data[""room_service""] == 0)
 & (train_data[""spa""] == 0)
 & (train_data[""food_court""] == 0) 
 & (train_data[""shopping_mall""] == 0)
 & (train_data[""vr_deck""] == 0)]

mean_age = not_billed_data[""age""].mean().round()
print(""There are {0} passengers on the board of the spaceship who have not spent money, their average age is {1}""
 .format(not_billed_data.shape[0], mean_age))",spaceship-titanic-eda-xgboost-80.ipynb
4.9 Get descriptive statistic of categoric variables of train dataset,"train_data.describe(include = ""object"").T",spaceship-titanic-eda-xgboost-80.ipynb
4.10 Check null values of train dataset,train_data.isnull().sum(),spaceship-titanic-eda-xgboost-80.ipynb
4.11 Visualize missing values,msno.matrix(train_data),spaceship-titanic-eda-xgboost-80.ipynb
4.12 Fill null values with median numeric and frequent values categoric ,"numeric_data =[column for column in train_data.select_dtypes ([""int"" , ""float""])] ",spaceship-titanic-eda-xgboost-80.ipynb
replace missing values in each categorical column with the most frequent value,for col in categoric_data : ,spaceship-titanic-eda-xgboost-80.ipynb
4.13 Check null values again,train_data.isnull().sum().sum() + test_data.isnull().sum().sum(),spaceship-titanic-eda-xgboost-80.ipynb
4.14 Get the names of categoric variables,"print(""Columns in object data type: \n"",
 list((train_data.select_dtypes(""object"").columns)))",spaceship-titanic-eda-xgboost-80.ipynb
4.15 Get the names of numeric variables,"print(""Columns in numeric data type: \n"",
 list((train_data.select_dtypes([""int"", ""float""]).columns)))",spaceship-titanic-eda-xgboost-80.ipynb
4.16 Get class frequencies of some variables,"print(""Class frequencies of 'home_planet' variable: \n\n"",
 train_data[""home_planet""].value_counts())
print(""___________________________________________"")

print(""Class frequencies of 'destination' variable: \n\n"",
 train_data[""destination""].value_counts())",spaceship-titanic-eda-xgboost-80.ipynb
4.17 Check correlation between the variables of train dataset,train_data.corr(),spaceship-titanic-eda-xgboost-80.ipynb
5.1 Histogram,"train_data.hist(bins = 25, figsize = (12, 12))
plt.show()",spaceship-titanic-eda-xgboost-80.ipynb
5.2 Get the number of age periods of passengers,"datasets = [train_data, test_data]
for data in datasets:
 baby = train_data[train_data[""age""] <= 5]
 kid = train_data[(train_data[""age""] > 5) & (train_data[""age""] <= 10)]
 teenager = train_data[(train_data[""age""] > 10) & (train_data[""age""] <= 20)]
 youth = train_data[(train_data[""age""] > 20) & (train_data[""age""] <= 40)]
 adult = train_data[(train_data[""age""] > 40) & (train_data[""age""] <= 60)]
 old = train_data[(train_data[""age""] > 60)]
 
print(""Number of baby passengers: "", baby.shape[0])
print(""Number of kid passengers: "", kid.shape[0])
print(""Number of teenager passengers: "", teenager.shape[0])
print(""Number of youth passengers: "", youth.shape[0])
print(""Number of adult passengers: "", adult.shape[0])
print(""Number of old passengers: "", old.shape[0])",spaceship-titanic-eda-xgboost-80.ipynb
5.3 Barplot,"ages = {""baby"" : baby.shape[0], ""kid"" : kid.shape[0],
 ""teenager"" : teenager.shape[0], ""youth"" : youth.shape[0],
 ""adult"" : adult.shape[0], ""old"" : old.shape[0]}
ages = pd.Series(ages)

plt.figure(figsize = (11, 6))
plt.title(""age categories"", fontsize = 15)
plt.xlabel(""number of passengers"", fontsize = 13)

plt.barh(ages.index, ages.values, color = sns.color_palette(""inferno_r"", 5),
 height = 0.5)
plt.show()",spaceship-titanic-eda-xgboost-80.ipynb
5.4 Catplot,"sns.catplot(x = ""transported"",
 y = ""age"",
 kind = ""box"",
 color = '#100C8E',
 data = train_data).set(title = ""Age and transporting condition of passenger"");",spaceship-titanic-eda-xgboost-80.ipynb
5.5 Regplot,"fig, axes = plt.subplots(2, 2, figsize = (20, 10))
axes = axes.flatten()

sns.regplot(ax = axes[0], x = ""age"", y = ""room_service"", data = train_data);
sns.regplot(ax = axes[1], x = ""age"", y = ""spa"", data = train_data);
sns.regplot(ax = axes[2], x = ""age"", y = ""shopping_mall"", data = train_data);
sns.regplot(ax = axes[3], x = ""age"", y = ""food_court"", data = train_data);",spaceship-titanic-eda-xgboost-80.ipynb
5.6 Heatmap,"plt.figure(figsize = [15, 10], clear = True, facecolor = 'white')
sns.heatmap(train_data.corr(), annot = True, square = False, linewidths = 2,
 linecolor = ""white"", cmap = ""summer"");",spaceship-titanic-eda-xgboost-80.ipynb
5.7 Pandas crosstab,"pd.crosstab(train_data[""vip""], train_data[""transported""],
 normalize = True).plot(kind = ""pie"",
 figsize = (17, 10), subplots = True, stacked=True);",spaceship-titanic-eda-xgboost-80.ipynb
5.8 Visualization with AutoViz,"!pip install autoviz
from autoviz.AutoViz_Class import AutoViz_Class
%matplotlib inline
av = AutoViz_Class()
dftc = av.AutoViz(filename = '', sep = '', dfte = train_data)",spaceship-titanic-eda-xgboost-80.ipynb
"What is ANOVA test? ANOVA stands for Analysis of Variance. It s a statistical test that was developed by Ronald Fisher in 1918 and has been in use ever since. Put simply, ANOVA tells you if there are any statistical differences between the means of three or more independent groups.For more information: ","numeric_data = [column for column in train_data.select_dtypes([""int"", ""float""])]

for column in numeric_data:
 df_anova = train_data[[column,'transported']]
 grouped_anova = df_anova.groupby(['transported'])
 f_value, p_value = stats.f_oneway(grouped_anova.get_group('Yes')[column],
 grouped_anova.get_group('No')[column])
 result = """"
 if p_value < 0.05:
 result = ""{} is important feature for prediction"".format(column)
 else:
 result = ""{} is not an important feature for prediction"".format(column)
 print(result)",spaceship-titanic-eda-xgboost-80.ipynb
7.1 Look at train dataset,train_data.head(),spaceship-titanic-eda-xgboost-80.ipynb
7.2 Managing outliers,"def outlier_detection_train(df, n, columns):
 rows = []
 will_drop_train = []
 for col in columns:
 Q1 = np.nanpercentile(df[col], 25)
 Q3 = np.nanpercentile(df[col], 75)
 IQR = Q3 - Q1
 outlier_point = 1.5 * IQR
 rows.extend(df[(df[col] < Q1 - outlier_point)|(df[col] > Q3 + outlier_point)].index)
 for r, c in Counter(rows).items():
 if c >= n: will_drop_train.append(r)
 return will_drop_train",spaceship-titanic-eda-xgboost-80.ipynb
Create deck and side features from cabin column,"train_data[""deck""]= train_data[""cabin""]. apply(lambda x : str(x). split(""/"")[ 0]) ",spaceship-titanic-eda-xgboost-80.ipynb
Financial situation of passengers,"train_data[""total_bill""]= train_data[""room_service""]+ train_data[""food_court""]+ train_data[""shopping_mall""]+ train_data[""spa""]+ train_data[""vr_deck""] ",spaceship-titanic-eda-xgboost-80.ipynb
create new feature in group from PassengerID variable,"train_data[""group_id""]= train_data[""id""]. apply(lambda x : x.split(""_"")[ 0]) ",spaceship-titanic-eda-xgboost-80.ipynb
get names from the name columns of train data,names_train_data = [] ,spaceship-titanic-eda-xgboost-80.ipynb
define function to get last words from the name,def gender_features(word): ,spaceship-titanic-eda-xgboost-80.ipynb
we use gender features function to extract the features,"featuresets =[( gender_features(n), gender)for(n , gender)in labeled_names] ",spaceship-titanic-eda-xgboost-80.ipynb
create new column called gender ,"train_data[""gender""]= names_gender ",spaceship-titanic-eda-xgboost-80.ipynb
get names from the name columns of test data,names_test_data = [] ,spaceship-titanic-eda-xgboost-80.ipynb
we use gender features function to extract the features,"featuresets =[( gender_features(n), gender)for(n , gender)in labeled_names] ",spaceship-titanic-eda-xgboost-80.ipynb
create new column called gender ,"test_data[""gender""]= names_gender ",spaceship-titanic-eda-xgboost-80.ipynb
7.4 Encoding the variables of train dataset,"lbe = LabelEncoder()
lbe.fit_transform(train_data[""transported""])
train_data[""transported""] = lbe.fit_transform(train_data[""transported""])

y = train_data[""transported""]

train_data = train_data.drop([""id"", ""name"", ""cabin"", ""total_bill"", ""group_id"", ""group_number"", ""transported""], axis = 1)
x = pd.get_dummies(train_data, drop_first = True)",spaceship-titanic-eda-xgboost-80.ipynb
7.5 Get shapes of x and y sets,"print(x.shape)
print(y.shape)",spaceship-titanic-eda-xgboost-80.ipynb
7.6 Look at one hot encoded version of x and y sets,x.head(),spaceship-titanic-eda-xgboost-80.ipynb
7.7 Splitting the train dataset into x train y train and x test y test sets,"x_train, x_test, y_train, y_test = train_test_split(x, y,
 test_size = 0.10,
 shuffle = True,
 random_state = 1)
print(x_train.shape)
print(x_test.shape)",spaceship-titanic-eda-xgboost-80.ipynb
7.8 Standartization,"scaler = StandardScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)",spaceship-titanic-eda-xgboost-80.ipynb
8.1 Build XGBoost model and search best hyperparameters with GridSearchCV method,"'''

xgbc = XGBClassifier()

xgbc_params = {
 ""gamma"": [0.5, 1, 1.5],
 ""subsample"": [0.6, 0.8, 1.0],
 ""colsample_bytree"": [0.6, 0.8, 1.0],
 ""max_depth"": [3, 4, 5],
 ""n_estimators"": [100, 130, 150]
}

xgbc_cv_model = GridSearchCV(xgbc, xgbc_params, cv = 10, n_jobs = -1)
xgbc_cv_model.fit(x_train, y_train)

print(""Best hyperparametres of the model: \n"", xgbc_cv_model.best_params_)
'''",spaceship-titanic-eda-xgboost-80.ipynb
8.3 Make initial prediction,"y_pred = xgbc_tuned.predict(x_test)
accuracy_score(y_test, y_pred)",spaceship-titanic-eda-xgboost-80.ipynb
8.4 Classification report,"print(classification_report(y_test, y_pred))",spaceship-titanic-eda-xgboost-80.ipynb
8.5 ROC AUC,"xgb_roc_auc = roc_auc_score(y_test, xgbc_tuned.predict(x_test))
fpr, tpr, thresholds = roc_curve(y_test, xgbc_tuned.predict_proba(x_test)[:,1])

plt.figure()
plt.plot(fpr, tpr, label = 'AUC (area = %0.2f)' % xgb_roc_auc)
plt.plot([0, 1], [0, 1],'g--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC AUC')
plt.show()",spaceship-titanic-eda-xgboost-80.ipynb
8.6 Confusion matrix,"plot_confusion_matrix(xgbc_tuned,
 x_test,
 y_test,
 cmap = ""summer"",
 normalize = ""true"");",spaceship-titanic-eda-xgboost-80.ipynb
8.7 Build model with full data,"xgbc_model_full = XGBClassifier(gamma = 1.5,
 subsample = 1.0,
 max_depth = 5,
 colsample_bytree = 1.0,
 n_estimators = 100)
xgbc_model_full = xgbc_model_full.fit(x, y)",spaceship-titanic-eda-xgboost-80.ipynb
9.1 Look at test data,test_data.head(),spaceship-titanic-eda-xgboost-80.ipynb
9.2 Change values of some variables,"test_data[""vip""] = test_data[""vip""].replace(to_replace = [False, True],
 value = [""No"", ""Yes""]).astype(""object"")
test_data[""cryo_sleep""] = test_data[""cryo_sleep""].replace(to_replace = [False, True],
 value = [""No"", ""Yes""]).astype(""object"")",spaceship-titanic-eda-xgboost-80.ipynb
9.3 Keep outside id variable,"PassengerID = test_data[""id""]
PassengerID.head()",spaceship-titanic-eda-xgboost-80.ipynb
9.4 One hot encoding test data,"test_data = test_data.drop([""id"", ""total_bill"", ""cabin"", ""group_id"", ""group_number"", ""name""], axis = 1)
test_data = pd.get_dummies(test_data, drop_first = True)",spaceship-titanic-eda-xgboost-80.ipynb
9.5 Make prediction,"y_pred = pd.Series(xgbc_model_full.predict(test_data)).map({0:False, 1:True})",spaceship-titanic-eda-xgboost-80.ipynb
9.6 Create submission file,"submission = pd.DataFrame({""PassengerId"": PassengerID.values, ""Transported"": y_pred})
submission.head()",spaceship-titanic-eda-xgboost-80.ipynb
9.7 Save submission file,"submission.to_csv('submission.csv', index = False)
print(""My competition submission: \n\n"", submission)",spaceship-titanic-eda-xgboost-80.ipynb
Intel Extension for Scikit learn installation:,!pip install scikit-learn-intelex -q --progress-bar off,spaceship-titanic-fast-kernel-using-sklearnex.ipynb
Import Libraries,"import pandas as pd
import numpy as np
import warnings
import gc
from IPython.display import HTML
warnings.filterwarnings(""ignore"")

from math import sin, cos, pi
from timeit import default_timer as timer
import matplotlib.pyplot as plt

random_state = 42",spaceship-titanic-fast-kernel-using-sklearnex.ipynb
Reading Data,"PATH_TRAIN = '../input/spaceship-titanic/train.csv'
PATH_TEST = '../input/spaceship-titanic/test.csv'
PATH_SUBMISSION = '../input/spaceship-titanic/sample_submission.csv'",spaceship-titanic-fast-kernel-using-sklearnex.ipynb
Data Pre Processing,"from sklearn.impute import SimpleImputer

imputer_cols = [""Age"", ""FoodCourt"", ""ShoppingMall"", ""Spa"", ""VRDeck"" ,""RoomService""]
imputer = SimpleImputer(strategy = 'median')

imputer.fit(train_data[imputer_cols])

train_data[imputer_cols] = imputer.transform(train_data[imputer_cols])
test_data[imputer_cols] = imputer.transform(test_data[imputer_cols])

train_data[""HomePlanet""].fillna('Z', inplace=True)
test_data[""HomePlanet""].fillna('Z', inplace=True)",spaceship-titanic-fast-kernel-using-sklearnex.ipynb
Boruta Go to Feature selection,"from boruta import BorutaPy
from sklearn.ensemble import RandomForestClassifier

timeFirstI = timer()
clf = RandomForestClassifier(n_estimators = 200, n_jobs = -1, max_depth = 5)

trans = BorutaPy(clf, random_state = 777)
sel = trans.fit_transform(X.values, y.values)
timeSecondI = timer()",spaceship-titanic-fast-kernel-using-sklearnex.ipynb
Default Scikit learn,"from sklearnex import unpatch_sklearn
unpatch_sklearn()",spaceship-titanic-fast-kernel-using-sklearnex.ipynb
RFE Go to Feature selection,"from sklearnex import patch_sklearn
patch_sklearn()",spaceship-titanic-fast-kernel-using-sklearnex.ipynb
Default Scikit learn,"from sklearnex import unpatch_sklearn
unpatch_sklearn()",spaceship-titanic-fast-kernel-using-sklearnex.ipynb
Shap Go to Feature selection,import shap,spaceship-titanic-fast-kernel-using-sklearnex.ipynb
Default scikit learn,"from sklearnex import unpatch_sklearn
unpatch_sklearn()",spaceship-titanic-fast-kernel-using-sklearnex.ipynb
Modeling,"from catboost import CatBoostClassifier

model = CatBoostClassifier()
model.fit(X_train, y_train)",spaceship-titanic-fast-kernel-using-sklearnex.ipynb
1.0 Installing Required Libraries,"%%capture
!git clone https://github.com/analokmaus/kuma_utils.git",spaceship-titanic-nn-model-feature-eng.ipynb
Install Gokinjo...,! pip install gokinjo ,spaceship-titanic-nn-model-feature-eng.ipynb
2.0 Importing Libraries for the Model,% % time ,spaceship-titanic-nn-model-feature-eng.ipynb
linear algebra,import numpy as np ,spaceship-titanic-nn-model-feature-eng.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,spaceship-titanic-nn-model-feature-eng.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,spaceship-titanic-nn-model-feature-eng.ipynb
"You can also write temporary files to kaggle temp , but they won t be saved outside of the current session","%%capture
from sklearn.preprocessing import LabelEncoder
from gokinjo import knn_kfold_extract
from gokinjo import knn_extract",spaceship-titanic-nn-model-feature-eng.ipynb
3.0 Seeting Notebook Parameters...,% % time ,spaceship-titanic-nn-model-feature-eng.ipynb
I like to disable my Notebook Warnings.,import warnings ,spaceship-titanic-nn-model-feature-eng.ipynb
Amount of data we want to load into the Model...,DATA_ROWS = None ,spaceship-titanic-nn-model-feature-eng.ipynb
"Dataframe, the amount of rows and cols to visualize...",NROWS = 100 ,spaceship-titanic-nn-model-feature-eng.ipynb
Main data location path...,BASE_PATH = '...' ,spaceship-titanic-nn-model-feature-eng.ipynb
"Configure notebook display settings to only use 2 decimal places, tables look nicer.","pd.options.display.float_format = '{:,.2f}'.format ",spaceship-titanic-nn-model-feature-eng.ipynb
4.0 Loading Information from CSV...,"%%time
trn_data = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')
tst_data = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')

sub = pd.read_csv('/kaggle/input/spaceship-titanic/sample_submission.csv')",spaceship-titanic-nn-model-feature-eng.ipynb
5.0 Exploring the Information Available,% % time ,spaceship-titanic-nn-model-feature-eng.ipynb
Explore the shape of the DataFrame...,trn_data.shape ,spaceship-titanic-nn-model-feature-eng.ipynb
Display the first few rows of the DataFrame...,trn_data.head () ,spaceship-titanic-nn-model-feature-eng.ipynb
Display the information from the dataset...,trn_data.info () ,spaceship-titanic-nn-model-feature-eng.ipynb
Checking for empty or NaN values in the dataset by variable...,trn_data.isnull (). sum () ,spaceship-titanic-nn-model-feature-eng.ipynb
6.1 Filling NaNs by Using EDA Insights Age ,% % time ,spaceship-titanic-nn-model-feature-eng.ipynb
Filling NaNs Based on Feature Engineering...,"def fill_nans_by_age(df , age_limit = 13): ",spaceship-titanic-nn-model-feature-eng.ipynb
6.2 Filling NaNs by Using EDA Insights CryoSleep ,% % time ,spaceship-titanic-nn-model-feature-eng.ipynb
Filling NaNs Based on Feature Engineering...,"def fill_nans_by_cryo(df , age_limit = 13): ",spaceship-titanic-nn-model-feature-eng.ipynb
6.3 Creating Age Groups Using EDA Insights Age ,"%%time
def age_groups(df, age_limit = 13):
 df['AgeGroup'] = np.where(df['Age'] < age_limit, 0, 1)
 return df",spaceship-titanic-nn-model-feature-eng.ipynb
"6.7 Extracting Deck, Cabin Number and Side","%%time
def cabin_separation(df):
 '''
 Split the Cabin name into Deck, Number and Side
 
 '''
 
 df['CabinDeck'] = df['Cabin'].str.split('/', expand=True)[0]
 df['CabinNum'] = df['Cabin'].str.split('/', expand=True)[1]
 df['CabinSide'] = df['Cabin'].str.split('/', expand=True)[2]
 
 df.drop(columns = ['Cabin'], inplace = True)
 return df",spaceship-titanic-nn-model-feature-eng.ipynb
6.8 Extracting Family Name and Name,"%%time
def name_ext(df):
 '''
 Split the Name of the passenger into First and Family...
 
 '''
 
 df['FirstName'] = df['Name'].str.split(' ', expand=True)[0]
 df['FamilyName'] = df['Name'].str.split(' ', expand=True)[1]
 df.drop(columns = ['Name'], inplace = True)
 return df",spaceship-titanic-nn-model-feature-eng.ipynb
"6.9 Creating Age Groups, Based on EDA","%%time
def age_groups(df, age_limit = 13):
 df['AgeGroup'] = np.where(df['Age'] < age_limit, 0, 1)
 return df",spaceship-titanic-nn-model-feature-eng.ipynb
6.10 Extracting Group,"def extract_group(df):
 '''
 '''
 df['TravelGroup'] = df['PassengerId'].str.split('_', expand = True)[0]
 df['TravelGroupPos'] = df['PassengerId'].str.split('_', expand = True)[1]
 return df",spaceship-titanic-nn-model-feature-eng.ipynb
6.11 Imputing Using LightGBM,trn_data.columns,spaceship-titanic-nn-model-feature-eng.ipynb
Create a fill missing values function using the Simple Imputer,"def ml_imputer(df , cols , object_cols): ",spaceship-titanic-nn-model-feature-eng.ipynb
6.4 Filling NaNs by Mode and Mean Using Groups ,trn_data[trn_data['TravelGroup'] == '2926'],spaceship-titanic-nn-model-feature-eng.ipynb
6.4 Filling NaNs by Mode and Mean,"%%time
def fill_missing(df):
 '''
 Fill NaNs values or with mean or most commond value...
 
 '''
 
 numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
 
 numeric_tmp = df.select_dtypes(include = numerics)
 categ_tmp = df.select_dtypes(exclude = numerics)

 for col in numeric_tmp.columns:
 print(col)
 df[col] = df[col].fillna(value = df[col].mean())
 
 for col in categ_tmp.columns:
 print(col)
 df[col] = df[col].fillna(value = df[col].mode()[0])
 
 print('...')
 
 return df",spaceship-titanic-nn-model-feature-eng.ipynb
6.5 Calculating Total Expended in the Ship,"%%time
def total_billed(df):
 '''
 Calculates total amount billed in the trip to the passenger... 
 Args:
 Returns:
 
 '''
 
 df['TotalBilled'] = df['RoomService'] + df['FoodCourt'] + df['ShoppingMall'] + df['Spa'] + df['VRDeck']
 return df",spaceship-titanic-nn-model-feature-eng.ipynb
6.6 Filling NaNs by Using EDA Insights TotalBilled ,"%%time
def fill_nans_by_totalspend(df):
 df['CryoSleep'] = np.where(df['TotalBilled'] > 0, False, df['CryoSleep'])
 return df",spaceship-titanic-nn-model-feature-eng.ipynb
"6.11 Calculating Aggregated Features, Based on Cabin Deck","%%time
Weltiest_Deck = trn_data.groupby('CabinDeck').aggregate({'TotalBilled': 'sum', 'Transported': 'sum', 'CryoSleep': 'sum', 'PassengerId': 'size'}).reset_index()
Weltiest_Deck['AvgSpended'] = Weltiest_Deck['TotalBilled'] / Weltiest_Deck['PassengerId']
Weltiest_Deck['TransportedPercentage'] = Weltiest_Deck['Transported'] / Weltiest_Deck['PassengerId']
Weltiest_Deck['CryoSleepPercentage'] = Weltiest_Deck['CryoSleep'] / Weltiest_Deck['PassengerId']
Weltiest_Deck = Weltiest_Deck.sort_values('AvgSpended', ascending = False)
Weltiest_Deck.head(10)",spaceship-titanic-nn-model-feature-eng.ipynb
"6.12 Calulating the Number of Relatives, Using Family Name","%%time
trn_relatives = trn_data.groupby('FamilyName')['PassengerId'].count().reset_index()
tst_relatives = tst_data.groupby('FamilyName')['PassengerId'].count().reset_index()",spaceship-titanic-nn-model-feature-eng.ipynb
"6.13 Calulating the Number of People Traveling Together, Using Traveling Group ","%%time
trn_relatives = trn_data.groupby('TravelGroup')['PassengerId'].count().reset_index()
tst_relatives = tst_data.groupby('TravelGroup')['PassengerId'].count().reset_index()",spaceship-titanic-nn-model-feature-eng.ipynb
7.1 Separating the Fields by Type,trn_data.columns,spaceship-titanic-nn-model-feature-eng.ipynb
7.2 Encoding Categorical Variables,"%%time

def encode_categorical(train_df, test_df, categ_feat = categorical_features):
 '''
 
 '''
 encoder_dict = {}
 
 concat_data = pd.concat([trn_data[categ_feat], tst_data[categ_feat]])
 
 for col in concat_data.columns:
 print('Encoding: ', col, '...')
 encoder = LabelEncoder()
 encoder.fit(concat_data[col])
 encoder_dict[col] = encoder

 train_df[col + '_Enc'] = encoder.transform(train_df[col])
 test_df[col + '_Enc'] = encoder.transform(test_df[col])
 
 train_df = train_df.drop(columns = categ_feat, axis = 1)
 test_df = test_df.drop(columns = categ_feat, axis = 1)

 return train_df, test_df",spaceship-titanic-nn-model-feature-eng.ipynb
7.3 One Hot Encoding Categorical Variables,"%%time
def one_hot(df, one_hot_categ):
 for col in one_hot_categ:
 tmp = pd.get_dummies(df[col], prefix = col)
 df = pd.concat([df, tmp], axis = 1)
 df = df.drop(columns = one_hot_categ)
 return df",spaceship-titanic-nn-model-feature-eng.ipynb
8.0 Feature Selection for Baseline Model,% % time ,spaceship-titanic-nn-model-feature-eng.ipynb
"9.0 Advance Feature Engineering, KNN",% % time ,spaceship-titanic-nn-model-feature-eng.ipynb
Convert X and y to Numpy arrays as library requirements,X_array = trn_data[features]. to_numpy () ,spaceship-titanic-nn-model-feature-eng.ipynb
It Takes almost 17min 36s for K 1 and 50 000 rows...,"KNN_trn_features = knn_kfold_extract(X_array , y_array , k = K , normalize = 'standard') ",spaceship-titanic-nn-model-feature-eng.ipynb
10.0 Selection of Features for Training Stage,% % time ,spaceship-titanic-nn-model-feature-eng.ipynb
11.1 Importing all the required libraries...,"%%time
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping
from tensorflow.keras.layers import Dense, Input, InputLayer, Add, BatchNormalization, Dropout, Concatenate

from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
import random

from sklearn.model_selection import KFold, StratifiedKFold 
from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score
import datetime
import math",spaceship-titanic-nn-model-feature-eng.ipynb
11.2 Defining the Model Architecture...,% % time ,spaceship-titanic-nn-model-feature-eng.ipynb
11.3 Visualizing the Architecture Created...,"%%time
architecture = nn_model_one()
architecture.summary()",spaceship-titanic-nn-model-feature-eng.ipynb
11.4 Defining Model Parameters for Training...,% % time ,spaceship-titanic-nn-model-feature-eng.ipynb
Defining model parameters...,BATCH_SIZE = 128 ,spaceship-titanic-nn-model-feature-eng.ipynb
11.5 Defining Training Functions for the NN Model, % % time ,spaceship-titanic-nn-model-feature-eng.ipynb
Defining model training function...,"def fit_model(X_train , y_train , X_val , y_val , run = 0): ",spaceship-titanic-nn-model-feature-eng.ipynb
"12.0 Training the NN Model, Using a CV Loop",% % time ,spaceship-titanic-nn-model-feature-eng.ipynb
Create empty lists to store NN information...,history_list = [] ,spaceship-titanic-nn-model-feature-eng.ipynb
kf KFold n splits 5 ,"kf = StratifiedKFold(n_splits = 5 , random_state = 15 , shuffle = True) ",spaceship-titanic-nn-model-feature-eng.ipynb
13.0 Generating Predictions,% % time ,spaceship-titanic-nn-model-feature-eng.ipynb
Populated the prediction on the submission dataset and creates an output file,sub['Transported']= np.array(predictions). mean(axis = 0) ,spaceship-titanic-nn-model-feature-eng.ipynb
1.Importing Libraries ,import numpy as np ,spaceship-titanic-top-6-for-beginners.ipynb
Sklearn,from sklearn.model_selection import train_test_split ,spaceship-titanic-top-6-for-beginners.ipynb
Models,from xgboost import XGBClassifier ,spaceship-titanic-top-6-for-beginners.ipynb
2 Loading the Data ,"df_train = pd.read_csv('../input/spaceship-titanic/train.csv')
df_test = pd.read_csv('../input/spaceship-titanic/test.csv')

df_train.head()",spaceship-titanic-top-6-for-beginners.ipynb
3 Let s Explore ,"r1,c1 = df_train.shape
print('The training data has {} rows and {} columns'.format(r1,c1))
r2,c2 = df_test.shape
print('The validation data has {} rows and {} columns'.format(r2,c2))",spaceship-titanic-top-6-for-beginners.ipynb
c1 stands for the number of columns in the training data.,print('MISSING VALUES IN TRAINING DATASET:') ,spaceship-titanic-top-6-for-beginners.ipynb
3.C Null Replacement ,"df_train[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']] = df_train[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']].fillna(0)
df_test[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']] = df_test[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']].fillna(0)

df_train['Age'] =df_train['Age'].fillna(df_train['Age'].median())
df_test['Age'] =df_test['Age'].fillna(df_test['Age'].median())

df_train['VIP'] =df_train['VIP'].fillna(False)
df_test['VIP'] =df_test['VIP'].fillna(False)

df_train['HomePlanet'] =df_train['HomePlanet'].fillna('Mars')
df_test['HomePlanet'] =df_test['HomePlanet'].fillna('Mars')

df_train['Destination']=df_train['Destination'].fillna(""PSO J318.5-22"")
df_test['Destination']=df_test['Destination'].fillna(""PSO J318.5-22"")

df_train['CryoSleep'] =df_train['CryoSleep'].fillna(False)
df_test['CryoSleep'] =df_test['CryoSleep'].fillna(False)

df_train['Cabin'] =df_train['Cabin'].fillna('T/0/P')
df_test['Cabin'] =df_test['Cabin'].fillna('T/0/P')

",spaceship-titanic-top-6-for-beginners.ipynb
4.Exploration and Visualization ,"plt.figure(figsize=(15,18))
sns.heatmap(df_train.corr(), annot=True);",spaceship-titanic-top-6-for-beginners.ipynb
"Dude, Europa is gone","sns.countplot(df_train.VIP,hue=df_train.Transported);",spaceship-titanic-top-6-for-beginners.ipynb
The people in CryoSleep are majorly TransportedDo not sleep during travel alright ,"sns.countplot(df_train.Destination,hue=df_train.Transported)
plt.xticks(rotation=90);",spaceship-titanic-top-6-for-beginners.ipynb
"Cabin The cabin number where the passenger is staying. Takes the form deck num side, where side can be either P for Port or S for Starboard.","df_train[[ 'Deck' , 'Num' , 'Side']] = df_train.Cabin.str.split('/' , expand = True) ",spaceship-titanic-top-6-for-beginners.ipynb
Let s look into them,"sns.countplot(df_train.Deck,hue=df_train.Transported);",spaceship-titanic-top-6-for-beginners.ipynb
5.Feature Engineering ,"df_train['total_spent']= df_train['RoomService']+ df_train['FoodCourt']+ df_train['ShoppingMall']+ df_train['Spa']+ df_train['VRDeck']
df_test['total_spent']=df_test['RoomService']+df_test['FoodCourt']+df_test['ShoppingMall']+df_test['Spa']+df_test['VRDeck']",spaceship-titanic-top-6-for-beginners.ipynb
Same for test data,df_test['AgeGroup']= 0 ,spaceship-titanic-top-6-for-beginners.ipynb
6.A Encoding,"from sklearn.preprocessing import LabelEncoder

categorical_cols= ['HomePlanet','CryoSleep','Destination','VIP','Deck','Side','Num']
for i in categorical_cols:
 print(i)
 le=LabelEncoder()
 arr=np.concatenate((df_train[i], df_test[i])).astype(str)
 le.fit(arr)
 df_train[i]=le.transform(df_train[i].astype(str))
 df_test[i]=le.transform(df_test[i].astype(str))",spaceship-titanic-top-6-for-beginners.ipynb
6.B Dropping Columns,"df_train= df_train.drop(['Name','Cabin'],axis=1)
df_test= df_test.drop(['Name','Cabin'],axis=1)",spaceship-titanic-top-6-for-beginners.ipynb
6.C Splitting Columns,"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=0)",spaceship-titanic-top-6-for-beginners.ipynb
Cat Boost ,"
from catboost import CatBoostClassifier
model=CatBoostClassifier(iterations=1500,
 eval_metric='Accuracy',
 verbose=0)",spaceship-titanic-top-6-for-beginners.ipynb
Gradient Boosting,"from sklearn.ensemble import GradientBoostingClassifier
gb=GradientBoostingClassifier(random_state=1,n_estimators=250,learning_rate=0.15,max_depth=3)
gb.fit(X_train,y_train)",spaceship-titanic-top-6-for-beginners.ipynb
lets re fit the model on the entire data,"gcv.fit(X , y) ",spaceship-titanic-top-6-for-beginners.ipynb
 Prediction and Submission ,"y_pred = gcv.predict(df_test)

sub=pd.DataFrame({'Transported':y_pred.astype(bool)},index=df_test.index)

sub.head()",spaceship-titanic-top-6-for-beginners.ipynb
Thanks for reading: Upvote! and Leave some suggestions,140/24458100,spaceship-titanic-top-6-for-beginners.ipynb
Will import libraries,import numpy as np ,spam-detection-strata2013after-party.ipynb
Reading data,data = pd.read_csv('../input/just-the-basics-the-after-party/train.csv') ,spam-detection-strata2013after-party.ipynb
"Since the dataset has no headers, let s name the columns for further incrimination.","colums = list(( range(0 , 100))) ",spam-detection-strata2013after-party.ipynb
And let s fill in the missing values with the median,for i in colums : ,spam-detection-strata2013after-party.ipynb
Let s bring y to the required shape,y_train = np.ravel(y) ,spam-detection-strata2013after-party.ipynb
"Data is full, no need delete outliers NEED MORE Explanations ",X_train = data ,spam-detection-strata2013after-party.ipynb
For penalty will use Lasso l1 . Tune C parameter,"param_grid = { 'C' :[0.01 , 0.05 , 0.1 , 0.5 , 1 , 5 , 10]} ",spam-detection-strata2013after-party.ipynb
RidgeClassifier,"param_grid = {'alpha': [0.01, 0.05, 0.1, 0.5, 1, 2, 5]}

estimator = linear_model.RidgeClassifier( random_state = 1)
optimizerR = GridSearchCV(estimator, param_grid, scoring = 'roc_auc',cv = 3) 
optimizerR.fit(X_train, y_train)

print('score_train_opt', optimizerR.best_score_)
print('param_opt', optimizerR.best_params_)",spam-detection-strata2013after-party.ipynb
"RandomForestClassifier We should have a loose stopping criterion and then use pruning to remove branches that contribute to overfitting. But pruning is a tradeoff between accuracy and generalizability, so our train scores might lower but the difference between train and test scores will also get lower. This is what we need. details ","rf_class = ensemble.RandomForestClassifier(random_state = 1)
train_scores, test_scores = model_selection.validation_curve(rf_class, X_train, y_train, 'max_depth', list(range(1, 11)), cv=3, scoring='roc_auc')
print('max_depth=', list(range(1, 10)))
print(train_scores.mean(axis = 1))
print(test_scores.mean(axis = 1))",spam-detection-strata2013after-party.ipynb
We get the same difference between train and test scores on by max depth 4 9 And we have the bigger score ROC AUC by max depth 4,"param_grid = {'n_estimators': list(range(20, 100, 5)), 'min_weight_fraction_leaf': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5] } 

estimator = ensemble.RandomForestClassifier(max_depth=4, random_state = 1)
optimizerRF = GridSearchCV(estimator, param_grid, scoring = 'roc_auc',cv = 3) 
optimizerRF.fit(X_train, y_train)

print('score_train_opt', optimizerRF.best_score_)
print('param_opt', optimizerRF.best_params_)",spam-detection-strata2013after-party.ipynb
Extreme Gradient Boosting,"param_grid = {'max_depth': list(range(1, 7)), 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1, 1.5], 'n_estimators': list(range(10, 100, 5)) }
estimator = xgb.XGBClassifier( random_state = 1, min_child_weight=3)
optimizer = GridSearchCV(estimator, param_grid, scoring = 'roc_auc',cv = 3) 
optimizer.fit(X_train, y_train)

print('score_train_opt', optimizer.best_score_)
print('param_opt', optimizer.best_params_)",spam-detection-strata2013after-party.ipynb
Writting answers,ans = optimizerRF.predict(X_test) ,spam-detection-strata2013after-party.ipynb
Importing relevant libraries ,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from termcolor import colored
import warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, plot_confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV as gscv
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import VotingClassifier
from scipy.stats import expon, uniform

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Input, Model
from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization
warnings.simplefilter('ignore')",sst-eda-17-models-dl-top-7.ipynb
Reading Data ,"train_data = pd.read_csv(""../input/spaceship-titanic/train.csv"")
test_data = pd.read_csv(""../input/spaceship-titanic/test.csv"")",sst-eda-17-models-dl-top-7.ipynb
Train Data,train_data.head(),sst-eda-17-models-dl-top-7.ipynb
Test Data,test_data.head(),sst-eda-17-models-dl-top-7.ipynb
View Data Types of Predictors and Target Variables ,train_data.dtypes,sst-eda-17-models-dl-top-7.ipynb
Missing Values Train Data ,"nan_cols = train_data.columns[train_data.isna().any()].tolist()
plt.figure(figsize=(19,8))
nan_count_cols = train_data[nan_cols].isna().sum()
print(""MISSING VALS IN THE TRAINING SET:"")
print(colored(nan_count_cols, ""green""))
sns.barplot(y=nan_count_cols, x=nan_cols, palette='mako')
plt.show()",sst-eda-17-models-dl-top-7.ipynb
Missing Values Test Data ,"nan_cols = test_data.columns[test_data.isna().any()].tolist()
plt.figure(figsize=(30,8))
nan_count_cols = test_data[nan_cols].isna().sum()
sns.barplot(y=nan_count_cols, x=nan_cols, palette='mako')
plt.show()
print(colored(""MISSING VALS IN THE TESTING SET:\n"", 'magenta', attrs=['bold', 'underline']))
print(colored(nan_count_cols, ""cyan"", attrs=['bold']),'\n')",sst-eda-17-models-dl-top-7.ipynb
Group Size train set ,"MAX_GROUP_SIZE = 8
gggg_pp = train_data['PassengerId'].apply(lambda x: x.split('_')).values
gggg = list(map(lambda x: x[0], gggg_pp))
pp = list(map(lambda x: x[1], gggg_pp))
train_data['gggg'] = gggg
train_data['pp'] = pp

train_data['pp'] = train_data['pp'].astype('int64')
train_data['group_size'] = 0
for i in range(MAX_GROUP_SIZE):
 curr_gggg = train_data[train_data['pp'] == i + 1]['gggg'].to_numpy()
 train_data.loc[train_data['gggg'].isin(curr_gggg), ['group_size']] = i + 1

plt.figure(figsize=(19,8))
print(colored(""Value Counts based on the group size:\n"", 'green', attrs=['underline', 'bold']))
print(colored(""Group Size, Count"", 'magenta', attrs=['bold']))
print(colored(train_data['group_size'].value_counts(), ""cyan"", attrs=['bold']))
sns.barplot(y=train_data['group_size'].value_counts(), x=np.unique(train_data['pp']), palette='mako')
plt.show()
sns.catplot(x=""group_size"", kind=""count"", hue='Transported', data=train_data, palette='mako').set(title='Group Size and Transported Count')
plt.show()
",sst-eda-17-models-dl-top-7.ipynb
Group Size test set ,"gggg_pp = test_data['PassengerId'].apply(lambda x: x.split('_')).values
gggg = list(map(lambda x: x[0], gggg_pp))
pp = list(map(lambda x: x[1], gggg_pp))
 
plt.figure(figsize=(19,8))
test_data['pp'] = pp
test_data['gggg'] = gggg

test_data['pp'] = test_data['pp'].astype('int64')
test_data['group_size'] = 0

for i in range(MAX_GROUP_SIZE):
 curr_gggg = test_data[test_data['pp'] == i + 1]['gggg'].to_numpy()
 test_data.loc[test_data['gggg'].isin(curr_gggg), ['group_size']] = i + 1

print(colored(""Value Counts based on the group size:\n"", 'green', attrs=['underline', 'bold']))
print(colored(""Group Size, Count"", 'magenta', attrs=['bold']))
print(colored(test_data['group_size'].value_counts(), ""cyan"", attrs=['bold']))
sns.barplot(y=test_data['group_size'].value_counts(), x=np.unique(test_data['group_size']), palette='mako')
plt.show()",sst-eda-17-models-dl-top-7.ipynb
The possible values present in the HomePlanet predictor,"HOME_PLANET_UNIQUE_VALS =['Earth' , 'Europa' , 'Mars'] ",sst-eda-17-models-dl-top-7.ipynb
HomePlanet with respect to Transported ,"homeplanet_transported_count = train_data[['HomePlanet', 'Transported']].value_counts()
sns.catplot(x=""HomePlanet"", kind=""count"", hue='Transported', data=train_data, palette='mako')
plt.show()
print(colored(""COUNT STATISTICS:\n"", 'magenta', attrs=['underline', 'bold']))
print(colored(homeplanet_transported_count, 'cyan', attrs=['bold']))",sst-eda-17-models-dl-top-7.ipynb
CryoSleep Count ,"sns.barplot(y=train_data['CryoSleep'].value_counts(), x=[True, False], palette='mako').set(title=""Train Set CryoSleep value count"")
plt.show()
print(colored(""CryoSleep Count Train data:\n"", 'magenta', attrs=['underline', 'bold']))
print(colored(train_data['CryoSleep'].value_counts(), 'cyan', attrs=['bold']), '\n')
sns.barplot(y=test_data['CryoSleep'].value_counts(), x=[True, False], palette='mako').set(title=""Test Set CryoSleep value count"")
plt.show()
print(colored(""CryoSleep Count Test data:\n"", 'magenta', attrs=['underline', 'bold']))
print(colored(test_data['CryoSleep'].value_counts(), 'cyan', attrs=['bold']))",sst-eda-17-models-dl-top-7.ipynb
CryoSleep with respect to Transported ,"cryosleep_transported_count = train_data[['CryoSleep', 'Transported']].value_counts()
sns.catplot(x=""CryoSleep"", kind=""count"", hue='Transported', data=train_data, palette='mako')
plt.show()
print(colored(""COUNT STATISTICS:\n"", 'magenta', attrs=['underline', 'bold']))
print(colored(cryosleep_transported_count, 'cyan', attrs=['bold']))",sst-eda-17-models-dl-top-7.ipynb
Cabin Visualization ,"train_data['Cabin'].fillna('N/N/N', inplace=True)
test_data['Cabin'].fillna('N/N/N', inplace=True)

deck_num_side = train_data['Cabin'].apply(lambda x: x.split('/'))
side = list(map(lambda x: x[-1], deck_num_side))
side_order_vals = ['S', 'P', 'N']
train_data['side'] = side
test_data['side'] = list(map(lambda x: x[-1], test_data['Cabin'].apply(lambda x: x.split('/'))))

plt.figure(figsize=(19,8))
sns.countplot(x='side', data=train_data, order=side_order_vals, palette='mako')
plt.show()
print(colored(""Value Counts based on the Cabin side:\n"", 'green', attrs=['underline', 'bold']))
print(colored(""Cabin Side, Count"", 'magenta', attrs=['bold']))
print(colored(train_data['side'].value_counts(), ""cyan"", attrs=['bold']))
sns.catplot(x=""side"", kind=""count"", hue='Transported', data=train_data, palette='mako').set(title='Cabin Side and Transported Count')
plt.show()",sst-eda-17-models-dl-top-7.ipynb
Deck Visualization ,"deck = list(map(lambda x: x[0], deck_num_side))

train_data['deck'] = deck
test_data['deck'] = list(map(lambda x: x[0], test_data['Cabin'].apply(lambda x: x.split('/'))))

deck_order_vals = ['A','B','C','D','E','F','G','T','N']
plt.figure(figsize=(19,8))
sns.countplot(x='deck', data=train_data, order=deck_order_vals, palette='mako')
plt.show()
print(colored(""Value Counts based on the Cabin side:\n"", 'green', attrs=['underline', 'bold']))
print(colored(""Cabin Side, Count"", 'magenta', attrs=['bold']))
print(colored(train_data['deck'].value_counts(), ""cyan"", attrs=['bold']))
sns.catplot(x=""deck"", kind=""count"", hue='Transported', data=train_data, palette='mako').set(title='Cabin Deck and Transported Count')
plt.show()",sst-eda-17-models-dl-top-7.ipynb
Destination Visualization I will simply perform a count based on all the values in Destination and also compare it with the target,"plt.figure(figsize=(19,8))
DESTINATION_UNIQUE_VALS = train_data['Destination'].unique()
sns.countplot(x='Destination', data=train_data, order=DESTINATION_UNIQUE_VALS, palette='mako')
plt.show()
print(colored(""Value Counts based on the Destination:\n"", 'green', attrs=['underline', 'bold']))
print(colored(""Destination, Count"", 'magenta', attrs=['bold']))
print(colored(train_data['Destination'].value_counts(), ""cyan"", attrs=['bold']))
sns.catplot(x=""Destination"", kind=""count"", hue='Transported', data=train_data, palette='mako').set(title='Cabin Side and Transported Count')
plt.show()",sst-eda-17-models-dl-top-7.ipynb
"Age Visualization Below I provide information about the min, mean and max age values for both Target scenarios Transported 1 and Transported 0 . Lastly I also provide visualization of the data with a Box Plot and a Histogram.","fig, ax = plt.subplots(1, 2, figsize=(19,8))
fig.suptitle('Age Histogram and Boxplot', size=22)
sns.histplot(x='Age', data=train_data, hue='Transported', palette='mako', kde=True, element='step', ax=ax[0])
sns.boxplot(x='Transported', y='Age', data=train_data, palette='mako', ax=ax[1])
plt.show()

print(colored(""Age Min, Mean and Max:"", 'green', attrs=['underline', 'bold']))

transported_1 = train_data[train_data['Transported']==True]['Age']
transported_0 = train_data[train_data['Transported']==False]['Age']

print(colored(""\tTransported == 1"", 'magenta', attrs=['bold']))
print('\tAge Minimum: ', colored(transported_1.describe()['min'], ""cyan"", attrs=['bold']))
print('\tAge Mean:', colored(transported_1.describe()['mean'], ""cyan"", attrs=['bold']))
print('\tAge Maximum:', colored(transported_1.describe()['max'], ""cyan"", attrs=['bold']), '\n')

print(colored(""\tTransported == 0"", 'magenta', attrs=['bold']))
print('\tAge Minimum: ', colored(transported_0.describe()['min'], ""cyan"", attrs=['bold']))
print('\tAge Mean:', colored(transported_0.describe()['mean'], ""cyan"", attrs=['bold']))
print('\tAge Maximum:', colored(transported_0.describe()['max'], ""cyan"", attrs=['bold']))",sst-eda-17-models-dl-top-7.ipynb
"Conclusions Based on the visualization, passengers with lower age have a higher chance of being Transported, which is reflected in the histogram. ","VIP_transported_count = train_data[['VIP', 'Transported']].value_counts()
sns.catplot(x=""VIP"", kind=""count"", hue='Transported', data=train_data, palette='mako')
plt.show()
print(colored(""COUNT STATISTICS:\n"", 'magenta', attrs=['underline', 'bold']))
print(colored(VIP_transported_count, 'cyan', attrs=['bold']))",sst-eda-17-models-dl-top-7.ipynb
"RoomService, FoodCourt, ShoppingMall, Spa, VRDeck Below I provide a summary regarding the following predictors: RoomService, FoodCourt, ShoppingMall, Spa, VRDeck","fig,ax = plt.subplots(1, 2, figsize=(22,7))
fig.suptitle('RoomService Histogram and Boxplot', size=22)
sns.histplot(x='RoomService', data=train_data, hue='Transported', palette='mako', kde=True, element='step', ax=ax[0])
sns.boxplot(x='Transported', y='Age', data=train_data, palette='mako', ax=ax[1])
plt.show()

print(colored(""Age Min, Mean and Max:"", 'green', attrs=['underline', 'bold']))

transported_1 = train_data[train_data['Transported']==True]['RoomService']
transported_0 = train_data[train_data['Transported']==False]['RoomService']

print(colored(""\tTransported == 1"", 'magenta', attrs=['bold']))
print('\tRoomService Minimum: ', colored(transported_1.describe()['min'], ""cyan"", attrs=['bold']))
print('\tRoomService Mean:', colored(transported_1.describe()['mean'], ""cyan"", attrs=['bold']))
print('\tRoomService Maximum:', colored(transported_1.describe()['max'], ""cyan"", attrs=['bold']), '\n')

print(colored(""\tTransported == 0"", 'magenta', attrs=['bold']))
print('\tRoomService Minimum: ', colored(transported_0.describe()['min'], ""cyan"", attrs=['bold']))
print('\tRoomService Mean:', colored(transported_0.describe()['mean'], ""cyan"", attrs=['bold']))
print('\tRoomService Maximum:', colored(transported_0.describe()['max'], ""cyan"", attrs=['bold']))",sst-eda-17-models-dl-top-7.ipynb
Visualizing Age vs Money Spent on Amenities,"AMENITIES = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']
fig, ax = plt.subplots(3, 2, figsize=(18,5))

for i, amenity in enumerate(AMENITIES):
 sns.scatterplot(x='Age', y=amenity, data=train_data, hue='Transported', palette='mako', ax=fig.axes[i])
 fig.axes[i].set_title(f'{amenity} vs Age', weight='bold')
 
 
plt.show()",sst-eda-17-models-dl-top-7.ipynb
Create a new column Amenities which is the sum of all the amenities,"train_data['Amenities'] = train_data[AMENITIES].sum(axis=1)
test_data['Amenities'] = test_data[AMENITIES].sum(axis=1)
train_data['NoAmenities'] = train_data['Amenities']==0
test_data['NoAmenities'] = test_data['Amenities']==0",sst-eda-17-models-dl-top-7.ipynb
Visualizing the correlation matrix of the dataset If features are highly correlated then they need to be handled accordingly,"plt.matshow(train_data.corr())
plt.colorbar()
plt.show()",sst-eda-17-models-dl-top-7.ipynb
SAVE PASSENGERID s for submission,PASSENGER_ID = test_data[[ 'PassengerId']] ,sst-eda-17-models-dl-top-7.ipynb
The remaining predictors are: HomePlanet: object CryoSleep: object Destination: object Age: float64 VIP: bool RoomService: float64 FoodCourt: float64 ShoppingMall: float64 Spa: float64 VRDeck: float64 group size: int64 side: object deck: object Amenities: float64 NoAmenities: bool ,train_data,sst-eda-17-models-dl-top-7.ipynb
Filling Missing values,"print(colored(f""Total NaN values before:"", 'green', attrs=['underline', 'bold']))
print(colored(train_data.isna().sum().sum(), attrs=['bold']))
LABELS = test_data.columns
for col in LABELS:
 if col in ['Age', 'RoomService',
 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']:
 train_data[col].fillna(train_data[col].median(), inplace=True)
 test_data[col].fillna(train_data[col].median(), inplace=True)
 else:
 train_data[col].fillna(train_data[col].mode()[0], inplace=True)
 test_data[col].fillna(train_data[col].mode()[0], inplace=True)

print(colored(f""Total NaN values after:"", 'green', attrs=['underline', 'bold']))
print(colored(train_data.isna().sum().sum(), attrs=['bold']))",sst-eda-17-models-dl-top-7.ipynb
Encode labels and Scale Objects will use scikit learn s LabelEncoder Booleans will be converted to int Floats will be scaled with MinMaxScaler ,for col in LABELS : ,sst-eda-17-models-dl-top-7.ipynb
Check if object, if train_data[col]. dtype == 'O' : ,sst-eda-17-models-dl-top-7.ipynb
Apply Min Max Scaling,train_data[LABELS_MM]= mm_scaler.fit_transform(train_data[LABELS_MM]) ,sst-eda-17-models-dl-top-7.ipynb
Apply Standard Scaling,train_data[LABELS_SS]= ss_scaler.fit_transform(train_data[LABELS_SS]) ,sst-eda-17-models-dl-top-7.ipynb
Splitting Data into Training and Validation Sets ,"X, y = train_data.drop('Transported', axis=1), train_data[['Transported']]",sst-eda-17-models-dl-top-7.ipynb
Define dictionary with model accuracies,model_dict = { } ,sst-eda-17-models-dl-top-7.ipynb
Naive Bayes Model 1,"classifer = GaussianNB()
predictor = classifer.fit(X_train, y_train)
y_pred = predictor.predict(X_val)
accuracy_naive_bayes = accuracy_score(y_val, y_pred)
model_dict['naive_bayes'] = accuracy_naive_bayes
print(accuracy_naive_bayes)",sst-eda-17-models-dl-top-7.ipynb
Linear Discriminant Analysis Model 2,"classifer = LinearDiscriminantAnalysis()
predictor = classifer.fit(X_train, y_train)
y_pred = predictor.predict(X_val)
accuracy_lda = accuracy_score(y_val, y_pred)
model_dict['linear_discriminant_analysis'] = accuracy_lda
print(accuracy_lda)",sst-eda-17-models-dl-top-7.ipynb
Logistic Regression Model 3,"classifier = LogisticRegression(random_state=42)
predictor = classifier.fit(X_train, y_train)
y_pred = predictor.predict(X_val)
accuracy_log_reg = accuracy_score(y_val, y_pred)
model_dict['logistic_regression'] = accuracy_log_reg
print(accuracy_log_reg)",sst-eda-17-models-dl-top-7.ipynb
Support Vector Classifier Model 4,"classifier = SVC(random_state=42)
predictor_svc = classifier.fit(X_train, y_train)
y_pred = predictor_svc.predict(X_val)
accuracy_svc = accuracy_score(y_val, y_pred)
model_dict['SVC'] = accuracy_svc
print(accuracy_svc)",sst-eda-17-models-dl-top-7.ipynb
K Neighbors Classifier Model 5,"from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier()
predictor = classifier.fit(X_train, y_train)
y_pred = predictor.predict(X_val)
accuracy_knn = accuracy_score(y_val, y_pred)
model_dict['kneighbors_classifier'] = accuracy_knn
print(accuracy_knn)",sst-eda-17-models-dl-top-7.ipynb
Stochastic Gradient Descent Classifier Model 6,"classifier = SGDClassifier(random_state=42)
predictor = classifier.fit(X_train, y_train)
y_pred = predictor.predict(X_val)
accuracy_sgdc = accuracy_score(y_val, y_pred)
model_dict['sgd_classifier'] = accuracy_sgdc
print(accuracy_sgdc)",sst-eda-17-models-dl-top-7.ipynb
Random Forest Classifier Model 7,"classifier = RandomForestClassifier(random_state=42)
predictor = classifier.fit(X_train, y_train)
y_pred = predictor.predict(X_val)
accuracy_rfc = accuracy_score(y_val, y_pred)
model_dict['random_forest_classifier'] = accuracy_rfc
print(accuracy_rfc)",sst-eda-17-models-dl-top-7.ipynb
Gradient Boosting Classifier Model 8,"classifier = GradientBoostingClassifier(random_state=42)
predictor_gbc = classifier.fit(X_train, y_train)
y_pred = predictor_gbc.predict(X_val)
accuracy_gbc = accuracy_score(y_val, y_pred)
model_dict['gradient_boosting_classifier'] = accuracy_gbc
print(accuracy_gbc)",sst-eda-17-models-dl-top-7.ipynb
XGBoost Classifier Model 9,"classifier = XGBClassifier(random_state=42, eval_metric='logloss')
predictor_xgb = classifier.fit(X_train, y_train)
y_pred = predictor_xgb.predict(X_val)
accuracy_xgb = accuracy_score(y_val, y_pred)
model_dict['xgboost_classifier'] = accuracy_xgb
print(accuracy_xgb)",sst-eda-17-models-dl-top-7.ipynb
AdaBoost Classifier Model 10,"dtc=DecisionTreeClassifier(criterion='entropy', random_state=42)
classifier = AdaBoostClassifier(dtc,random_state=42)
predictor = classifier.fit(X_train, y_train)
y_pred = predictor.predict(X_val)
accuracy_ada = accuracy_score(y_val, y_pred)
model_dict['adaboost_classifier'] = accuracy_ada
print(accuracy_ada)",sst-eda-17-models-dl-top-7.ipynb
LGBM Classifier Model 11 If you want to learn more about the LGBM classifier I suggest looking at this kernel: ,"classifier = LGBMClassifier(random_state=42)
predictor_lgbm = classifier.fit(X_train, y_train)
y_pred = predictor_lgbm.predict(X_val)
accuracy_lgbm = accuracy_score(y_val, y_pred)
model_dict['lgbm_classifier'] = accuracy_lgbm
print(accuracy_lgbm)",sst-eda-17-models-dl-top-7.ipynb
Extra Trees Classifier Model 12,"classifier = ExtraTreesClassifier(random_state=42)
predictor_etc = classifier.fit(X_train, y_train)
y_pred = predictor_etc.predict(X_val)
accuracy_etc = accuracy_score(y_val, y_pred)
model_dict['etc_classifier'] = accuracy_etc
print(accuracy_etc)",sst-eda-17-models-dl-top-7.ipynb
Function to visualizing model accuracies,"def visualize_model_accuracies(model_dict):
 model_accuracies_df = pd.DataFrame(columns=['Model', 'Accuracy'])
 model_accuracies_df['Model'] = model_dict.keys()
 model_accuracies_df['Accuracy'] = model_dict.values()
 model_accuracies_df.sort_values('Accuracy', inplace=True, ascending=False)

 plt.figure(figsize=(28,8),)
 plt.ylabel(""Models"", fontsize=16)
 plt.xlabel(""Accuracy"", fontsize=16)
 plt.title(""Model Accuracies"", fontsize=22)
 sns.barplot(y = pd.to_numeric(model_accuracies_df['Accuracy']), x = model_accuracies_df['Model'], palette='mako')
 plt.margins(x=0.005)
 plt.show()

 print(colored(""The 3 models with the highest accuracies are:""))
 print(f""{model_accuracies_df.iloc[:3, ]}"")",sst-eda-17-models-dl-top-7.ipynb
"I will now perform Grid Search on the three candidate models Below I define yet another dictionary to keep track of the models Note: XGBoost Classifier is a very similar model to GB classifier, therefore I will use SVC as the 3rd option despite it being the 4th best performing model",gs_model_dict = {},sst-eda-17-models-dl-top-7.ipynb
Results after performing grid search,"final_parameters_xgbc = { 'learning_rate' : 0.09 , 'max_depth' : 6 , 'n_estimators' : 100 } ",sst-eda-17-models-dl-top-7.ipynb
Results after performing grid search,"final_parameters_svc = { 'C' : 91 , 'class_weight' : None , 'kernel' : 'rbf' } ",sst-eda-17-models-dl-top-7.ipynb
Visualize accuracies,visualize_model_accuracies(gs_model_dict),sst-eda-17-models-dl-top-7.ipynb
Voting Ensemble Model 16,"voting_ensemble = VotingClassifier(estimators= 
 [('SVC', clf_svc),
 ('XBG', clf_xgbc),
 ('LGBM', clf_lgbm)],
 voting = 'hard')

ensemble_predictor = voting_ensemble.fit(X_train, y_train)

y_pred = ensemble_predictor.predict(X_val)
print(f'Accuracy score for the first ensemble: {accuracy_score(y_val, y_pred)}')
",sst-eda-17-models-dl-top-7.ipynb
"Confusion matrix for ensemble composed of SVC, XGBC, and LGBM","print(""Performance on validation data:"", f1_score(y_val, y_pred, average='micro'))
plot_confusion_matrix(ensemble_predictor, X_val, y_val) 
plt.show()",sst-eda-17-models-dl-top-7.ipynb
Final Classifier The model with the higher accuracy isn t that good,classifier = ensemble_predictor,sst-eda-17-models-dl-top-7.ipynb
Change data to numpy format,"X_train = X_train.to_numpy()
y_train = y_train.to_numpy()
X_val = X_val.to_numpy()
y_val = y_val.to_numpy()",sst-eda-17-models-dl-top-7.ipynb
Model Architecture Model 17,LOSS_CONST = 30e-6 ,sst-eda-17-models-dl-top-7.ipynb
Hidden Layers,"x = Dense(1024 , kernel_regularizer = tf.keras.regularizers.l2(LOSS_CONST), activation = 'relu' , name = 'dense_1')( inputs) ",sst-eda-17-models-dl-top-7.ipynb
"x Dropout 0.025, name dropout 2 x ","x = Dense(512 , kernel_regularizer = tf.keras.regularizers.l2(LOSS_CONST), activation = 'relu' , name = 'dense_3')( inputs) ",sst-eda-17-models-dl-top-7.ipynb
"x Dropout 0.025, name dropout 2 x ","x = Dense(128 , kernel_regularizer = tf.keras.regularizers.l2(LOSS_CONST), activation = 'relu' , name = 'dense_3')( inputs) ",sst-eda-17-models-dl-top-7.ipynb
"x Dropout 0.025, name dropout 2 x ","x = Dense(32 , kernel_regularizer = tf.keras.regularizers.l2(LOSS_CONST), activation = 'relu' , name = 'dense_3')( inputs) ",sst-eda-17-models-dl-top-7.ipynb
"x Dropout DROPOUT, name dropout 3 x ","y = Dense(1 , activation = 'sigmoid' , name = 'output_layer')( x) ",sst-eda-17-models-dl-top-7.ipynb
"For now we submit the ML voting classifier, since the DL model needs a lot of work","submission = classifier.predict(test_data)

data = {
 'PassengerId': np.array(PASSENGER_ID).reshape(len(PASSENGER_ID)),
 'Transported': np.array(submission).astype('bool')
}

submission_df = pd.DataFrame(data=data).reset_index()
submission_df.drop(columns=['index'], inplace=True, axis=1)
submission_df.to_csv(""submission.csv"", index=False)",sst-eda-17-models-dl-top-7.ipynb
linear algebra,import numpy as np ,stacked-regressions-top-4-on-leaderboard.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,stacked-regressions-top-4-on-leaderboard.ipynb
Matlab style plotting,import matplotlib.pyplot as plt ,stacked-regressions-top-4-on-leaderboard.ipynb
ignore annoying warning from sklearn and seaborn ,warnings.warn = ignore_warn ,stacked-regressions-top-4-on-leaderboard.ipynb
for some statistics,"from scipy.stats import norm , skew ",stacked-regressions-top-4-on-leaderboard.ipynb
Limiting floats output to 3 decimal points,"pd.set_option('display.float_format' , lambda x : '{:.3f}'.format(x)) ",stacked-regressions-top-4-on-leaderboard.ipynb
check the files available in the directory,"print(check_output ([""ls"" , ""../input""]).decode(""utf8"")) ",stacked-regressions-top-4-on-leaderboard.ipynb
Now let s import and put the train and test datasets in pandas dataframe,train = pd.read_csv('../input/train.csv') ,stacked-regressions-top-4-on-leaderboard.ipynb
display the first five rows of the train dataset.,train.head(5) ,stacked-regressions-top-4-on-leaderboard.ipynb
display the first five rows of the test dataset.,test.head(5) ,stacked-regressions-top-4-on-leaderboard.ipynb
check the numbers of samples and features,"print(""The train data size before dropping Id feature is : {} "".format(train.shape)) ",stacked-regressions-top-4-on-leaderboard.ipynb
Save the Id column,train_ID = train['Id'] ,stacked-regressions-top-4-on-leaderboard.ipynb
Now drop the Id colum since it s unnecessary for the prediction process.,"train.drop(""Id"" , axis = 1 , inplace = True) ",stacked-regressions-top-4-on-leaderboard.ipynb
check again the data size after dropping the Id variable,"print(""\nThe train data size after dropping Id feature is : {} "".format(train.shape)) ",stacked-regressions-top-4-on-leaderboard.ipynb
Let s explore these outliers,"
fig, ax = plt.subplots()
ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])
plt.ylabel('SalePrice', fontsize=13)
plt.xlabel('GrLivArea', fontsize=13)
plt.show()
",stacked-regressions-top-4-on-leaderboard.ipynb
Deleting outliers,train = train.drop(train[( train['GrLivArea']> 4000)&(train['SalePrice']< 300000 )]. index) ,stacked-regressions-top-4-on-leaderboard.ipynb
Check the graphic again,"fig , ax = plt.subplots () ",stacked-regressions-top-4-on-leaderboard.ipynb
SalePrice is the variable we need to predict. So let s do some analysis on this variable first.,"sns.distplot(train['SalePrice'], fit = norm); ",stacked-regressions-top-4-on-leaderboard.ipynb
Get the fitted parameters used by the function,"( mu , sigma)= norm.fit(train['SalePrice']) ",stacked-regressions-top-4-on-leaderboard.ipynb
We use the numpy fuction log1p which applies log 1 x to all elements of the column,"train[""SalePrice""]= np.log1p(train[""SalePrice""]) ",stacked-regressions-top-4-on-leaderboard.ipynb
Check the new distribution,"sns.distplot(train['SalePrice'], fit = norm); ",stacked-regressions-top-4-on-leaderboard.ipynb
Get the fitted parameters used by the function,"( mu , sigma)= norm.fit(train['SalePrice']) ",stacked-regressions-top-4-on-leaderboard.ipynb
let s first concatenate the train and test data in the same dataframe,"ntrain = train.shape[0]
ntest = test.shape[0]
y_train = train.SalePrice.values
all_data = pd.concat((train, test)).reset_index(drop=True)
all_data.drop(['SalePrice'], axis=1, inplace=True)
print(""all_data size is : {}"".format(all_data.shape))",stacked-regressions-top-4-on-leaderboard.ipynb
Missing Data,"all_data_na = (all_data.isnull().sum() / len(all_data)) * 100
all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]
missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})
missing_data.head(20)",stacked-regressions-top-4-on-leaderboard.ipynb
Correlation map to see how features are correlated with SalePrice,corrmat = train.corr () ,stacked-regressions-top-4-on-leaderboard.ipynb
"PoolQC : data description says NA means No Pool . That make sense, given the huge ratio of missing value 99 and majority of houses have no Pool at all in general. ","all_data[""PoolQC""] = all_data[""PoolQC""].fillna(""None"")",stacked-regressions-top-4-on-leaderboard.ipynb
MiscFeature : data description says NA means no misc feature ,"all_data[""MiscFeature""] = all_data[""MiscFeature""].fillna(""None"")",stacked-regressions-top-4-on-leaderboard.ipynb
Alley : data description says NA means no alley access ,"all_data[""Alley""] = all_data[""Alley""].fillna(""None"")",stacked-regressions-top-4-on-leaderboard.ipynb
Fence : data description says NA means no fence ,"all_data[""Fence""] = all_data[""Fence""].fillna(""None"")",stacked-regressions-top-4-on-leaderboard.ipynb
FireplaceQu : data description says NA means no fireplace ,"all_data[""FireplaceQu""] = all_data[""FireplaceQu""].fillna(""None"")",stacked-regressions-top-4-on-leaderboard.ipynb
"GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None ","for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):
 all_data[col] = all_data[col].fillna('None')",stacked-regressions-top-4-on-leaderboard.ipynb
"GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 Since No garage no cars in such garage. ","for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):
 all_data[col] = all_data[col].fillna(0)",stacked-regressions-top-4-on-leaderboard.ipynb
"BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement ","for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):
 all_data[col] = all_data[col].fillna(0)",stacked-regressions-top-4-on-leaderboard.ipynb
"BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement related features, NaN means that there is no basement. ","for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):
 all_data[col] = all_data[col].fillna('None')",stacked-regressions-top-4-on-leaderboard.ipynb
MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. ,"all_data[""MasVnrType""] = all_data[""MasVnrType""].fillna(""None"")
all_data[""MasVnrArea""] = all_data[""MasVnrArea""].fillna(0)",stacked-regressions-top-4-on-leaderboard.ipynb
MSZoning The general zoning classification : RL is by far the most common value. So we can fill in missing values with RL ,all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0]),stacked-regressions-top-4-on-leaderboard.ipynb
"Utilities : For this categorical feature all records are AllPub , except for one NoSeWa and 2 NA . Since the house with NoSewa is in the training set, this feature won t help in predictive modelling. We can then safely remove it. ","all_data = all_data.drop(['Utilities'], axis=1)",stacked-regressions-top-4-on-leaderboard.ipynb
Functional : data description says NA means typical ,"all_data[""Functional""] = all_data[""Functional""].fillna(""Typ"")",stacked-regressions-top-4-on-leaderboard.ipynb
"Electrical : It has one NA value. Since this feature has mostly SBrkr , we can set that for the missing value. ",all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0]),stacked-regressions-top-4-on-leaderboard.ipynb
"KitchenQual: Only one NA value, and same as Electrical, we set TA which is the most frequent for the missing value in KitchenQual. ",all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0]),stacked-regressions-top-4-on-leaderboard.ipynb
Exterior1st and Exterior2nd : Again Both Exterior 1 2 have only one missing value. We will just substitute in the most common string ,"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])
all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])",stacked-regressions-top-4-on-leaderboard.ipynb
SaleType : Fill in again with most frequent which is WD ,all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0]),stacked-regressions-top-4-on-leaderboard.ipynb
MSSubClass : Na most likely means No building class. We can replace missing values with None ,"
all_data['MSSubClass'] = all_data['MSSubClass'].fillna(""None"")

",stacked-regressions-top-4-on-leaderboard.ipynb
Check remaining missing values if any,all_data_na =(all_data.isnull (). sum ()/ len(all_data)) * 100 ,stacked-regressions-top-4-on-leaderboard.ipynb
MSSubClass The building class,all_data['MSSubClass']= all_data['MSSubClass']. apply(str) ,stacked-regressions-top-4-on-leaderboard.ipynb
Changing OverallCond into a categorical variable,all_data['OverallCond']= all_data['OverallCond']. astype(str) ,stacked-regressions-top-4-on-leaderboard.ipynb
Year and month sold are transformed into categorical features.,all_data['YrSold']= all_data['YrSold']. astype(str) ,stacked-regressions-top-4-on-leaderboard.ipynb
Label Encoding some categorical variables that may contain information in their ordering set ,from sklearn.preprocessing import LabelEncoder ,stacked-regressions-top-4-on-leaderboard.ipynb
Adding total sqfootage feature,all_data['TotalSF']= all_data['TotalBsmtSF']+ all_data['1stFlrSF']+ all_data['2ndFlrSF'] ,stacked-regressions-top-4-on-leaderboard.ipynb
Skewed features,"numeric_feats = all_data.dtypes[all_data.dtypes != ""object""]. index ",stacked-regressions-top-4-on-leaderboard.ipynb
Check the skew of all numerical features,skewed_feats = all_data[numeric_feats]. apply(lambda x : skew(x.dropna ())). sort_values(ascending = False) ,stacked-regressions-top-4-on-leaderboard.ipynb
We use the scipy function boxcox1p which computes the Box Cox transformation of 1 x . Note that setting  0 is equivalent to log1p used above for the target variable. See this page for more details on Box Cox Transformation as well as the scipy function s page,skewness = skewness[abs(skewness)> 0.75] ,stacked-regressions-top-4-on-leaderboard.ipynb
all data feat 1," all_data[feat]= boxcox1p(all_data[feat], lam) ",stacked-regressions-top-4-on-leaderboard.ipynb
Getting dummy categorical features,"
all_data = pd.get_dummies(all_data)
print(all_data.shape)",stacked-regressions-top-4-on-leaderboard.ipynb
Getting the new train and test sets. ,"train = all_data[:ntrain]
test = all_data[ntrain:]
",stacked-regressions-top-4-on-leaderboard.ipynb
Import librairies,"from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.kernel_ridge import KernelRidge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import lightgbm as lgb

",stacked-regressions-top-4-on-leaderboard.ipynb
Validation function,n_folds = 5 ,stacked-regressions-top-4-on-leaderboard.ipynb
LASSO Regression : This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn s Robustscaler method on pipeline ,"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))",stacked-regressions-top-4-on-leaderboard.ipynb
Elastic Net Regression : again made robust to outliers,"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))",stacked-regressions-top-4-on-leaderboard.ipynb
Kernel Ridge Regression : ,"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)",stacked-regressions-top-4-on-leaderboard.ipynb
Gradient Boosting Regression : With huber loss that makes it robust to outliers,"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,
 max_depth=4, max_features='sqrt',
 min_samples_leaf=15, min_samples_split=10, 
 loss='huber', random_state =5)",stacked-regressions-top-4-on-leaderboard.ipynb
XGBoost : ,"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, 
 learning_rate=0.05, max_depth=3, 
 min_child_weight=1.7817, n_estimators=2200,
 reg_alpha=0.4640, reg_lambda=0.8571,
 subsample=0.5213, silent=1,
 random_state =7, nthread = -1)

",stacked-regressions-top-4-on-leaderboard.ipynb
LightGBM : ,"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,
 learning_rate=0.05, n_estimators=720,
 max_bin = 55, bagging_fraction = 0.8,
 bagging_freq = 5, feature_fraction = 0.2319,
 feature_fraction_seed=9, bagging_seed=9,
 min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)",stacked-regressions-top-4-on-leaderboard.ipynb
Let s see how these base models perform on the data by evaluating the cross validation rmsle error,"score = rmsle_cv(lasso)
print(""\nLasso score: {:.4f} ({:.4f})\n"".format(score.mean(), score.std()))",stacked-regressions-top-4-on-leaderboard.ipynb
Averaged base models class,"class AveragingModels(BaseEstimator , RegressorMixin , TransformerMixin): ",stacked-regressions-top-4-on-leaderboard.ipynb
we define clones of the original models to fit the data in," def fit(self , X , y): ",stacked-regressions-top-4-on-leaderboard.ipynb
Train cloned base models, for model in self.models_ : ,stacked-regressions-top-4-on-leaderboard.ipynb
Now we do the predictions for cloned models and average them," def predict(self , X): ",stacked-regressions-top-4-on-leaderboard.ipynb
"We just average four models here ENet, GBoost, KRR and lasso. Of course we could easily add more models in the mix. ","averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))

score = rmsle_cv(averaged_models)
print("" Averaged base models score: {:.4f} ({:.4f})\n"".format(score.mean(), score.std()))",stacked-regressions-top-4-on-leaderboard.ipynb
Stacking averaged Models Class,"class StackingAveragedModels(BaseEstimator , RegressorMixin , TransformerMixin): ",stacked-regressions-top-4-on-leaderboard.ipynb
We again fit the data on clones of the original models," def fit(self , X , y): ",stacked-regressions-top-4-on-leaderboard.ipynb
that are needed to train the cloned meta model," out_of_fold_predictions = np.zeros(( X.shape[0], len(self.base_models))) ",stacked-regressions-top-4-on-leaderboard.ipynb
Now train the cloned meta model using the out of fold predictions as new feature," self.meta_model_.fit(out_of_fold_predictions , y) ",stacked-regressions-top-4-on-leaderboard.ipynb
meta features for the final prediction which is done by the meta model," def predict(self , X): ",stacked-regressions-top-4-on-leaderboard.ipynb
"To make the two approaches comparable by using the same number of models , we just average Enet KRR and Gboost, then we add lasso as meta model.","stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),
 meta_model = lasso)

score = rmsle_cv(stacked_averaged_models)
print(""Stacking Averaged models score: {:.4f} ({:.4f})"".format(score.mean(), score.std()))",stacked-regressions-top-4-on-leaderboard.ipynb
We first define a rmsle evaluation function ,"def rmsle(y, y_pred):
 return np.sqrt(mean_squared_error(y, y_pred))",stacked-regressions-top-4-on-leaderboard.ipynb
StackedRegressor:,"stacked_averaged_models.fit(train.values, y_train)
stacked_train_pred = stacked_averaged_models.predict(train.values)
stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))
print(rmsle(y_train, stacked_train_pred))",stacked-regressions-top-4-on-leaderboard.ipynb
XGBoost:,"model_xgb.fit(train, y_train)
xgb_train_pred = model_xgb.predict(train)
xgb_pred = np.expm1(model_xgb.predict(test))
print(rmsle(y_train, xgb_train_pred))",stacked-regressions-top-4-on-leaderboard.ipynb
LightGBM:,"model_lgb.fit(train, y_train)
lgb_train_pred = model_lgb.predict(train)
lgb_pred = np.expm1(model_lgb.predict(test.values))
print(rmsle(y_train, lgb_train_pred))",stacked-regressions-top-4-on-leaderboard.ipynb
Ensemble prediction:,ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15,stacked-regressions-top-4-on-leaderboard.ipynb
Submission,"sub = pd.DataFrame()
sub['Id'] = test_ID
sub['SalePrice'] = ensemble
sub.to_csv('submission.csv',index=False)",stacked-regressions-top-4-on-leaderboard.ipynb
linear algebra,import numpy as np ,start-from-here-disaster-tweets-eda-basic-model.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,start-from-here-disaster-tweets-eda-basic-model.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,start-from-here-disaster-tweets-eda-basic-model.ipynb
Installing textstat package.,!pip install textstat,start-from-here-disaster-tweets-eda-basic-model.ipynb
 Importing necessary modules ,import string ,start-from-here-disaster-tweets-eda-basic-model.ipynb
utility functions:,"def plot_readability(a , b , title , bins = 0.1 , colors =['#3A4750' , '#F64E8B']) : ",start-from-here-disaster-tweets-eda-basic-model.ipynb
 Importing Dataframes ,"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')
test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')
sub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')",start-from-here-disaster-tweets-eda-basic-model.ipynb
glimpse at train dataset,train.head () ,start-from-here-disaster-tweets-eda-basic-model.ipynb
glimpse at test dataset,test.head () ,start-from-here-disaster-tweets-eda-basic-model.ipynb
some basic cleaning,train['text']= train['text']. apply(lambda x : cleanhtml(x)) ,start-from-here-disaster-tweets-eda-basic-model.ipynb
removing url tags,train['text']= train['text']. apply(lambda x : removeurl(x)) ,start-from-here-disaster-tweets-eda-basic-model.ipynb
 Target Value Distribution ,cnt_srs = train['target']. value_counts () ,start-from-here-disaster-tweets-eda-basic-model.ipynb
4.1 Number of tweets according to location top 20 ,"cnt_ = train['location'].value_counts()
cnt_.reset_index()
cnt_ = cnt_[:20,]
trace1 = go.Bar(
 x = cnt_.index,
 y = cnt_.values,
 name = ""Number of tweets in dataset according to location"",
 marker = dict(color = 'rgba(200, 74, 55, 0.5)',
 line=dict(color='rgb(0,0,0)',width=1.5)),
 )

data = [trace1]
layout = go.Layout(barmode = ""group"",title = 'Number of tweets in dataset according to location')
fig = go.Figure(data = data, layout = layout)
py.iplot(fig)",start-from-here-disaster-tweets-eda-basic-model.ipynb
4.2 Number of tweets according to location per class 0 or1 ,"train1_df = train[train[""target""]==1]
train0_df = train[train[""target""]==0]
cnt_1 = train1_df['location'].value_counts()
cnt_1.reset_index()
cnt_1 = cnt_1[:20,]

cnt_0 = train0_df['location'].value_counts()
cnt_0.reset_index()
cnt_0 = cnt_0[:20,]

trace1 = go.Bar(
 x = cnt_1.index,
 y = cnt_1.values,
 name = ""Number of tweets about real disaster location wise"",
 marker = dict(color = 'rgba(255, 74, 55, 0.5)',
 line=dict(color='rgb(0,0,0)',width=1.5)),
 )
trace0 = go.Bar(
 x = cnt_0.index,
 y = cnt_0.values,
 name = ""Number of tweets other than real disaster location wise"",
 marker = dict(color = 'rgba(79, 82, 97, 0.5)',
 line=dict(color='rgb(0,0,0)',width=1.5)),
 )


data = [trace0,trace1]
layout = go.Layout(barmode = 'stack',title = 'Number of tweets in dataset according to location')
fig = go.Figure(data = data, layout = layout)
py.iplot(fig)",start-from-here-disaster-tweets-eda-basic-model.ipynb
Getting latitude and longitudes of locations for plotting.,"df = train['location'].value_counts()[:20,]
df = pd.DataFrame(df)
df = df.reset_index()
df.columns = ['location', 'counts'] 
geolocator = Nominatim(user_agent=""specify_your_app_name_here"")
geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)
dictt_latitude = {}
dictt_longitude = {}
for i in df['location'].values:
 print(i)
 location = geocode(i)
 dictt_latitude[i] = location.latitude
 dictt_longitude[i] = location.longitude
df['latitude']= df['location'].map(dictt_latitude)
df['longitude'] = df['location'].map(dictt_longitude)",start-from-here-disaster-tweets-eda-basic-model.ipynb
 Word clouds of each class ,"from wordcloud import WordCloud , STOPWORDS ",start-from-here-disaster-tweets-eda-basic-model.ipynb
"By observing the above word cloud we can see some words like earthquake,fire,wildfires etc.., which refer to real disasters. ","plot_wordcloud(train[train[""target""]==0], title=""Word Cloud of tweets if not a real disaster"")",start-from-here-disaster-tweets-eda-basic-model.ipynb
6.1 Word Frequency,from collections import defaultdict ,start-from-here-disaster-tweets-eda-basic-model.ipynb
custom function for ngram generation,"def generate_ngrams(text , n_gram = 1): ",start-from-here-disaster-tweets-eda-basic-model.ipynb
custom function for horizontal bar chart,"def horizontal_bar_chart(df , color): ",start-from-here-disaster-tweets-eda-basic-model.ipynb
6.2 Bigram plots,freq_dict = defaultdict(int) ,start-from-here-disaster-tweets-eda-basic-model.ipynb
6.3 Trigram plots,freq_dict = defaultdict(int) ,start-from-here-disaster-tweets-eda-basic-model.ipynb
 Creating Meta Features:Now we will create some meta features and then look at how they are distributed between the classes. The ones that we will create are Number of words in the text Number of unique words in the text Number of characters in the text Number of stopwords Number of punctuations Number of upper case words Number of title case words Average length of the words ,"train[""num_words""]= train[""text""]. apply(lambda x : len(str(x). split ())) ",start-from-here-disaster-tweets-eda-basic-model.ipynb
Number of unique words in the text,"train[""num_unique_words""]= train[""text""]. apply(lambda x : len(set(str(x). split ()))) ",start-from-here-disaster-tweets-eda-basic-model.ipynb
Number of characters in the text,"train[""num_chars""]= train[""text""]. apply(lambda x : len(str(x))) ",start-from-here-disaster-tweets-eda-basic-model.ipynb
Number of stopwords in the text,"train[""num_stopwords""]= train[""text""]. apply(lambda x : len ([w for w in str(x). lower (). split ()if w in STOPWORDS])) ",start-from-here-disaster-tweets-eda-basic-model.ipynb
Number of punctuations in the text,"train[""num_punctuations""]= train['text']. apply(lambda x : len ([c for c in str(x)if c in string.punctuation])) ",start-from-here-disaster-tweets-eda-basic-model.ipynb
Number of title case words in the text,"train[""num_words_upper""]= train[""text""]. apply(lambda x : len ([w for w in str(x). split ()if w.isupper()])) ",start-from-here-disaster-tweets-eda-basic-model.ipynb
Number of title case words in the text,"train[""num_words_title""]= train[""text""]. apply(lambda x : len ([w for w in str(x). split ()if w.istitle()])) ",start-from-here-disaster-tweets-eda-basic-model.ipynb
Average length of the words in the text,"train[""mean_word_len""]= train[""text""]. apply(lambda x : np.mean ([len(w)for w in str(x). split()])) ",start-from-here-disaster-tweets-eda-basic-model.ipynb
truncation for better visuals,train['num_words']. loc[train['num_words']> 60]= 100 ,start-from-here-disaster-tweets-eda-basic-model.ipynb
truncation for better visuals,train['num_punctuations']. loc[train['num_punctuations']> 25]= 25 ,start-from-here-disaster-tweets-eda-basic-model.ipynb
truncation for better visuals,train['num_chars']. loc[train['num_chars']> 350]= 350 ,start-from-here-disaster-tweets-eda-basic-model.ipynb
8.1 Histogram Plots of number of words per each class 0 or 1 ,"train1_df = train[train[""target""]== 1] ",start-from-here-disaster-tweets-eda-basic-model.ipynb
Overlay both histograms,fig.update_layout(barmode = 'stack') ,start-from-here-disaster-tweets-eda-basic-model.ipynb
Reduce opacity to see both histograms,fig.update_traces(opacity = 0.75) ,start-from-here-disaster-tweets-eda-basic-model.ipynb
8.2 Histogram Plots of number of characters per each class 0 or 1 ,fig = go.Figure () ,start-from-here-disaster-tweets-eda-basic-model.ipynb
Overlay both histograms,fig.update_layout(barmode = 'stack') ,start-from-here-disaster-tweets-eda-basic-model.ipynb
Reduce opacity to see both histograms,fig.update_traces(opacity = 0.75) ,start-from-here-disaster-tweets-eda-basic-model.ipynb
8.3 Histogram Plots of number of punctuations per each class 0 or 1 ,fig = go.Figure () ,start-from-here-disaster-tweets-eda-basic-model.ipynb
Overlay both histograms,fig.update_layout(barmode = 'stack') ,start-from-here-disaster-tweets-eda-basic-model.ipynb
Reduce opacity to see both histograms,fig.update_traces(opacity = 1) ,start-from-here-disaster-tweets-eda-basic-model.ipynb
8.4 Histogram plots of number of words in train and test sets,fig = go.Figure () ,start-from-here-disaster-tweets-eda-basic-model.ipynb
Overlay both histograms,fig.update_layout(barmode = 'stack') ,start-from-here-disaster-tweets-eda-basic-model.ipynb
Reduce opacity to see both histograms,fig.update_traces(opacity = 0.75) ,start-from-here-disaster-tweets-eda-basic-model.ipynb
8.5 Histogram plots of number of chars in train and test sets,fig = go.Figure () ,start-from-here-disaster-tweets-eda-basic-model.ipynb
Overlay both histograms,fig.update_layout(barmode = 'stack') ,start-from-here-disaster-tweets-eda-basic-model.ipynb
Reduce opacity to see both histograms,fig.update_traces(opacity = 0.75) ,start-from-here-disaster-tweets-eda-basic-model.ipynb
8.6 Histogram plots of number of punctuations in train and test sets,fig = go.Figure () ,start-from-here-disaster-tweets-eda-basic-model.ipynb
Overlay both histograms,fig.update_layout(barmode = 'stack') ,start-from-here-disaster-tweets-eda-basic-model.ipynb
Reduce opacity to see both histograms,fig.update_traces(opacity = 0.75) ,start-from-here-disaster-tweets-eda-basic-model.ipynb
Score Difficulty 90 100 Very Easy 80 89 Easy 70 79 Fairly Easy 60 69 Standard 50 59 Fairly Difficult 30 49 Difficult 0 29 Very Confusing Read More : Wikipedia,"tqdm.pandas()
fre_notreal = np.array(train[""text""][train[""target""] == 0].progress_apply(textstat.flesch_reading_ease))
fre_real = np.array(train[""text""][train[""target""] == 1].progress_apply(textstat.flesch_reading_ease))
plot_readability(fre_notreal,fre_real,""Flesch Reading Ease"",20)",start-from-here-disaster-tweets-eda-basic-model.ipynb
"9.2 The Flesch Kincaid Grade Level These readability tests are used extensively in the field of education. The Flesch Kincaid Grade Level Formula instead presents a score as a U.S. grade level, making it easier for teachers, parents, librarians, and others to judge the readability level of various books and texts. It can also mean the number of years of education generally required to understand this text, relevant when the formula results in a number greater than 10. The grade level is calculated with the following formula: A score of 9.3 means that a ninth grader would be able to read the document. Read more: Wikipedia ","fkg_notreal = np.array(train[""text""][train[""target""] == 0].progress_apply(textstat.flesch_kincaid_grade))
fkg_real = np.array(train[""text""][train[""target""] == 1].progress_apply(textstat.flesch_kincaid_grade))
plot_readability(fkg_notreal,fkg_real,""Flesch Kincaid Grade"",4,['#C1D37F','#491F21'])",start-from-here-disaster-tweets-eda-basic-model.ipynb
"9.3 The Fog Scale Gunning FOG Formula In linguistics, the Gunning fog index is a readability test for English writing. The index estimates the years of formal education a person needs to understand the text on the first reading. For instance, a fog index of 12 requires the reading level of a United States high school senior around 18 years old . The formula to calculate Fog scale: Read more : Wikipedia ","fog_notreal = np.array(train[""text""][train[""target""] == 0].progress_apply(textstat.gunning_fog))
fog_real = np.array(train[""text""][train[""target""] == 1].progress_apply(textstat.gunning_fog))
plot_readability(fog_notreal,fog_real,""The Fog Scale (Gunning FOG Formula)"",4,['#E2D58B','#CDE77F'])",start-from-here-disaster-tweets-eda-basic-model.ipynb
"9.4 Automated Readability Index Returns the ARI Automated Readability Index which outputs a number that approximates the grade level needed to comprehend the text.For example if the ARI is 6.5, then the grade level to comprehend the text is 6th to 7th grade. Formula to calculate ARI: Read More: Wikipedia ","ari_notreal = np.array(train[""text""][train[""target""] == 0].progress_apply(textstat.automated_readability_index))
ari_real = np.array(train[""text""][train[""target""] == 1].progress_apply(textstat.automated_readability_index))
plot_readability(ari_notreal,ari_real,""Automated Readability Index"",10,['#488286','#FF934F'])",start-from-here-disaster-tweets-eda-basic-model.ipynb
9.5 The Coleman Liau Index Returns the grade level of the text using the Coleman Liau Formula. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document. The Coleman Liau index is calculated with the following formula: L is the average number of letters per 100 words and S is the average number of sentences per 100 words. Read More : Wikipedia ,"cli_notreal = np.array(train[""text""][train[""target""] == 0].progress_apply(textstat.coleman_liau_index))
cli_real = np.array(train[""text""][train[""target""] == 1].progress_apply(textstat.coleman_liau_index))
plot_readability(cli_notreal,cli_real,""The Coleman-Liau Index"",10,['#8491A3','#2B2D42'])",start-from-here-disaster-tweets-eda-basic-model.ipynb
9.6 Linsear Write Formula Returns the grade level of the text using the Coleman Liau Formula. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document. Read More : Wikipedia ,"lwf_notreal = np.array(train[""text""][train[""target""] == 0].progress_apply(textstat.linsear_write_formula))
lwf_real = np.array(train[""text""][train[""target""] == 1].progress_apply(textstat.linsear_write_formula))
plot_readability(lwf_notreal,lwf_real,""Linsear Write Formula"",2,['#8D99AE','#EF233C'])",start-from-here-disaster-tweets-eda-basic-model.ipynb
"9.7 Dale Chall Readability Score Different from other tests, since it uses a lookup table of the most commonly used 3000 English words. Thus it returns the grade level using the New Dale Chall Formula. The formula for calculating the raw score of the Dale Chall readability score is given below: Score Understood by 4.9 or lower average 4th grade student or lower 5.0 5.9 average 5th or 6th grade student 6.0 6.9 average 7th or 8th grade student 7.0 7.9 average 9th or 10th grade student 8.0 8.9 average 11th or 12th grade student 9.0 9.9 average 13th to 15th grade college student Read More : Wikipedia ","dcr_notreal = np.array(train[""text""][train[""target""] == 0].progress_apply(textstat.dale_chall_readability_score))
dcr_real = np.array(train[""text""][train[""target""] == 1].progress_apply(textstat.dale_chall_readability_score))
plot_readability(dcr_notreal,dcr_real,""Dale-Chall Readability Score"",1,['#C65D17','#DDB967'])",start-from-here-disaster-tweets-eda-basic-model.ipynb
"9.8 Readability Consensus based upon all the above tests Based upon all the above tests, returns the estimated school grade level required to understand the text. ","def consensus_all(text):
 return textstat.text_standard(text,float_output=True)

con_notreal = np.array(train[""text""][train[""target""] == 0].progress_apply(consensus_all))
con_real = np.array(train[""text""][train[""target""] == 1].progress_apply(consensus_all))
plot_readability(con_notreal,con_real,""Readability Consensus based upon all the above tests"",2)",start-from-here-disaster-tweets-eda-basic-model.ipynb
 Topic Modelling Topic modeling is a machine learning technique that automatically analyzes text data to determine cluster words for a set of documents. This is known as unsupervised machine learning because it doesn t require a predefined list of tags or training data that s been previously classified by humans. Topic modeling involves counting words and grouping similar word patterns to infer topics within unstructured data. Read More : Here ,"notreal_text = train[""text""][ train[""target""]== 0]. progress_apply(spacy_tokenizer) ",start-from-here-disaster-tweets-eda-basic-model.ipynb
count vectorization,"vectorizer_notreal = CountVectorizer(min_df = 5 , max_df = 0.9 , stop_words = 'english' , lowercase = True , token_pattern = '[a-zA-Z\-][a-zA-Z\-]{2,}') ",start-from-here-disaster-tweets-eda-basic-model.ipynb
Latent Dirichlet Allocation Model,"lda_notreal = LatentDirichletAllocation(n_components = 10 , max_iter = 5 , learning_method = 'online' , verbose = True) ",start-from-here-disaster-tweets-eda-basic-model.ipynb
10.2 Printing keywords,"def selected_topics(model, vectorizer, top_n=10):
 for idx, topic in enumerate(model.components_):
 print(""Topic %d:"" % (idx))
 print([(vectorizer.get_feature_names()[i], topic[i])
 for i in topic.argsort()[:-top_n - 1:-1]]) ",start-from-here-disaster-tweets-eda-basic-model.ipynb
10.3 Visualizing LDA results of not a real disaster tweets with pyLDAvis,"pyLDAvis.enable_notebook()
dash = pyLDAvis.sklearn.prepare(lda_notreal, notreal_vectorized, vectorizer_notreal, mds='tsne')
dash",start-from-here-disaster-tweets-eda-basic-model.ipynb
10.4 Visualizing LDA results of a real disaster tweets with pyLDAvis,"pyLDAvis.enable_notebook()
dash = pyLDAvis.sklearn.prepare(lda_real, real_vectorized, vectorizer_real, mds='tsne')
dash",start-from-here-disaster-tweets-eda-basic-model.ipynb
 Tfidf Vectorization ,"tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))
tfidf_vec.fit_transform(train['text'].values.tolist() + test['text'].values.tolist())
train_tfidf = tfidf_vec.transform(train['text'].values.tolist())
test_tfidf = tfidf_vec.transform(test['text'].values.tolist())",start-from-here-disaster-tweets-eda-basic-model.ipynb
 Building basic Logistic regression model ,"train_y = train[""target""].values

def runModel(train_X, train_y, test_X, test_y, test_X2):
 model = linear_model.LogisticRegression(C=5., solver='sag')
 model.fit(train_X, train_y)
 pred_test_y = model.predict_proba(test_X)[:,1]
 pred_test_y2 = model.predict_proba(test_X2)[:,1]
 return pred_test_y, pred_test_y2, model

print(""Building model."")
cv_scores = []
pred_full_test = 0
pred_train = np.zeros([train.shape[0]])
kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)
for dev_index, val_index in kf.split(train):
 dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]
 dev_y, val_y = train_y[dev_index], train_y[val_index]
 pred_val_y, pred_test_y, model = runModel(dev_X, dev_y, val_X, val_y, test_tfidf)
 pred_full_test = pred_full_test + pred_test_y
 pred_train[val_index] = pred_val_y
 cv_scores.append(metrics.log_loss(val_y, pred_val_y))
 break",start-from-here-disaster-tweets-eda-basic-model.ipynb
 Threshold value search for better score ,from tqdm import tqdm ,start-from-here-disaster-tweets-eda-basic-model.ipynb
reference: , best_threshold = 0 ,start-from-here-disaster-tweets-eda-basic-model.ipynb
Now let us look at the important words used for classifying when target 1.,"import eli5
eli5.show_weights(model, vec=tfidf_vec, top=100, feature_filter=lambda x: x != '<BIAS>')",start-from-here-disaster-tweets-eda-basic-model.ipynb
loading markovify module,import markovify ,start-from-here-disaster-tweets-eda-basic-model.ipynb
preparing dataset,"data_notreal = train[""text""][ train[""target""]== 0] ",start-from-here-disaster-tweets-eda-basic-model.ipynb
14.1 Building text model,"text_model_notreal = markovify.NewlineText(data_notreal, state_size = 2)
text_model_real = markovify.NewlineText(data_real, state_size = 2)",start-from-here-disaster-tweets-eda-basic-model.ipynb
14.2 Generating tweets about not a real disaster,"for i in range(10):
 print(text_model_notreal.make_sentence())",start-from-here-disaster-tweets-eda-basic-model.ipynb
14.3 Generating tweets about real disaster,"for i in range(10):
 print(text_model_real.make_sentence())",start-from-here-disaster-tweets-eda-basic-model.ipynb
"Example Red Wine QualityNow we know everything we need to start training deep learning models. So let s see it in action! We ll use the Red Wine Quality dataset.This dataset consists of physiochemical measurements from about 1600 Portuguese red wines. Also included is a quality rating for each wine from blind taste tests. How well can we predict a wine s perceived quality from these measurements?We ve put all of the data preparation into this next hidden cell. It s not essential to what follows so feel free to skip it. One thing you might note for now though is that we ve rescaled each feature to lie in the interval . As we ll discuss more in Lesson 5, neural networks tend to perform best when their inputs are on a common scale.",import pandas as pd ,stochastic-gradient-descent.ipynb
Create training and validation splits,"df_train = red_wine.sample(frac = 0.7 , random_state = 0) ",stochastic-gradient-descent.ipynb
"Scale to 0, 1 ",max_ = df_train.max(axis = 0) ,stochastic-gradient-descent.ipynb
Split features and target,"X_train = df_train.drop('quality' , axis = 1) ",stochastic-gradient-descent.ipynb
How many inputs should this network have? We can discover this by looking at the number of columns in the data matrix. Be sure not to include the target quality here only the input features.,print(X_train.shape),stochastic-gradient-descent.ipynb
Eleven columns means eleven inputs.We ve chosen a three layer network with over 1500 neurons. This network should be capable of learning fairly complex relationships in the data.,"from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
 layers.Dense(512, activation='relu', input_shape=[11]),
 layers.Dense(512, activation='relu'),
 layers.Dense(512, activation='relu'),
 layers.Dense(1),
])",stochastic-gradient-descent.ipynb
"Deciding the architecture of your model should be part of a process. Start simple and use the validation loss as your guide. You ll learn more about model development in the exercises.After defining the model, we compile in the optimizer and loss function.","model.compile(
 optimizer='adam',
 loss='mae',
)",stochastic-gradient-descent.ipynb
Now we re ready to start the training! We ve told Keras to feed the optimizer 256 rows of the training data at a time the batch size and to do that 10 times all the way through the dataset the epochs .,"history = model.fit(
 X_train, y_train,
 validation_data=(X_valid, y_valid),
 batch_size=256,
 epochs=10,
)",stochastic-gradient-descent.ipynb
"You can see that Keras will keep you updated on the loss as the model trains.Often, a better way to view the loss though is to plot it. The fit method in fact keeps a record of the loss produced during training in a History object. We ll convert the data to a Pandas dataframe, which makes the plotting easy.",import pandas as pd ,stochastic-gradient-descent.ipynb
convert the training history to a dataframe,history_df = pd.DataFrame(history.history) ,stochastic-gradient-descent.ipynb
use Pandas native plot method,history_df['loss']. plot (); ,stochastic-gradient-descent.ipynb
Importing the library,"import pandas as pd
import numpy as np
import calendar

import plotly.express as px
from plotly.subplots import make_subplots
import plotly.figure_factory as ff
import plotly.offline as offline
import plotly.graph_objs as go
offline.init_notebook_mode(connected = True)",store-sales-analysis-time-serie.ipynb
Dataset The data is about store sales forcasting where contaning 54 stores having 33 products in 16 states. After combining the data we have df train1 dataset. ,"df_holi = pd.read_csv('../input/store-sales-time-series-forecasting/holidays_events.csv')
df_oil = pd.read_csv('../input/store-sales-time-series-forecasting/oil.csv')
df_stores = pd.read_csv('../input/store-sales-time-series-forecasting/stores.csv')
df_trans = pd.read_csv('../input/store-sales-time-series-forecasting/transactions.csv')

df_train = pd.read_csv('../input/store-sales-time-series-forecasting/train.csv')
df_test = pd.read_csv('../input/store-sales-time-series-forecasting/test.csv')",store-sales-analysis-time-serie.ipynb
copying of train data and merging other data,"df_train1 = df_train.merge(df_holi , on = 'date' , how = 'left') ",store-sales-analysis-time-serie.ipynb
data,"df_st_sa = df_train1.groupby('store_type'). agg({ ""sales"" : ""mean"" }). reset_index (). sort_values(by = 'sales' , ascending = False) ",store-sales-analysis-time-serie.ipynb
chart color,df_fa_sa['color']= '#496595' ,store-sales-analysis-time-serie.ipynb
data,"df_2013 = df_train1[df_train1['year']== 2013][['month' , 'sales']] ",store-sales-analysis-time-serie.ipynb
top levels,"top_labels =['2013' , '2014' , '2015' , '2016' , '2017'] ",store-sales-analysis-time-serie.ipynb
data,"df_m_sa = df_train1.groupby('month'). agg({ ""sales"" : ""mean"" }). reset_index () ",store-sales-analysis-time-serie.ipynb
chart color,df_m_sa['color']= '#496595' ,store-sales-analysis-time-serie.ipynb
data,"df_dw_sa = df_train1.groupby('day_of_week'). agg({ ""sales"" : ""mean"" }). reset_index () ",store-sales-analysis-time-serie.ipynb
data,"df_st_ht = df_train1.groupby (['store_type' , 'holiday_type']).agg({ ""sales"" : ""mean"" }). reset_index () ",store-sales-analysis-time-serie.ipynb
data,"df_y_m_st = df_train1.groupby (['year' , 'month' , 'store_type']).agg({ ""sales"" : ""mean"" }). reset_index () ",store-sales-analysis-time-serie.ipynb
data,"df_m_ht = df_train1.groupby (['month' , 'holiday_type']).agg({ ""sales"" : ""mean"" }). reset_index () ",store-sales-analysis-time-serie.ipynb
data,"df_y_m_ht = df_train1.groupby (['year' , 'month' , 'holiday_type']).agg({ ""sales"" : ""mean"" }). reset_index () ",store-sales-analysis-time-serie.ipynb
linear algebra,import numpy as np ,store-sales-eda-prediction-with-ts.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,store-sales-eda-prediction-with-ts.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,store-sales-eda-prediction-with-ts.ipynb
back to top 1.1 Loading of Libraries Load all the libraries to be used,import warnings ,store-sales-eda-prediction-with-ts.ipynb
basic dates,import numpy as np ,store-sales-eda-prediction-with-ts.ipynb
data visualization,import matplotlib.pyplot as plt ,store-sales-eda-prediction-with-ts.ipynb
advanced vizs,import seaborn as sns ,store-sales-eda-prediction-with-ts.ipynb
statistics,from statsmodels.distributions.empirical_distribution import ECDF ,store-sales-eda-prediction-with-ts.ipynb
time series analysis,from statsmodels.tsa.seasonal import seasonal_decompose ,store-sales-eda-prediction-with-ts.ipynb
prophet by Facebook,from fbprophet import Prophet ,store-sales-eda-prediction-with-ts.ipynb
back to top 1.2 Loading of Datasets Let us load the datasets,path = '/kaggle/input/store-sales-time-series-forecasting/' ,store-sales-eda-prediction-with-ts.ipynb
time series as indexes,train.index ,store-sales-eda-prediction-with-ts.ipynb
back to top 2.1 Number of Roww and Columns. Let us load the datasets. Check the shape and info about the datasets,"print('Number of train samples: ', train.shape)
print('Number of test samples: ', test.shape)
print('Number of store data: ', store_data.shape)
print('Number of Holiday data: ', holidays_data.shape)
print('Number of Oil Price data: ', oil_data.shape)
print('Number of features: ', len(train.columns))
print(train.info())
print(train.columns)
print(train.head())",store-sales-eda-prediction-with-ts.ipynb
Short description of Key Fields date: Date of entry store nbr: Store Number sales: the turnover for any given day target variable . family: Product Family store type: Type of Store cluster: Store Cluster city: City state: State the store is located in onpromotion: indicates whether a store is running a promo on that day. holiday type locale : If it was a holiday and type of holiday transactions: Number of transactions sales: sales volume ,"train.info()
train.isnull().sum()",store-sales-eda-prediction-with-ts.ipynb
data extraction,train['Year']= train.index.year ,store-sales-eda-prediction-with-ts.ipynb
Merge the Training data with Store Data for an expanded view,"train_store=train.merge(store_data, on = 'store_nbr', how='left')
train_store.head()",store-sales-eda-prediction-with-ts.ipynb
Check for Unique Values,"ts_unique = train_store.nunique()
for index, value in ts_unique.items():
 print(f""Index : {index}, Value : {value}"")
 if value < 50:
 print('*'*50)
 print('keys')
 print('*'*50)
 print(train_store[index].unique())
 else:
 print('*'*50)
 print('skipping', index)
 print('More than 50 unique elements')
 print('*'*50)
 ",store-sales-eda-prediction-with-ts.ipynb
"Family , Cluster, Type, State, City and Date elements with less than 50 unique values",import matplotlib.pyplot as plt ,store-sales-eda-prediction-with-ts.ipynb
data,"ts_sales_type = train_store.groupby('type'). agg({ ""sales"" : ""mean"" }). reset_index (). sort_values(by = 'sales' , ascending = False) ",store-sales-eda-prediction-with-ts.ipynb
to format into seaborn,"sns.set(style = ""ticks"") ",store-sales-eda-prediction-with-ts.ipynb
basic color for plots,c = '#386B7F' ,store-sales-eda-prediction-with-ts.ipynb
plot second ECDF,plt.subplot(212) ,store-sales-eda-prediction-with-ts.ipynb
Let us ejoin store data with stores by number,"print(""Joining train set with an additional store information."") ",store-sales-eda-prediction-with-ts.ipynb
that are present in both train and store sets are merged together,"train_store = pd.merge(train , store_data , how = 'inner' , on = 'store_nbr') ",store-sales-eda-prediction-with-ts.ipynb
Plot a Barplot on Sales and Family columns,"plt.figure(figsize=(16,12))
sns.barplot(x=""sales"", y=""family"", data=train_store.sort_values(by=['sales','family'], ascending=False),
 label=""Sales"", color=""b"")",store-sales-eda-prediction-with-ts.ipynb
Plot a Seaborn Relplot,"plt.figure(figsize=(24,18))
sns.relplot(x=""onpromotion"", y=""sales"", hue=""city"", size=""sales"",
 sizes=(40, 400), alpha=.5, palette=""muted"",
 height=6, data=train_store)",store-sales-eda-prediction-with-ts.ipynb
Some EDA on Holidays,"plt.subplots(1,2,figsize=(20,20))
plt.subplot(211)
plt.title('Counts of Type Of Holidays')
sns.countplot(x=holidays_data.type, hue=holidays_data.locale)
plt.legend(loc='upper right')
plt.subplot(212)
plt.title('Counts of type of holiday')
sns.countplot(x=holidays_data.locale)",store-sales-eda-prediction-with-ts.ipynb
"Plot a relplot on City , Sales and Family with a hue on State","g = sns.relplot(
 data=train_store,
 x=""city"", y=""sales"", col=""family"", hue=""state"",
 kind=""line"", palette=""crest"", linewidth=4, zorder=5,
 col_wrap=3, height=2, aspect=1.5, legend=False,
)",store-sales-eda-prediction-with-ts.ipynb
Some Store Data EDA,"plt.subplots(1,3,figsize=(21,20))
plt.subplot(311)
sns.countplot(x=store_data.type, order = store_data.type.value_counts().index)
plt.subplot(312)
sns.countplot(y=store_data.cluster, order = store_data.cluster.value_counts().index)
plt.subplot(313)
sns.countplot(y=store_data.city, order = store_data.city.value_counts().index)",store-sales-eda-prediction-with-ts.ipynb
Plot a Category Plot on Type and Cluster Use Strip Plot,"sns.catplot(x = 'type', y='cluster',data=store_data, kind='strip')",store-sales-eda-prediction-with-ts.ipynb
Plot a Category Plot on Type and Cluster Use Swarm Plot,"sns.catplot(x = 'type', y='cluster',data=store_data, kind='swarm')",store-sales-eda-prediction-with-ts.ipynb
Plot Barchart on Store Sales by Store and Sales Value,"from matplotlib.ticker import ScalarFormatter , FormatStrFormatter ",store-sales-eda-prediction-with-ts.ipynb
annotation here,for p in ax.patches : ,store-sales-eda-prediction-with-ts.ipynb
Plot Barchart on Store Sales by City and Sales Value,"from matplotlib.ticker import ScalarFormatter , FormatStrFormatter ",store-sales-eda-prediction-with-ts.ipynb
annotation here,for p in ax.patches : ,store-sales-eda-prediction-with-ts.ipynb
Plot Barchart on Store Sales by State and Sales Value,"from matplotlib.ticker import ScalarFormatter, FormatStrFormatter
sns.set(font_scale=0.8)
x=train_store.groupby('state')['store_nbr','type','sales'].sum()
x=x.sort_values(by='sales',ascending=False)
x=x.iloc[0:55].reset_index()

plt.figure(figsize=(20,15))
ax= sns.barplot( x.sales ,x.state, alpha=0.8, palette=""Spectral"")

for p in ax.patches:
 ax.annotate(""%.2f"" % round((p.get_width()/1000000),2), (p.get_x() + p.get_width(), p.get_y() + 1.2),
 xytext=(5, 15), textcoords='offset points')

sns.set(font_scale=0.8)
plt.title(""Sales per State"", fontsize=20)
plt.ylabel('Total Sales', fontsize=12)
plt.xlabel('State', fontsize=12)
loc, labels = plt.xticks()
ax.set_xticklabels(labels, rotation=60)
sns.despine(left=True, bottom=True)
sns.set_style('whitegrid')
plt.show()",store-sales-eda-prediction-with-ts.ipynb
importing data,"train = pd.read_csv(path + 'train.csv' , parse_dates = True , low_memory = False , index_col = 'date') ",store-sales-eda-prediction-with-ts.ipynb
sales for the store number 1 StoreType C ,"sales = train[train.store_nbr == 44]. loc[: ,['date' , 'sales']] ",store-sales-eda-prediction-with-ts.ipynb
to datetime64,sales['date']= pd.DatetimeIndex(sales['date']) ,store-sales-eda-prediction-with-ts.ipynb
plot daily sales,"ax = sales.set_index('ds'). plot(figsize =(12 , 4), color = c) ",store-sales-eda-prediction-with-ts.ipynb
predictions,forecast = my_model.predict(future_dates) ,store-sales-eda-prediction-with-ts.ipynb
preditions for last week,"forecast[[ 'ds' , 'yhat' , 'yhat_lower' , 'yhat_upper']].tail(7) ",store-sales-eda-prediction-with-ts.ipynb
visualizing predicions,my_model.plot(forecast); ,store-sales-eda-prediction-with-ts.ipynb
Trying FastAI Tabular LibrariesThanks to the inspiration ,"from fastai.tabular.all import *
from sklearn.metrics import mean_squared_log_error",store-sales-eda-prediction-with-ts.ipynb
"Store Sales Forecasting Exploration NotebookIn this competition, the task is to predict the sales for the thousands of product families sold at Favorita stores located in Ecuador. The training data includes dates, store and product information, whether that item was being promoted, as well as the sales numbers. In this notebook, I have shared a started exploration eda for the dataset. I will keep updating1. Load DatasetLet s first load the dataset files","import numpy as np 
import pandas as pd 
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go

train = pd.read_csv(""/kaggle/input/store-sales-time-series-forecasting/train.csv"")
test = pd.read_csv(""/kaggle/input/store-sales-time-series-forecasting/test.csv"")
oil_df = pd.read_csv(""/kaggle/input/store-sales-time-series-forecasting/oil.csv"")
holidays_events = pd.read_csv(""/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv"")
stores = pd.read_csv(""/kaggle/input/store-sales-time-series-forecasting/stores.csv"")
txn = pd.read_csv(""/kaggle/input/store-sales-time-series-forecasting/transactions.csv"")

print (""Training Data Shape: "", train.shape)
print (""Testing Data Shape"", test.shape)

train.head()",store-sales-forecasting-exploration.ipynb
combine datasets,"train1 = train.merge(oil_df , on = 'date' , how = 'left') ",store-sales-forecasting-exploration.ipynb
" Exploratory AnalysisLet s take a look at the time series patterns in the dataset such as Average Sales over time, average sales over time by store type, by store name etc. ","agg = train1.groupby('date').agg({""sales"" : ""mean""}).reset_index()
fig = px.line(agg, x='date', y=""sales"")
fig.update_layout(title = ""Average Sales by Date"")
fig.show()

agg = train1.groupby('date').agg({""transactions"" : ""mean""}).reset_index()
fig = px.line(agg, x='date', y=""transactions"")
fig.update_layout(title = ""Average Transactions by Date"")
fig.show()",store-sales-forecasting-exploration.ipynb
Let s now look at various other columns and their related average sales ,"def vbar(col):
 temp = train1.groupby(col).agg({""sales"" : ""mean""}).reset_index()
 temp = temp.sort_values('sales', ascending = False)
 c = {
 'x' : list(temp['sales'])[:15][::-1], 
 'y' : list(temp[col])[:15][::-1],
 'title' : ""Average sales by ""+col
 }
 trace = go.Bar(y=[str(_) + "" "" for _ in c['y']], x=c['x'], orientation=""h"", marker=dict(color=""#f77e90""))
 return trace 

 layout = go.Layout(title=c['title'], 
 paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',
 xaxis_title="""", yaxis_title="""", width=650)
 fig = go.Figure([trace], layout=layout)
 fig.update_xaxes(tickangle=45, tickfont=dict(color='crimson'))
 fig.update_yaxes(tickangle=0, tickfont=dict(color='crimson'))
 fig.show()
 
trace1 = vbar('family') 
trace2 = vbar('store_type') 
trace3 = vbar('state') 
trace4 = vbar('city') 

titles = ['Store Family', 'Store Type', 'State', 'City']
titles = ['Top ' + _ + "" by Average Sales"" for _ in titles]
fig = make_subplots(rows=2, cols=2, subplot_titles = titles)

fig.add_trace(trace1, row=1, col=1)
fig.add_trace(trace2, row=1, col=2)
fig.add_trace(trace3, row=2, col=1)
fig.add_trace(trace4, row=2, col=2)

fig.update_layout(height=800, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', showlegend = False)
fig.show()",store-sales-forecasting-exploration.ipynb
" Feature EngineeringWe can create some additional features from the date column such as dayofweek, month, year etc. ","def create_ts_features(df):
 df['date'] = pd.to_datetime(df['date'])
 df['dayofweek'] = df['date'].dt.dayofweek
 df['quarter'] = df['date'].dt.quarter
 df['month'] = df['date'].dt.month
 df['year'] = df['date'].dt.year
 df['dayofyear'] = df['date'].dt.dayofyear
 df['dayofmonth'] = df['date'].dt.day
 return df
 
train1 = create_ts_features(train1)
test1 = create_ts_features(test1)
train1.head()",store-sales-forecasting-exploration.ipynb
 Import libraries ,import math ,store-sales-time-series-forecast-visualization.ipynb
Model 1 trend ,from pyearth import Earth ,store-sales-time-series-forecast-visualization.ipynb
Model 2,"from sklearn.ensemble import ExtraTreesRegressor , RandomForestRegressor ",store-sales-time-series-forecast-visualization.ipynb
switch off the warnings,"warnings.filterwarnings(""ignore"") ",store-sales-time-series-forecast-visualization.ipynb
 Read data ,"df_holidays = pd.read_csv('../input/store-sales-time-series-forecasting/holidays_events.csv', header = 0)
df_oil = pd.read_csv('../input/store-sales-time-series-forecasting/oil.csv', header = 0)
df_stores = pd.read_csv('../input/store-sales-time-series-forecasting/stores.csv', header = 0)
df_trans = pd.read_csv('../input/store-sales-time-series-forecasting/transactions.csv', header = 0)

df_train = pd.read_csv('../input/store-sales-time-series-forecasting/train.csv', header = 0)
df_test = pd.read_csv('../input/store-sales-time-series-forecasting/test.csv', header = 0)",store-sales-time-series-forecast-visualization.ipynb
"Also, we need to convert all date columns to datetime Pandas format:","df_holidays['date'] = pd.to_datetime(df_holidays['date'], format = ""%Y-%m-%d"")
df_oil['date'] = pd.to_datetime(df_oil['date'], format = ""%Y-%m-%d"")
df_trans['date'] = pd.to_datetime(df_trans['date'], format = ""%Y-%m-%d"")
df_train['date'] = pd.to_datetime(df_train['date'], format = ""%Y-%m-%d"")
df_test['date'] = pd.to_datetime(df_test['date'], format = ""%Y-%m-%d"")",store-sales-time-series-forecast-visualization.ipynb
check data,df_holidays.head(10) ,store-sales-time-series-forecast-visualization.ipynb
check data,df_oil.head(3) ,store-sales-time-series-forecast-visualization.ipynb
check data,df_stores.head(10) ,store-sales-time-series-forecast-visualization.ipynb
check data,df_trans.head(3) ,store-sales-time-series-forecast-visualization.ipynb
check data,df_train.head(10) ,store-sales-time-series-forecast-visualization.ipynb
check data,df_test.head(5) ,store-sales-time-series-forecast-visualization.ipynb
" Visualize data One of the biggest parts of the notebook. Here we can look through some variables and see some dependencies. Firstly, let s check the dependency of the oil from the date: ","fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(25,15))
df_oil.plot.line(x=""date"", y=""dcoilwtico"", color='b', title =""dcoilwtico"", ax = axes, rot=0)
plt.show()",store-sales-time-series-forecast-visualization.ipynb
"As we have so much rows in out dataset, it will be easier to group data, as example, by week or month. The aggregation will be made by mean.","def grouped(df, key, freq, col):
 """""" GROUP DATA WITH CERTAIN FREQUENCY """"""
 df_grouped = df.groupby([pd.Grouper(key=key, freq=freq)]).agg(mean = (col, 'mean'))
 df_grouped = df_grouped.reset_index()
 return df_grouped",store-sales-time-series-forecast-visualization.ipynb
check grouped data,"df_grouped_trans_w = grouped(df_trans , 'date' , 'W' , 'transactions') ",store-sales-time-series-forecast-visualization.ipynb
"And, for better forecasting we ll add time column to our dataframe.","def add_time(df, key, freq, col):
 """""" ADD COLUMN 'TIME' TO DF """"""
 df_grouped = grouped(df, key, freq, col)
 df_grouped['time'] = np.arange(len(df_grouped.index))
 column_time = df_grouped.pop('time')
 df_grouped.insert(1, 'time', column_time)
 return df_grouped",store-sales-time-series-forecast-visualization.ipynb
"So, now we can check the results of grouping on the example of df train grouped by weeks on sales, after that, mean was counted .","df_grouped_train_w = add_time(df_train , 'date' , 'W' , 'sales') ",store-sales-time-series-forecast-visualization.ipynb
check results,df_grouped_train_w.head () ,store-sales-time-series-forecast-visualization.ipynb
"3.1. Linear Regression After that, we can build some more plots. Linear regression is widely used in practice and adapts naturally to even complex forecasting tasks. The linear regression algorithm learns how to make a weighted sum from its input features.","fig , axes = plt.subplots(nrows = 3 , ncols = 1 , figsize =(30 , 20)) ",store-sales-time-series-forecast-visualization.ipynb
TRANSACTIONS WEEKLY ,"axes[0]. plot('date' , 'mean' , data = df_grouped_trans_w , color = 'grey' , marker = 'o') ",store-sales-time-series-forecast-visualization.ipynb
SALES WEEKLY ,"axes[1]. plot('time' , 'mean' , data = df_grouped_train_w , color = '0.75') ",store-sales-time-series-forecast-visualization.ipynb
"3.2 Lag feature To make a lag feature we shift the observations of the target series so that they appear to have occured later in time. Here we ve created a 1 step lag feature, though shifting by multiple steps is possible too. So, firstly, we should add lag to our data:","def add_lag(df, key, freq, col, lag):
 """""" ADD LAG """"""
 df_grouped = grouped(df, key, freq, col)
 name = 'Lag_' + str(lag)
 df_grouped['Lag'] = df_grouped['mean'].shift(lag)
 return df_grouped",store-sales-time-series-forecast-visualization.ipynb
Here we can check grouped data with lag:,"df_grouped_train_w_lag1 = add_lag(df_train , 'date' , 'W' , 'sales' , 1) ",store-sales-time-series-forecast-visualization.ipynb
check data,df_grouped_train_w_lag1.head () ,store-sales-time-series-forecast-visualization.ipynb
"So lag features let us fit curves to lag plots where each observation in a series is plotted against the previous observation. Let s build same plots, but with lag feature:","fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(30,20))
axes[0].plot('Lag', 'mean', data=df_grouped_train_w_lag1, color='0.75', linestyle=(0, (1, 10)))
axes[0].set_title(""Sales (grouped by week)"", fontsize=20)
axes[0] = sns.regplot(x='Lag', 
 y='mean', 
 data=df_grouped_train_w_lag1, 
 scatter_kws=dict(color='0.75'), 
 ax = axes[0])


axes[1].plot('Lag', 'mean', data=df_grouped_train_m_lag1, color='0.75', linestyle=(0, (1, 10)))
axes[1].set_title(""Sales (grouped by month)"", fontsize=20)
axes[1] = sns.regplot(x='Lag', 
 y='mean', 
 data=df_grouped_train_m_lag1, 
 scatter_kws=dict(color='0.75'), 
 line_kws={""color"": ""red""},
 ax = axes[1])

plt.show()",store-sales-time-series-forecast-visualization.ipynb
"3.3 Some more statistics visualizations In this block we are going to explore data. Firstly, let s count for each category in each dataset, value counts :","def plot_stats(df, column, ax, color, angle):
 """""" PLOT STATS OF DIFFERENT COLUMNS """"""
 count_classes = df[column].value_counts()
 ax = sns.barplot(x=count_classes.index, y=count_classes, ax=ax, palette=color)
 ax.set_title(column.upper(), fontsize=18)
 for tick in ax.get_xticklabels():
 tick.set_rotation(angle)",store-sales-time-series-forecast-visualization.ipynb
Here we can see stats for df holidays:,"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))
fig.autofmt_xdate()
fig.suptitle(""Stats of df_holidays"".upper())
plot_stats(df_holidays, ""type"", axes[0], ""pastel"", 45)
plot_stats(df_holidays, ""locale"", axes[1], ""rocket"", 45)
plt.show()",store-sales-time-series-forecast-visualization.ipynb
Here we count values for some columns of df stores:,"fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(20,40))
plot_stats(df_stores, ""city"", axes[0], ""mako_r"", 45)
plot_stats(df_stores, ""state"", axes[1], ""rocket_r"", 45)
plot_stats(df_stores, ""type"", axes[2], ""magma"", 0)
plot_stats(df_stores, ""cluster"", axes[3], ""viridis"", 0)
plt.show()",store-sales-time-series-forecast-visualization.ipynb
Let s plot pie chart for family of df train:,"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(20,10))
count_classes = df_train['family'].value_counts()
plt.title(""Stats of df_train"".upper())
colors = ['#ff9999','#66b3ff','#99ff99',
 '#ffcc99', '#ffccf9', '#ff99f8', 
 '#ff99af', '#ffe299', '#a8ff99',
 '#cc99ff', '#9e99ff', '#99c9ff',
 '#99f5ff', '#99ffe4', '#99ffaf']

plt.pie(count_classes, 
 labels = count_classes.index, 
 autopct='%1.1f%%',
 shadow=True, 
 startangle=90, 
 colors=colors)

plt.show()",store-sales-time-series-forecast-visualization.ipynb
"3.4 BoxPlot In addition, we can build some boxplots: for df oil df trans.","def plot_boxplot(palette, x, y, hue, ax, title):
 sns.set_theme(style=""ticks"", palette=palette)
 ax = sns.boxplot(x=x, y=y, hue=hue, ax=ax)
 ax.set_title(title, fontsize=18)",store-sales-time-series-forecast-visualization.ipynb
"3.5 Trend. Moving Average The trend component of a time series represents a persistent, long term change in the mean of the series. The trend is the slowest moving part of a series, the part representing the largest time scale of importance. In a time series of product sales, an increasing trend might be the effect of a market expansion as more people become aware of the product year by year.To see what kind of trend a time series might have, we can use a moving average plot. To compute a moving average of a time series, we compute the average of the values within a sliding window of some defined width. Each point on the graph represents the average of all the values in the series that fall within the window on either side. The idea is to smooth out any short term fluctuations in the series so that only long term changes remain.Below we can see the moving average plots for Transactions and Sales, colored in green.","def plot_moving_average(df, key, freq, col, window, min_periods, ax, title):
 df_grouped = grouped(df, key, freq, col)
 moving_average = df_grouped['mean'].rolling(window=window, center=True, min_periods=min_periods).mean() 
 ax = df_grouped['mean'].plot(color='0.75', linestyle='dashdot', ax=ax)
 ax = moving_average.plot(linewidth=3, color='g', ax=ax)
 ax.set_title(title, fontsize=18)",store-sales-time-series-forecast-visualization.ipynb
"3.6. Trend. Forecasting Trend We ll use a function from the statsmodels library called DeterministicProcess. Using this function will help us avoid some tricky failure cases that can arise with time series and linear regression. The order argument refers to polynomial order: 1 for linear, 2 for quadratic, 3 for cubic, and so on.","def plot_deterministic_process(df , key , freq , col , ax1 , title1 , ax2 , title2): ",store-sales-time-series-forecast-visualization.ipynb
manually set the frequency of the index, dp.index.freq = freq ,store-sales-time-series-forecast-visualization.ipynb
 in sample creates features for the dates given in the index argument, X1 = dp.in_sample () ,store-sales-time-series-forecast-visualization.ipynb
the target," y1 = df_grouped[""mean""] ",store-sales-time-series-forecast-visualization.ipynb
"features, so we need to be sure to exclude it here.", model = LinearRegression(fit_intercept = False) ,store-sales-time-series-forecast-visualization.ipynb
forecast Trend for future 30 steps, steps = 30 ,store-sales-time-series-forecast-visualization.ipynb
"Here we can see Linear Trend Linear Trend Forecast for Transactions plots 1,2 and Sales plots 3,4 .","fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(30,30))
plot_deterministic_process(df_trans, 'date', 'W', 'transactions', 
 axes[0], ""Transactions Linear Trend"", 
 axes[1], ""Transactions Linear Trend Forecast"")
plot_deterministic_process(df_train, 'date', 'W', 'sales', 
 axes[2], ""Sales Linear Trend"", 
 axes[3], ""Sales Linear Trend Forecast"")
plt.show()",store-sales-time-series-forecast-visualization.ipynb
"3.7 Seasonality Time series exhibits seasonality whenever there is a regular, periodic change in the mean of the series. Seasonal changes generally follow the clock and calendar repetitions over a day, a week, or a year are common. Seasonality is often driven by the cycles of the natural world over days and years or by conventions of social behavior surrounding dates and times. Just like we used a moving average plot to discover the trend in a series, we can use a seasonal plot to discover seasonal patterns.","def seasonal_plot(X, y, period, freq, ax=None):
 if ax is None:
 _, ax = plt.subplots()
 palette = sns.color_palette(""husl"", n_colors=X[period].nunique(),)
 ax = sns.lineplot(x=X[freq], 
 y=X[y],
 ax=ax, 
 hue=X[period],
 palette=palette, 
 legend=False)
 ax.set_title(f""Seasonal Plot ({period}/{freq})"")
 for line, name in zip(ax.lines, X[period].unique()):
 y_ = line.get_ydata()[-1]
 ax.annotate(name, 
 xy=(1, y_), 
 xytext=(6, 0), 
 color=line.get_color(), 
 xycoords=ax.get_yaxis_transform(), 
 textcoords=""offset points"", 
 size=14, 
 va=""center"")
 return ax",store-sales-time-series-forecast-visualization.ipynb
manually set the frequency of the index, df_grouped.index.freq = freq ,store-sales-time-series-forecast-visualization.ipynb
the x axis freq ," X[""day""]= X.index.dayofweek ",store-sales-time-series-forecast-visualization.ipynb
the seasonal period period ," X[""week""]= pd.Int64Index(X.index.isocalendar (). week) ",store-sales-time-series-forecast-visualization.ipynb
days within a year," X[""dayofyear""]= X.index.dayofyear ",store-sales-time-series-forecast-visualization.ipynb
"df trans, grouped by day","seasonality(df_trans , 'date' , 'D' , 'transactions') ",store-sales-time-series-forecast-visualization.ipynb
"df train, grouped by day","seasonality(df_train , 'date' , 'D' , 'sales') ",store-sales-time-series-forecast-visualization.ipynb
"After that, we can predict seasonality, using DeterministicProcess, as we used for Trend. We are going to forecast seasonality for Transactions and Sales.","def predict_seasonality(df , key , freq , col , ax1 , title1): ",store-sales-time-series-forecast-visualization.ipynb
10 sin cos pairs for A nnual seasonality," fourier = CalendarFourier(freq = ""A"" , order = 10) ",store-sales-time-series-forecast-visualization.ipynb
manually set the frequency of the index, df_grouped['date']. freq = freq ,store-sales-time-series-forecast-visualization.ipynb
 Time Series as Features ,"store_sales = df_train.copy()
store_sales['date'] = store_sales.date.dt.to_period('D')
store_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()

family_sales = (
 store_sales
 .groupby(['family', 'date'])
 .mean() 
 .unstack('family')
 .loc['2017', ['sales', 'onpromotion']]
)

mag_sales = family_sales.loc(axis=1)[:, 'MAGAZINES']",store-sales-time-series-forecast-visualization.ipynb
Here we can check data store sales:,store_sales.head(),store-sales-time-series-forecast-visualization.ipynb
Here we can check data mag sales:,mag_sales.head(),store-sales-time-series-forecast-visualization.ipynb
Here we can plot data:,"y = mag_sales.loc[:, 'sales'].squeeze()

fourier = CalendarFourier(freq='M', order=4)
dp = DeterministicProcess(
 constant=True,
 index=y.index,
 order=1,
 seasonal=True,
 drop=True,
 additional_terms=[fourier],
)
X_time = dp.in_sample()
X_time['NewYearsDay'] = (X_time.index.dayofyear == 1)

model = LinearRegression(fit_intercept=False)
model.fit(X_time, y)
y_deseason = y - model.predict(X_time)
y_deseason.name = 'sales_deseasoned'

ax = y_deseason.plot()
ax.set_title(""Magazine Sales (deseasonalized)"");",store-sales-time-series-forecast-visualization.ipynb
"4.1 Lag plot A lag plot of a time series shows its values plotted against its lags. Serial dependence in a time series will often become apparent by looking at a lag plot. The most commonly used measure of serial dependence is known as autocorrelation, which is simply the correlation a time series has with one of its lags. The partial autocorrelation tells you the correlation of a lag accounting for all of the previous lags the amount of new correlation the lag contributes, so to speak. Plotting the partial autocorrelation can help you choose which lag features to use.","def lagplot(x, y=None, lag=1, standardize=False, ax=None, **kwargs):
 from matplotlib.offsetbox import AnchoredText
 x_ = x.shift(lag)
 if standardize:
 x_ = (x_ - x_.mean()) / x_.std()
 if y is not None:
 y_ = (y - y.mean()) / y.std() if standardize else y
 else:
 y_ = x
 corr = y_.corr(x_)
 if ax is None:
 fig, ax = plt.subplots()
 scatter_kws = dict(alpha=0.75,s=3)
 line_kws = dict(color='C3', )
 ax = sns.regplot(x=x_, y=y_, scatter_kws=scatter_kws, line_kws=line_kws, lowess=True, ax=ax, **kwargs)
 at = AnchoredText(f""{corr:.2f}"",prop=dict(size=""large""), frameon=True, loc=""upper left"")
 at.patch.set_boxstyle(""square, pad=0.0"")
 ax.add_artist(at)
 ax.set(title=f""Lag {lag}"", xlabel=x_.name, ylabel=y_.name)
 return ax


def plot_lags(x, y=None, lags=6, nrows=1, lagplot_kwargs={}, **kwargs):
 import math
 kwargs.setdefault('nrows', nrows)
 kwargs.setdefault('ncols', math.ceil(lags / nrows))
 kwargs.setdefault('figsize', (kwargs['ncols'] * 2 + 10, nrows * 2 + 5))
 fig, axs = plt.subplots(sharex=True, sharey=True, squeeze=False, **kwargs)
 for ax, k in zip(fig.get_axes(), range(kwargs['nrows'] * kwargs['ncols'])):
 if k + 1 <= lags:
 ax = lagplot(x, y, lag=k + 1, ax=ax, **lagplot_kwargs)
 ax.set_title(f""Lag {k + 1}"", fontdict=dict(fontsize=14))
 ax.set(xlabel="""", ylabel="""")
 else:
 ax.axis('off')
 plt.setp(axs[-1, :], xlabel=x.name)
 plt.setp(axs[:, 0], ylabel=y.name if y is not None else x.name)
 fig.tight_layout(w_pad=0.1, h_pad=0.1)
 return fig",store-sales-time-series-forecast-visualization.ipynb
Let s take a look at the lag and autocorrelation plots first:,"_ = plot_lags(y_deseason, lags=8, nrows=2)",store-sales-time-series-forecast-visualization.ipynb
Here we examine leading and lagging values for onpromotion plotted against magazine sales.,"onpromotion = mag_sales.loc[: , 'onpromotion']. squeeze (). rename('onpromotion') ",store-sales-time-series-forecast-visualization.ipynb
Drop the New Year outlier,"plot_lags(x = onpromotion.iloc[1 :], y = y_deseason.iloc[1 :], lags = 3 , nrows = 1) ",store-sales-time-series-forecast-visualization.ipynb
"4.2 Lags. Forecasting After that, we can make lags for future plots.","def make_lags(ts, lags):
 return pd.concat(
 {
 f'y_lag_{i}': ts.shift(i)
 for i in range(1, lags + 1)
 },
 axis=1)",store-sales-time-series-forecast-visualization.ipynb
Create target series and data splits,y = y_deseason.copy () ,store-sales-time-series-forecast-visualization.ipynb
fit intercept True since we didn t use DeterministicProcess,model = LinearRegression () ,store-sales-time-series-forecast-visualization.ipynb
" Hybrid Models Linear regression excels at extrapolating trends, but can t learn interactions. XGBoost excels at learning interactions, but can t extrapolate trends. Here we ll learn how to create hybrid forecasters that combine complementary learning algorithms and let the strengths of one make up for the weakness of the other. ","store_sales = df_train.copy()
store_sales['date'] = store_sales.date.dt.to_period('D')
store_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()

family_sales = (
 store_sales
 .groupby(['family', 'date'])
 .mean()
 .unstack('family')
 .loc['2017']
)",store-sales-time-series-forecast-visualization.ipynb
we ll add fit and predict methods to this minimal class,class BoostedHybrid : ,store-sales-time-series-forecast-visualization.ipynb
store column names from fit method, self.y_columns = None ,store-sales-time-series-forecast-visualization.ipynb
"Also, we need to create fit method:","def fit(self , X_1 , X_2 , y): ",store-sales-time-series-forecast-visualization.ipynb
train model 1," self.model_1.fit(X_1 , y) ",store-sales-time-series-forecast-visualization.ipynb
And predict method:,"def predict(self , X_1 , X_2): ",store-sales-time-series-forecast-visualization.ipynb
Target series,"y = family_sales.loc[: , 'sales'] ",store-sales-time-series-forecast-visualization.ipynb
X 1: Features for Linear Regression,"dp = DeterministicProcess(index = y.index , order = 1) ",store-sales-time-series-forecast-visualization.ipynb
onpromotion feature,"X_2 = family_sales.drop('sales' , axis = 1). stack () ",store-sales-time-series-forecast-visualization.ipynb
from sklearn.preprocessing,le = LabelEncoder () ,store-sales-time-series-forecast-visualization.ipynb
values are day of the month,"X_2[""day""]= X_2.index.day ",store-sales-time-series-forecast-visualization.ipynb
After that we train and plot,"y_train , y_valid = y[: ""2017-07-01""], y[""2017-07-02"" :] ",store-sales-time-series-forecast-visualization.ipynb
just a demo.,"model.fit(X1_train , X2_train , y_train) ",store-sales-time-series-forecast-visualization.ipynb
train data,store_sales = df_train.copy () ,store-sales-time-series-forecast-visualization.ipynb
test data,test = df_test.copy () ,store-sales-time-series-forecast-visualization.ipynb
make 4 lag features,"X = make_lags(y , lags = 5). dropna () ",store-sales-time-series-forecast-visualization.ipynb
make multistep target,"y = make_multistep_target(y , steps = 16). dropna () ",store-sales-time-series-forecast-visualization.ipynb
Here we prepare the data for XGBoost:,le = LabelEncoder () ,store-sales-time-series-forecast-visualization.ipynb
init model,model = RegressorChain(base_estimator = XGBRegressor ()) ,store-sales-time-series-forecast-visualization.ipynb
train model,"model.fit(X , y) ",store-sales-time-series-forecast-visualization.ipynb
helpful function,"def plot_multistep(y , every = 1 , ax = None , palette_kwargs = None): ",store-sales-time-series-forecast-visualization.ipynb
"So, now, we can plot results:","FAMILY = 'BEAUTY'
START = '2017-04-01'
EVERY = 16

y_pred_ = y_pred.xs(FAMILY, level='family', axis=0).loc[START:]
y_ = family_sales.loc[START:, 'sales'].loc[:, FAMILY]

fig, ax = plt.subplots(1, 1, figsize=(11, 4))
ax = y_.plot(color=""0.75"",style="".-"",markeredgecolor=""0.25"", markerfacecolor=""0.25"",ax=ax, alpha=0.5)
ax = plot_multistep(y_pred_, ax=ax, every=EVERY)
_ = ax.legend([FAMILY, FAMILY + ' Forecast'])",store-sales-time-series-forecast-visualization.ipynb
 ,import numpy as np ,store-sales-ts-forecasting-a-comprehensive-guide.ipynb
 ,import statsmodels.api as sm ,store-sales-ts-forecasting-a-comprehensive-guide.ipynb
 ,import matplotlib.pyplot as plt ,store-sales-ts-forecasting-a-comprehensive-guide.ipynb
 ,"pd.set_option('display.max_columns' , None) ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Import,"train = pd.read_csv(""../input/store-sales-time-series-forecasting/train.csv"") ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
sub pd.read csv .. input store sales time series forecasting sample submission.csv ,"transactions = pd.read_csv(""../input/store-sales-time-series-forecasting/transactions.csv""). sort_values ([""store_nbr"" , ""date""]) ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Datetime,"train[""date""]= pd.to_datetime(train.date) ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Data types,"train.onpromotion = train.onpromotion.astype(""float16"") ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
 TransactionsLet s start with the transaction data ,transactions.head(10),store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"This feature is highly correlated with sales but first, you are supposed to sum the sales feature to find relationship. Transactions means how many people came to the store or how many invoices created in a day.Sales gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units 1.5 kg of cheese, for instance, as opposed to 1 bag of chips .That s why, transactions will be one of the relevant features in the model. In the following sections, we will generate new features by using transactions.","temp = pd.merge(train.groupby([""date"", ""store_nbr""]).sales.sum().reset_index(), transactions, how = ""left"")
print(""Spearman Correlation between Total Sales and Transactions: {:,.4f}"".format(temp.corr(""spearman"").sales.loc[""transactions""]))
px.line(transactions.sort_values([""store_nbr"", ""date""]), x='date', y='transactions', color='store_nbr',title = ""Transactions"" )",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"There is a stable pattern in Transaction. All months are similar except December from 2013 to 2017 by boxplot. In addition, we ve just seen same pattern for each store in previous plot. Store sales had always increased at the end of the year.","a = transactions.copy()
a[""year""] = a.date.dt.year
a[""month""] = a.date.dt.month
px.box(a, x=""year"", y=""transactions"" , color = ""month"", title = ""Transactions"")",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Let s take a look at transactions by using monthly average sales!We ve just learned a pattern what increases sales. It was the end of the year. We can see that transactions increase in spring and decrease after spring.,"a = transactions.set_index(""date"").resample(""M"").transactions.mean().reset_index()
a[""year""] = a.date.dt.year
px.line(a, x='date', y='transactions', color='year',title = ""Monthly Average Transactions"" )",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"When we look at their relationship, we can see that there is a highly correlation between total sales and transactions also. ","px.scatter(temp, x = ""transactions"", y = ""sales"", trendline = ""ols"", trendline_color_override = ""red"")",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"The days of week is very important for shopping. It shows us a great pattern. Stores make more transactions at weekends. Almost, the patterns are same from 2013 to 2017 and Saturday is the most important day for shopping.","a = transactions.copy()
a[""year""] = a.date.dt.year
a[""dayofweek""] = a.date.dt.dayofweek+1
a = a.groupby([""year"", ""dayofweek""]).transactions.mean().reset_index()
px.line(a, x=""dayofweek"", y=""transactions"" , color = ""year"", title = ""Transactions"")",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Import,"oil = pd.read_csv(""../input/store-sales-time-series-forecasting/oil.csv"") ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Resample,"oil = oil.set_index(""date""). dcoilwtico.resample(""D""). sum (). reset_index () ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Interpolate,"oil[""dcoilwtico""]= np.where(oil[""dcoilwtico""]== 0 , np.nan , oil[""dcoilwtico""]) ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Plot,"p = oil.melt(id_vars =['date']+ list(oil.keys ()[ 5 :]) , var_name = 'Legend') ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"I just said, Ecuador is a oil dependent country but is it true? Can we really see that from the data by looking at?First of all, let s look at the correlations for sales and transactions. The correlation values are not strong but the sign of sales is negative. Maybe, we can catch a clue. Logically, if daily oil price is high, we expect that the Ecuador s economy is bad and it means the price of product increases and sales decreases. There is a negative relationship here. ","temp = pd.merge(temp, oil, how = ""left"")
print(""Correlation with Daily Oil Prices"")
print(temp.drop([""store_nbr"", ""dcoilwtico""], axis = 1).corr(""spearman"").dcoilwtico_interpolated.loc[[""sales"", ""transactions""]], ""\n"")


fig, axes = plt.subplots(1, 2, figsize = (15,5))
temp.plot.scatter(x = ""dcoilwtico_interpolated"", y = ""transactions"", ax=axes[0])
temp.plot.scatter(x = ""dcoilwtico_interpolated"", y = ""sales"", ax=axes[1], color = ""r"")
axes[0].set_title('Daily oil price & Transactions', fontsize = 15)
axes[1].set_title('Daily Oil Price & Sales', fontsize = 15);",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"You should never decide what you will do by looking at a graph or result! You are supposed to change your view and define new hypotheses.We would have been wrong if we had looked at some simple outputs just like above and we had said that there is no relationship with oil prices and let s not use oil price data.All right! We are aware of analyzing deeply now. Let s draw a scatter plot but let s pay attention for product families this time. All of the plots almost contains same pattern. When daily oil price is under about 70, there are more sales in the data. There are 2 cluster here. They are over 70 and under 70. It seems pretty understandable actually. We are in a good way I think. What do you think? Just now, we couldn t see a pattern for daily oil price, but now we extracted a new pattern from it.","a = pd.merge(train.groupby([""date"", ""family""]).sales.sum().reset_index(), oil.drop(""dcoilwtico"", axis = 1), how = ""left"")
c = a.groupby(""family"").corr(""spearman"").reset_index()
c = c[c.level_1 == ""dcoilwtico_interpolated""][[""family"", ""sales""]].sort_values(""sales"")

fig, axes = plt.subplots(7, 5, figsize = (20,20))
for i, fam in enumerate(c.family):
 if i < 6:
 a[a.family == fam].plot.scatter(x = ""dcoilwtico_interpolated"", y = ""sales"", ax=axes[0, i-1])
 axes[0, i-1].set_title(fam+""\n Correlation:""+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)
 axes[0, i-1].axvline(x=70, color='r', linestyle='--')
 if i >= 6 and i<11:
 a[a.family == fam].plot.scatter(x = ""dcoilwtico_interpolated"", y = ""sales"", ax=axes[1, i-6])
 axes[1, i-6].set_title(fam+""\n Correlation:""+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)
 axes[1, i-6].axvline(x=70, color='r', linestyle='--')
 if i >= 11 and i<16:
 a[a.family == fam].plot.scatter(x = ""dcoilwtico_interpolated"", y = ""sales"", ax=axes[2, i-11])
 axes[2, i-11].set_title(fam+""\n Correlation:""+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)
 axes[2, i-11].axvline(x=70, color='r', linestyle='--')
 if i >= 16 and i<21:
 a[a.family == fam].plot.scatter(x = ""dcoilwtico_interpolated"", y = ""sales"", ax=axes[3, i-16])
 axes[3, i-16].set_title(fam+""\n Correlation:""+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)
 axes[3, i-16].axvline(x=70, color='r', linestyle='--')
 if i >= 21 and i<26:
 a[a.family == fam].plot.scatter(x = ""dcoilwtico_interpolated"", y = ""sales"", ax=axes[4, i-21])
 axes[4, i-21].set_title(fam+""\n Correlation:""+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)
 axes[4, i-21].axvline(x=70, color='r', linestyle='--')
 if i >= 26 and i < 31:
 a[a.family == fam].plot.scatter(x = ""dcoilwtico_interpolated"", y = ""sales"", ax=axes[5, i-26])
 axes[5, i-26].set_title(fam+""\n Correlation:""+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)
 axes[5, i-26].axvline(x=70, color='r', linestyle='--')
 if i >= 31 :
 a[a.family == fam].plot.scatter(x = ""dcoilwtico_interpolated"", y = ""sales"", ax=axes[6, i-31])
 axes[6, i-31].set_title(fam+""\n Correlation:""+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)
 axes[6, i-31].axvline(x=70, color='r', linestyle='--')
 
 
plt.tight_layout(pad=5)
plt.suptitle(""Daily Oil Product & Total Family Sales \n"", fontsize = 20);
plt.show()
 ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"Most of the stores are similar to each other, when we examine them with correlation matrix. Some stores, such as 20, 21, 22, and 52 may be a little different.","a = train[[""store_nbr"", ""sales""]]
a[""ind""] = 1
a[""ind""] = a.groupby(""store_nbr"").ind.cumsum().values
a = pd.pivot(a, index = ""ind"", columns = ""store_nbr"", values = ""sales"").corr()
mask = np.triu(a.corr())
plt.figure(figsize=(20, 20))
sns.heatmap(a,
 annot=True,
 fmt='.1f',
 cmap='coolwarm',
 square=True,
 mask=mask,
 linewidths=1,
 cbar=False)
plt.title(""Correlations among stores"",fontsize = 20)
plt.show()",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
There is a graph that shows us daily total sales below.,"a = train.set_index(""date"").groupby(""store_nbr"").resample(""D"").sales.sum().reset_index()
px.line(a, x = ""date"", y= ""sales"", color = ""store_nbr"", title = ""Daily total sales of the stores"")",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"I realized some unnecessary rows in the data while I was looking at the time serie of the stores one by one. If you select the stores from above, some of them have no sales at the beginning of 2013. You can see them, if you look at the those stores 20, 21, 22, 29, 36, 42, 52 and 53. I decided to remove those rows before the stores opened. In the following codes, we will get rid of them.","print(train.shape)
train = train[~((train.store_nbr == 52) & (train.date < ""2017-04-20""))]
train = train[~((train.store_nbr == 22) & (train.date < ""2015-10-09""))]
train = train[~((train.store_nbr == 42) & (train.date < ""2015-08-21""))]
train = train[~((train.store_nbr == 21) & (train.date < ""2015-07-24""))]
train = train[~((train.store_nbr == 29) & (train.date < ""2015-03-20""))]
train = train[~((train.store_nbr == 20) & (train.date < ""2015-02-13""))]
train = train[~((train.store_nbr == 53) & (train.date < ""2014-05-29""))]
train = train[~((train.store_nbr == 36) & (train.date < ""2013-05-09""))]
train.shape",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"Zero ForecastingSome stores don t sell some product families. In the following code, you can see which products aren t sold in which stores. It isn t difficult to forecast them next 15 days. Their forecasts must be 0 next 15 days.I will remove them from the data and create a new data frame for product families which never sell. Then, when we are at submission part, I will combine that data frame with our predictions.","c = train.groupby([""store_nbr"", ""family""]).sales.sum().reset_index().sort_values([""family"",""store_nbr""])
c = c[c.sales == 0]
c",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Anti Join,"outer_join = train.merge(c[c.sales == 0]. drop(""sales"" , axis = 1), how = 'outer' , indicator = True) ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"Are The Product Families Active or Passive?Some products can sell rarely in the stores. When I worked on a product supply demand for restuarants project at my previous job, some products were passive if they never bought in the last two months. I want to apply this domain knowledge here and I will look on the last 60 days.However, some product families depends on seasonality. Some of them might not active on the last 60 days but it doesn t mean it is passive.","c = train.groupby([""family"", ""store_nbr""]).tail(60).groupby([""family"", ""store_nbr""]).sales.sum().reset_index()
c[c.sales == 0]",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"As you can see below, these examples are too rare and also the sales are low. I m open your suggestions for these families. I won t do anything for now but, you would like to improve your model you can focus on that.But still, I want to use that knowledge whether it is simple and I will create a new feature. It shows that the product family is active or not.","fig, ax = plt.subplots(1,5, figsize = (20,4))
train[(train.store_nbr == 10) & (train.family == ""LAWN AND GARDEN"")].set_index(""date"").sales.plot(ax = ax[0], title = ""STORE 10 - LAWN AND GARDEN"")
train[(train.store_nbr == 36) & (train.family == ""LADIESWEAR"")].set_index(""date"").sales.plot(ax = ax[1], title = ""STORE 36 - LADIESWEAR"")
train[(train.store_nbr == 6) & (train.family == ""SCHOOL AND OFFICE SUPPLIES"")].set_index(""date"").sales.plot(ax = ax[2], title = ""STORE 6 - SCHOOL AND OFFICE SUPPLIES"")
train[(train.store_nbr == 14) & (train.family == ""BABY CARE"")].set_index(""date"").sales.plot(ax = ax[3], title = ""STORE 14 - BABY CARE"")
train[(train.store_nbr == 53) & (train.family == ""BOOKS"")].set_index(""date"").sales.plot(ax = ax[4], title = ""STORE 43 - BOOKS"")
plt.show()",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"We can catch the trends, seasonality and anomalies for families.","a = train.set_index(""date"").groupby(""family"").resample(""D"").sales.sum().reset_index()
px.line(a, x = ""date"", y= ""sales"", color = ""family"", title = ""Daily total sales of the family"")",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"We are working with the stores. Well, there are plenty of products in the stores and we need to know which product family sells much more? Let s make a barplot to see that.The graph shows us GROCERY I and BEVERAGES are the top selling families.","a = train.groupby(""family"").sales.mean().sort_values(ascending = False).reset_index()
px.bar(a, y = ""family"", x=""sales"", color = ""family"", title = ""Which product family preferred more?"")",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Does onpromotion column cause a data leakage problem?It is really a good question. The Data Leakage is one of the biggest problem when we will fit a model. There is a great discussion from Nesterenko Marina . You should look at it before fitting a model. ,"print(""Spearman Correlation between Sales and Onpromotion: {:,.4f}"".format(train.corr(""spearman"").sales.loc[""onpromotion""]))",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
How different can stores be from each other? I couldn t find a major pattern among the stores actually. But I only looked at a single plot. There may be some latent patterns. ,"d = pd.merge(train, stores)
d[""store_nbr""] = d[""store_nbr""].astype(""int8"")
d[""year""] = d.date.dt.year
px.line(d.groupby([""city"", ""year""]).sales.mean().reset_index(), x = ""year"", y = ""sales"", color = ""city"")",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
" Holidays and EventsWhat a mess! Probably, you are confused due to the holidays and events data. It contains a lot of information inside but, don t worry. You just need to take a breathe and think! It is a meta data so you have to split it logically and make the data useful.What are our problems? Some national holidays have been transferred. There might be a few holidays in one day. When we merged all of data, number of rows might increase. We don t want duplicates. What is the scope of holidays? It can be regional or national or local. You need to split them by the scope. Work day issue Some specific events Creating new features etc.End of the section, they won t be a problem anymore! ","holidays = pd.read_csv(""../input/store-sales-time-series-forecasting/holidays_events.csv"") ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Transferred Holidays,"tr1 = holidays[( holidays.type == ""Holiday"")&(holidays.transferred == True )]. drop(""transferred"" , axis = 1). reset_index(drop = True) ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Additional Holidays,"holidays[""description""]= holidays[""description""]. str.replace(""-"" , """"). str.replace(""+"" , """"). str.replace('\d+' , '') ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Bridge Holidays,"holidays[""description""]= holidays[""description""]. str.replace(""Puente "" , """") ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"Work Day Holidays, that is meant to payback the Bridge.","work_day = holidays[holidays.type == ""Work Day""] ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Events are national,"events = holidays[holidays.type == ""Event""]. drop ([""type"" , ""locale"" , ""locale_name""], axis = 1). rename({ ""description"" : ""events"" } , axis = 1) ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"d pd.merge d, events, how left ","d = pd.merge(d , national , how = ""left"") ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Regional,"d = pd.merge(d , regional , how = ""left"" , on =[""date"" , ""state""]) ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Local,"d = pd.merge(d , local , how = ""left"" , on =[""date"" , ""city""]) ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Work Day: It will be removed when real work day colum created,"d = pd.merge(d , work_day[[ ""date"" , ""type""]].rename({ ""type"" : ""IsWorkDay"" } , axis = 1), how = ""left"") ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
EVENTS,"events[""events""]= np.where(events.events.str.contains(""futbol""), ""Futbol"" , events.events) ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
categorical columns col for col in df.columns if df col .dtype object ," df = pd.get_dummies(df , columns = categorical_columns , dummy_na = nan_as_category) ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
New features,"d[""holiday_national_binary""]= np.where(d.holiday_national.notnull (), 1 , 0) ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Let s apply an AB test to Events and Holidays features. Are they statistically significant? Also it can be a good way for first feature selection. H0: The sales are equal M1 M2 H1: The sales are not equal M1 ! M2 ,"def AB_Test(dataframe , group , target): ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Packages, from scipy.stats import shapiro ,store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Split A B, groupA = dataframe[dataframe[group]== 1][ target] ,store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Assumption: Normality, ntA = shapiro(groupA)[ 1]< 0.05 ,store-sales-ts-forecasting-a-comprehensive-guide.ipynb
 H0: Normal Distribution , if(ntA == False)&(ntB == False): ,store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Assumption: Homogeneity of variances," leveneTest = stats.levene(groupA , groupB)[ 1]< 0.05 ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
H1: Heterogeneous: True, if leveneTest == False : ,store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Homogeneity," ttest = stats.ttest_ind(groupA , groupB , equal_var = True)[ 1] ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
H1: M1 ! M2 True, else : ,store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Heterogeneous," ttest = stats.ttest_ind(groupA , groupB , equal_var = False)[ 1] ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
H1: M1 ! M2 True, else : ,store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Non Parametric Test," ttest = stats.mannwhitneyu(groupA , groupB)[ 1] ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Result,"d.groupby([""family"",""events_Futbol""]).sales.mean()[:60]",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Time Related Features,def create_date_features(df): ,store-sales-ts-forecasting-a-comprehensive-guide.ipynb
0: Winter 1: Spring 2: Summer 3: Fall," df[""season""]= np.where(df.month.isin ([12 , 1 , 2]) , 0 , 1) ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Workday column,"d[""workday""]= np.where(( d.holiday_national_binary == 1)|(d.holiday_local_binary == 1)|(d.holiday_regional_binary == 1)|(d['day_of_week']. isin ([6 , 7])), 0 , 1) ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Supermarket sales could be affected by this.,"d[""wageday""]= pd.Series(np.where(( d['is_month_end']== 1)|(d[""day_of_month""]== 15), 1 , 0)).astype(""int8"") ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
" Did Earhquake affect the store sales?A magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake.Comparing average sales by year, month and product family will be one of the best ways to be able to understand how earthquake had affected the store sales.We can use the data of March, April, May and June and there may be increasing or decrasing sales for some product families.Lastly, we extracted a column for earthquake from Holidays and Events data. events Terremoto Manabi column will help to fit a better model. ","d[(d.month.isin([4,5]))].groupby([""year""]).sales.mean()",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
March,"pd.pivot_table(d[(d.month.isin([3]))], index=""year"", columns=""family"", values=""sales"", aggfunc=""mean"")",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
April May,"pd.pivot_table(d[(d.month.isin([4,5]))], index=""year"", columns=""family"", values=""sales"", aggfunc=""mean"")",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
June,"pd.pivot_table(d[(d.month.isin([6]))], index=""year"", columns=""family"", values=""sales"", aggfunc=""mean"")",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
a d d store nbr 1 .set index date ,"a = d[( d.sales.notnull ())]. groupby ([""date"" , ""family""]).sales.mean (). reset_index (). set_index(""date"") ",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
 a.sales.notnull , temp = a[( a.family == i )] ,store-sales-ts-forecasting-a-comprehensive-guide.ipynb
"I decided to chose these lags 16, 20, 30, 45, 365, 730 from PACF. I don t know that they will help me to improve the model but especially, 365th and 730th lags may be helpful. If you compare 2016 and 2017 years for sales, you can see that they are highly correlated.","a = d[d.year.isin([2016,2017])].groupby([""year"", ""day_of_year""]).sales.mean().reset_index()
px.line(a, x = ""day_of_year"", y = ""sales"", color = ""year"", title = ""Average sales for 2016 and 2017"")",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
 Simple Moving Average ,"a = train.sort_values([""store_nbr"", ""family"", ""date""])
for i in [20, 30, 45, 60, 90, 120, 365, 730]:
 a[""SMA""+str(i)+""_sales_lag16""] = a.groupby([""store_nbr"", ""family""]).rolling(i).sales.mean().shift(16).values
 a[""SMA""+str(i)+""_sales_lag30""] = a.groupby([""store_nbr"", ""family""]).rolling(i).sales.mean().shift(30).values
 a[""SMA""+str(i)+""_sales_lag60""] = a.groupby([""store_nbr"", ""family""]).rolling(i).sales.mean().shift(60).values
print(""Correlation"")
a[[""sales""]+a.columns[a.columns.str.startswith(""SMA"")].tolist()].corr()",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
 Exponential Moving Average ,"def ewm_features(dataframe, alphas, lags):
 dataframe = dataframe.copy()
 for alpha in alphas:
 for lag in lags:
 dataframe['sales_ewm_alpha_' + str(alpha).replace(""."", """") + ""_lag_"" + str(lag)] = \
 dataframe.groupby([""store_nbr"", ""family""])['sales']. \
 transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())
 return dataframe

alphas = [0.95, 0.9, 0.8, 0.7, 0.5]
lags = [16, 30, 60, 90]

a = ewm_features(a, alphas, lags)",store-sales-ts-forecasting-a-comprehensive-guide.ipynb
Go to Table of Content,"import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from tqdm.notebook import tqdm
import seaborn as sns
from statsmodels.graphics.gofplots import qqplot
!pip install scikit-uplift -q
from sklift.metrics import uplift_at_k, uplift_auc_score, qini_auc_score, weighted_average_uplift
from sklift.viz import plot_uplift_preds
from sklift.models import SoloModel, TwoModels
import xgboost as xgb",study-series-uplift-modeling.ipynb
"We see numerous hidden features X 1 X 50, binary treatment in object format and binary conversion ",train.head(),study-series-uplift-modeling.ipynb
Features are not standartized or normalized ,train.describe(),study-series-uplift-modeling.ipynb
"However, highly likely this features have been cleared. Every feature dist looks like normal. ","rows , cols = 10 , 5 ",study-series-uplift-modeling.ipynb
Just to make sure look at the qq plot. ,"rows , cols = 10 , 5 ",study-series-uplift-modeling.ipynb
"edgecolor 264653 , data train, ax axs row, col , color w "," qqplot(train[f'X_{n_feat}'], ax = axs[row , col], line = 'q') ",study-series-uplift-modeling.ipynb
hide last empty graphs, except IndexError : ,study-series-uplift-modeling.ipynb
"Due to that fact that we have no Uplift before research we cannot use classic metrics for Meta Learners. However, we need to compare models and understand their accuracy. 1. Uplift k All what we need is to sort values DESC and calculate the difference of mean target Y on treatment and control subsets: target on top k 2. Uplift by percentile decile The same approarch, but here we calculate the difference in every decile separately Using Uplift by percentile, we may calculate Weighted average uplift: Weighted average uplift size of the treatment subset in i percentile3. Uplift Curve and AUUC Uplift curve is a cummulative uplift function dependent on quantity of objects: AUUC area under Unplift Curve is an area between random uplift curve and curve from model, normalized by area under ideal Uplift curve 4. Qini Curve and AUQC Qini curve is a another approach of cummulative function: AUQC or Qini coef area under Qini Curve is an area between random qini curve and curve from model, normalized by area under ideal Qini curve ",train.columns,study-series-uplift-modeling.ipynb
The good news is that we can use classic ML classifiers! Let s do it with xgboost. You may also try it with other classifiers and compare the results. ,"def get_metrics(y_val , uplift , treatment_val): ",study-series-uplift-modeling.ipynb
uplift at top 30 . Strategy by group means sorting control and treatment separately. Overall together," upliftk = uplift_at_k(y_true = y_val , uplift = uplift , treatment = treatment_val , strategy = 'by_group' , k = 0.3) ",study-series-uplift-modeling.ipynb
"The main idea of T learner is to train two independent models: one based on observations after treatment T , another one on control rows C . The uplift effect is the difference between predictions on data from model T and model C. ","xgb_T = xgb.XGBClassifier(random_state=42, objective='binary:logistic', use_label_encoder=False)
xgb_C = xgb.XGBClassifier(random_state=42, objective='binary:logistic', use_label_encoder=False)
sm = TwoModels(estimator_trmnt=xgb_T, estimator_ctrl=xgb_C)
sm = sm.fit(X_train, y_train, treatment_train, estimator_trmnt_fit_params={}, estimator_ctrl_fit_params={})

uplift_sm = sm.predict(X_val)
res = get_metrics(y_val, uplift_sm, treatment_val)",study-series-uplift-modeling.ipynb
First approach: ,"xgb_T = xgb.XGBClassifier(random_state=42, objective='binary:logistic', use_label_encoder=False)
xgb_C = xgb.XGBClassifier(random_state=42, objective='binary:logistic', use_label_encoder=False)
sm = TwoModels(estimator_trmnt=xgb_T, estimator_ctrl=xgb_C, method='ddr_control')
sm = sm.fit(X_train, y_train, treatment_train, estimator_trmnt_fit_params={}, estimator_ctrl_fit_params={})

uplift_sm = sm.predict(X_val)
res = get_metrics(y_val, uplift_sm, treatment_val)",study-series-uplift-modeling.ipynb
Second approach: ,"xgb_T = xgb.XGBClassifier(random_state=42, objective='binary:logistic', use_label_encoder=False)
xgb_C = xgb.XGBClassifier(random_state=42, objective='binary:logistic', use_label_encoder=False)
sm = TwoModels(estimator_trmnt=xgb_T, estimator_ctrl=xgb_C, method='ddr_treatment')
sm = sm.fit(X_train, y_train, treatment_train, estimator_trmnt_fit_params={}, estimator_ctrl_fit_params={})

uplift_sm = sm.predict(X_val)
res = get_metrics(y_val, uplift_sm, treatment_val)",study-series-uplift-modeling.ipynb
"This is part of the Machine Learning course. In this step, you will learn to submit your model to a machine learning competition. It s fun, and it will give you a way to see your progress as your skills keep improving.Introduction Machine learning competitions are a great way to improve your skills and measure your progress as a data scientist. If you are using data from a competition on Kaggle, you can easily submit it from your notebook. Here s how you do it.Example We re doing very minimal data set up here so we can focus on how to submit modeling results to competitions. Other tutorials will teach you how build great models. So the model in this example will be fairly simple. We ll start with the code to read data, select predictors, and fit a model.",import numpy as np ,submitting-from-a-kernel.ipynb
Read the data,train = pd.read_csv('../input/train.csv') ,submitting-from-a-kernel.ipynb
pull data into target y and predictors X ,train_y = train.SalePrice ,submitting-from-a-kernel.ipynb
Create training predictors data,train_X = train[predictor_cols] ,submitting-from-a-kernel.ipynb
Read the test data,test = pd.read_csv('../input/test.csv') ,submitting-from-a-kernel.ipynb
"Treat the test data in the same way as training data. In this case, pull same columns.",test_X = test[predictor_cols] ,submitting-from-a-kernel.ipynb
Use the model to make predictions,predicted_prices = my_model.predict(test_X) ,submitting-from-a-kernel.ipynb
We will look at the predicted prices to ensure we have something sensible.,print(predicted_prices) ,submitting-from-a-kernel.ipynb
"Prepare Submission File We make submissions in CSV files. Your submissions usually have two columns: an ID column and a prediction column. The ID field comes from the test data keeping whatever name the ID field had in that data, which for the housing data is the string Id . The prediction column will use the name of the target field.We will create a DataFrame with this data, and then use the dataframe s to csv method to write our submission file. Explicitly include the argument index False to prevent pandas from adding another column in our csv file.","my_submission = pd.DataFrame({ 'Id' : test.Id , 'SalePrice' : predicted_prices }) ",submitting-from-a-kernel.ipynb
you could use any filename. We choose submission here,"my_submission.to_csv('submission.csv' , index = False) ",submitting-from-a-kernel.ipynb
"IntroductionMost of the techniques we ve seen in this course have been for numerical features. The technique we ll look at in this lesson, target encoding, is instead meant for categorical features. It s a method of encoding categories as numbers, like one hot or label encoding, with the difference that it also uses the target to create the encoding. This makes it what we call a supervised feature engineering technique.","
import pandas as pd

autos = pd.read_csv(""../input/fe-course-data/autos.csv"")",target-encoding.ipynb
"Target EncodingA target encoding is any kind of encoding that replaces a feature s categories with some number derived from the target.A simple and effective version is to apply a group aggregation from Lesson 3, like the mean. Using the Automobiles dataset, this computes the average price of each vehicle s make:","autos[""make_encoded""] = autos.groupby(""make"")[""price""].transform(""mean"")

autos[[""make"", ""price"", ""make_encoded""]].head(10)",target-encoding.ipynb
"This kind of target encoding is sometimes called a mean encoding. Applied to a binary target, it s also called bin counting. Other names you might come across include: likelihood encoding, impact encoding, and leave one out encoding. SmoothingAn encoding like this presents a couple of problems, however. First are unknown categories. Target encodings create a special risk of overfitting, which means they need to be trained on an independent encoding split. When you join the encoding to future splits, Pandas will fill in missing values for any categories not present in the encoding split. These missing values you would have to impute somehow.Second are rare categories. When a category only occurs a few times in the dataset, any statistics calculated on its group are unlikely to be very accurate. In the Automobiles dataset, the mercurcy make only occurs once. The mean price we calculated is just the price of that one vehicle, which might not be very representative of any Mercuries we might see in the future. Target encoding rare categories can make overfitting more likely.A solution to these problems is to add smoothing. The idea is to blend the in category average with the overall average. Rare categories get less weight on their category average, while missing categories just get the overall average.In pseudocode: encoding weight in category 1 weight overall where weight is a value between 0 and 1 calculated from the category frequency.An easy way to determine the value for weight is to compute an m estimate: weight n n m where n is the total number of times that category occurs in the data. The parameter m determines the smoothing factor . Larger values of m put more weight on the overall estimate. In the Automobiles dataset there are three cars with the make chevrolet. If you chose m 2.0, then the chevrolet category would be encoded with 60 of the average Chevrolet price plus 40 of the overall average price. chevrolet 0.6 6000.00 0.4 13285.03When choosing a value for m, consider how noisy you expect the categories to be. Does the price of a vehicle vary a great deal within each make? Would you need a lot of data to get good estimates? If so, it could be better to choose a larger value for m if the average price for each make were relatively stable, a smaller value could be okay. Use Cases for Target Encoding Target encoding is great for: High cardinality features: A feature with a large number of categories can be troublesome to encode: a one hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature s most important property: its relationship with the target. Domain motivated features: From prior experience, you might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature s true informativeness. Example MovieLens1MThe MovieLens1M dataset contains one million movie ratings by users of the MovieLens website, with features describing each user and movie. This hidden cell sets everything up:",import matplotlib.pyplot as plt ,target-encoding.ipynb
"With over 3000 categories, the Zipcode feature makes a good candidate for target encoding, and the size of this dataset over one million rows means we can spare some data to create the encoding.We ll start by creating a 25 split to train the target encoder.","X = df.copy()
y = X.pop('Rating')

X_encode = X.sample(frac=0.25)
y_encode = y[X_encode.index]
X_pretrain = X.drop(X_encode.index)
y_train = y[X_pretrain.index]",target-encoding.ipynb
"The category encoders package in scikit learn contrib implements an m estimate encoder, which we ll use to encode our Zipcode feature.",from category_encoders import MEstimateEncoder ,target-encoding.ipynb
Create the encoder instance. Choose m to control noise.,"encoder = MEstimateEncoder(cols =[""Zipcode""], m = 5.0) ",target-encoding.ipynb
Fit the encoder on the encoding split.,"encoder.fit(X_encode , y_encode) ",target-encoding.ipynb
Encode the Zipcode column to create the final training data,X_train = encoder.transform(X_pretrain) ,target-encoding.ipynb
Let s compare the encoded values to the target to see how informative our encoding might be.,"plt.figure(dpi=90)
ax = sns.distplot(y, kde=False, norm_hist=True)
ax = sns.kdeplot(X_train.Zipcode, color='r', ax=ax)
ax.set_xlabel(""Rating"")
ax.legend(labels=['Zipcode', 'Rating']);",target-encoding.ipynb
"TensorFlow deep NN A high level tutorial into Deep Learning using MNIST data and TensorFlow library. by and : 0.99Prerequisites: fundamental coding skills, a bit of linear algebra, especially matrix operations and perhaps understanding how images are stored in computer memory. To start with machine learning, we suggest coursera course by Andrew Ng.Note: Feel free to fork and adjust CONSTANTS to tweak network behaviour and explore how it changes algorithm performance and accuracy. Besides TensorFlow graph section can also be modified for learning purposes.It is highly recommended printing every variable that isn t 100 clear for you. Also, tensorboard can be used on a local environment for visualisation and debugging. Libraries and settings",import numpy as np ,tensorflow-deep-nn.ipynb
settings,LEARNING_RATE = 1e-4 ,tensorflow-deep-nn.ipynb
set to 20000 on local environment to get 0.99 accuracy,TRAINING_ITERATIONS = 2500 ,tensorflow-deep-nn.ipynb
set to 0 to train on all available data,VALIDATION_SIZE = 2000 ,tensorflow-deep-nn.ipynb
image number to output,IMAGE_TO_DISPLAY = 10 ,tensorflow-deep-nn.ipynb
read training data from CSV file,data = pd.read_csv('../input/train.csv') ,tensorflow-deep-nn.ipynb
Every image is a stretched array of pixel values.,"images = data.iloc[: , 1 :]. values ",tensorflow-deep-nn.ipynb
convert from 0:255 0.0:1.0 ,"images = np.multiply(images , 1.0 / 255.0) ",tensorflow-deep-nn.ipynb
In this case it s 784 pixels 28 28px,image_size = images.shape[1] ,tensorflow-deep-nn.ipynb
in this case all images are square,image_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8) ,tensorflow-deep-nn.ipynb
display image,def display(img): ,tensorflow-deep-nn.ipynb
" 784 28,28 "," one_image = img.reshape(image_width , image_height) ",tensorflow-deep-nn.ipynb
output image,display(images[IMAGE_TO_DISPLAY]) ,tensorflow-deep-nn.ipynb
"The corresponding labels are numbers between 0 and 9, describing which digit a given image is of.","labels_flat = data[[0]].values.ravel()

print('labels_flat({0})'.format(len(labels_flat)))
print ('labels_flat[{0}] => {1}'.format(IMAGE_TO_DISPLAY,labels_flat[IMAGE_TO_DISPLAY]))",tensorflow-deep-nn.ipynb
"In this case, there are ten different digits labels classes.","labels_count = np.unique(labels_flat).shape[0]

print('labels_count => {0}'.format(labels_count))",tensorflow-deep-nn.ipynb
9 0 0 0 0 0 0 0 0 0 1 ,"def dense_to_one_hot(labels_dense , num_classes): ",tensorflow-deep-nn.ipynb
split data into training validation,validation_images = images[: VALIDATION_SIZE] ,tensorflow-deep-nn.ipynb
weight initialization,def weight_variable(shape): ,tensorflow-deep-nn.ipynb
convolution,"def conv2d(x , W): ",tensorflow-deep-nn.ipynb
" 1,1 1",def max_pool_2x2(x): ,tensorflow-deep-nn.ipynb
images,"x = tf.placeholder('float' , shape =[None , image_size]) ",tensorflow-deep-nn.ipynb
labels,"y_ = tf.placeholder('float' , shape =[None , labels_count]) ",tensorflow-deep-nn.ipynb
first convolutional layer,"W_conv1 = weight_variable ([5 , 5 , 1 , 32]) ",tensorflow-deep-nn.ipynb
" 40000,784 40000,28,28,1 ","image = tf.reshape(x ,[- 1 , image_width , image_height , 1]) ",tensorflow-deep-nn.ipynb
"print image.get shape 40000,28,28,1 ","h_conv1 = tf.nn.relu(conv2d(image , W_conv1)+ b_conv1) ",tensorflow-deep-nn.ipynb
"print h conv1.get shape 40000, 28, 28, 32 ",h_pool1 = max_pool_2x2(h_conv1) ,tensorflow-deep-nn.ipynb
display 32 fetures in 4 by 8 grid,"layer1 = tf.reshape(h_conv1 ,(- 1 , image_height , image_width , 4 , 8)) ",tensorflow-deep-nn.ipynb
"reorder so the channels are in the first dimension, x and y follow.","layer1 = tf.transpose(layer1 ,(0 , 3 , 1 , 4 , 2)) ",tensorflow-deep-nn.ipynb
second convolutional layer,"W_conv2 = weight_variable ([5 , 5 , 32 , 64]) ",tensorflow-deep-nn.ipynb
"print h conv2.get shape 40000, 14,14, 64 ",h_pool2 = max_pool_2x2(h_conv2) ,tensorflow-deep-nn.ipynb
display 64 fetures in 4 by 16 grid,"layer2 = tf.reshape(h_conv2 ,(- 1 , 14 , 14 , 4 , 16)) ",tensorflow-deep-nn.ipynb
"reorder so the channels are in the first dimension, x and y follow.","layer2 = tf.transpose(layer2 ,(0 , 3 , 1 , 4 , 2)) ",tensorflow-deep-nn.ipynb
densely connected layer,"W_fc1 = weight_variable ([7 * 7 * 64 , 1024]) ",tensorflow-deep-nn.ipynb
" 40000, 7, 7, 64 40000, 3136 ","h_pool2_flat = tf.reshape(h_pool2 ,[- 1 , 7 * 7 * 64]) ",tensorflow-deep-nn.ipynb
dropout,keep_prob = tf.placeholder('float') ,tensorflow-deep-nn.ipynb
readout layer for deep net,"W_fc2 = weight_variable ([1024 , labels_count]) ",tensorflow-deep-nn.ipynb
cost function,cross_entropy = - tf.reduce_sum(y_ * tf.log(y)) ,tensorflow-deep-nn.ipynb
optimisation function,train_step = tf.train.AdamOptimizer(LEARNING_RATE). minimize(cross_entropy) ,tensorflow-deep-nn.ipynb
evaluation,"correct_prediction = tf.equal(tf.argmax(y , 1), tf.argmax(y_ , 1)) ",tensorflow-deep-nn.ipynb
" 0.1, 0.9, 0.2, 0.1, 0.1 0.3, 0.5, 0.1, 0.2, 0.3 1","predict = tf.argmax(y , 1) ",tensorflow-deep-nn.ipynb
"Finally neural network structure is defined and TensorFlow graph is ready for training. Train, validate and predict Helper functionsIdeally, we should use all data for every step of the training, but that s expensive. So, instead, we use small batches of random data. This method is called stochastic training. It is cheaper, faster and gives much of the same result.",epochs_completed = 0 ,tensorflow-deep-nn.ipynb
serve data by batches,def next_batch(batch_size): ,tensorflow-deep-nn.ipynb
"when all trainig data have been already used, it is reorder randomly", if index_in_epoch > num_examples : ,tensorflow-deep-nn.ipynb
finished epoch, epochs_completed += 1 ,tensorflow-deep-nn.ipynb
shuffle the data, perm = np.arange(num_examples) ,tensorflow-deep-nn.ipynb
start next epoch, start = 0 ,tensorflow-deep-nn.ipynb
start TensorFlow session,init = tf.initialize_all_variables () ,tensorflow-deep-nn.ipynb
visualisation variables,train_accuracies = [] ,tensorflow-deep-nn.ipynb
get new batch," batch_xs , batch_ys = next_batch(BATCH_SIZE) ",tensorflow-deep-nn.ipynb
"check progress on every 1st,2nd,...,10th,20th,...,100th... step", if i % display_step == 0 or(i + 1)== TRAINING_ITERATIONS : ,tensorflow-deep-nn.ipynb
check final accuracy on validation set,if(VALIDATION_SIZE): ,tensorflow-deep-nn.ipynb
read test data from CSV file,test_images = pd.read_csv('../input/test.csv'). values ,tensorflow-deep-nn.ipynb
convert from 0:255 0.0:1.0 ,"test_images = np.multiply(test_images , 1.0 / 255.0) ",tensorflow-deep-nn.ipynb
using batches is more resource efficient,predicted_lables = np.zeros(test_images.shape[0]) ,tensorflow-deep-nn.ipynb
"Appendix As it was mentioned before, it is good to output some variables for a better understanding of the process. Here we pull an output of the first convolution layer from TensorFlow graph. 32 features are transformed into an image grid, and it s quite interesting to see how filters picked by NN outline characteristics of different digits.","layer1_grid = layer1.eval(feed_dict={x: test_images[IMAGE_TO_DISPLAY:IMAGE_TO_DISPLAY+1], keep_prob: 1.0})
plt.axis('off')
plt.imshow(layer1_grid[0], cmap=cm.seismic )",tensorflow-deep-nn.ipynb
"Welcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We ve received a transmission from four lightyears away and things aren t looking good.The Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.While rounding Alpha Centauri en route to its first destination the torrid 55 Cancri E the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!GoalThe goal of this competition is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic s collision with the spacetime anomaly.DataFilestrain.csv Personal records for about two thirds 8700 of the passengers, to be used as training data.test.csv Personal records for the remaining one third 4300 of the passengers, to be used as test data. Your task is to predict the value of Transported for the passengers in this set. sample submission.csv a sample submission file in the correct formatColumnsPassengerId A unique Id for each passenger. Each Id takes the form gggg pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.HomePlanet The planet the passenger departed from, typically their planet of permanent residence.CryoSleep Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.Cabin The cabin number where the passenger is staying. Takes the form deck num side, where side can be either P for Port or S for Starboard.Destination The planet the passenger will be debarking to.Age The age of the passenger.VIP Whether the passenger has paid for special VIP service during the voyage.RoomService, FoodCourt, ShoppingMall, Spa, VRDeck Amount the passenger has billed at each of the Spaceship Titanic s many luxury amenities.Name The first and last names of the passenger.Transported Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.Evaluation MetricSubmissions are evaluated based on their classification accuracy, the percentage of predicted labels that are correct. I will be integrating W B for visualizations and logging artifacts! Space Titanic project on W B Dashboard To get the API key, create an account in the website . Use secrets to use API Keys more securely ",import numpy as np ,tensorflow-spaceship-neuraldecisionforests.ipynb
ignore warnings,import warnings ,tensorflow-spaceship-neuraldecisionforests.ipynb
Missing Values,"plt.figure(figsize = (25,11))
sns.heatmap(df_train.isna().values, cmap = ['#fef9ef','#fe6d73'], xticklabels=df_train.columns)
plt.title(""Missing values in training Data"", size=20);",tensorflow-spaceship-neuraldecisionforests.ipynb
basic stats of features,"df_train.describe (). style.background_gradient(cmap = ""Pastel1"") ",tensorflow-spaceship-neuraldecisionforests.ipynb
Target variable distribution,"plt.figure(figsize = (15, 7)) 
sns.countplot(y = df_train['Transported'], color = '#fe6d73') 
plt.title(""Target Distribution"") 
plt.show()",tensorflow-spaceship-neuraldecisionforests.ipynb
Numerical Feature Distribution,"fig, ax = plt.subplots(2,3, figsize=(18, 18))
for i, feature in enumerate(NUMERIC_FEATURE_NAMES):
 sns.distplot(df_train[feature], color = '#fe6d73', ax=ax[math.floor(i/3),i%3]).set_title(f'{feature} Distribution')
fig.show()",tensorflow-spaceship-neuraldecisionforests.ipynb
Categorical Feature Distribution,"def countplot_features(df_train , feature , title): ",tensorflow-spaceship-neuraldecisionforests.ipynb
plot distributions of categorical features,for feature in CATEGORICAL_FEATURE_NAMES1 : ,tensorflow-spaceship-neuraldecisionforests.ipynb
Correlation of Features,"plt.figure(figsize=(25, 9))
sns.heatmap(df_train[[f'{feature}' for feature in df_train.columns]].corr(),annot=True ,cmap = ""Pastel1"")",tensorflow-spaceship-neuraldecisionforests.ipynb
Logging Plots to Weights and Biases,"def create_wandb_hist(x_data = None , x_name = None , title = None , log = None): ",tensorflow-spaceship-neuraldecisionforests.ipynb
Preprocessing,"df_train.drop(['PassengerId', 'Cabin','Name','CryoSleep','VIP'],axis =1 , inplace=True)
df_test.drop(['PassengerId', 'Cabin','Name','CryoSleep','VIP'],axis = 1 ,inplace=True)
df_train.Transported.replace([True,False], [""1"", ""0""], inplace=True)",tensorflow-spaceship-neuraldecisionforests.ipynb
Save train data to W B Artifacts,"run = wandb.init(project = 'SpaceTitanic' , name = 'training_data' , anonymous = anony , config = CONFIG) ",tensorflow-spaceship-neuraldecisionforests.ipynb
"tf.datatf.data API is used for building efficient input pipelines which can handle large amounts of data and perform complex data transformations . tf.data API has provisions for handling different data formats . tf.data.Datasettf.data.Dataset is an abstraction introduced by tf.data API and consists of sequence of elements where each element has one or more components . For example , in a tabular data pipeline , an element might be a single training example , with a pair of tensor components representing the input features and its labeltf.data.Dataset can be created using two distinct waysConstructing a dataset using data stored in memory by a data sourceConstructing a dataset from one or more tf.data.Dataset objects by a data transformation","def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):
 dataset = tf.data.experimental.make_csv_dataset(
 csv_file_path,
 batch_size=batch_size,
 column_names=CSV_HEADER,
 label_name=TARGET_FEATURE_NAME,
 num_epochs=1,
 shuffle=shuffle
 )
 return dataset.cache()",tensorflow-spaceship-neuraldecisionforests.ipynb
Model Inputs,"def create_model_inputs():
 inputs = {}
 for feature_name in FEATURE_NAMES:
 if feature_name in NUMERIC_FEATURE_NAMES:
 inputs[feature_name] = layers.Input(
 name=feature_name, shape=(), dtype=tf.float32
 )
 else:
 inputs[feature_name] = layers.Input(
 name=feature_name, shape=(), dtype=tf.string
 )
 return inputs",tensorflow-spaceship-neuraldecisionforests.ipynb
"Feature representation using Keras Preprocessing LayersFeature representations can be one of the crucial aspect in model developement workflows . It is a experimental process and there is no perfect solution . Keras preprocessing Layers helps us create more flexible preprocessing pipeline where new data transformations can be applied while changing the model architecture .ImageSourceKeras Preprocessing Layers Numerical FeaturesThe Keras preprocessing layers available for numerical features are below tf.keras.layers.Normalization: performs feature wise normalization of input features. tf.keras.layers.Discretization: turns continuous numerical features into integer categorical features.adapt :Adapt is an optional utility function which helps in setting the internal state of layers from input data . adapt is available on all stateful processing layerrs and it computes mean and variance for the layerrs and stores them as layers weights . adapt is called before fit , evaluate or predict Keras Preprocessing Layers Categorical FeaturesThe various keras preprocessing layers available for categorical variables are below .tf.keras.layers.CategoryEncoding: turns integer categorical features into one hot, multi hot, or count dense representations.tf.keras.layers.Hashing: performs categorical feature hashing, also known as the hashing trick .tf.keras.layers.StringLookup: turns string categorical values an encoded representation that can be read by an Embedding layer or Dense layer.tf.keras.layers.IntegerLookup: turns integer categorical values into an encoded representation that can be read by an Embedding layer or Dense layer.",def encode_inputs(inputs): ,tensorflow-spaceship-neuraldecisionforests.ipynb
" oov token, we set mask token to None and num oov indices to 0.",class NeuralDecisionTree(keras.Model): ,tensorflow-spaceship-neuraldecisionforests.ipynb
Create a mask for the randomly selected features., num_used_features = int(num_features * used_features_rate) ,tensorflow-spaceship-neuraldecisionforests.ipynb
Each tree will have its own randomly selected input features to use., for _ in range(num_trees): ,tensorflow-spaceship-neuraldecisionforests.ipynb
Import Libraries,import os ,text-analysis-topic-modelling-with-spacy-gensim.ipynb
"Gathering Data The dataset we will be working with will be the Lee corpus which is a shortened of the Lee Background Corpus, and the 20NG dataset. ","test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])
print(test_data_dir)
lee_train_file = test_data_dir + os.sep + 'lee_background.cor'
print(lee_train_file)
text = open(lee_train_file).read()",text-analysis-topic-modelling-with-spacy-gensim.ipynb
"Cleaning Data We can t have state of the art results without data which is as good. Let s spend this section working on cleaning and understanding our data set. We will be checking out spaCy, an industry grade text processing package. ",nlp = spacy.load('en'),text-analysis-topic-modelling-with-spacy-gensim.ipynb
"For safe measure, let s add some stopwords. It s a newspaper corpus, so it is likely we will be coming across variations of said , Mister , and Mr ... which will not really add any value to the topic models. ","my_stop_words = ['say', '\s', 'mr', 'Mr', 'said', 'says', 'saying', 'today', 'be']
for stopword in my_stop_words:
 lexeme = nlp.vocab[stopword]
 lexeme.is_stop = True",text-analysis-topic-modelling-with-spacy-gensim.ipynb
"Computational LinguisticsNow that we have our doc object. We can see that the doc object now contains the entire corpus. This is important because we will be using the doc object to create our corpus for the machine learning algorithms. When creating a corpus for gensim scikit learn, we sometimes forget the incredible power which spaCy packs in its pipeline, so we will briefly demonstrate the same in this section with a smaller example sentence.","sent = nlp('Last Thursday, Manchester United defeated AC Milan at San Siro.')",text-analysis-topic-modelling-with-spacy-gensim.ipynb
POS Tagging,"for token in sent:
 print(token.text, token.pos_, token.tag_)",text-analysis-topic-modelling-with-spacy-gensim.ipynb
NER Tagging,"for token in sent:
 print(token.text, token.ent_type_)",text-analysis-topic-modelling-with-spacy-gensim.ipynb
Dependency Parsing,"for chunk in sent.noun_chunks:
 print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)",text-analysis-topic-modelling-with-spacy-gensim.ipynb
We add some words to the stop word list,"texts , article = [], [] ",text-analysis-topic-modelling-with-spacy-gensim.ipynb
"And this is the magic of spaCy just like that, we ve managed to get rid of stopwords, puctuation markers, and added lemmatized word. Sometimes topic modeling make more sense when New and York are treated as New York we can do this by creating a bigram model and modifying our corpus accordingly. ","bigram = gensim.models.phrases.Phrases(texts)
texts = [bigram[line] for line in texts]
texts = [bigram[line] for line in texts]",text-analysis-topic-modelling-with-spacy-gensim.ipynb
LSI Latent Semantic Indexing LSI stands for Latent Semantic Indexing It is a popular information retreival method which works by decomposing the original matrix of words to maintain key topics. ,"lsi_model = LsiModel(corpus=corpus, num_topics=10, id2word=dictionary)
lsi_model.show_topics(num_topics=5)",text-analysis-topic-modelling-with-spacy-gensim.ipynb
"HDP Hierarchical Drichlet Process HDP, the Hierarchical Drichlet Process is an unsupervised topic model which figures out the number of topics on it s own. ","hdp_model = HdpModel(corpus=corpus, id2word=dictionary)
hdp_model.show_topics()",text-analysis-topic-modelling-with-spacy-gensim.ipynb
"LDA Latent Dirchlet Allocation LDA, or Latent Dirchlet Allocation is arguably the most famous topic modeling algorithm out there. Out here we create a simple topic model with 10 topics. ","lda_model = LdaModel(corpus=corpus, num_topics=10, id2word=dictionary)
lda_model.show_topics()",text-analysis-topic-modelling-with-spacy-gensim.ipynb
pyLDAvis,import pyLDAvis.gensim,text-analysis-topic-modelling-with-spacy-gensim.ipynb
Load a Clean Dataset Kaggle Datasets is one of the best sources to get a clean dataset for this notebook I will be using Twitter US Airline Sentiment dataset.,clean_data = pd.read_csv('../input/twitter-airline-sentiment/Tweets.csv'),text-representations.ipynb
First of all let s drop the columns which we don t required,data.head(),text-representations.ipynb
we will remove them,"tags = r""@\w*"" ",text-representations.ipynb
this is an example vocabulary just to make concept clear,"sample_vocab =['the' , 'cat' , 'sat' , 'on' , 'mat' , 'dog' , 'run' , 'green' , 'tree'] ",text-representations.ipynb
vocabulary of words present in dataset,data_vocab = [] ,text-representations.ipynb
function to return one hot representation of passed text,"def get_onehot_representation(text , vocab = data_vocab): ",text-representations.ipynb
"We have 14276 different words in a given dataset thus this implies each word representation for one hot encoding schema will be of 14276 dimensional vector mark that this much big representation is just for a single word if we consider the representation of a sentence which consist of let say 20 words in it then it will be represented with 20,14276 sized matrix.","sample_one_hot_rep = get_onehot_representation(data.text[7], data_vocab)
print(f""Shapes of a single sentence : {np.array(sample_one_hot_rep).shape}"")",text-representations.ipynb
if you run this cell it will give you a memory error,data.head(),text-representations.ipynb
"Bag of words Bag of words BoW is a classical text representation technique that has been used commonly in NLP, especially in text classification problems. The key idea behind it is as follows: represent the text under consideration as a bag collection of words while ignoring the order and context.Similar to one hot encoding, BoW maps words to unique integer IDs between 1 and V . Each document in the corpus is then converted into a vector of V dimensions were in the ith component of the vector, i wid, is simply the number of times the word w occurs in the document, i.e., we simply score each word in V by their occurrence count in the document.Consider an example:let say we have a vocabulary V consisting of words the, cat, sat, in, hat, with then the bag of word representation of a few sentences will be given as ",from sklearn.feature_extraction.text import CountVectorizer ,text-representations.ipynb
" the , cat , with , the , hat ","sample_corpus =[""the cat sat"" , ""the cat sat in the hat"" , ""the cat with the hat""] ",text-representations.ipynb
"Sometimes, we don t care about the frequency of occurrence of words in the text and we only want to represent whether a word exists in the text or not. In such cases, we just initialize CountVectorizer with the binary True","sample_bow = CountVectorizer(binary = True)

sample_corpus = [""the cat sat"", ""the cat sat in the hat"", ""the cat with the hat""]

sample_bow.fit(sample_corpus)

def get_bow_representation(text):
 return sample_bow.transform(text)
 
print(f""Vacabulary mapping for given sample corpus : \n {sample_bow.vocabulary_}"")
print(""\nBag of word Representation of sentence 'the the the the cat cat sat in the hat'"")
print(get_bow_representation([""the the the the cat cat sat in the hat""]).toarray())",text-representations.ipynb
generate bag of word representation for given dataset,bow = CountVectorizer () ,text-representations.ipynb
bow.vocabulary ,"print(f""Shape of Bag of word representaion matrix : {bow_rep.toarray().shape}"")",text-representations.ipynb
Bag of 1 gram unigram ,from sklearn.feature_extraction.text import CountVectorizer ,text-representations.ipynb
Bag of 2 gram bigram ,from sklearn.feature_extraction.text import CountVectorizer ,text-representations.ipynb
Bag of 3 gram trigram ,from sklearn.feature_extraction.text import CountVectorizer ,text-representations.ipynb
"TF IDF In all the three approaches we ve seen so far, all the words in the text are treated as equally important there s no notion of some words in the document being more important than others. TF IDF, or term frequency inverse document frequency, addresses this issue. It aims to quantify the importance of a given word relative to other words in the document and in the corpus.The intuition behind TF IDF is as follows: if a word w appears many times in a sentence S1 but does not occur much in the rest of the Sentences Sn in the corpus, then the word w must be of great importance to the Sentence S1. The importance of w should increase in proportion to its frequency in S1 how many times that word occurs in sentence S1 , but at the same time, its importance should decrease in proportion to the word s frequency in other Sentence Sn in the corpus. Mathematically, this is captured using two quantities: TF and IDF. The two are then multiplied to arrive at the TF IDF score.TF term frequency measures how often a term or word occurs in a given document.Mathematical Expression of TFIDF inverse document frequency measures the importance of the term across a corpus. In computing TF, all terms are given equal importance weightage . However, it s a well known fact that stop words like is, are, am, etc., are not important, even though they occur frequently. To account for such cases, IDF weighs down the terms that are very common across a corpus and weighs up the rare terms. IDF of a term t is calculated as follows:The TF IDF score is a product of these two terms. Thus, TF IDF score TF IDF. Let s consider an example.Sentence A The Car is Driven on the Road Sentence B The Truck is Driven on the highway Computation of TF IDF scores are shown below","from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()

sample_corpus = [""the cat sat"", ""the cat sat in the hat"", ""the cat with the hat""]
tfidf_rep = tfidf.fit_transform(sample_corpus)
print(f""IDF Values for sample corpus : {tfidf.idf_}"")


print(""TF-IDF Representation for sentence 'the cat sat in the hat' :"") 
print(tfidf.transform([""the cat sat in the hat""]).toarray())",text-representations.ipynb
"Word2vec Word Embeddings Word Embeddings : They are a real valued vector representation of words that allows words with the same meaning to have similar representation. Thus we can say word embeddings are the projection of meanings of words in a real valued vector Word2vec is a Word Embedding Technique published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text.It is the representation of words that allows words with the same meaning to have similar representation, Word2vec operationalizes this by projecting the meaning of the words in a vector space where words with similar meanings will tend to cluster together, and works with very different meanings are far from one another.Using Pre trained word2vec word embeddings Training your own word embeddings is a pretty expensive process in terms of both time and computing . Thankfully, for many scenarios, it s not necessary to train your own embeddings Someone has done the hard work of training word embeddings on a large corpus, such as Wikipedia, news articles, or even the entire web, and has put words and their corresponding vectors on the web. These embeddings can be downloaded and used to get the vectors for the words you want. Some of the most popular pre trained embeddings are Word2vec by Google, GloVe by Stanford, and fasttext embeddings by Facebook, to name a few.Below code, cell demonstrates how to use pre trained word2vec word embeddings.","from gensim.models import Word2Vec, KeyedVectors
pretrained_path = ""../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin""

Word2VecModel = KeyedVectors.load_word2vec_format(pretrained_path, binary = True)",text-representations.ipynb
As we learned Word2vec do have a similar vector representation for words with the same meaning so let s check the similar words for the word good ,print(Word2VecModel.most_similar('good')),text-representations.ipynb
Next few code cells shows implementation of CBOW,"from gensim.test.utils import common_texts
from gensim.models import Word2Vec

print(""Sentences on Which We are gonna train our CBOW Word2Vec Model:\n"")
print(common_texts)

Our_CBOW_Word2Vec_Model = Word2Vec(common_texts, vector_size = 10, window = 5, min_count = 1, workers = 8, sg = 0)
Our_CBOW_Word2Vec_Model.save(""Our_CBOW_Word2Vec_Model.w2v"")
print(""Model Saved"")",text-representations.ipynb
"Glove Word Embeddings GloVe Stands for Global Vectors for word representation is another word embedding technique that was developed as an open source project at Stanford and was launched in 2014. Just to refresh, word vectors put words to a nice vector space, where similar words cluster together and different words repel. The advantage of GloVe is that, unlike Word2vec, GloVe does not rely just on local statistics local context information of words , but incorporates global statistics word co occurrence to obtain word vectors. But keep in mind that there s quite a bit of synergy between the GloVe and Word2vec. The gloVe can be used to find relations between words like synonyms, company product relations, zip codes, and cities, etc.The question may arise Why do we need Glove if we have word2vec as a good word embedding technique Because Word2vec relies only on local information of language. That is, the semantics learned for a given word, are only affected by the surrounding words.For example, take the sentence,The cat sat on the matIf you use Word2vec, it wouldn t capture information like,is the a special context of the words cat and mat ?oris the just a stopword?This can be suboptimal, especially in the eye of theoreticians.GloVe method is built on an important idea, You can derive semantic relationships between words from the co occurrence matrix. Given a corpus having V words, the co occurrence matrix X will be a V x V matrix, where the i th row and j th column of X, X ij denotes how many times word i has co occurred with word j. An example co occurrence matrix might look as follows.The co occurrence matrix for the sentence the cat sat on the mat with a window size of 1. As you probably noticed it is a symmetric matrix.For detailed knowledge about Glove word embedding, you can refer This article","Glove_path = ""../input/glove6b/glove.6B.100d.txt""
from gensim.scripts.glove2word2vec import glove2word2vec
word2vec_output_file = 'glove.6B.100d.txt.word2vec'
glove2word2vec(Glove_path, word2vec_output_file)",text-representations.ipynb
load the Stanford GloVe model,filename = './glove.6B.100d.txt.word2vec' ,text-representations.ipynb
"FastText Word Embeddings A word can be represented by its constituent character ngrams. Following a similar architecture to Word2vec, fastText learns embeddings for words and character n grams together and views a word s embedding vector as an aggregation of its constituent character n grams. This makes it possible to generate embeddings even for words that are not present in the vocabulary. Say there s a word, gregarious, that s not found in the embedding s word vocabulary. We break it into character n grams gre, reg, ega, .ous and combine these embeddings of the ngrams to arrive at the embedding of gregarious. How FastText Works?FastText is a modified version of word2vec i.e.. Skip Gram and CBOW . The only difference between fastText vs word2vec is its pooling strategies what are the input, output, and dictionary of the model . In word2vec each word is represented as a bag of words but in FastText each word is represented as a bag of character n gram.character n grams the contiguous sequence of n items from a given sample of a character or word. It may be bigram, trigram, etc. For example character trigram n 3 of the word where will be:In FastText architecture, they have also included the word itself with the character n gram. That means input data to the model for the word eating will be:Now the model I am referring same is word2vec which is a shallow neural network with one hidden layer as discussed above.Now to prepare training data for the Skip Gram based FastText model, we define context word as the word which follows a given word in the text which will be our target word . That means we will be predicting the surrounding word for a given word.Note: FastText word embeddings support both Continuous Bag of Words CBOW and Skip Gram models. I will explain and implement the skip gram model in the below cell to learn vector representation FastText word embeddings . Now let s construct our training examples like Skip Gram , scanning through the text with a window will prepare a context word and a target word.Consider the sentence : i like natural language processingFor the above example, for context words i and natural the target word will be like . Full training data for FastText word embedding will look like below. By observing the below training data, your confusion of fastText vs word2vec should be clear.Now you know in word2vec skip gram each word is represented as a bag of words but in FastText each word is represented as a bag of character n gram. This training data preparation is the only difference between FastText word embeddings and skip gram or CBOW word embeddings.After training data preparation of FastText, training the word embedding, finding word similarity, etc. are the same as the word2vec model for our example similar to the skip gram model .Now let s see how to implement FastText word embeddings in python using Gensim library.",from gensim.models.fasttext import load_facebook_model ,text-representations.ipynb
fasttext model FastText.load fasttext format .. input fasttext crawl 300d 2m crawl 300d 2M.vec ,"print(""Most similar words to word 'human' : "") ",text-representations.ipynb
"Training our own fasttext model using python s gensim library by settings up below listed hyperparameters: size: Dimensionality of the word vectors. window window size, min count: The model ignores all words with total frequency lower than this. sample: The threshold for configuring which higher frequency words are randomly down sampled, useful range is 0, 1e 5 . workers: Use these many worker threads to train the model faster training with multicore machines . sg: Training algorithm: skip gram if sg 1, otherwise CBOW. iter: Number of iterations epochs over the corpus. ","from gensim.models import FastText
from gensim.test.utils import common_texts

our_fasttext_model = FastText(common_texts, vector_size = 100, min_count = 1, window = 5, sg = 1)",text-representations.ipynb
Word Embedding for Word Computer ,our_fasttext_model.wv['computer'] ,text-representations.ipynb
Visualizing Word2vec Word Embedding," 
from sklearn.manifold import TSNE

embedding_array = np.array(embeddings_clusters)
n, m, k = embedding_array.shape

tsne_2d_model = TSNE(perplexity = 15, n_components = 2, n_iter = 4000, random_state = 11, init = 'pca')
tsne_embeddings = np.array(tsne_2d_model.fit_transform(embedding_array.reshape(n * m, k))).reshape(n, m, 2)",text-representations.ipynb
"Data Pipelines. To keep things simple, we ve used tf flowers dataset, a multi class classification problem. To apply image augmentaiton, we have used the vectorized implementaiton of CutMix and MixUp, derived from the KerasCV. Additionally, we have also used Jax library to write image augmentaiton layer and used it in the tf.data API. FYI, yes we can use Jax code to build keras layer. Also, we have replaced all possible numpy code to jax.numpy wherever possible. Code Style: Keras.io. JIT Compilation Mixed Precision Gradient Accumulation Label Smoothing TensorFlow Lite Conversion Note: If the accelerator is set to GPU, training will be the first to begin, followed by inference. However, if the accelerator is set to CPU, only inference will be performed, and a trained weight file will be used. Also, this notebook is tested on Jax 0.3.13 and TensorFlow 2.6.4 on Kaggle, TF 2.8 on Colab, and TF 2.9 on Deepnote.","!pip install gdown -q
!pip install jax==0.3.13 jaxlib==0.3.10 -q",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Utils,"def get_model_weight(model_id):
 """"""Get the trained weights.""""""
 if not os.path.exists(""model.h5""):
 model_weight = gdown.download(id=model_id, quiet=False)
 else:
 model_weight = ""model.h5""
 return model_weight


def get_model_history(history_id):
 """"""Get the history / log file.""""""
 if not os.path.exists(""history.csv""):
 history_file = gdown.download(id=history_id, quiet=False)
 else:
 history_file = ""history.csv""
 return history_file


def make_plot(tfdata, take_batch=1, title=True, figsize=(20, 20)):
 """"""ref: https://gist.github.com/innat/4dc4080cfdf5cf20ef0fc93d3623ca9b""""""

 font = {
 ""family"": ""serif"",
 ""color"": ""darkred"",
 ""weight"": ""normal"",
 ""size"": 15,
 }

 for images, labels in tfdata.take(take_batch):
 plt.figure(figsize=figsize)
 xy = int(np.ceil(images.shape[0] * 0.5))

 for i in range(images.shape[0]):
 plt.subplot(xy, xy, i + 1)
 plt.imshow(tf.cast(images[i], dtype=tf.uint8))
 if title:
 plt.title(tcls_names[tf.argmax(labels[i], axis=-1)], fontdict=font)
 plt.axis(""off"")

 plt.tight_layout()
 plt.show()
",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Acquiring Data,"import pathlib

dataset_url = ""https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz""
data_dir = keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

image_count = len(list(data_dir.glob('*/*.jpg')))
print('Total Samples: ', image_count)",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Parameter Settings,class Parameters : ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
data level, image_size = 384 ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
hparams, epochs = 20 ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
" or, exponential, cosine, linear, constant ", lr_sched = 'cosine_restart' ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Data Loading,"train_set = keras.utils.image_dataset_from_directory(
 data_dir,
 validation_split=params.val_split,
 subset=""training"",
 label_mode='categorical',
 seed=params.image_size,
 image_size=(params.image_size, params.image_size),
 batch_size=params.batch_size,
)

val_set = keras.utils.image_dataset_from_directory(
 data_dir,
 validation_split=params.val_split,
 subset=""validation"",
 label_mode='categorical',
 seed=params.image_size,
 image_size=(params.image_size, params.image_size),
 batch_size=params.batch_size,
)

tcls_names, vcls_names = train_set.class_names , val_set.class_names
tcls_names, vcls_names ",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Visualize Raw Samples,"make_plot(train_set, take_batch=1, title=True) 
make_plot(val_set, take_batch=1, title=True) ",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
"Advance Image AugmentationAs mentioned, we will be using CutMix and MixUp, written in tf.keras. And along with it, we will be using two simple Jax coded augmentation layers and convert them to work in tf.data API. However, currently, there is no such probability parameter to control the occurrence of the keras built in augmentation layer. So, we will use a wrapper class for the augmentation layers and make the image transformation random in action.","class RandomApply(layers.Layer):
 """"""RandomApply will randomly apply the transformation layer
 based on the given probability.
 
 Ref. https://stackoverflow.com/a/72558994/9215780
 """"""

 def __init__(self, layer, probability, **kwargs):
 super().__init__(**kwargs)
 self.layer = layer
 self.probability = probability

 def call(self, inputs, training=True):
 apply_layer = tf.random.uniform([]) < self.probability
 outputs = tf.cond(
 pred=tf.logical_and(apply_layer, training),
 true_fn=lambda: self.layer(inputs),
 false_fn=lambda: inputs,
 )
 return outputs

 def get_config(self):
 config = super().get_config()
 config.update(
 {
 ""layer"": layers.serialize(self.layer),
 ""probability"": self.probability,
 }
 )
 return config",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
MixUpImplemented in tf.keras.,"class MixUp(layers.Layer):
 """"""Original implementation: https://github.com/keras-team/keras-cv.
 The original implementaiton provide more interface to apply mixup on
 various CV related task, i.e. object detection etc. It also provides
 many effective validation check.

 Derived and modified for simpler usages: M.Innat.
 Ref. https://gist.github.com/innat/0ee2b6155d663aac2617fe596e1d8d49
 """"""

 def __init__(self, alpha=0.2, seed=None, **kwargs):
 super().__init__(**kwargs)
 self.alpha = alpha
 self.seed = seed

 @staticmethod
 def _sample_from_beta(alpha, beta, shape):
 sample_alpha = tf.random.gamma(shape, 1.0, beta=alpha)
 sample_beta = tf.random.gamma(shape, 1.0, beta=beta)
 return sample_alpha / (sample_alpha + sample_beta)

 def _mixup_samples(self, images):
 batch_size = tf.shape(images)[0]
 permutation_order = tf.random.shuffle(tf.range(0, batch_size), seed=self.seed)

 lambda_sample = MixUp._sample_from_beta(self.alpha, self.alpha, (batch_size,))
 lambda_sample = tf.reshape(lambda_sample, [-1, 1, 1, 1])

 mixup_images = tf.gather(images, permutation_order)
 images = lambda_sample * images + (1.0 - lambda_sample) * mixup_images

 return images, tf.squeeze(lambda_sample), permutation_order

 def _mixup_labels(self, labels, lambda_sample, permutation_order):
 labels_for_mixup = tf.gather(labels, permutation_order)

 lambda_sample = tf.reshape(lambda_sample, [-1, 1])
 labels = lambda_sample * labels + (1.0 - lambda_sample) * labels_for_mixup

 return labels

 def call(self, batch_inputs):
 bs_images = tf.cast(batch_inputs[0], dtype=tf.float32) 
 bs_labels = tf.cast(batch_inputs[1], dtype=tf.float32) 

 mixup_images, lambda_sample, permutation_order = self._mixup_samples(bs_images)
 mixup_labels = self._mixup_labels(bs_labels, lambda_sample, permutation_order)

 return [mixup_images, mixup_labels]

 def get_config(self):
 config = super().get_config()
 config.update(
 {
 ""alpha"": self.alpha,
 ""seed"": self.seed,
 }
 )
 return config
",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
CutMixImplemented in tf.keras.,class CutMix(layers.Layer): ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Channel Shuffle jax2tf Implemented in jax.,"import jax
from jax import jit
from jax import random
from jax import numpy as jnp
from jax.experimental import jax2tf",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
GrayScaling jax2tf Implemented in jax.,"class RandomGrayscale(layers.Layer):
 """"""Grayscale is a preprocessing layer that transforms
 RGB images to Grayscale images.

 Ref. https://gist.github.com/innat/4e89725ccdcd763e0a6ba19216fd60bf
 """"""

 def __init__(self, output_channel=1, prob=1, **kwargs):
 super().__init__(**kwargs)
 self.output_channel = self._check_input_params(output_channel)

 def _check_input_params(self, output_channels):
 if output_channels not in [1, 3]:
 raise ValueError(
 ""Received invalid argument output_channels. ""
 f""output_channels must be in 1 or 3. Got {output_channels}""
 )
 return output_channels

 @partial(jit, static_argnums=0)
 def _jax_gray_scale(self, images):
 rgb_weights = jnp.array([0.2989, 0.5870, 0.1140], dtype=images.dtype)
 grayscale = (rgb_weights * images).sum(axis=-1)

 if self.output_channel == 1:
 grayscale = jnp.expand_dims(grayscale, axis=-1)
 return grayscale
 elif self.output_channel == 3:
 return jnp.stack([grayscale] * 3, axis=-1)
 else:
 raise ValueError(""Unsupported value for `output_channels`."")

 def call(self, images, training=True):
 if training:
 return jax2tf.convert(
 self._jax_gray_scale, polymorphic_shapes=(""batch, ..."")
 )(images)
 else:
 return images

 def get_config(self):
 config = {
 ""output_channel"": self.output_channel,
 }
 base_config = super().get_config()
 return dict(list(base_config.items()) + list(config.items()))
",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Combine All Augmentation,"jax_to_keras_augment = keras.Sequential(
 [
 RandomApply(RandomGrayscale(output_channel=3), probability=0.2),
 RandomApply(RandomChannelShuffle(), probability=0.5),
 ],
 name=""jax2keras_augment"",
)


tf_to_keras_augment = keras.Sequential(
 [
 RandomApply(layers.RandomFlip(""horizontal""), probability=0.5),
 RandomApply(layers.RandomZoom(0.2, 0.3), probability=0.2),
 RandomApply(
 layers.RandomRotation((0.2, 0.3), fill_mode=""reflect""), probability=0.8
 ),
 ],
 name=""tf2keras_augment"",
)
",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Visualize Augmented Training Set,"make_plot(train_ds, take_batch=5, title=False) ",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Visualize Validation Set,"make_plot(val_set, take_batch=1, title=False) ",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
additional memory to store prefetched elements.,train_ds = train_ds.prefetch(buffer_size = params.autotune) ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
2 by 2 sized patches,"patch_size =(2 , 2) ",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Dropout rate,dropout_rate = 0.5 ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Attention heads,num_heads = 8 ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Embedding dimension,embed_dim = 64 ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
MLP layer size,num_mlp = 128 ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
"Convert embedded patches to query, key, and values",qkv_bias = True ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Size of attention window,window_size = 2 ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Size of shifting window,shift_size = 1 ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Initial image size Input size of the transformer model,image_dimension = 24 ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
base cnn models,class HybridModel(keras.Model): ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
"Implement Gradient AccumulationWe also like to implement gradient accumulation technique in our model building pipelines. Usually transformer based models are computationally expensive and thus constrain the batch size limit. To overcome, we like to split up the batch into smaller mini batches which are run sequentially, while accumulating their results. Because gradient accumulation technique calculates the loss and gradients after each mini batch, but instead of updating the model parameters, it waits and accumulates the gradients over consecutive batches, so it can overcoming memory constraints, i.e using less memory to training the model like it using large batch size. So, if we run a gradient accumulation with steps of 8 and batch size of 8 images, it serves almost the same purpose of running with a batch size of 64 images.source.",class GradientAccumulation(HybridModel): ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Pre Setting for Training,"from tensorflow.keras import losses
from tensorflow.keras import metrics
from tensorflow.keras import callbacks
from tensorflow.keras import optimizers

ckp = callbacks.ModelCheckpoint(
 ""model.h5"",
 monitor=""val_accuracy"",
 verbose=1,
 save_best_only=True,
 save_weights_only=True,
 mode=""max"",
)
log = callbacks.CSVLogger(""history.csv"", separator="","", append=False)",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
training,if physical_devices : ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Plotting,"plt.figure(figsize =(20 , 10)) ",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Grad CAM : Hybrid EfficientNet Swin Transformer,"def plot_stuff(inputs , features_a , features_b): ",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
ref: ,def get_img_array(img): ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
ref: ,"def make_gradcam_heatmap(img_array , grad_model , pred_index = None): ",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Get val images,img_arrays = next(iter(val_ds))[0] ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
plot utils,for img_array in img_arrays[: 3]: ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Generate class activation heatmap, img_array = get_img_array(img_array) ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Display heatmap," plot_stuff(img_array , cnn_heatmap , swin_heatmap) ",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
ref: ,"samples , labels = next(iter(val_ds.shuffle(params.batch_size))) ",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
preparing," img_array = sample[tf.newaxis , ...] ",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
get heatmaps," heatmap_a , heatmap_b = make_gradcam_heatmap(img_array , model) ",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
overaly heatmap and input sample," overaly_a = save_and_display_gradcam(sample , heatmap_a) ",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
ploting stuff," plot_stuff(img_array , overaly_a , overlay_b) ",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Calling save my model creates a SavedModel folder my model.,"model.save(""saved_model"") ",tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
Let s check: weight matching,assert len(model.weights)== len(reconstructed_model.weights) ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
"TensorFlow Lite ConversionThis HybridSwinTransformer model can be converted into TensorFlow Lite format for mobile and edge devices. Below is the conversion code. Note that, the TFLite can be used to minimize the complexity of optimizing inference. You can read more details about it from here.",from tensorflow import lite ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
and convert.,converter = lite.TFLiteConverter.from_keras_model(model) ,tf-hybrid-efficientnet-swin-transformer-gradcam.ipynb
SerializationA TFRecord is a kind of file that TensorFlow uses to store binary data. TFRecords contain sequences of byte strings. Here is a very simple TFRecord:,import tensorflow as tf ,tfrecords-basics.ipynb
write one record, f.write(b'123') ,tfrecords-basics.ipynb
write another record, f.write(b'xyz314') ,tfrecords-basics.ipynb
"A TFRecord is a sequence of bytes, so we have to turn our data into byte strings before it can go into a TFRecord. We can use tf.io.serialize tensor to turn a tensor into a byte string and tf.io.parse tensor to turn it back. It s important to keep track of your tensor s datatype in this case tf.uint8 since you have to specify it when parsing the string back to a tensor again.","x = tf.constant([[1, 2], [3, 4]], dtype=tf.uint8)
print('x:', x, '\n')

x_bytes = tf.io.serialize_tensor(x)
print('x_bytes:', x_bytes, '\n')

print('x:', tf.io.parse_tensor(x_bytes, out_type=tf.uint8))",tfrecords-basics.ipynb
"tf.dataSo how do we write a dataset as a TFRecord? If your dataset is composed of byte strings, you can use data.TFRecordWriter. To read it back again, use data.TFRecordsDataset.","from tensorflow.data import Dataset , TFRecordDataset ",tfrecords-basics.ipynb
Construct a small dataset,"ds = Dataset.from_tensor_slices ([b'abc' , b'123']) ",tfrecords-basics.ipynb
Write the dataset to a TFRecord,writer = TFRecordWriter(PATH) ,tfrecords-basics.ipynb
Read the dataset from the TFRecord,ds_2 = TFRecordDataset(PATH) ,tfrecords-basics.ipynb
"Serializing ImagesImages can be encoded in several ways: raw encode with tf.io.serialize tensor, decode with tf.io.parse tensor jpeg encode with tf.io.encode jpeg, decode with tf.io.decode jpeg or tf.io.decode and crop jpeg png encode with tf.io.encode png, decode with tf.io.decode pngJust be sure to use whichever decoder goes with the encoder you chose. Generally, using jpeg encoding for images is a good idea when using TPUs since this can compress the data some, potentially improving data transfer time.",from sklearn.datasets import load_sample_image ,tfrecords-basics.ipynb
Load numpy array,image_raw = load_sample_image('flower.jpg') ,tfrecords-basics.ipynb
jpeg encode decode,image_jpeg = tf.io.encode_jpeg(image_raw) ,tfrecords-basics.ipynb
"tf.ExampleWhat if you have structured data, like image, label pairs? TensorFlow also includes an API for structured data, tf.Example. They are based on Google s Protocol Buffers.A single Example is meant to represent a single instance in a dataset, like a single image, label pair. Each Example has Features, described as a dict of feature names and values. A value can be either a BytesList, a FloatList, or an Int64List, each wrapped as a single Feature. There s no value type for tensors instead, serialize tensors with tf.io.serialize tensor, get the bytestring with the numpy method, and encode them in a BytesList.Here s how we could encode labeled image data:","from tensorflow.train import BytesList , FloatList , Int64List ",tfrecords-basics.ipynb
All of the data is stored as attributes of the Example instance.,print(example.features.feature['label']),tfrecords-basics.ipynb
"Once everything is encoded as an Example, you can serialize it with the SerializeToString method.","example_bytes = example.SerializeToString()
print(example_bytes)",tfrecords-basics.ipynb
It s nice to wrap all this in a function.,"def make_example(image, label, class_name):
 image_feature = Feature(
 bytes_list=BytesList(value=[
 tf.io.serialize_tensor(image).numpy(),
 ])
 )
 label_feature = Feature(
 int64_list=Int64List(value=[
 label,
 ])
 )
 class_name_feature = Feature(
 bytes_list=BytesList(value=[
 class_name.encode(),
 ])
 )

 features = Features(feature={
 'image': image_feature,
 'label': label_feature,
 'class_name': class_name_feature,
 })
 
 example = Example(features=features)
 
 return example.SerializeToString()",tfrecords-basics.ipynb
"Parsing Serialized ExamplesTo decode a serialized example, we need to give TensorFlow a description of what kind of data to expect. We have just scalar entries for each so we can use FixedLenFeature. Pass the description to tf.io.parse single example along with the serialized example.","from tensorflow.io import FixedLenFeature, VarLenFeature

feature_description = {
 'image': FixedLenFeature([], tf.string),
 'label': FixedLenFeature([], tf.int64),
 'class_name': FixedLenFeature([], tf.string),
}

example_2 = tf.io.parse_single_example(example, feature_description)
print(""Parsed: "", example_2)",tfrecords-basics.ipynb
"These are the functions from the helper script that assemble the dataset from the TFRecords. We haven t covered everything here, but hopefully it s a little more clear what s going on.",def decode_image(image_data): ,tfrecords-basics.ipynb
"convert image to floats in 0, 1 range"," image = tf.cast(image , tf.float32)/ 255.0 ",tfrecords-basics.ipynb
explicit size needed for TPU," image = tf.reshape(image ,[* IMAGE_SIZE , 3]) ",tfrecords-basics.ipynb
"Goal of CycleGAN The goal of a CycleGAN is simple, learn a mapping between some dataset, X, and another dataset, Y. For example, X could be a dataset of horse images and Y a dataset of zebra images. The beauty of CycleGAN is that X and Y do not have to be paired. This means that we can give CycleGAN any images for X and any images for Y, even if each image in Y is not the direct mapping of the related image in X.Thanks to this feature you can quickly apply properties from any data set to another.What else can CycleGANs do? A lot. CycleGANs can do practically anything involving photo editing. Including: Swapping night day images Changing season temperature Rendering photo realistic images from outlines Adding or removing elements from an image, e.g., trees Automatically coloring black and white images And much, much more.","import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa

from kaggle_datasets import KaggleDatasets
import matplotlib.pyplot as plt
import numpy as np

try:
 tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
 print('Device:', tpu.master())
 tf.config.experimental_connect_to_cluster(tpu)
 tf.tpu.experimental.initialize_tpu_system(tpu)
 strategy = tf.distribute.experimental.TPUStrategy(tpu)
except:
 strategy = tf.distribute.get_strategy()
print('Number of replicas:', strategy.num_replicas_in_sync)

AUTO = tf.data.experimental.AUTOTUNE
print(tf.__version__)",the-beauty-of-cyclegan.ipynb
Build the Generator,"OUTPUT_CHANNELS = 3

def downsample(filters, size, apply_instancenorm=True):
 initializer = tf.random_normal_initializer(0.01, 0.02)
 gamma_init = keras.initializers.RandomNormal(mean=0.01, stddev=0.02)

 result = keras.Sequential()
 result.add(layers.Conv2D(filters, size, strides=2, padding='same',
 kernel_initializer=initializer, use_bias=False))

 if apply_instancenorm:
 result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))

 result.add(layers.LeakyReLU())

 return result",the-beauty-of-cyclegan.ipynb
Build the Discriminator,def Discriminator (): ,the-beauty-of-cyclegan.ipynb
" bs, 128, 128, 64 "," down1 = downsample(64 , 4 , False)( x) ",the-beauty-of-cyclegan.ipynb
" bs, 64, 64, 128 "," down2 = downsample(128 , 4)( down1) ",the-beauty-of-cyclegan.ipynb
" bs, 32, 32, 256 "," down3 = downsample(256 , 4)( down2) ",the-beauty-of-cyclegan.ipynb
" bs, 34, 34, 256 ", zero_pad1 = layers.ZeroPadding2D ()( down3) ,the-beauty-of-cyclegan.ipynb
transforms photos to Monet esque paintings, monet_generator = Generator () ,the-beauty-of-cyclegan.ipynb
transforms Monet paintings to be more like photos, photo_generator = Generator () ,the-beauty-of-cyclegan.ipynb
differentiates real Monet paintings and generated Monet paintings, monet_discriminator = Discriminator () ,the-beauty-of-cyclegan.ipynb
differentiates real photos and generated photos, photo_discriminator = Discriminator () ,the-beauty-of-cyclegan.ipynb
CycleGAN define,class CycleGan(keras.Model): ,the-beauty-of-cyclegan.ipynb
monet generator.save weights generator.h5 ,"monet_generator.load_weights(""../input/monetcheckpointtoimages/generator.h5"")",the-beauty-of-cyclegan.ipynb
Make prediction,import PIL ,the-beauty-of-cyclegan.ipynb
make predition," prediction = generator_model(img , training = False)[ 0]. numpy () ",the-beauty-of-cyclegan.ipynb
re scale, prediction =(prediction * 127.5 + 127.5). astype(np.uint8) ,the-beauty-of-cyclegan.ipynb
Create folder to save generated images,"predict_and_save(load_dataset(PHOTO_FILENAMES , * IMAGE_SIZE). map(data_augment , num_parallel_calls = AUTO). batch(1), monet_generator , '../images/') ",the-beauty-of-cyclegan.ipynb
Create submission,import shutil ,the-beauty-of-cyclegan.ipynb
"shutil.make archive kaggle working images , zip , .. images ","print(f""Generated samples: {len([name for name in os.listdir('../images/') if os.path.isfile(os.path.join('../images/', name))])}"") ",the-beauty-of-cyclegan.ipynb
Import image,"image = cv2.imread(""../images/14.jpg"") ",the-beauty-of-cyclegan.ipynb
Show the image with matplotlib,plt.imshow(image) ,the-beauty-of-cyclegan.ipynb
Imports,"import os , warnings ",the-convolutional-classifier.ipynb
Reproducability,def set_seed(seed = 31415): ,the-convolutional-classifier.ipynb
Set Matplotlib defaults,"plt.rc('figure' , autolayout = True) ",the-convolutional-classifier.ipynb
Let s take a look at a few examples from the training set.,"
import matplotlib.pyplot as plt",the-convolutional-classifier.ipynb
"Step 2 Define Pretrained BaseThe most commonly used dataset for pretraining is ImageNet, a large dataset of many kind of natural images. Keras includes a variety models pretrained on ImageNet in its applications module. The pretrained model we ll use is called VGG16.","pretrained_base = tf.keras.models.load_model(
 '../input/cv-course-models/cv-course-models/vgg16-pretrained-base',
)
pretrained_base.trainable = False",the-convolutional-classifier.ipynb
"Step 3 Attach HeadNext, we attach the classifier head. For this example, we ll use a layer of hidden units the first Dense layer followed by a layer to transform the outputs to a probability score for class 1, Truck. The Flatten layer transforms the two dimensional outputs of the base into the one dimensional inputs needed by the head.","from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
 pretrained_base,
 layers.Flatten(),
 layers.Dense(6, activation='relu'),
 layers.Dense(1, activation='sigmoid'),
])",the-convolutional-classifier.ipynb
"Step 4 TrainFinally, let s train the model. Since this is a two class problem, we ll use the binary versions of crossentropy and accuracy. The adam optimizer generally performs well, so we ll choose it as well.","model.compile(
 optimizer='adam',
 loss='binary_crossentropy',
 metrics=['binary_accuracy'],
)

history = model.fit(
 ds_train,
 validation_data=ds_valid,
 epochs=30,
 verbose=0,
)",the-convolutional-classifier.ipynb
"When training a neural network, it s always a good idea to examine the loss and metric plots. The history object contains this information in a dictionary history.history. We can use Pandas to convert this dictionary to a dataframe and plot it with a built in method.","import pandas as pd

history_frame = pd.DataFrame(history.history)
history_frame.loc[:, ['loss', 'val_loss']].plot()
history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();",the-convolutional-classifier.ipynb
"IntroductionIn the previous two lessons, we learned about the three operations that carry out feature extraction from an image: 1. filter with a convolution layer 2. detect with ReLU activation 3. condense with a maximum pooling layerThe convolution and pooling operations share a common feature: they are both performed over a sliding window. With convolution, this window is given by the dimensions of the kernel, the parameter kernel size. With pooling, it is the pooling window, given by pool size. There are two additional parameters affecting both convolution and pooling layers these are the strides of the window and whether to use padding at the image edges. The strides parameter says how far the window should move at each step, and the padding parameter describes how we handle the pixels at the edges of the input.With these two parameters, defining the two layers becomes:",from tensorflow import keras ,the-sliding-window.ipynb
"StrideThe distance the window moves at each step is called the stride. We need to specify the stride in both dimensions of the image: one for moving left to right and one for moving top to bottom. This animation shows strides 2, 2 , a movement of 2 pixels each step. What effect does the stride have? Whenever the stride in either direction is greater than 1, the sliding window will skip over some of the pixels in the input at each step.Because we want high quality features to use for classification, convolutional layers will most often have strides 1, 1 . Increasing the stride means that we miss out on potentially valuble information in our summary. Maximum pooling layers, however, will almost always have stride values greater than 1, like 2, 2 or 3, 3 , but not larger than the window itself.Finally, note that when the value of the strides is the same number in both directions, you only need to set that number for instance, instead of strides 2, 2 , you could use strides 2 for the parameter setting.PaddingWhen performing the sliding window computation, there is a question as to what to do at the boundaries of the input. Staying entirely inside the input image means the window will never sit squarely over these boundary pixels like it does for every other pixel in the input. Since we aren t treating all the pixels exactly the same, could there be a problem?What the convolution does with these boundary values is determined by its padding parameter. In TensorFlow, you have two choices: either padding same or padding valid . There are trade offs with each.When we set padding valid , the convolution window will stay entirely inside the input. The drawback is that the output shrinks loses pixels , and shrinks more for larger kernels. This will limit the number of layers the network can contain, especially when inputs are small in size.The alternative is to use padding same . The trick here is to pad the input with 0 s around its borders, using just enough 0 s to make the size of the output the same as the size of the input. This can have the effect however of diluting the influence of pixels at the borders. The animation below shows a sliding window with same padding. The VGG model we ve been looking at uses same padding for all of its convolutional layers. Most modern convnets will use some combination of the two. Another parameter to tune! Example Exploring Sliding WindowsTo better understand the effect of the sliding window parameters, it can help to observe a feature extraction on a low resolution image so that we can see the individual pixels. Let s just look at a simple circle.This next hidden cell will create an image and kernel for us.",import tensorflow as tf ,the-sliding-window.ipynb
 Data and Modules ,"import numpy as np 
import pandas as pd 

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns 
import plotly.express as px

import warnings
warnings.filterwarnings('ignore')

import os
train=pd.read_csv(""/kaggle/input/tabular-playground-series-sep-2022/train.csv"")
test=pd.read_csv(""/kaggle/input/tabular-playground-series-sep-2022/test.csv"")",time-series-analysis-forecasting.ipynb
 Data Vizualization ,"sns.set(rc={'figure.figsize':(24,8)})
ax=sns.lineplot(data=train,x='date',y='num_sold',hue='product')
ax.axes.set_title(""\nBasic Time Series of Sales\n"",fontsize=20);",time-series-analysis-forecasting.ipynb
"How to detrend a time series?Detrending a time series is to remove the trend component from a time series. But how to extract the trend? There are multiple approaches. Subtract the line of best fit from the time series. The line of best fit may be obtained from a linear regression model with the time steps as the predictor. For more complex trends, you may want to use quadratic terms x 2 in the model. Subtract the trend component obtained from time series decomposition we saw earlier. Subtract the mean Apply a filter like Baxter King filter statsmodels.tsa.filters.bkfilter or the Hodrick Prescott Filter statsmodels.tsa.filters.hpfilter to remove the moving average trend lines or the cyclical components. ","from scipy import signal
df = train[(train['country']=='Belgium')&(train['product']=='Kaggle Advanced Techniques')&(train['store']=='KaggleMart')]
detrended = signal.detrend(df.num_sold.values)
plt.plot(detrended)",time-series-analysis-forecasting.ipynb
How to deseasonalize a time series? There are multiple approaches to deseasonalize a time series as well. Below are a few: Take a moving average with length as the seasonal window. This will smoothen in series in the process. Seasonal difference the series subtract the value of previous season from the current value Divide the series by the seasonal index obtained from STL decomposition ,df.info(),time-series-analysis-forecasting.ipynb
"How to test for seasonality of a time series?The common way is to plot the series and check for repeatable patterns in fixed time intervals. So, the types of seasonality is determined by the clock or the calendar: Hour of day Day of month Weekly Monthly Yearly","from pandas.plotting import autocorrelation_plot
df = train[(train['country']=='France')&(train['product']=='Kaggle Advanced Techniques')&(train['store']=='KaggleMart')]

plt.rcParams.update({'figure.figsize':(9,5), 'figure.dpi':120})
autocorrelation_plot(df.num_sold.tolist())
plt.title('Autocorrelation', fontsize=16)
plt.plot()",time-series-analysis-forecasting.ipynb
"How to test for stationarity? The stationarity of a series can be established by looking at the plot of the series like we did earlier.Another method is to split the series into 2 or more contiguous parts and computing the summary statistics like the mean, variance and the autocorrelation. If the stats are quite different, then the series is not likely to be stationary.Nevertheless, you need a method to quantitatively determine if a given series is stationary or not. This can be done using statistical tests called Unit Root Tests . There are multiple variations of this, where the tests check if a time series is non stationary and possess a unit root.There are multiple implementations of Unit Root tests like: Augmented Dickey Fuller test ADH Test Kwiatkowski Phillips Schmidt Shin KPSS test trend stationary Philips Perron test PP Test The most commonly used is the ADF test, where the null hypothesis is the time series possesses a unit root and is non stationary. So, id the P Value in ADH test is less than the significance level 0.05 , you reject the null hypothesis.The KPSS test, on the other hand, is used to test for trend stationarity. The null hypothesis and the P Value interpretation is just the opposite of ADH test. The below code implements these two tests using statsmodels package in python.",df = train[(train['country']=='Belgium')&(train['product']=='Kaggle Advanced Techniques')&(train['store']=='KaggleMart')],time-series-analysis-forecasting.ipynb
Augmented Dickey Fuller test ADH Test ,"from statsmodels.tsa.stattools import adfuller
result = adfuller(df.num_sold.values, autolag='AIC')
print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')
for key, value in result[4].items():
 print('Critial Values:')
 print(f' {key}, {value}')",time-series-analysis-forecasting.ipynb
Kwiatkowski Phillips Schmidt Shin KPSS test trend stationary ,"from statsmodels.tsa.stattools import kpss
result = kpss(df.num_sold.values, regression='c')
print('\nKPSS Statistic: %f' % result[0])
print('p-value: %f' % result[1])
for key, value in result[3].items():
 print('Critial Values:')
 print(f' {key}, {value}');",time-series-analysis-forecasting.ipynb
"Autocorrelation is the correlation between two observations at different points in a time series. For example, values that are separated by an interval might have a strong positive or negative correlation. When these correlations are present, they indicate that past values influence the current value. Analysts use the autocorrelation and partial autocorrelation functions to understand the properties of time series data, fit the appropriate models, and make forecasts.In this post, I cover both the autocorrelation function and partial autocorrelation function. You ll learn about the differences between these functions and what they can tell you about your data. In later posts, I ll show you how to incorporate this information in regression models of time series data and other time series analyses.Autocorrelation and Partial Autocorrelation Basics Autocorrelation is the correlation between two values in a time series. In other words, the time series data correlate with themselves hence, the name. We talk about these correlations using the term lags. Analysts record time series data by measuring a characteristic at evenly spaced intervals such as daily, monthly, or yearly. The number of intervals between the two observations is the lag. For example, the lag between the current and previous observation is one. If you go back one more interval, the lag is two, and so on.In mathematical terms, the observations at yt and yt k are separated by k time units. K is the lag. This lag can be days, quarters, or years depending on the nature of the data. When k 1, you re assessing adjacent observations. For each lag, there is a correlation.Autocorrelation Function ACF Use the autocorrelation function ACF to identify which lags have significant correlations, understand the patterns and properties of the time series, and then use that information to model the time series data. From the ACF, you can assess the randomness and stationarity of a time series. You can also determine whether trends and seasonal patterns are present.In an ACF plot, each bar represents the size and direction of the correlation. Bars that extend across the red line are statistically significant.Partial Autocorrelation Function PACF The partial autocorrelation function is similar to the ACF except that it displays only the correlation between two observations that the shorter lags between those observations do not explain. For example, the partial autocorrelation for lag 3 is only the correlation that lags 1 and 2 do not explain. In other words, the partial correlation for each lag is the unique correlation between those two observations after partialling out the intervening correlations.As you saw, the autocorrelation function helps assess the properties of a time series. In contrast, the partial autocorrelation function PACF is more useful during the specification process for an autoregressive model. Analysts use partial autocorrelation plots to specify regression models with time series data and Auto Regressive Integrated Moving Average ARIMA models. I ll focus on that aspect in posts about those methods.","from statsmodels.tsa.stattools import acf , pacf ",time-series-analysis-forecasting.ipynb
Draw Plot,"fig , axes = plt.subplots(1 , 2 , figsize =(16 , 3), dpi = 100) ",time-series-analysis-forecasting.ipynb
 Lag PlotsA Lag plot is a scatter plot of a time series against a lag of itself. It is normally used to check for autocorrelation. ,"from pandas.plotting import lag_plot
plt.rcParams.update({'ytick.left' : False, 'axes.titlepad':10})


ss = train[(train['country']=='Belgium')&(train['product']=='Kaggle Advanced Techniques')&(train['store']=='KaggleMart')]
ss['value']=ss['num_sold']
a10 = train[(train['country']=='France')&(train['product']=='Kaggle Advanced Techniques')&(train['store']=='KaggleMart')]
a10['value']=a10['num_sold']


fig, axes = plt.subplots(1, 4, figsize=(10,3), sharex=True, sharey=True, dpi=100)
for i, ax in enumerate(axes.flatten()[:4]):
 lag_plot(ss.value, lag=i+1, ax=ax, c='firebrick')
 ax.set_title('Lag ' + str(i+1))

fig.suptitle('Lag Plots of Sun Spots Area \n(Points get wide and scattered with increasing lag -> lesser correlation)\n', y=1.15) 

fig, axes = plt.subplots(1, 4, figsize=(10,3), sharex=True, sharey=True, dpi=100)
for i, ax in enumerate(axes.flatten()[:4]):
 lag_plot(a10.value, lag=i+1, ax=ax, c='firebrick')
 ax.set_title('Lag ' + str(i+1))

fig.suptitle('Lag Plots of Drug Sales', y=1.05) 
plt.show()",time-series-analysis-forecasting.ipynb
"Smoothening of a time series may be useful in: Reducing the effect of noise in a signal get a fair approximation of the noise filtered series. The smoothed version of series can be used as a feature to explain the original series itself. Visualize the underlying trend better So how to smoothen a series? Let s discuss the following methods: Take a moving average Do a LOESS smoothing Localized Regression Do a LOWESS smoothing Locally Weighted Regression Moving average is nothing but the average of a rolling window of defined width. But you must choose the window width wisely, because, large window size will over smooth the series. For example, a window size equal to the seasonal duration ex: 12 for a month wise series , will effectively nullify the seasonal effect. ",from statsmodels.nonparametric.smoothers_lowess import lowess ,time-series-analysis-forecasting.ipynb
Plot,"fig , axes = plt.subplots(4 , 1 , figsize =(7 , 7), sharex = True , dpi = 120) ",time-series-analysis-forecasting.ipynb
"ARIMA Model stands for Auto Regressive Integrated Moving Average. It is used to predict the future values of a time series using its past values and forecast errors. The below diagram shows the components of an ARIMA model: Auto Regressive Model Auto Regressive models predict future behavior using past behavior where there is some correlation between past and future data. The formula below represents the autoregressive model. It is a modified version of the slope formula with the target value being expressed as the sum of the intercept, the product of a coefficient and the previous output, and an error correction term.Moving Average Moving Average is a statistical method that takes the updated average of values to help cut down on noise. It takes the average over a specific interval of time. You can get it by taking different subsets of your data and finding their respective averages.You first consider a bunch of data points and take their average. You then find the next average by removing the first value of the data and including the next value of the series.Integration Integration is the difference between present and previous observations. It is used to make the time series stationary. Each of these values acts as a parameter for our ARIMA model. Instead of representing the ARIMA model by these various operators and models, you use parameters to represent them. These parameters are: p: Previous lagged values for each time point. Derived from the Auto Regressive Model. q: Previous lagged values for the error term. Derived from the Moving Average. d: Number of times data is differenced to make it stationary. It is the number of times it performs integration. ","df = train[(train['country']=='Belgium')&(train['product']=='Kaggle Advanced Techniques')&(train['store']=='KaggleMart')]
series=pd.DataFrame()
series['value']=df['num_sold']
series=series.set_index(df['date'])
series.index = pd.to_datetime(series.index)

series",time-series-analysis-forecasting.ipynb
plot residual errors,residuals = pd.DataFrame(model_fit.resid) ,time-series-analysis-forecasting.ipynb
"What is Serial Dependence?In earlier lessons, we investigated properties of time series that were most easily modeled as time dependent properties, that is, with features we could derive directly from the time index. Some time series properties, however, can only be modeled as serially dependent properties, that is, using as features past values of the target series. The structure of these time series may not be apparent from a plot over time plotted against past values, however, the structure becomes clear as we see in the figure below below. These two series have serial dependence, but not time dependence. Points on the right have coordinates value at time t 1, value at time t . With trend and seasonality, we trained models to fit curves to plots like those on the left in the figure above the models were learning time dependence. The goal in this lesson is to train models to fit curves to plots like those on the right we want them to learn serial dependence.CyclesOne especially common way for serial dependence to manifest is in cycles. Cycles are patterns of growth and decay in a time series associated with how the value in a series at one time depends on values at previous times, but not necessarily on the time step itself. Cyclic behavior is characteristic of systems that can affect themselves or whose reactions persist over time. Economies, epidemics, animal populations, volcano eruptions, and similar natural phenomena often display cyclic behavior. Four time series with cyclic behavior. What distinguishes cyclic behavior from seasonality is that cycles are not necessarily time dependent, as seasons are. What happens in a cycle is less about the particular date of occurence, and more about what has happened in the recent past. The at least relative independence from time means that cyclic behavior can be much more irregular than seasonality.Lagged Series and Lag PlotsTo investigate possible serial dependence like cycles in a time series, we need to create lagged copies of the series. Lagging a time series means to shift its values forward one or more time steps, or equivalently, to shift the times in its index backward one or more steps. In either case, the effect is that the observations in the lagged series will appear to have happened later in time.This shows the monthly unemployment rate in the US y together with its first and second lagged series y lag 1 and y lag 2, respectively . Notice how the values of the lagged series are shifted forward in time.",import pandas as pd ,time-series-as-features.ipynb
"By lagging a time series, we can make its past values appear contemporaneous with the values we are trying to predict in the same row, in other words . This makes lagged series useful as features for modeling serial dependence. To forecast the US unemployment rate series, we could use y lag 1 and y lag 2 as features to predict the target y. This would forecast the future unemployment rate as a function of the unemployment rate in the prior two months.Lag plotsA lag plot of a time series shows its values plotted against its lags. Serial dependence in a time series will often become apparent by looking at a lag plot. We can see from this lag plot of US Unemployment that there is a strong and apparently linear relationship between the current unemployment rate and past rates. Lag plot of US Unemployment with autocorrelations indicated. The most commonly used measure of serial dependence is known as autocorrelation, which is simply the correlation a time series has with one of its lags. US Unemployment has an autocorrelation of 0.99 at lag 1, 0.98 at lag 2, and so on.Choosing lagsWhen choosing lags to use as features, it generally won t be useful to include every lag with a large autocorrelation. In US Unemployment, for instance, the autocorrelation at lag 2 might result entirely from decayed information from lag 1 just correlation that s carried over from the previous step. If lag 2 doesn t contain anything new, there would be no reason to include it if we already have lag 1.The partial autocorrelation tells you the correlation of a lag accounting for all of the previous lags the amount of new correlation the lag contributes, so to speak. Plotting the partial autocorrelation can help you choose which lag features to use. In the figure below, lag 1 through lag 6 fall outside the intervals of no correlation in blue , so we might choose lags 1 through lag 6 as features for US Unemployment. Lag 11 is likely a false positive. Partial autocorrelations of US Unemployment through lag 12 with 95 confidence intervals of no correlation. A plot like that above is known as a correlogram. The correlogram is for lag features essentially what the periodogram is for Fourier features.Finally, we need to be mindful that autocorrelation and partial autocorrelation are measures of linear dependence. Because real world time series often have substantial non linear dependences, it s best to look at a lag plot or use some more general measure of dependence, like mutual information when choosing lag features. The Sunspots series has lags with non linear dependence which we might overlook with autocorrelation. Lag plot of the Sunspots series. Non linear relationships like these can either be transformed to be linear or else learned by an appropriate algorithm.Example Flu TrendsThe Flu Trends dataset contains records of doctor s visits for the flu for weeks between 2009 and 2016. Our goal is to forecast the number of flu cases for the coming weeks.We will take two approaches. In the first we ll forecast doctor s visits using lag features. Our second approach will be to forecast doctor s visits using lags of another set of time series: flu related search terms as captured by Google Trends.",from pathlib import Path ,time-series-as-features.ipynb
Set Matplotlib defaults,"plt.style.use(""seaborn-whitegrid"") ",time-series-as-features.ipynb
"Our Flu Trends data shows irregular cycles instead of a regular seasonality: the peak tends to occur around the new year, but sometimes earlier or later, sometimes larger or smaller. Modeling these cycles with lag features will allow our forecaster to react dynamically to changing conditions instead of being constrained to exact dates and times as with seasonal features.Let s take a look at the lag and autocorrelation plots first:","_ = plot_lags(flu_trends.FluVisits, lags=12, nrows=2)
_ = plot_pacf(flu_trends.FluVisits, lags=12)",time-series-as-features.ipynb
"The lag plots indicate that the relationship of FluVisits to its lags is mostly linear, while the partial autocorrelations suggest the dependence can be captured using lags 1, 2, 3, and 4. We can lag a time series in Pandas with the shift method. For this problem, we ll fill in the missing values the lagging creates with 0.0.","def make_lags(ts, lags):
 return pd.concat(
 {
 f'y_lag_{i}': ts.shift(i)
 for i in range(1, lags + 1)
 },
 axis=1)


X = make_lags(flu_trends.FluVisits, lags=4)
X = X.fillna(0.0)",time-series-as-features.ipynb
Create target series and data splits,y = flu_trends.FluVisits.copy () ,time-series-as-features.ipynb
fit intercept True since we didn t use DeterministicProcess,model = LinearRegression () ,time-series-as-features.ipynb
"Looking just at the forecast values, we can see how our model needs a time step to react to sudden changes in the target series. This is a common limitation of models using only lags of the target series as features.","
ax = y_test.plot(**plot_params)
_ = y_fore.plot(ax=ax, color='C3')",time-series-as-features.ipynb
"To improve the forecast we could try to find leading indicators, time series that could provide an early warning for changes in flu cases. For our second approach then we ll add to our training data the popularity of some flu related search terms as measured by Google Trends.Plotting the search phrase FluCough against the target FluVisits suggests such search terms could be useful as leading indicators: flu related searches tend to become more popular in the weeks prior to office visits.","
ax = flu_trends.plot(
 y=[""FluCough"", ""FluVisits""],
 secondary_y=""FluCough"",
)",time-series-as-features.ipynb
"The dataset contains 129 such terms, but we ll just use a few.","search_terms =[""FluContagious"" , ""FluCough"" , ""FluFever"" , ""InfluenzaA"" , ""TreatFlu"" , ""IHaveTheFlu"" , ""OverTheCounterFlu"" , ""HowLongFlu""] ",time-series-as-features.ipynb
Create three lags for each search term,"X0 = make_lags(flu_trends[search_terms], lags = 3) ",time-series-as-features.ipynb
"Create four lags for the target, as before","X1 = make_lags(flu_trends['FluVisits'], lags = 4) ",time-series-as-features.ipynb
Combine to create the training data,"X = pd.concat ([X0 , X1], axis = 1). fillna(0.0) ",time-series-as-features.ipynb
"Our forecasts are a bit rougher, but our model appears to be better able to anticipate sudden increases in flu visits, suggesting that the several time series of search popularity were indeed effective as leading indicators.","
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=60, shuffle=False)

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = pd.Series(model.predict(X_train), index=y_train.index)
y_fore = pd.Series(model.predict(X_test), index=y_test.index)

ax = y_test.plot(**plot_params)
_ = y_fore.plot(ax=ax, color='C3')",time-series-as-features.ipynb
"Getting StartedJump right in and do the following after forking this notebook : Familiarize yourself with the two cells full of variables just after imports To speed things up: Change full train start day to a value that is just a few months earlier than full train end day Set val end day to 2017, 8, 1 Optionally, to make it really fast, comment out all lag features and set hybrid forecasting type to direct Run the notebook in interactive mode one cell at a time. As you re doing it: Add cells to examine variables Examine plots carefully and rerun the plots on different store numbers and or families if you like Once you feel comfortable playing around with variables and plots, make this notebook your own by adding and subtracting features. Read the commentary and comments throughout the notebook to see where and how to add and subtract features.If you get into trouble, you can make use of certain print display statements I used a lot when debugging, which are commented out by default.",from pathlib import Path ,time-series-bonus-lesson-unofficial.ipynb
"data processing, CSV file I O",import pandas as pd ,time-series-bonus-lesson-unofficial.ipynb
linear algebra,import numpy as np ,time-series-bonus-lesson-unofficial.ipynb
List input data files available in read only .. input directory,import os ,time-series-bonus-lesson-unofficial.ipynb
data time range to train on the full training set,"full_train_start_day = datetime.datetime(2015 , 6 , 16) ",time-series-bonus-lesson-unofficial.ipynb
data time range for train validation split,train_start_day = full_train_start_day ,time-series-bonus-lesson-unofficial.ipynb
data time range of test set,"test_start_day = datetime.datetime(2017 , 8 , 16) ",time-series-bonus-lesson-unofficial.ipynb
"If you use direct for hybrid forecasting type, you can set max lag to 0 but you ll need to comment out features that are lagged or depend on lagged features",max_lag = 7 ,time-series-bonus-lesson-unofficial.ipynb
"possible values: day by day refit all days, day by day fixed past, or direct","hybrid_forecasting_type = ""day_by_day_refit_all_days"" ",time-series-bonus-lesson-unofficial.ipynb
plot style settings from learntools.time series.style,"plt.style.use(""seaborn-whitegrid"") ",time-series-bonus-lesson-unofficial.ipynb
"If I had to make a change, I removed the function or class from this cell and put the revised versions just before they re used","def seasonal_plot(X , y , period , freq , ax = None): ",time-series-bonus-lesson-unofficial.ipynb
load raw data,comp_dir = Path('../input/store-sales-time-series-forecasting') ,time-series-bonus-lesson-unofficial.ipynb
"The next cell loads some of the data again, but this time beginning to get the data in the right format for model building.",comp_dir = Path('../input/store-sales-time-series-forecasting') ,time-series-bonus-lesson-unofficial.ipynb
"Any time a cell is slow to run, it s a nice idea to start the cell with time. This gives you an idea which of the slow cells are taking an especially long time to run, which is particularly useful when saving committing the notebook.The next cell takes a couple minutes to run due to a time consuming MultiIndex.from product operation. It s there in order to get Christmas days into the time index, as some algorithms to follow depend on the MuliIndex being complete with every possible date, family, and store.If you never run your notebook with data prior to 12 25 16, you could replace the 2 lines with the simpler MultiIndex line that is currently commented out.",% % time ,time-series-bonus-lesson-unofficial.ipynb
credit to KDJ2020: ,"calendar = pd.DataFrame(index = pd.date_range('2013-01-01' , '2017-08-31')).to_period('D') ",time-series-bonus-lesson-unofficial.ipynb
National level only for simplicity,df_hev = holidays_events[holidays_events.locale == 'National'] ,time-series-bonus-lesson-unofficial.ipynb
Keep one event only,df_hev = df_hev.groupby(df_hev.index). first () ,time-series-bonus-lesson-unofficial.ipynb
type Additional: days added to a regular calendar holiday such as happens around Christmas,calendar.tail(23),time-series-bonus-lesson-unofficial.ipynb
"BoostedHybrid ClassThe idea of Boosted Hybrids is to have the best of both worlds: model 1: use time based features to extrapolate long term and seasonal trends suitable: Linear Regression model 2: use any features possibly some that are time based , including lags, to find more complex interrelationships suitable: XGB The BoostedHybrid class as originally presented in lesson 5 exercises needed modifications to work with forecasting one day at a time as explained in lesson 6.If model 2 is going to have lag features based on the residuals as explained in lesson 6, then it needs access to residuals, y resid. So y resid is part of the BoostedHybrid class. We need to split the fit method into two methods, fit1 and fit2. We run fit1 on X1 to generate y resid, which is then available to create lag features for X2. After X2 features are created including lag features using y resid , fit2 can run.Also, we need a way to deal with the NaNs that get created from lag features. In lesson 6, this is done by aligning two dataframes shifting created mismatched indices . But if you do one step forecasting over and over, and you keep aligning, then you ll effectively drop rows with each forecasting step. Instead, we can delay dealing with the NaN rows until the last instant, in the fit2 method of BoostedHybrid. Then, we can ignore the rows instead of dropping them. By doing it this way, the rows can be reused over and over for things like rolling averages or rolling means, rather than throwing out the earliest chunk of the data set with each step.So that s what the max lag variable is for: it s passed as first n rows to ignore so that the appropriate number of rows are ignored in fit2 and predict methods.",class BoostedHybrid : ,time-series-bonus-lesson-unofficial.ipynb
train model 1," self.model_1.fit(X_1 , y) ",time-series-bonus-lesson-unofficial.ipynb
might make sense to make dp into a BoostedHybrid Class variable,def make_dp_features(df): ,time-series-bonus-lesson-unofficial.ipynb
"fourier a CalendarFourier freq A , order 4 "," fourier_m = CalendarFourier(freq = 'M' , order = 4) ",time-series-bonus-lesson-unofficial.ipynb
can be used to model long term trends and seasonal trends,"def make_X1_features(df , start_date , end_date , is_test_set = False): ",time-series-bonus-lesson-unofficial.ipynb
seasonal weekly and fourier longer time frame features are generated using DeterministicProcess, X1 = dp.in_sample () ,time-series-bonus-lesson-unofficial.ipynb
X1 wage day lag 1 X1.index.day 1 X1.index.day 16 , X1['NewYear']=(X1.index.dayofyear == 1) ,time-series-bonus-lesson-unofficial.ipynb
"X1.drop type Work Day , type Event , axis 1, inplace True ", if is_test_set : ,time-series-bonus-lesson-unofficial.ipynb
to further refine the modeling and forecasting,"def encode_categoricals(df , columns): ",time-series-bonus-lesson-unofficial.ipynb
from sklearn.preprocessing, le = LabelEncoder () ,time-series-bonus-lesson-unofficial.ipynb
"Test the Feature SetThough the next few cells are not necessary, it is helpful after adding new features to run these cells in interactive mode first to see what happens. They are a lot faster to run then doing one day at a time forecasts that happen later in the notebook. You can get a quick read as to whether you introduced a bug and whether the features did what you expected them to do. Tabular displays are good in both interactive mode and when saving committing, as it makes it easy to see what features are present and if there are any glaring issues with them.Also, this fit the train data code is briefer and easier to understand than the more complicated code that follows for one day at a time forecasts.",% % time ,time-series-bonus-lesson-unofficial.ipynb
unstack pivots MultiIndex to 54 x 33 1782 y columns,"store_sales_in_date_range = store_sales.unstack (['store_nbr' , 'family']).loc[full_train_start_day : full_train_end_day] ",time-series-bonus-lesson-unofficial.ipynb
Boosted Hybrid,"model = BoostedHybrid(model_1 = mod_1 , model_2 = mod_2) ",time-series-bonus-lesson-unofficial.ipynb
preparing X1 for hybrid model 1,"X_1 , y , dp = make_X1_features(store_sales_in_date_range , full_train_start_day , full_train_end_day) ",time-series-bonus-lesson-unofficial.ipynb
"fit1 before make X2 features, since X2 may want to create lag features from model.y resid","model.fit1(X_1 , y , stack_cols =['store_nbr' , 'family']) ",time-series-bonus-lesson-unofficial.ipynb
preparing X2 for hybrid model 2,def truncateFloat(data): ,time-series-bonus-lesson-unofficial.ipynb
comment out next line if don t want to see nan rows,"temp.iloc[max_lag : , :]. apply(lambda s : truncateFloat(s)) ",time-series-bonus-lesson-unofficial.ipynb
note that the fit method of BoostedHybrid class skips over nan rows,temp.apply(lambda s : truncateFloat(s)).head(10) ,time-series-bonus-lesson-unofficial.ipynb
1 54,STORE_NBR = '1' ,time-series-bonus-lesson-unofficial.ipynb
display store sales.index.get level values family .unique ,FAMILY = 'BEVERAGES' ,time-series-bonus-lesson-unofficial.ipynb
train val split prep that is the same for any of the hybrid forecasting methods,training_days =(train_end_day - train_start_day). days + 1 ,time-series-bonus-lesson-unofficial.ipynb
"use y to evaluate validation set, though we will treat y as unknown when training",y_val = y[val_start_day : val_end_day] ,time-series-bonus-lesson-unofficial.ipynb
"DIRECT hybrid version of train validate can t use with lagged rolling features on y, promo, etc. ","if hybrid_forecasting_type == ""direct"" : ",time-series-bonus-lesson-unofficial.ipynb
preparing X1 for hybrid part 1: LinearRegression," X_1_train , y_train , dp_val = make_X1_features(store_sales_in_date_range , train_start_day , train_end_day) ",time-series-bonus-lesson-unofficial.ipynb
"fit1 before make X2 features, since X2 may want to create lag features from model.y resid"," model_for_val.fit1(X_1_train , y_train , stack_cols =['store_nbr' , 'family']) ",time-series-bonus-lesson-unofficial.ipynb
Each new day y is fixed after it s initial prediction no y row is ever predicted more than once,"if hybrid_forecasting_type == ""day_by_day_fixed_past"" : ",time-series-bonus-lesson-unofficial.ipynb
preparing X1 for hybrid part 1: LinearRegression," X_1_train , y_train , dp_val = make_X1_features(store_sales_in_date_range , train_start_day , train_end_day) ",time-series-bonus-lesson-unofficial.ipynb
"fit1 before make X2 features, since X2 may want to create lag features from model.y resid"," model_for_val.fit1(X_1_train , y_train , stack_cols =['store_nbr' , 'family']) ",time-series-bonus-lesson-unofficial.ipynb
each new forecast causes y for all days in training and test or validation set to be reforecast.,"if hybrid_forecasting_type == ""day_by_day_refit_all_days"" : ",time-series-bonus-lesson-unofficial.ipynb
preparing X1 for hybrid part 1: LinearRegression," X_1_train , y_train , dp_val = make_X1_features(store_sales_in_date_range , train_start_day , train_end_day) ",time-series-bonus-lesson-unofficial.ipynb
"fit1 before make X2 features, since X2 may want to create lag features from model.y resid"," model_for_val.fit1(X_1_train , y_train , stack_cols =['store_nbr' , 'family']) ",time-series-bonus-lesson-unofficial.ipynb
evaluation metric for Store Sales forecasting learning competition:,"rmsle_train = mean_squared_log_error(y_train.iloc[max_lag : , :]. clip(0.0), y_fit)** 0.5 ",time-series-bonus-lesson-unofficial.ipynb
Sales should be 0,y_target['sales_pred']= y_predict[0]. clip(0.0) ,time-series-bonus-lesson-unofficial.ipynb
train test split prep that is the same for any of the hybrid forecasting methods,train_days =(full_train_end_day - full_train_start_day). days + 1 ,time-series-bonus-lesson-unofficial.ipynb
previously prepared data and fit model from data ranging from full train start day to full train end day. Can be used by when fitting test.,"model_for_test = BoostedHybrid(model_1 = mod_1 , model_2 = mod_2) ",time-series-bonus-lesson-unofficial.ipynb
model.fit 1 and 2 already happened previously on training set with a time range ending on most recent date ,"if hybrid_forecasting_type == ""direct"" : ",time-series-bonus-lesson-unofficial.ipynb
Each new day y is fixed after it s initial prediction no y row is ever predicted more than once,"if hybrid_forecasting_type == ""day_by_day_fixed_past"" : ",time-series-bonus-lesson-unofficial.ipynb
preparing X1 for hybrid part 1: LinearRegression," X_1_train , y_train , dp_test = make_X1_features(store_sales_in_date_range , full_train_start_day , full_train_end_day) ",time-series-bonus-lesson-unofficial.ipynb
"fit1 before make X2 features, since X2 may want to create lag features from model.y resid"," model_for_test.fit1(X_1_train , y_train , stack_cols =['store_nbr' , 'family']) ",time-series-bonus-lesson-unofficial.ipynb
each new forecast causes y for all days in training and test or validation set to be reforecast.,"if hybrid_forecasting_type == ""day_by_day_refit_all_days"" : ",time-series-bonus-lesson-unofficial.ipynb
preparing X1 for hybrid part 1: LinearRegression," X_1_train , y_train , dp_test = make_X1_features(store_sales_in_date_range , full_train_start_day , full_train_end_day) ",time-series-bonus-lesson-unofficial.ipynb
"fit1 before make X2 features, since X2 may want to create lag features from model.y resid"," model_for_test.fit1(X_1_train , y_train , stack_cols =['store_nbr' , 'family']) ",time-series-bonus-lesson-unofficial.ipynb
1 54,STORE_NBR = '1' ,time-series-bonus-lesson-unofficial.ipynb
display store sales.index.get level values family .unique ,FAMILY = 'BEVERAGES' ,time-series-bonus-lesson-unofficial.ipynb
markers: big size for tiny validation sets 1 2 days ,"ax = y_pred.loc(axis = 1)[ STORE_NBR , FAMILY]. plot(ax = ax , marker = '.' , color = 'red' , markersize = 12) ",time-series-bonus-lesson-unofficial.ipynb
1 33,NUM_FAMILIES = 33 ,time-series-bonus-lesson-unofficial.ipynb
1 54,STORE_NBR = '1' ,time-series-bonus-lesson-unofficial.ipynb
creates submission file submission.csv,"y_submit = y_forecast.stack (['store_nbr' , 'family']) ",time-series-bonus-lesson-unofficial.ipynb
linear algebra,import numpy as np ,time-series-forecasting-with-python.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,time-series-forecasting-with-python.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,time-series-forecasting-with-python.ipynb
Import required libraries,"import pandas as pd
import numpy as np
import datetime as dt
import os
import warnings
from collections import UserDict
from glob import glob
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from IPython.display import Image
%matplotlib inline
import matplotlib.dates as mpl_dates
import seaborn as sns
from statsmodels.tsa.ar_model import AutoReg, ar_select_order
from statsmodels.tsa.api import acf, pacf, graphics
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.preprocessing import MinMaxScaler
import math
from keras.models import Model, Sequential
from keras.layers import GRU, Dense
from keras.callbacks import EarlyStopping",time-series-forecasting-with-python.ipynb
Read the datasets,"holiday = pd.read_csv ('../input/store-sales-time-series-forecasting/holidays_events.csv')
holiday.head (10)",time-series-forecasting-with-python.ipynb
Check the rows with empty values,holiday.isna ().sum (),time-series-forecasting-with-python.ipynb
drop the missing values,oil = oil.dropna () ,time-series-forecasting-with-python.ipynb
convert datasets to time series,holiday['date']= pd.to_datetime(holiday['date']) ,time-series-forecasting-with-python.ipynb
Create a lag plot for datasets. Lag plots are used to check if a time series is random: random data should not exhibit any structure in the lag plot,"pd.options.display.float_format = ""{:, .2f}"".format ",time-series-forecasting-with-python.ipynb
import lag plot function,from pandas.plotting import lag_plot ,time-series-forecasting-with-python.ipynb
plot our holiday data set,lag_plot(holiday['date']) ,time-series-forecasting-with-python.ipynb
plot our oil data set,lag_plot(oil['date']) ,time-series-forecasting-with-python.ipynb
plot our transactions data set,lag_plot(transactions['date']) ,time-series-forecasting-with-python.ipynb
import autocorrelation plot function,from pandas.plotting import autocorrelation_plot ,time-series-forecasting-with-python.ipynb
pass the autocorrelation argument and plot the values holiday,holiday.reset_index(inplace = True) ,time-series-forecasting-with-python.ipynb
pass the autocorrelation argument and plot the values oil,oil.reset_index(inplace = True) ,time-series-forecasting-with-python.ipynb
pass the autocorrelation argument and plot the values transactions,transactions.reset_index(inplace = True) ,time-series-forecasting-with-python.ipynb
import plot acf function,from statsmodels.graphics.tsaplots import plot_acf ,time-series-forecasting-with-python.ipynb
plot the acf function on the holiday data set,plot_acf(holiday['date']) ,time-series-forecasting-with-python.ipynb
plot the acf function for oil data set,plot_acf(oil['date']) ,time-series-forecasting-with-python.ipynb
plot the acf function for transactions data set,plot_acf(transactions['date']) ,time-series-forecasting-with-python.ipynb
import plor pacf function,from statsmodels.graphics.tsaplots import plot_pacf ,time-series-forecasting-with-python.ipynb
plot the pacf fucntion on the holiday dataset,"plot_pacf(holiday['date'], lags = 20) ",time-series-forecasting-with-python.ipynb
plot the pacf function on the oil dataset,"plot_pacf(oil['date'], lags = 30) ",time-series-forecasting-with-python.ipynb
plot the pacf function on the transactions dataset,"plot_pacf(transactions['date'], lags = 40) ",time-series-forecasting-with-python.ipynb
apply AutoReg model for holiday dataset,"modelH = AutoReg(oil['date'], 1) ",time-series-forecasting-with-python.ipynb
apply AutoReg model for oil dataset,"modelO = AutoReg(oil['date'], 1) ",time-series-forecasting-with-python.ipynb
apply AutoReg model for transactions dataset,"modelT = AutoReg(transactions['date'], 1) ",time-series-forecasting-with-python.ipynb
"define figure style, plot package and default figure size",sns.set_style('darkgrid') ,time-series-forecasting-with-python.ipynb
default figure size,"sns.mpl.rc('figure' , figsize =(18 , 8)) ",time-series-forecasting-with-python.ipynb
use plot predict and visualize forecasts for holiday dataset,"figure = resultsH.plot_predict(120 , 490) ",time-series-forecasting-with-python.ipynb
use plot predict and visualize forecasts for oil dataset,"figure = resultsO.plot_predict(120 , 490) ",time-series-forecasting-with-python.ipynb
results plot predict and visualize forecasts for transactions dataset,"figure = resultsT.plot_predict(120 , 490) ",time-series-forecasting-with-python.ipynb
define default figure size,"fig = plt.figure(figsize =(18 , 10)) ",time-series-forecasting-with-python.ipynb
use plot predict and visualize forecasts for holiday dataset,"fig = resultsH.plot_diagnostics(fig = fig , lags = 30) ",time-series-forecasting-with-python.ipynb
define default figure size,"fig = plt.figure(figsize =(18 , 10)) ",time-series-forecasting-with-python.ipynb
use plot predict and visualize forecasts for oil dataset,"fig = resultsO.plot_diagnostics(fig = fig , lags = 30) ",time-series-forecasting-with-python.ipynb
define default figure size,"fig = plt.figure(figsize =(18 , 10)) ",time-series-forecasting-with-python.ipynb
use plot predict and visualize forecasts for oil dataset,"fig = resultsT.plot_diagnostics(fig = fig , lags = 30) ",time-series-forecasting-with-python.ipynb
Training the model for oil dataset,from sklearn.model_selection import train_test_split ,time-series-forecasting-with-python.ipynb
create train set containing only the model features,X = oil ,time-series-forecasting-with-python.ipynb
"scale train data to be in range 0, 1 ",from sklearn.preprocessing import MinMaxScaler ,time-series-forecasting-with-python.ipynb
"scale test data to be in range 0, 1 ",X_test = scaler.transform(X_test) ,time-series-forecasting-with-python.ipynb
specify the number of steps to forecast ahead,HORIZON = 3 ,time-series-forecasting-with-python.ipynb
make predictions on the test data,training_window = 720 ,time-series-forecasting-with-python.ipynb
move the trainig window, history.append(obs[0]) ,time-series-forecasting-with-python.ipynb
read the data,X = pd.read_csv('../input/store-sales-time-series-forecasting/test.csv') ,time-series-forecasting-with-python.ipynb
sampling the data in X,X = X['date']. sample(5000) ,time-series-forecasting-with-python.ipynb
sampling the data in y,y = y['date']. sample(5000) ,time-series-forecasting-with-python.ipynb
setting T the number of lag variables,T = 1 ,time-series-forecasting-with-python.ipynb
"setting the horizon, as we interesting in predicting next day",HORIZON = 1 ,time-series-forecasting-with-python.ipynb
convert datasets to time series,X = pd.to_datetime(X) ,time-series-forecasting-with-python.ipynb
create train set containing only the model features,"X_train , y_train , X_test , y_test = train_test_split(X , y , test_size = 0.5 , random_state = 42) ",time-series-forecasting-with-python.ipynb
create a validation set,"X_train , X_valid , y_train , y_valid = train_test_split(X_train , y_train , train_size = 0.5) ",time-series-forecasting-with-python.ipynb
converting data,X_train = np.asarray(X_train). astype(np.float32) ,time-series-forecasting-with-python.ipynb
rescale the data to 0 1 scale,"min_max_scaler = MinMaxScaler(feature_range =(0 , 1)) ",time-series-forecasting-with-python.ipynb
reshape the data,"X_train = np.reshape(X_train ,(X_train.shape[0], T , X_train.shape[1])) ",time-series-forecasting-with-python.ipynb
number of units in the RNN layer,LATENT_DIM = 5 ,time-series-forecasting-with-python.ipynb
number of samples per mini batch,BATCH_SIZE = 32 ,time-series-forecasting-with-python.ipynb
maximum number of times the training algorithm will cycle through all samples,EPOCHS = 15 ,time-series-forecasting-with-python.ipynb
define model and create a Sequential model,model = Sequential () ,time-series-forecasting-with-python.ipynb
specify early stop criteria,"GRU_earlystop = EarlyStopping(monitor = 'val_loss' , min_delta = 0 , patience = 5) ",time-series-forecasting-with-python.ipynb
plot the epochs and train loss and val loss,"plot_df = pd.DataFrame.from_dict({ 'train_loss' : history.history['loss'], 'val_loss' : history.history['val_loss']}) ",time-series-forecasting-with-python.ipynb
make the predictiondson the X test and compare those predictions on the y test,ts_predictions = model.predict(X_test) ,time-series-forecasting-with-python.ipynb
evaluate our model and compute MAPE mean absolute percentage error ,"def mape(ts_predictions , actuals): ",time-series-forecasting-with-python.ipynb
plot the count of predictions,"ev_ts_data['actual' < '0.3']. plot(x = 'prediction' , style =['r' , 'b'], figsize =(15 , 8)) ",time-series-forecasting-with-python.ipynb
Let s try to apply machine learning methods to transactions datasets,transactions.info (),time-series-forecasting-with-python.ipynb
converting data,X_train = np.asarray(X_train). astype(np.float32) ,time-series-forecasting-with-python.ipynb
rescale the data to 0 1 scale,"min_max_scaler = MinMaxScaler(feature_range =(0 , 1)) ",time-series-forecasting-with-python.ipynb
reshape the data,"X_train = np.reshape(X_train ,(X_train.shape[0], X_train.shape[1])) ",time-series-forecasting-with-python.ipynb
check the data shape,"print('Training data shape X: ' , X_train.shape) ",time-series-forecasting-with-python.ipynb
" IntroductionI decided to write this kernel because Titanic: Machine Learning from Disaster is one of my favorite competitions on Kaggle. This is a beginner level kernel which focuses on Exploratory Data Analysis and Feature Engineering. A lot of people start Kaggle with this competition and they get lost in extremely long tutorial kernels. This is a short kernel compared to the other ones. I hope this will be a good guide for starters and inspire them with new feature engineering ideas.Titanic: Machine Learning from Disaster is a great competition to apply domain knowledge for feature engineering, so I made a research and learned a lot about Titanic. There are many secrets to be revealed beneath the Titanic dataset. I tried to find out some of those secret factors that had affected the survival of passengers when the Titanic was sinking. I believe there are other features still waiting to be discovered. This kernel has 3 main sections Exploratory Data Analysis, Feature Engineering and Model, and it can achieve top 2 0.83732 public leaderboard score with a tuned Random Forest Classifier. It takes 60 seconds to run whole notebook. If you have any idea that might improve this kernel, please be sure to comment, or fork and experiment as you like. If you didn t understand any part, feel free to ask. ","import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style=""darkgrid"")

from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import StratifiedKFold

import string
import warnings
warnings.filterwarnings('ignore')

SEED = 42",titanic-advanced-feature-engineering-tutorial.ipynb
"Training set has 891 rows and test set has 418 rows Training set have 12 features and test set have 11 features One extra feature in training set is Survived feature, which is the target variable ","def concat_df(train_data , test_data): ",titanic-advanced-feature-engineering-tutorial.ipynb
Returns a concatenated df of training and test set," return pd.concat ([train_data , test_data], sort = True). reset_index(drop = True) ",titanic-advanced-feature-engineering-tutorial.ipynb
Returns divided dfs of training and test set," return all_data.loc[: 890], all_data.loc[891 :]. drop (['Survived'], axis = 1) ",titanic-advanced-feature-engineering-tutorial.ipynb
"1.1 Overview PassengerId is the unique id of the row and it doesn t have any effect on target Survived is the target variable we are trying to predict 0 or 1 : 1 Survived 0 Not Survived Pclass Passenger Class is the socio economic status of the passenger and it is a categorical ordinal feature which has 3 unique values 1, 2 or 3 : 1 Upper Class 2 Middle Class 3 Lower Class Name, Sex and Age are self explanatory SibSp is the total number of the passengers siblings and spouse Parch is the total number of the passengers parents and children Ticket is the ticket number of the passenger Fare is the passenger fare Cabin is the cabin number of the passenger Embarked is port of embarkation and it is a categorical feature which has 3 unique values C, Q or S : C Cherbourg Q Queenstown S Southampton ","print(df_train.info())
df_train.sample(3)",titanic-advanced-feature-engineering-tutorial.ipynb
"1.2 Missing Values As seen from below, some columns have missing values. display missing function shows the count of missing values in every column in both training and test set. Training set have missing values in Age, Cabin and Embarked columns Test set have missing values in Age, Cabin and Fare columnsIt is convenient to work on concatenated training and test set while dealing with missing values, otherwise filled data may overfit to training or test set samples. The count of missing values in Age, Embarked and Fare are smaller compared to total sample, but roughly 80 of the Cabin is missing. Missing values in Age, Embarked and Fare can be filled with descriptive statistical measures but that wouldn t work for Cabin.","def display_missing(df): 
 for col in df.columns.tolist(): 
 print('{} column missing values: {}'.format(col, df[col].isnull().sum()))
 print('\n')
 
for df in dfs:
 print('{}'.format(df.name))
 display_missing(df)",titanic-advanced-feature-engineering-tutorial.ipynb
"1.2.1 Age Missing values in Age are filled with median age, but using median age of the whole data set is not a good choice. Median age of Pclass groups is the best choice because of its high correlation with Age 0.408106 and Survived 0.338481 . It is also more logical to group ages by passenger classes instead of other features.","df_all_corr = df_all.corr().abs().unstack().sort_values(kind=""quicksort"", ascending=False).reset_index()
df_all_corr.rename(columns={""level_0"": ""Feature 1"", ""level_1"": ""Feature 2"", 0: 'Correlation Coefficient'}, inplace=True)
df_all_corr[df_all_corr['Feature 1'] == 'Age']",titanic-advanced-feature-engineering-tutorial.ipynb
"In order to be more accurate, Sex feature is used as the second level of groupby while filling the missing Age values. As seen from below, Pclass and Sex groups have distinct median Age values. When passenger class increases, the median age for both males and females also increases. However, females tend to have slightly lower median Age than males. The median ages below are used for filling the missing values in Age feature.","age_by_pclass_sex = df_all.groupby (['Sex' , 'Pclass']).median ()[ 'Age'] ",titanic-advanced-feature-engineering-tutorial.ipynb
Filling the missing values in Age with the medians of Sex and Pclass groups,"df_all['Age']= df_all.groupby (['Sex' , 'Pclass'])['Age']. apply(lambda x : x.fillna(x.median ())) ",titanic-advanced-feature-engineering-tutorial.ipynb
"1.2.2 Embarked Embarked is a categorical feature and there are only 2 missing values in whole data set. Both of those passengers are female, upper class and they have the same ticket number. This means that they know each other and embarked from the same port together. The mode Embarked value for an upper class female passenger is C Cherbourg , but this doesn t necessarily mean that they embarked from that port.",df_all[df_all['Embarked'].isnull()],titanic-advanced-feature-engineering-tutorial.ipynb
Filling the missing values in Embarked with S,df_all['Embarked']= df_all['Embarked']. fillna('S') ,titanic-advanced-feature-engineering-tutorial.ipynb
1.2.3 Fare There is only one passenger with missing Fare value. We can assume that Fare is related to family size Parch and SibSp and Pclass features. Median Fare value of a male with a third class ticket and no family is a logical choice to fill the missing value.,df_all[df_all['Fare'].isnull()],titanic-advanced-feature-engineering-tutorial.ipynb
Filling the missing value in Fare with the median Fare of 3rd class alone passenger,df_all['Fare']= df_all['Fare']. fillna(med_fare) ,titanic-advanced-feature-engineering-tutorial.ipynb
Creating Deck column from the first letter of the Cabin column M stands for Missing ,df_all['Deck']= df_all['Cabin']. apply(lambda s : s[0]if pd.notnull(s)else 'M') ,titanic-advanced-feature-engineering-tutorial.ipynb
Passenger in the T deck is changed to A,idx = df_all[df_all['Deck']== 'T']. index ,titanic-advanced-feature-engineering-tutorial.ipynb
"As I suspected, every deck has different survival rates and that information can t be discarded. Deck B, C, D and E have the highest survival rates. Those decks are mostly occupied by 1st class passengers. M has the lowest survival rate which is mostly occupied by 2nd and 3rd class passengers. To conclude, cabins used by 1st class passengers have higher survival rates than cabins used by 2nd and 3rd class passengers. In my opinion M Missing Cabin values has the lowest survival rate because they couldn t retrieve the cabin data of the victims. That s why I believe labeling that group as M is a reasonable way to handle the missing data. It is a unique group with shared characteristics. Deck feature has high cardinality right now so some of the values are grouped with each other based on their similarities. A, B and C decks are labeled as ABC because all of them have only 1st class passengers D and E decks are labeled as DE because both of them have similar passenger class distribution and same survival rate F and G decks are labeled as FG because of the same reason above M deck doesn t need to be grouped with other decks because it is very different from others and has the lowest survival rate.","df_all['Deck'] = df_all['Deck'].replace(['A', 'B', 'C'], 'ABC')
df_all['Deck'] = df_all['Deck'].replace(['D', 'E'], 'DE')
df_all['Deck'] = df_all['Deck'].replace(['F', 'G'], 'FG')

df_all['Deck'].value_counts()",titanic-advanced-feature-engineering-tutorial.ipynb
Dropping the Cabin feature,"df_all.drop (['Cabin'], inplace = True , axis = 1) ",titanic-advanced-feature-engineering-tutorial.ipynb
1.3 Target Distribution 38.38 342 891 of training set is Class 1 61.62 549 891 of training set is Class 0 ,"survived = df_train['Survived'].value_counts()[1]
not_survived = df_train['Survived'].value_counts()[0]
survived_per = survived / df_train.shape[0] * 100
not_survived_per = not_survived / df_train.shape[0] * 100

print('{} of {} passengers survived and it is the {:.2f}% of the training set.'.format(survived, df_train.shape[0], survived_per))
print('{} of {} passengers didnt survive and it is the {:.2f}% of the training set.'.format(not_survived, df_train.shape[0], not_survived_per))

plt.figure(figsize=(10, 8))
sns.countplot(df_train['Survived'])

plt.xlabel('Survival', size=15, labelpad=15)
plt.ylabel('Passenger Count', size=15, labelpad=15)
plt.xticks((0, 1), ['Not Survived ({0:.2f}%)'.format(not_survived_per), 'Survived ({0:.2f}%)'.format(survived_per)])
plt.tick_params(axis='x', labelsize=13)
plt.tick_params(axis='y', labelsize=13)

plt.title('Training Set Survival Distribution', size=15, y=1.05)

plt.show()",titanic-advanced-feature-engineering-tutorial.ipynb
1.4 Correlations Features are highly correlated with each other and dependent to each other. The highest correlation between features is 0.549500 in training set and 0.577147 in test set between Fare and Pclass . The other features are also highly correlated. There are 9 correlations in training set and 6 correlations in test set that are higher than 0.1.,"df_train_corr = df_train.drop(['PassengerId'], axis=1).corr().abs().unstack().sort_values(kind=""quicksort"", ascending=False).reset_index()
df_train_corr.rename(columns={""level_0"": ""Feature 1"", ""level_1"": ""Feature 2"", 0: 'Correlation Coefficient'}, inplace=True)
df_train_corr.drop(df_train_corr.iloc[1::2].index, inplace=True)
df_train_corr_nd = df_train_corr.drop(df_train_corr[df_train_corr['Correlation Coefficient'] == 1.0].index)

df_test_corr = df_test.corr().abs().unstack().sort_values(kind=""quicksort"", ascending=False).reset_index()
df_test_corr.rename(columns={""level_0"": ""Feature 1"", ""level_1"": ""Feature 2"", 0: 'Correlation Coefficient'}, inplace=True)
df_test_corr.drop(df_test_corr.iloc[1::2].index, inplace=True)
df_test_corr_nd = df_test_corr.drop(df_test_corr[df_test_corr['Correlation Coefficient'] == 1.0].index)",titanic-advanced-feature-engineering-tutorial.ipynb
Training set high correlations,corr = df_train_corr_nd['Correlation Coefficient']> 0.1 ,titanic-advanced-feature-engineering-tutorial.ipynb
Test set high correlations,corr = df_test_corr_nd['Correlation Coefficient']> 0.1 ,titanic-advanced-feature-engineering-tutorial.ipynb
"1.5.1 Continuous Features Both of the continuous features Age and Fare have good split points and spikes for a decision tree to learn. One potential problem for both features is, the distribution has more spikes and bumps in training set, but it is smoother in test set. Model may not be able to generalize to test set because of this reason. Distribution of Age feature clearly shows that children younger than 15 has a higher survival rate than any of the other age groups In distribution of Fare feature, the survival rate is higher on distribution tails. The distribution also has positive skew because of the extremely large outliers ","cont_features =['Age' , 'Fare'] ",titanic-advanced-feature-engineering-tutorial.ipynb
Distribution of survival in feature," sns.distplot(df_train[~ surv][ feature], label = 'Not Survived' , hist = True , color = '#e74c3c' , ax = axs[0][ i]) ",titanic-advanced-feature-engineering-tutorial.ipynb
Distribution of feature in dataset," sns.distplot(df_train[feature], label = 'Training Set' , hist = False , color = '#e74c3c' , ax = axs[1][ i]) ",titanic-advanced-feature-engineering-tutorial.ipynb
1.5.2 Categorical Features Every categorical feature has at least one class with high mortality rate. Those classes are very helpful to predict whether the passenger is a survivor or victim. Best categorical features are Pclass and Sex because they have the most homogenous distributions. Passengers boarded from Southampton has a lower survival rate unlike other ports. More than half of the passengers boarded from Cherbourg had survived. This observation could be related to Pclass feature Parch and SibSp features show that passengers with only one family member has a higher survival rate ,"cat_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Deck']

fig, axs = plt.subplots(ncols=2, nrows=3, figsize=(20, 20))
plt.subplots_adjust(right=1.5, top=1.25)

for i, feature in enumerate(cat_features, 1): 
 plt.subplot(2, 3, i)
 sns.countplot(x=feature, hue='Survived', data=df_train)
 
 plt.xlabel('{}'.format(feature), size=20, labelpad=15)
 plt.ylabel('Passenger Count', size=20, labelpad=15) 
 plt.tick_params(axis='x', labelsize=20)
 plt.tick_params(axis='y', labelsize=20)
 
 plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})
 plt.title('Count of Survival in {} Feature'.format(feature), size=20, y=1.05)

plt.show()",titanic-advanced-feature-engineering-tutorial.ipynb
"1.6 Conclusion Most of the features are correlated with each other. This relationship can be used to create new features with feature transformation and feature interaction. Target encoding could be very useful as well because of the high correlations with Survived feature.Split points and spikes are visible in continuous features. They can be captured easily with a decision tree model, but linear models may not be able to spot them.Categorical features have very distinct distributions with different survival rates. Those features can be one hot encoded. Some of those features may be combined with each other to make new features.Created a new feature called Deck and dropped Cabin feature at the Exploratory Data Analysis part.","df_all = concat_df(df_train, df_test)
df_all.head()",titanic-advanced-feature-engineering-tutorial.ipynb
"2.1.1 Fare Fare feature is positively skewed and survival rate is extremely high on the right end. 13 quantile based bins are used for Fare feature. Even though the bins are too much, they provide decent amount of information gain. The groups at the left side of the graph has the lowest survival rate and the groups at the right side of the graph has the highest survival rate. This high survival rate was not visible in the distribution graph. There is also an unusual group 15.742, 23.25 in the middle with high survival rate that is captured in this process.","df_all['Fare'] = pd.qcut(df_all['Fare'], 13)",titanic-advanced-feature-engineering-tutorial.ipynb
"2.1.2 Age Age feature has a normal distribution with some spikes and bumps and 10 quantile based bins are used for Age. The first bin has the highest survival rate and 4th bin has the lowest survival rate. Those were the biggest spikes in the distribution. There is also an unusual group 34.0, 40.0 with high survival rate that is captured in this process.","df_all['Age'] = pd.qcut(df_all['Age'], 10)",titanic-advanced-feature-engineering-tutorial.ipynb
"2.2 Frequency Encoding Family Size is created by adding SibSp, Parch and 1. SibSp is the count of siblings and spouse, and Parch is the count of parents and children. Those columns are added in order to find the total size of families. Adding 1 at the end, is the current passenger. Graphs have clearly shown that family size is a predictor of survival because different values have different survival rates. Family Size with 1 are labeled as Alone Family Size with 2, 3 and 4 are labeled as Small Family Size with 5 and 6 are labeled as Medium Family Size with 7, 8 and 11 are labeled as Large","df_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1

fig, axs = plt.subplots(figsize=(20, 20), ncols=2, nrows=2)
plt.subplots_adjust(right=1.5)

sns.barplot(x=df_all['Family_Size'].value_counts().index, y=df_all['Family_Size'].value_counts().values, ax=axs[0][0])
sns.countplot(x='Family_Size', hue='Survived', data=df_all, ax=axs[0][1])

axs[0][0].set_title('Family Size Feature Value Counts', size=20, y=1.05)
axs[0][1].set_title('Survival Counts in Family Size ', size=20, y=1.05)

family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}
df_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)

sns.barplot(x=df_all['Family_Size_Grouped'].value_counts().index, y=df_all['Family_Size_Grouped'].value_counts().values, ax=axs[1][0])
sns.countplot(x='Family_Size_Grouped', hue='Survived', data=df_all, ax=axs[1][1])

axs[1][0].set_title('Family Size Feature Value Counts After Grouping', size=20, y=1.05)
axs[1][1].set_title('Survival Counts in Family Size After Grouping', size=20, y=1.05)

for i in range(2):
 axs[i][1].legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 20})
 for j in range(2):
 axs[i][j].tick_params(axis='x', labelsize=20)
 axs[i][j].tick_params(axis='y', labelsize=20)
 axs[i][j].set_xlabel('')
 axs[i][j].set_ylabel('')

plt.show()",titanic-advanced-feature-engineering-tutorial.ipynb
"There are too many unique Ticket values to analyze, so grouping them up by their frequencies makes things easier.How is this feature different than Family Size? Many passengers travelled along with groups. Those groups consist of friends, nannies, maids and etc. They weren t counted as family, but they used the same ticket.Why not grouping tickets by their prefixes? If prefixes in Ticket feature has any meaning, then they are already captured in Pclass or Embarked features because that could be the only logical information which can be derived from the Ticket feature.According to the graph below, groups with 2,3 and 4 members had a higher survival rate. Passengers who travel alone has the lowest survival rate. After 4 group members, survival rate decreases drastically. This pattern is very similar to Family Size feature but there are minor differences. Ticket Frequency values are not grouped like Family Size because that would basically create the same feature with perfect correlation. This kind of feature wouldn t provide any additional information gain.",df_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count'),titanic-advanced-feature-engineering-tutorial.ipynb
"2.3 Title Is Married Title is created by extracting the prefix before Name feature. According to graph below, there are many titles that are occuring very few times. Some of those titles doesn t seem correct and they need to be replaced. Miss, Mrs, Ms, Mlle, Lady, Mme, the Countess, Dona titles are replaced with Miss Mrs Ms because all of them are female. Values like Mlle, Mme and Dona are actually the name of the passengers, but they are classified as titles because Name feature is split by comma. Dr, Col, Major, Jonkheer, Capt, Sir, Don and Rev titles are replaced with Dr Military Noble Clergy because those passengers have similar characteristics. Master is a unique title. It is given to male passengers below age 26. They have the highest survival rate among all males.Is Married is a binary feature based on the Mrs title. Mrs title has the highest survival rate among other female titles. This title needs to be a feature because all female titles are grouped with each other.","df_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]
df_all['Is_Married'] = 0
df_all['Is_Married'].loc[df_all['Title'] == 'Mrs'] = 1",titanic-advanced-feature-engineering-tutorial.ipynb
2.4 Target Encoding extract surname function is used for extracting surnames of passengers from the Name feature. Family feature is created with the extracted surname. This is necessary for grouping passengers in the same family. ,"def extract_surname(data): 
 
 families = []
 
 for i in range(len(data)): 
 name = data.iloc[i]

 if '(' in name:
 name_no_bracket = name.split('(')[0] 
 else:
 name_no_bracket = name
 
 family = name_no_bracket.split(',')[0]
 title = name_no_bracket.split(',')[1].strip().split(' ')[0]
 
 for c in string.punctuation:
 family = family.replace(c, '').strip()
 
 families.append(family)
 
 return families

df_all['Family'] = extract_surname(df_all['Name'])
df_train = df_all.loc[:890]
df_test = df_all.loc[891:]
dfs = [df_train, df_test]",titanic-advanced-feature-engineering-tutorial.ipynb
Creating a list of families and tickets that are occuring in both training and test set,non_unique_families =[x for x in df_train['Family']. unique ()if x in df_test['Family']. unique()] ,titanic-advanced-feature-engineering-tutorial.ipynb
"Checking a family exists in both training and test set, and has members more than 1"," if df_family_survival_rate.index[i]in non_unique_families and df_family_survival_rate.iloc[i , 1]> 1 : ",titanic-advanced-feature-engineering-tutorial.ipynb
"Checking a ticket exists in both training and test set, and has members more than 1"," if df_ticket_survival_rate.index[i]in non_unique_tickets and df_ticket_survival_rate.iloc[i , 1]> 1 : ",titanic-advanced-feature-engineering-tutorial.ipynb
"2.5.1 Label Encoding Non Numerical Features Embarked, Sex, Deck , Title and Family Size Grouped are object type, and Age and Fare features are category type. They are converted to numerical type with LabelEncoder. LabelEncoder basically labels the classes from 0 to n. This process is necessary for models to learn from those features.","non_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare']

for df in dfs:
 for feature in non_numeric_features: 
 df[feature] = LabelEncoder().fit_transform(df[feature])",titanic-advanced-feature-engineering-tutorial.ipynb
"2.5.2 One Hot Encoding the Categorical Features The categorical features Pclass, Sex, Deck, Embarked, Title are converted to one hot encoded features with OneHotEncoder. Age and Fare features are not converted because they are ordinal unlike the previous ones.","cat_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']
encoded_features = []

for df in dfs:
 for feature in cat_features:
 encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()
 n = df[feature].nunique()
 cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]
 encoded_df = pd.DataFrame(encoded_feat, columns=cols)
 encoded_df.index = df.index
 encoded_features.append(encoded_df)

df_train = pd.concat([df_train, *encoded_features[:6]], axis=1)
df_test = pd.concat([df_test, *encoded_features[6:]], axis=1)",titanic-advanced-feature-engineering-tutorial.ipynb
"2.6 Conclusion Age and Fare features are binned. Binning helped dealing with outliers and it revealed some homogeneous groups in those features. Family Size is created by adding Parch and SibSp features and 1. Ticket Frequency is created by counting the occurence of Ticket values.Name feature is very useful. First, Title and Is Married features are created from the title prefix in the names. Second, Family Survival Rate and Family Survival Rate NA features are created by target encoding the surname of the passengers. Ticket Survival Rate is created by target encoding the Ticket feature. Survival Rate feature is created by averaging the Family Survival Rate and Ticket Survival Rate features.Finally, the non numeric type features are label encoded and categorical features are one hot encoded. Created 5 new features Family Size, Title, Is Married, Survival Rate and Survival Rate NA and dropped the useless features after encoding.","df_all = concat_df(df_train, df_test)
drop_cols = ['Deck', 'Embarked', 'Family', 'Family_Size', 'Family_Size_Grouped', 'Survived',
 'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Ticket', 'Title',
 'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA']

df_all.drop(columns=drop_cols, inplace=True)

df_all.head()",titanic-advanced-feature-engineering-tutorial.ipynb
 Model ,"X_train = StandardScaler().fit_transform(df_train.drop(columns=drop_cols))
y_train = df_train['Survived'].values
X_test = StandardScaler().fit_transform(df_test.drop(columns=drop_cols))

print('X_train shape: {}'.format(X_train.shape))
print('y_train shape: {}'.format(y_train.shape))
print('X_test shape: {}'.format(X_test.shape))",titanic-advanced-feature-engineering-tutorial.ipynb
"3.1 Random Forest Created 2 RandomForestClassifier s. One of them is a single model and the other is for k fold cross validation.The highest accuracy of the single best model is 0.82775 in public leaderboard. However, it doesn t perform better in k fold cross validation. It is a good model to start experimenting and hyperparameter tuning.The highest accuracy of leaderboard model is 0.83732 in public leaderboard with 5 fold cross validation. This model is created for leaderboard score and it is tuned to overfit slightly. It is designed to overfit because the estimated probabilities of X test in every fold are going to be divided by N fold count . If this model is used as a single model, it would struggle to predict lots of samples correctly.Which model should I use? leaderboard model overfits to test set so it s not suggested to use models like this in real life projects. single best model is a good model to start experimenting and learning about decision trees.","single_best_model = RandomForestClassifier(criterion='gini', 
 n_estimators=1100,
 max_depth=5,
 min_samples_split=4,
 min_samples_leaf=5,
 max_features='auto',
 oob_score=True,
 random_state=SEED,
 n_jobs=-1,
 verbose=1)

leaderboard_model = RandomForestClassifier(criterion='gini',
 n_estimators=1750,
 max_depth=7,
 min_samples_split=6,
 min_samples_leaf=6,
 max_features='auto',
 oob_score=True,
 random_state=SEED,
 n_jobs=-1,
 verbose=1) ",titanic-advanced-feature-engineering-tutorial.ipynb
StratifiedKFold is used for stratifying the target variable. The folds are made by preserving the percentage of samples for each class in target variable Survived .,N = 5 ,titanic-advanced-feature-engineering-tutorial.ipynb
Fitting the model," leaderboard_model.fit(X_train[trn_idx], y_train[trn_idx]) ",titanic-advanced-feature-engineering-tutorial.ipynb
Computing Train AUC score," trn_fpr , trn_tpr , trn_thresholds = roc_curve(y_train[trn_idx], leaderboard_model.predict_proba(X_train[trn_idx])[: , 1]) ",titanic-advanced-feature-engineering-tutorial.ipynb
Computing Validation AUC score," val_fpr , val_tpr , val_thresholds = roc_curve(y_train[val_idx], leaderboard_model.predict_proba(X_train[val_idx])[: , 1]) ",titanic-advanced-feature-engineering-tutorial.ipynb
X test probabilities," probs.loc[: , 'Fold_{}_Prob_0'.format(fold )]= leaderboard_model.predict_proba(X_test)[ : , 0] ",titanic-advanced-feature-engineering-tutorial.ipynb
3.2 Feature Importance,"importances['Mean_Importance'] = importances.mean(axis=1)
importances.sort_values(by='Mean_Importance', inplace=True, ascending=False)

plt.figure(figsize=(15, 20))
sns.barplot(x='Mean_Importance', y=importances.index, data=importances)

plt.xlabel('')
plt.tick_params(axis='x', labelsize=15)
plt.tick_params(axis='y', labelsize=15)
plt.title('Random Forest Classifier Mean Feature Importance Between Folds', size=15)

plt.show()",titanic-advanced-feature-engineering-tutorial.ipynb
3.3 ROC Curve,"def plot_roc_curve(fprs , tprs): ",titanic-advanced-feature-engineering-tutorial.ipynb
Plotting ROC for each fold and computing AUC scores," for i ,(fpr , tpr)in enumerate(zip(fprs , tprs), 1): ",titanic-advanced-feature-engineering-tutorial.ipynb
Plotting ROC for random guessing," plt.plot ([0 , 1],[0 , 1], linestyle = '--' , lw = 2 , color = 'r' , alpha = 0.8 , label = 'Random Guessing') ",titanic-advanced-feature-engineering-tutorial.ipynb
Plotting the mean ROC," ax.plot(mean_fpr , mean_tpr , color = 'b' , label = 'Mean ROC (AUC = {:.3f} $\pm$ {:.3f})'.format(mean_auc , std_auc), lw = 2 , alpha = 0.8) ",titanic-advanced-feature-engineering-tutorial.ipynb
Plotting the standard deviation around the mean ROC Curve," std_tpr = np.std(tprs_interp , axis = 0) ",titanic-advanced-feature-engineering-tutorial.ipynb
3.4 Submission,"class_survived = [col for col in probs.columns if col.endswith('Prob_1')]
probs['1'] = probs[class_survived].sum(axis=1) / N
probs['0'] = probs.drop(columns=class_survived).sum(axis=1) / N
probs['pred'] = 0
pos = probs[probs['1'] >= 0.5].index
probs.loc[pos, 'pred'] = 1

y_pred = probs['pred'].astype(int)

submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])
submission_df['PassengerId'] = df_test['PassengerId']
submission_df['Survived'] = y_pred.values
submission_df.to_csv('submissions.csv', header=True, index=False)
submission_df.head(10)",titanic-advanced-feature-engineering-tutorial.ipynb
"IntroductionThis is my first work of machine learning. the notebook is written in python and has inspired from Exploring Survival on Titanic by Megan Risdal, a Kernel in R on Kaggle 1 .","%matplotlib inline
import numpy as np
import pandas as pd
import re as re

train = pd.read_csv('../input/train.csv', header = 0, dtype={'Age': np.float64})
test = pd.read_csv('../input/test.csv' , header = 0, dtype={'Age': np.float64})
full_data = [train, test]

print (train.info())",titanic-best-working-classifier.ipynb
 Pclass there is no missing value on this feature and already a numerical value. so let s check it s impact on our train set. ,"print (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())",titanic-best-working-classifier.ipynb
 Sex ,"print (train[[""Sex"", ""Survived""]].groupby(['Sex'], as_index=False).mean())",titanic-best-working-classifier.ipynb
 SibSp and Parch With the number of siblings spouse and the number of children parents we can create new feature called Family Size. ,"for dataset in full_data:
 dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1
print (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())",titanic-best-working-classifier.ipynb
it seems has a good effect on our prediction but let s go further and categorize people to check whether they are alone in this ship or not.,"for dataset in full_data:
 dataset['IsAlone'] = 0
 dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1
print (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())",titanic-best-working-classifier.ipynb
 Embarked the embarked feature has some missing value. and we try to fill those with the most occurred value S . ,"for dataset in full_data:
 dataset['Embarked'] = dataset['Embarked'].fillna('S')
print (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())",titanic-best-working-classifier.ipynb
 Fare Fare also has some missing value and we will replace it with the median. then we categorize it into 4 ranges. ,"for dataset in full_data:
 dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())
train['CategoricalFare'] = pd.qcut(train['Fare'], 4)
print (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())",titanic-best-working-classifier.ipynb
 Age we have plenty of missing values in this feature. generate random numbers between mean std and mean std . then we categorize age into 5 range. ,"for dataset in full_data:
 age_avg 	 = dataset['Age'].mean()
 age_std 	 = dataset['Age'].std()
 age_null_count = dataset['Age'].isnull().sum()
 
 age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)
 dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list
 dataset['Age'] = dataset['Age'].astype(int)
 
train['CategoricalAge'] = pd.cut(train['Age'], 5)

print (train[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())",titanic-best-working-classifier.ipynb
 Name inside this feature we can find the title of people. ,def get_title(name): ,titanic-best-working-classifier.ipynb
"If the title exists, extract and return it.",	 if title_search : ,titanic-best-working-classifier.ipynb
so we have titles. let s categorize it and check the title impact on survival rate.,"for dataset in full_data:
 dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\
 	'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')

 dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')
 dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')
 dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')

print (train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())",titanic-best-working-classifier.ipynb
Data Cleaning great! now let s clean our data and map our features into numerical values.,for dataset in full_data : ,titanic-best-working-classifier.ipynb
Mapping Sex," dataset['Sex']= dataset['Sex']. map({ 'female' : 0 , 'male' : 1 }). astype(int) ",titanic-best-working-classifier.ipynb
Mapping titles," title_mapping = { ""Mr"" : 1 , ""Miss"" : 2 , ""Mrs"" : 3 , ""Master"" : 4 , ""Rare"" : 5 } ",titanic-best-working-classifier.ipynb
Mapping Embarked," dataset['Embarked']= dataset['Embarked']. map({ 'S' : 0 , 'C' : 1 , 'Q' : 2 }). astype(int) ",titanic-best-working-classifier.ipynb
Mapping Fare," dataset.loc[dataset['Fare']<= 7.91 , 'Fare']= 0 ",titanic-best-working-classifier.ipynb
Mapping Age," dataset.loc[dataset['Age']<= 16 , 'Age']= 0 ",titanic-best-working-classifier.ipynb
Classifier Comparison,"import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import accuracy_score, log_loss
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression

classifiers = [
 KNeighborsClassifier(3),
 SVC(probability=True),
 DecisionTreeClassifier(),
 RandomForestClassifier(),
	AdaBoostClassifier(),
 GradientBoostingClassifier(),
 GaussianNB(),
 LinearDiscriminantAnalysis(),
 QuadraticDiscriminantAnalysis(),
 LogisticRegression()]

log_cols = [""Classifier"", ""Accuracy""]
log 	 = pd.DataFrame(columns=log_cols)

sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)

X = train[0::, 1::]
y = train[0::, 0]

acc_dict = {}

for train_index, test_index in sss.split(X, y):
	X_train, X_test = X[train_index], X[test_index]
	y_train, y_test = y[train_index], y[test_index]
	
	for clf in classifiers:
		name = clf.__class__.__name__
		clf.fit(X_train, y_train)
		train_predictions = clf.predict(X_test)
		acc = accuracy_score(y_test, train_predictions)
		if name in acc_dict:
			acc_dict[name] += acc
		else:
			acc_dict[name] = acc

for clf in acc_dict:
	acc_dict[clf] = acc_dict[clf] / 10.0
	log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)
	log = log.append(log_entry)

plt.xlabel('Accuracy')
plt.title('Classifier Accuracy')

sns.set_color_codes(""muted"")
sns.barplot(x='Accuracy', y='Classifier', data=log, color=""b"")",titanic-best-working-classifier.ipynb
Prediction now we can use SVC classifier to predict our data.,"candidate_classifier = SVC()
candidate_classifier.fit(train[0::, 1::], train[0::, 0])
result = candidate_classifier.predict(test)",titanic-best-working-classifier.ipynb
data analysis and wrangling,import pandas as pd ,titanic-data-science-solutions.ipynb
visualization,import seaborn as sns ,titanic-data-science-solutions.ipynb
machine learning,from sklearn.linear_model import LogisticRegression ,titanic-data-science-solutions.ipynb
Acquire dataThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.,"train_df = pd.read_csv('../input/train.csv')
test_df = pd.read_csv('../input/test.csv')
combine = [train_df, test_df]",titanic-data-science-solutions.ipynb
Analyze by describing dataPandas also helps describe the datasets answering following questions early in our project.Which features are available in the dataset?Noting the feature names for directly manipulating or analyzing these. These feature names are described on the Kaggle data page here.,print(train_df.columns.values),titanic-data-science-solutions.ipynb
preview the data,train_df.head () ,titanic-data-science-solutions.ipynb
"Which features are mixed data types?Numerical, alphanumeric data within same feature. These are candidates for correcting goal. Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanumeric. Which features may contain errors or typos?This is harder to review for a large dataset, however reviewing a few samples from a smaller dataset may just tell us outright, which features may require correcting. Name feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names. ",train_df.tail(),titanic-data-science-solutions.ipynb
"Which features contain blank, null or empty values?These will require correcting. Cabin Age Embarked features contain a number of null values in that order for the training dataset. Cabin Age are incomplete in case of test dataset. What are the data types for various features?Helping us during converting goal. Seven features are integer or floats. Six in case of test dataset. Five features are strings object . ","train_df.info()
print('_'*40)
test_df.info()",titanic-data-science-solutions.ipynb
"What is the distribution of numerical feature values across the samples?This helps us determine, among other early insights, how representative is the training dataset of the actual problem domain. Total samples are 891 or 40 of the actual number of passengers on board the Titanic 2,224 . Survived is a categorical feature with 0 or 1 values. Around 38 samples survived representative of the actual survival rate at 32 . Most passengers 75 did not travel with parents or children. Nearly 30 of the passengers had siblings and or spouse aboard. Fares varied significantly with few passengers 1 paying as high as 512. Few elderly passengers 1 within age range 65 80. ",train_df.describe () ,titanic-data-science-solutions.ipynb
"What is the distribution of categorical features? Names are unique across the dataset count unique 891 Sex variable as two possible values with 65 male top male, freq 577 count 891 . Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin. Embarked takes three possible values. S port used by most passengers top S Ticket feature has high ratio 22 of duplicate values unique 681 . ",train_df.describe(include=['O']),titanic-data-science-solutions.ipynb
"Analyze by pivoting featuresTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical Sex , ordinal Pclass or discrete SibSp, Parch type. Pclass We observe significant correlation 0.5 among Pclass 1 and Survived classifying 3 . We decide to include this feature in our model. Sex We confirm the observation during problem definition that Sex female had very high survival rate at 74 classifying 1 . SibSp and Parch These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features creating 1 . ","train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)",titanic-data-science-solutions.ipynb
Analyze by visualizing dataNow we can continue confirming some of our assumptions using visualizations for analyzing the data.Correlating numerical featuresLet us start by understanding correlations between numerical features and our solution goal Survived .A histogram chart is useful for analyzing continous numerical variables like Age where banding or ranges will help identify useful patterns. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. This helps us answer questions relating to specific bands Did infants have better survival rate? Note that x axis in historgram visualizations represents the count of samples or passengers.Observations. Infants Age 4 had high survival rate. Oldest passengers Age 80 survived. Large number of 15 25 year olds did not survive. Most passengers are in 15 35 age range. Decisions.This simple analysis confirms our assumptions as decisions for subsequent workflow stages. We should consider Age our assumption classifying 2 in our model training. Complete the Age feature for null values completing 1 . We should band age groups creating 3 . ,"g = sns.FacetGrid(train_df, col='Survived')
g.map(plt.hist, 'Age', bins=20)",titanic-data-science-solutions.ipynb
"grid sns.FacetGrid train df, col Pclass , hue Survived ","grid = sns.FacetGrid(train_df , col = 'Survived' , row = 'Pclass' , size = 2.2 , aspect = 1.6) ",titanic-data-science-solutions.ipynb
"grid sns.FacetGrid train df, col Embarked ","grid = sns.FacetGrid(train_df , row = 'Embarked' , size = 2.2 , aspect = 1.6) ",titanic-data-science-solutions.ipynb
"grid sns.FacetGrid train df, col Embarked , hue Survived , palette 0: k , 1: w ","grid = sns.FacetGrid(train_df , row = 'Embarked' , col = 'Survived' , size = 2.2 , aspect = 1.6) ",titanic-data-science-solutions.ipynb
"Wrangle dataWe have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.Correcting by dropping featuresThis is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.Based on our assumptions and decisions we want to drop the Cabin correcting 2 and Ticket correcting 1 features.Note that where applicable we perform operations on both training and testing datasets together to stay consistent.","print(""Before"", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)

train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)
test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)
combine = [train_df, test_df]

""After"", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape",titanic-data-science-solutions.ipynb
"Creating new feature extracting from existingWe want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.In the following code we extract Title feature using regular expressions. The RegEx pattern w . matches the first word which ends with a dot character within Name feature. The expand False flag returns a DataFrame.Observations.When we plot Title, Age, and Survived, we note the following observations. Most titles band Age groups accurately. For example: Master title has Age mean of 5 years. Survival among Title Age bands varies slightly. Certain titles mostly survived Mme, Lady, Sir or did not Don, Rev, Jonkheer . Decision. We decide to retain the new Title feature for model training. ","for dataset in combine:
 dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand=False)

pd.crosstab(train_df['Title'], train_df['Sex'])",titanic-data-science-solutions.ipynb
We can replace many titles with a more common name or classify them as Rare.,"for dataset in combine:
 dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\
 	'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')

 dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')
 dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')
 dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')
 
train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()",titanic-data-science-solutions.ipynb
We can convert the categorical titles to ordinal.,"title_mapping = {""Mr"": 1, ""Miss"": 2, ""Mrs"": 3, ""Master"": 4, ""Rare"": 5}
for dataset in combine:
 dataset['Title'] = dataset['Title'].map(title_mapping)
 dataset['Title'] = dataset['Title'].fillna(0)

train_df.head()",titanic-data-science-solutions.ipynb
Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.,"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)
test_df = test_df.drop(['Name'], axis=1)
combine = [train_df, test_df]
train_df.shape, test_df.shape",titanic-data-science-solutions.ipynb
Converting a categorical featureNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.Let us start by converting Sex feature to a new feature called Gender where female 1 and male 0.,"for dataset in combine:
 dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)

train_df.head()",titanic-data-science-solutions.ipynb
"grid sns.FacetGrid train df, col Pclass , hue Gender ","grid = sns.FacetGrid(train_df , row = 'Pclass' , col = 'Sex' , size = 2.2 , aspect = 1.6) ",titanic-data-science-solutions.ipynb
Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.,"guess_ages = np.zeros((2,3))
guess_ages",titanic-data-science-solutions.ipynb
"Now we iterate over Sex 0 or 1 and Pclass 1, 2, 3 to calculate guessed values of Age for the six combinations.",for dataset in combine : ,titanic-data-science-solutions.ipynb
Let us create Age bands and determine correlations with Survived.,"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)
train_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)",titanic-data-science-solutions.ipynb
Let us replace Age with ordinals based on these bands.,"for dataset in combine: 
 dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0
 dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1
 dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2
 dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3
 dataset.loc[ dataset['Age'] > 64, 'Age']
train_df.head()",titanic-data-science-solutions.ipynb
We can not remove the AgeBand feature.,"train_df = train_df.drop(['AgeBand'], axis=1)
combine = [train_df, test_df]
train_df.head()",titanic-data-science-solutions.ipynb
Create new feature combining existing featuresWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.,"for dataset in combine:
 dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1

train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)",titanic-data-science-solutions.ipynb
We can create another feature called IsAlone.,"for dataset in combine:
 dataset['IsAlone'] = 0
 dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1

train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()",titanic-data-science-solutions.ipynb
"Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.","train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)
test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)
combine = [train_df, test_df]

train_df.head()",titanic-data-science-solutions.ipynb
We can also create an artificial feature combining Pclass and Age.,"for dataset in combine:
 dataset['Age*Class'] = dataset.Age * dataset.Pclass

train_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)",titanic-data-science-solutions.ipynb
"Completing a categorical featureEmbarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance.","freq_port = train_df.Embarked.dropna().mode()[0]
freq_port",titanic-data-science-solutions.ipynb
Converting categorical feature to numericWe can now convert the EmbarkedFill feature by creating a new numeric Port feature.,"for dataset in combine:
 dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)

train_df.head()",titanic-data-science-solutions.ipynb
Quick completing and converting a numeric featureWe can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. We do this in a single line of code.Note that we are not creating an intermediate new feature or doing any further analysis for correlation to guess missing feature as we are replacing only a single value. The completion goal achieves desired requirement for model algorithm to operate on non null values.We may also want round off the fare to two decimals as it represents currency.,"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)
test_df.head()",titanic-data-science-solutions.ipynb
We can not create FareBand.,"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)
train_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)",titanic-data-science-solutions.ipynb
Convert the Fare feature to ordinal values based on the FareBand.,"for dataset in combine:
 dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0
 dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1
 dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2
 dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3
 dataset['Fare'] = dataset['Fare'].astype(int)

train_df = train_df.drop(['FareBand'], axis=1)
combine = [train_df, test_df]
 
train_df.head(10)",titanic-data-science-solutions.ipynb
And the test dataset.,test_df.head(10),titanic-data-science-solutions.ipynb
"Model, predict and solveNow we are ready to train a model and predict the required solution. There are 60 predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output Survived or not with other variables or features Gender, Age, Port... . We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include: Logistic Regression KNN or k Nearest Neighbors Support Vector Machines Naive Bayes classifier Decision Tree Random Forrest Perceptron Artificial neural network RVM or Relevance Vector Machine ","X_train = train_df.drop(""Survived"", axis=1)
Y_train = train_df[""Survived""]
X_test = test_df.drop(""PassengerId"", axis=1).copy()
X_train.shape, Y_train.shape, X_test.shape",titanic-data-science-solutions.ipynb
Logistic Regression,logreg = LogisticRegression () ,titanic-data-science-solutions.ipynb
"We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.Positive coefficients increase the log odds of the response and thus increase the probability , and negative coefficients decrease the log odds of the response and thus decrease the probability . Sex is highest positivie coefficient, implying as the Sex value increases male: 0 to female: 1 , the probability of Survived 1 increases the most. Inversely as Pclass increases, probability of Survived 1 decreases the most. This way Age Class is a good artificial feature to model as it has second highest negative correlation with Survived. So is Title as second highest positive correlation. ","coeff_df = pd.DataFrame(train_df.columns.delete(0))
coeff_df.columns = ['Feature']
coeff_df[""Correlation""] = pd.Series(logreg.coef_[0])

coeff_df.sort_values(by='Correlation', ascending=False)",titanic-data-science-solutions.ipynb
Support Vector Machines,svc = SVC () ,titanic-data-science-solutions.ipynb
"In pattern recognition, the k Nearest Neighbors algorithm or k NN for short is a non parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors k is a positive integer, typically small . If k 1, then the object is simply assigned to the class of that single nearest neighbor. Reference Wikipedia.KNN confidence score is better than Logistics Regression but worse than SVM.","knn = KNeighborsClassifier(n_neighbors = 3)
knn.fit(X_train, Y_train)
Y_pred = knn.predict(X_test)
acc_knn = round(knn.score(X_train, Y_train) * 100, 2)
acc_knn",titanic-data-science-solutions.ipynb
Gaussian Naive Bayes,gaussian = GaussianNB () ,titanic-data-science-solutions.ipynb
Perceptron,perceptron = Perceptron () ,titanic-data-science-solutions.ipynb
Linear SVC,linear_svc = LinearSVC () ,titanic-data-science-solutions.ipynb
Stochastic Gradient Descent,sgd = SGDClassifier () ,titanic-data-science-solutions.ipynb
Decision Tree,decision_tree = DecisionTreeClassifier () ,titanic-data-science-solutions.ipynb
Random Forest,random_forest = RandomForestClassifier(n_estimators = 100) ,titanic-data-science-solutions.ipynb
"Model evaluationWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees habit of overfitting to their training set. ","models = pd.DataFrame({
 'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 
 'Random Forest', 'Naive Bayes', 'Perceptron', 
 'Stochastic Gradient Decent', 'Linear SVC', 
 'Decision Tree'],
 'Score': [acc_svc, acc_knn, acc_log, 
 acc_random_forest, acc_gaussian, acc_perceptron, 
 acc_sgd, acc_linear_svc, acc_decision_tree]})
models.sort_values(by='Score', ascending=False)",titanic-data-science-solutions.ipynb
linear algebra,import numpy as np ,titanic-project-example.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,titanic-project-example.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,titanic-project-example.ipynb
"Here we import the data. For this analysis, we will be exclusively working with the Training set. We will be validating based on data from the training set as well. For our final submissions, we will make predictions based on the test set. ","training = pd.read_csv('/kaggle/input/titanic/train.csv')
test = pd.read_csv('/kaggle/input/titanic/test.csv')

training['train_test'] = 1
test['train_test'] = 0
test['Survived'] = np.NaN
all_data = pd.concat([training,test])

%matplotlib inline
all_data.columns",titanic-project-example.ipynb
quick look at our data types null counts,training.info () ,titanic-project-example.ipynb
"to better understand the numeric data, we want to use the .describe method. This gives us an understanding of the central tendencies of the data",training.describe () ,titanic-project-example.ipynb
quick way to separate numeric columns,training.describe (). columns ,titanic-project-example.ipynb
look at numeric and categorical values separately,"df_num = training[[ 'Age' , 'SibSp' , 'Parch' , 'Fare']] ",titanic-project-example.ipynb
distributions for all numeric variables,for i in df_num.columns : ,titanic-project-example.ipynb
Perhaps we should take the non normal distributions and consider normalizing them?,"print(df_num.corr())
sns.heatmap(df_num.corr())",titanic-project-example.ipynb
"compare survival rate across Age, SibSp, Parch, and Fare","pd.pivot_table(training , index = 'Survived' , values =['Age' , 'SibSp' , 'Parch' , 'Fare']) ",titanic-project-example.ipynb
Comparing survival and each of these categorical variables,"print(pd.pivot_table(training , index = 'Survived' , columns = 'Pclass' , values = 'Ticket' , aggfunc = 'count')) ",titanic-project-example.ipynb
Feature Engineering 1 Cabin Simplify cabins evaluated if cabin letter cabin adv or the purchase of tickets across multiple cabins cabin multiple impacted survival 2 Tickets Do different ticket types impact survival rates?3 Does a person s title relate to survival rates?,df_cat.Cabin ,titanic-project-example.ipynb
multiple letters,training['cabin_multiple']. value_counts () ,titanic-project-example.ipynb
in this case we will treat null values like it s own category,training['cabin_adv']= training.Cabin.apply(lambda x : str(x)[ 0]) ,titanic-project-example.ipynb
comparing surivial rate by cabin,print(training.cabin_adv.value_counts ()) ,titanic-project-example.ipynb
numeric vs non numeric,training['numeric_ticket']= training.Ticket.apply(lambda x : 1 if x.isnumeric ()else 0) ,titanic-project-example.ipynb
lets us view all rows in dataframe through scrolling. This is for convenience,"pd.set_option(""max_rows"" , None) ",titanic-project-example.ipynb
difference in numeric vs non numeric tickets in survival rate,"pd.pivot_table(training , index = 'Survived' , columns = 'numeric_ticket' , values = 'Ticket' , aggfunc = 'count') ",titanic-project-example.ipynb
survival rate across different tyicket types,"pd.pivot_table(training , index = 'Survived' , columns = 'ticket_letters' , values = 'Ticket' , aggfunc = 'count') ",titanic-project-example.ipynb
feature engineering on person s title,training.Name.head(50) ,titanic-project-example.ipynb
"mr., ms., master. etc",training['name_title'].value_counts(),titanic-project-example.ipynb
create all categorical variables that we did above for both training and test sets,all_data['cabin_multiple']= all_data.Cabin.apply(lambda x : 0 if pd.isna(x)else len(x.split(' '))) ,titanic-project-example.ipynb
all data.Age all data.Age.fillna training.Age.mean ,all_data.Age = all_data.Age.fillna(training.Age.median ()) ,titanic-project-example.ipynb
all data.Fare all data.Fare.fillna training.Fare.mean ,all_data.Fare = all_data.Fare.fillna(training.Fare.median ()) ,titanic-project-example.ipynb
drop null embarked rows. Only 2 instances of this in training and 0 in test,"all_data.dropna(subset =['Embarked'], inplace = True) ",titanic-project-example.ipynb
tried log norm of sibsp not used ,all_data['norm_sibsp']= np.log(all_data.SibSp + 1) ,titanic-project-example.ipynb
log norm of fare used ,all_data['norm_fare']= np.log(all_data.Fare + 1) ,titanic-project-example.ipynb
converted fare to category for pd.get dummies ,all_data.Pclass = all_data.Pclass.astype(str) ,titanic-project-example.ipynb
created dummy variables from categories also can use OneHotEncoder ,"all_dummies = pd.get_dummies(all_data[[ 'Pclass' , 'Sex' , 'Age' , 'SibSp' , 'Parch' , 'norm_fare' , 'Embarked' , 'cabin_adv' , 'cabin_multiple' , 'numeric_ticket' , 'name_title' , 'train_test']]) ",titanic-project-example.ipynb
Split to train test again,"X_train = all_dummies[all_dummies.train_test == 1]. drop (['train_test'], axis = 1) ",titanic-project-example.ipynb
Scale data,from sklearn.preprocessing import StandardScaler ,titanic-project-example.ipynb
"Model Building Baseline Validation Performance Before going further, I like to see how various different models perform with default parameters. I tried the following models using 5 fold cross validation to get a baseline. With a validation set basline, we can see how much tuning improves each of the models. Just because a model has a high basline on this validation set doesn t mean that it will actually do better on the eventual test set. Naive Bayes 72.6 Logistic Regression 82.1 Decision Tree 77.6 K Nearest Neighbor 80.5 Random Forest 80.6 Support Vector Classifier 83.2 Xtreme Gradient Boosting 81.8 Soft Voting Classifier All Models 82.8 ","from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC",titanic-project-example.ipynb
I usually use Naive Bayes as a baseline for my classification tasks,gnb = GaussianNB () ,titanic-project-example.ipynb
A soft classifier averages the confidence of each of the models. If a the average confidence is 50 that it is a 1 it will be counted as such,from sklearn.ensemble import VotingClassifier ,titanic-project-example.ipynb
"Model Tuned Performance After getting the baselines, let s see if we can improve on the indivdual model results!I mainly used grid search to tune the models. I also used Randomized Search for the Random Forest and XG boosted model to simplify testing time. Model Baseline Tuned Performance Naive Bayes 72.6 NA Logistic Regression 82.1 82.6 Decision Tree 77.6 NA K Nearest Neighbor 80.5 83.0 Random Forest 80.6 83.6 Support Vector Classifier 83.2 83.2 Xtreme Gradient Boosting 81.8 85.3 ","from sklearn.model_selection import GridSearchCV 
from sklearn.model_selection import RandomizedSearchCV ",titanic-project-example.ipynb
simple performance reporting function,"def clf_performance(classifier , model_name): ",titanic-project-example.ipynb
"Because the total feature space is so large, I used a randomized search to narrow down the paramters for the model. I took the best model from this and did a more granular search","rf = RandomForestClassifier(random_state = 1)
param_grid = {'n_estimators': [400,450,500,550],
 'criterion':['gini','entropy'],
 'bootstrap': [True],
 'max_depth': [15, 20, 25],
 'max_features': ['auto','sqrt', 10],
 'min_samples_leaf': [2,3],
 'min_samples_split': [2,3]}
 
clf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)
best_clf_rf = clf_rf.fit(X_train_scaled,y_train)
clf_performance(best_clf_rf,'Random Forest')",titanic-project-example.ipynb
"Model Additional Ensemble Approaches 1 Experimented with a hard voting classifier of three estimators KNN, SVM, RF 81.6 2 Experimented with a soft voting classifier of three estimators KNN, SVM, RF 82.3 Best Performance 3 Experimented with soft voting on all estimators performing better than 80 except xgb KNN, RF, LR, SVC 82.9 4 Experimented with soft voting on all estimators including XGB KNN, SVM, RF, LR, XGB 83.5 ","best_lr = best_clf_lr.best_estimator_
best_knn = best_clf_knn.best_estimator_
best_svc = best_clf_svc.best_estimator_
best_rf = best_clf_rf.best_estimator_
best_xgb = best_clf_xgb.best_estimator_

voting_clf_hard = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'hard') 
voting_clf_soft = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'soft') 
voting_clf_all = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('lr', best_lr)], voting = 'soft') 
voting_clf_xgb = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('xgb', best_xgb),('lr', best_lr)], voting = 'soft')

print('voting_clf_hard :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5))
print('voting_clf_hard mean :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5).mean())

print('voting_clf_soft :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5))
print('voting_clf_soft mean :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5).mean())

print('voting_clf_all :',cross_val_score(voting_clf_all,X_train,y_train,cv=5))
print('voting_clf_all mean :',cross_val_score(voting_clf_all,X_train,y_train,cv=5).mean())

print('voting_clf_xgb :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5))
print('voting_clf_xgb mean :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5).mean())
",titanic-project-example.ipynb
no new results here,"params = { 'weights' :[[ 1 , 1 , 1],[1 , 2 , 1],[1 , 1 , 2],[2 , 1 , 1],[2 , 2 , 1],[1 , 2 , 2],[2 , 1 , 2]] } ",titanic-project-example.ipynb
Make Predictions,"voting_clf_hard.fit(X_train_scaled , y_train) ",titanic-project-example.ipynb
convert output to dataframe,"final_data = { 'PassengerId' : test.PassengerId , 'Survived' : y_hat_rf } ",titanic-project-example.ipynb
track differences between outputs,"comparison['difference_rf_vc_hard']= comparison.apply(lambda x : 1 if x.Survived_vc_hard != x.Survived_rf else 0 , axis = 1) ",titanic-project-example.ipynb
prepare submission files,"submission.to_csv('submission_rf.csv' , index = False) ",titanic-project-example.ipynb
data analysis libraries,import numpy as np ,titanic-survival-predictions-beginner.ipynb
visualization libraries,import matplotlib.pyplot as plt ,titanic-survival-predictions-beginner.ipynb
ignore warnings,import warnings ,titanic-survival-predictions-beginner.ipynb
import train and test CSV files,"train = pd.read_csv(""../input/train.csv"") ",titanic-survival-predictions-beginner.ipynb
take a look at the training data,"train.describe(include = ""all"") ",titanic-survival-predictions-beginner.ipynb
get a list of the features within the dataset,print(train.columns) ,titanic-survival-predictions-beginner.ipynb
see a sample of the dataset to get an idea of the variables,train.sample(5) ,titanic-survival-predictions-beginner.ipynb
see a summary of the training dataset,"train.describe(include = ""all"") ",titanic-survival-predictions-beginner.ipynb
check for any other unusable values,print(pd.isnull(train). sum ()) ,titanic-survival-predictions-beginner.ipynb
draw a bar plot of survival by sex,"sns.barplot(x = ""Sex"" , y = ""Survived"" , data = train) ",titanic-survival-predictions-beginner.ipynb
print percentages of females vs. males that survive,"print(""Percentage of females who survived:"" , train[""Survived""][ train[""Sex""]== 'female']. value_counts(normalize = True)[ 1]* 100) ",titanic-survival-predictions-beginner.ipynb
draw a bar plot of survival by Pclass,"sns.barplot(x = ""Pclass"" , y = ""Survived"" , data = train) ",titanic-survival-predictions-beginner.ipynb
print percentage of people by Pclass that survived,"print(""Percentage of Pclass = 1 who survived:"" , train[""Survived""][ train[""Pclass""]== 1]. value_counts(normalize = True)[ 1]* 100) ",titanic-survival-predictions-beginner.ipynb
draw a bar plot for SibSp vs. survival,"sns.barplot(x = ""SibSp"" , y = ""Survived"" , data = train) ",titanic-survival-predictions-beginner.ipynb
I won t be printing individual percent values for all of these.,"print(""Percentage of SibSp = 0 who survived:"" , train[""Survived""][ train[""SibSp""]== 0]. value_counts(normalize = True)[ 1]* 100) ",titanic-survival-predictions-beginner.ipynb
draw a bar plot for Parch vs. survival,"sns.barplot(x = ""Parch"" , y = ""Survived"" , data = train) ",titanic-survival-predictions-beginner.ipynb
sort the ages into logical categories,"train[""Age""]= train[""Age""]. fillna(- 0.5) ",titanic-survival-predictions-beginner.ipynb
draw a bar plot of Age vs. survival,"sns.barplot(x = ""AgeGroup"" , y = ""Survived"" , data = train) ",titanic-survival-predictions-beginner.ipynb
"Cabin Feature I think the idea here is that people with recorded cabin numbers are of higher socioeconomic class, and thus more likely to survive. Thanks for the tips, and Daniel Ellis!","train[""CabinBool""]=(train[""Cabin""]. notnull (). astype('int')) ",titanic-survival-predictions-beginner.ipynb
calculate percentages of CabinBool vs. survived,"print(""Percentage of CabinBool = 1 who survived:"" , train[""Survived""][ train[""CabinBool""]== 1]. value_counts(normalize = True)[ 1]* 100) ",titanic-survival-predictions-beginner.ipynb
draw a bar plot of CabinBool vs. survival,"sns.barplot(x = ""CabinBool"" , y = ""Survived"" , data = train) ",titanic-survival-predictions-beginner.ipynb
Looking at the Test Data Let s see how our test data looks!,"test.describe(include=""all"")",titanic-survival-predictions-beginner.ipynb
we ll start off by dropping the Cabin feature since not a lot more useful information can be extracted from it.,"train = train.drop (['Cabin'], axis = 1) ",titanic-survival-predictions-beginner.ipynb
we can also drop the Ticket feature since it s unlikely to yield any useful information,"train = train.drop (['Ticket'], axis = 1) ",titanic-survival-predictions-beginner.ipynb
now we need to fill in the missing values in the Embarked feature,"print(""Number of people embarking in Southampton (S):"") ",titanic-survival-predictions-beginner.ipynb
replacing the missing values in the Embarked feature with S,"train = train.fillna({ ""Embarked"" : ""S"" }) ",titanic-survival-predictions-beginner.ipynb
create a combined group of both datasets,"combine =[train , test] ",titanic-survival-predictions-beginner.ipynb
extract a title for each Name in the train and test datasets,for dataset in combine : ,titanic-survival-predictions-beginner.ipynb
replace various titles with more common names,for dataset in combine : ,titanic-survival-predictions-beginner.ipynb
map each of the title groups to a numerical value,"title_mapping = { ""Mr"" : 1 , ""Miss"" : 2 , ""Mrs"" : 3 , ""Master"" : 4 , ""Royal"" : 5 , ""Rare"" : 6 } ",titanic-survival-predictions-beginner.ipynb
Young Adult,"mr_age = train[train[""Title""]== 1][ ""AgeGroup""]. mode () ",titanic-survival-predictions-beginner.ipynb
Student,"miss_age = train[train[""Title""]== 2][ ""AgeGroup""]. mode () ",titanic-survival-predictions-beginner.ipynb
Adult,"mrs_age = train[train[""Title""]== 3][ ""AgeGroup""]. mode () ",titanic-survival-predictions-beginner.ipynb
Baby,"master_age = train[train[""Title""]== 4][ ""AgeGroup""]. mode () ",titanic-survival-predictions-beginner.ipynb
Adult,"royal_age = train[train[""Title""]== 5][ ""AgeGroup""]. mode () ",titanic-survival-predictions-beginner.ipynb
Adult,"rare_age = train[train[""Title""]== 6][ ""AgeGroup""]. mode () ",titanic-survival-predictions-beginner.ipynb
test test.fillna Age : test Title .map age title mapping ,"for x in range(len(train[""AgeGroup""])): ",titanic-survival-predictions-beginner.ipynb
map each Age value to a numerical value,"age_mapping = { 'Baby' : 1 , 'Child' : 2 , 'Teenager' : 3 , 'Student' : 4 , 'Young Adult' : 5 , 'Adult' : 6 , 'Senior' : 7 } ",titanic-survival-predictions-beginner.ipynb
"dropping the Age feature for now, might change","train = train.drop (['Age'], axis = 1) ",titanic-survival-predictions-beginner.ipynb
drop the name feature since it contains no more useful information.,"train = train.drop (['Name'], axis = 1) ",titanic-survival-predictions-beginner.ipynb
map each Sex value to a numerical value,"sex_mapping = { ""male"" : 0 , ""female"" : 1 } ",titanic-survival-predictions-beginner.ipynb
map each Embarked value to a numerical value,"embarked_mapping = { ""S"" : 1 , ""C"" : 2 , ""Q"" : 3 } ",titanic-survival-predictions-beginner.ipynb
fill in missing Fare value in test set based on mean fare for that Pclass,"for x in range(len(test[""Fare""])): ",titanic-survival-predictions-beginner.ipynb
Pclass 3," pclass = test[""Pclass""][ x] ",titanic-survival-predictions-beginner.ipynb
map Fare values into groups of numerical values,"train['FareBand']= pd.qcut(train['Fare'], 4 , labels =[1 , 2 , 3 , 4]) ",titanic-survival-predictions-beginner.ipynb
drop Fare values,"train = train.drop (['Fare'], axis = 1) ",titanic-survival-predictions-beginner.ipynb
check train data,train.head () ,titanic-survival-predictions-beginner.ipynb
check test data,test.head () ,titanic-survival-predictions-beginner.ipynb
Splitting the Training Data We will use part of our training data 22 in this case to test the accuracy of our different models.,"from sklearn.model_selection import train_test_split

predictors = train.drop(['Survived', 'PassengerId'], axis=1)
target = train[""Survived""]
x_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)",titanic-survival-predictions-beginner.ipynb
Gaussian Naive Bayes,from sklearn.naive_bayes import GaussianNB ,titanic-survival-predictions-beginner.ipynb
Logistic Regression,from sklearn.linear_model import LogisticRegression ,titanic-survival-predictions-beginner.ipynb
Support Vector Machines,from sklearn.svm import SVC ,titanic-survival-predictions-beginner.ipynb
Linear SVC,from sklearn.svm import LinearSVC ,titanic-survival-predictions-beginner.ipynb
Perceptron,from sklearn.linear_model import Perceptron ,titanic-survival-predictions-beginner.ipynb
Decision Tree,from sklearn.tree import DecisionTreeClassifier ,titanic-survival-predictions-beginner.ipynb
Random Forest,from sklearn.ensemble import RandomForestClassifier ,titanic-survival-predictions-beginner.ipynb
KNN or k Nearest Neighbors,from sklearn.neighbors import KNeighborsClassifier ,titanic-survival-predictions-beginner.ipynb
Stochastic Gradient Descent,from sklearn.linear_model import SGDClassifier ,titanic-survival-predictions-beginner.ipynb
Gradient Boosting Classifier,from sklearn.ensemble import GradientBoostingClassifier ,titanic-survival-predictions-beginner.ipynb
Let s compare the accuracies of each model!,"models = pd.DataFrame({
 'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 
 'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', 
 'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],
 'Score': [acc_svc, acc_knn, acc_logreg, 
 acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,
 acc_sgd, acc_gbk]})
models.sort_values(by='Score', ascending=False)",titanic-survival-predictions-beginner.ipynb
set ids as PassengerId and predict survival,ids = test['PassengerId'] ,titanic-survival-predictions-beginner.ipynb
set the output as a dataframe and convert to csv file named submission.csv,"output = pd.DataFrame({ 'PassengerId' : ids , 'Survived' : predictions }) ",titanic-survival-predictions-beginner.ipynb
" IntroductionThis is my first kernel at Kaggle. I choosed the Titanic competition which is a good way to introduce feature engineering and ensemble modeling. Firstly, I will display some feature analyses then ill focus on the feature engineering. Last part concerns modeling and predicting the survival on the Titanic using an voting procedure. This script follows three main parts: Feature analysis Feature engineering Modeling ","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

from collections import Counter

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve

sns.set(style='white', context='notebook', palette='deep')
",titanic-top-4-with-ensemble-modeling.ipynb
Load train and Test set,"train = pd.read_csv(""../input/train.csv"") ",titanic-top-4-with-ensemble-modeling.ipynb
Outlier detection,"def detect_outliers(df , n , features): ",titanic-top-4-with-ensemble-modeling.ipynb
Show the outliers rows,train.loc[Outliers_to_drop] ,titanic-top-4-with-ensemble-modeling.ipynb
Drop outliers,"train = train.drop(Outliers_to_drop , axis = 0). reset_index(drop = True) ",titanic-top-4-with-ensemble-modeling.ipynb
Join train and test datasets in order to obtain the same number of features during categorical conversion,train_len = len(train) ,titanic-top-4-with-ensemble-modeling.ipynb
Fill empty and NaNs values with NaN,dataset = dataset.fillna(np.nan) ,titanic-top-4-with-ensemble-modeling.ipynb
Check for Null values,dataset.isnull (). sum () ,titanic-top-4-with-ensemble-modeling.ipynb
Infos,train.info () ,titanic-top-4-with-ensemble-modeling.ipynb
Summarie and statistics,train.describe () ,titanic-top-4-with-ensemble-modeling.ipynb
Correlation matrix between numerical values SibSp Parch Age and Fare values and Survived,"g = sns.heatmap(train[[ ""Survived"" , ""SibSp"" , ""Parch"" , ""Age"" , ""Fare""]].corr (), annot = True , fmt = "".2f"" , cmap = ""coolwarm"") ",titanic-top-4-with-ensemble-modeling.ipynb
Explore Age vs Survived,"g = sns.FacetGrid(train , col = 'Survived') ",titanic-top-4-with-ensemble-modeling.ipynb
Explore Age distibution,"g = sns.kdeplot(train[""Age""][(train[""Survived""]== 0)&(train[""Age""]. notnull ())], color = ""Red"" , shade = True) ",titanic-top-4-with-ensemble-modeling.ipynb
Fare,"dataset[""Fare""].isnull().sum()",titanic-top-4-with-ensemble-modeling.ipynb
Fill Fare missing values with the median value,"dataset[""Fare""]= dataset[""Fare""]. fillna(dataset[""Fare""]. median ()) ",titanic-top-4-with-ensemble-modeling.ipynb
Explore Fare distribution,"g = sns.distplot(dataset[""Fare""], color = ""m"" , label = ""Skewness : %.2f"" %(dataset[""Fare""]. skew ())) ",titanic-top-4-with-ensemble-modeling.ipynb
Apply log to Fare to reduce skewness distribution,"dataset[""Fare""]= dataset[""Fare""]. map(lambda i : np.log(i)if i > 0 else 0) ",titanic-top-4-with-ensemble-modeling.ipynb
3.2 Categorical values Sex,"g = sns.barplot(x=""Sex"",y=""Survived"",data=train)
g = g.set_ylabel(""Survival Probability"")",titanic-top-4-with-ensemble-modeling.ipynb
Embarked,"dataset[""Embarked""].isnull().sum()",titanic-top-4-with-ensemble-modeling.ipynb
Fill Embarked nan values of dataset set with S most frequent value,"dataset[""Embarked""]= dataset[""Embarked""]. fillna(""S"") ",titanic-top-4-with-ensemble-modeling.ipynb
"Explore Age vs Sex, Parch , Pclass and SibSP","g = sns.factorplot(y = ""Age"" , x = ""Sex"" , data = dataset , kind = ""box"") ",titanic-top-4-with-ensemble-modeling.ipynb
convert Sex into categorical value 0 for male and 1 for female,"dataset[""Sex""]= dataset[""Sex""]. map({ ""male"" : 0 , ""female"" : 1 }) ",titanic-top-4-with-ensemble-modeling.ipynb
Index of NaN age rows,"index_NaN_age = list(dataset[""Age""][ dataset[""Age""]. isnull()]. index) ",titanic-top-4-with-ensemble-modeling.ipynb
 Feature engineering 5.1 Name Title ,"dataset[""Name""].head()",titanic-top-4-with-ensemble-modeling.ipynb
Get Title from Name,"dataset_title =[i.split("","")[ 1]. split(""."")[ 0]. strip ()for i in dataset[""Name""]] ",titanic-top-4-with-ensemble-modeling.ipynb
Convert to categorical values Title,"dataset[""Title""]= dataset[""Title""]. replace (['Lady' , 'the Countess' , 'Countess' , 'Capt' , 'Col' , 'Don' , 'Dr' , 'Major' , 'Rev' , 'Sir' , 'Jonkheer' , 'Dona'], 'Rare') ",titanic-top-4-with-ensemble-modeling.ipynb
Drop Name variable,"dataset.drop(labels =[""Name""], axis = 1 , inplace = True) ",titanic-top-4-with-ensemble-modeling.ipynb
Create a family size descriptor from SibSp and Parch,"dataset[""Fsize""]= dataset[""SibSp""]+ dataset[""Parch""]+ 1 ",titanic-top-4-with-ensemble-modeling.ipynb
Create new feature of family size,dataset['Single']= dataset['Fsize']. map(lambda s : 1 if s == 1 else 0) ,titanic-top-4-with-ensemble-modeling.ipynb
convert to indicator values Title and Embarked,"dataset = pd.get_dummies(dataset , columns =[""Title""]) ",titanic-top-4-with-ensemble-modeling.ipynb
5.3 Cabin,"dataset[""Cabin""].head()",titanic-top-4-with-ensemble-modeling.ipynb
The Cabin feature column contains 292 values and 1007 missing values.I supposed that passengers without a cabin have a missing value displayed instead of the cabin number.,"dataset[""Cabin""][dataset[""Cabin""].notnull()].head()",titanic-top-4-with-ensemble-modeling.ipynb
Replace the Cabin number by the type of cabin X if not,"dataset[""Cabin""]= pd.Series ([i[0]if not pd.isnull(i)else 'X' for i in dataset['Cabin']]) ",titanic-top-4-with-ensemble-modeling.ipynb
"The first letter of the cabin indicates the Desk, i choosed to keep this information only, since it indicates the probable location of the passenger in the Titanic.","g = sns.countplot(dataset[""Cabin""],order=['A','B','C','D','E','F','G','T','X'])",titanic-top-4-with-ensemble-modeling.ipynb
"Because of the low number of passenger that have a cabin, survival probabilities have an important standard deviation and we can t distinguish between survival probability of passengers in the different desks. But we can see that passengers with a cabin have generally more chance to survive than passengers without X .It is particularly true for cabin B, C, D, E and F.","dataset = pd.get_dummies(dataset, columns = [""Cabin""],prefix=""Cabin"")",titanic-top-4-with-ensemble-modeling.ipynb
5.4 Ticket,"dataset[""Ticket""].head()",titanic-top-4-with-ensemble-modeling.ipynb
Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X.,Ticket = [] ,titanic-top-4-with-ensemble-modeling.ipynb
Take prefix," Ticket.append(i.replace(""."" , """"). replace(""/"" , """"). strip (). split(' ')[ 0]) ",titanic-top-4-with-ensemble-modeling.ipynb
Create categorical values for Pclass,"dataset[""Pclass""]= dataset[""Pclass""]. astype(""category"") ",titanic-top-4-with-ensemble-modeling.ipynb
Drop useless variables,"dataset.drop(labels =[""PassengerId""], axis = 1 , inplace = True) ",titanic-top-4-with-ensemble-modeling.ipynb
Separate train dataset and test dataset,train = dataset[: train_len] ,titanic-top-4-with-ensemble-modeling.ipynb
Separate train features and label,"train[""Survived""]= train[""Survived""]. astype(int) ",titanic-top-4-with-ensemble-modeling.ipynb
Cross validate model with Kfold stratified cross val,kfold = StratifiedKFold(n_splits = 10) ,titanic-top-4-with-ensemble-modeling.ipynb
Modeling step Test differents algorithms,random_state = 2 ,titanic-top-4-with-ensemble-modeling.ipynb
Adaboost,DTC = DecisionTreeClassifier () ,titanic-top-4-with-ensemble-modeling.ipynb
ExtraTrees,ExtC = ExtraTreesClassifier () ,titanic-top-4-with-ensemble-modeling.ipynb
RFC Parameters tunning,RFC = RandomForestClassifier () ,titanic-top-4-with-ensemble-modeling.ipynb
Gradient boosting tunning,GBC = GradientBoostingClassifier () ,titanic-top-4-with-ensemble-modeling.ipynb
SVC classifier,SVMC = SVC(probability = True) ,titanic-top-4-with-ensemble-modeling.ipynb
6.1.3 Plot learning curvesLearning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy.,"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
 n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):
 """"""Generate a simple plot of the test and training learning curve""""""
 plt.figure()
 plt.title(title)
 if ylim is not None:
 plt.ylim(*ylim)
 plt.xlabel(""Training examples"")
 plt.ylabel(""Score"")
 train_sizes, train_scores, test_scores = learning_curve(
 estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
 train_scores_mean = np.mean(train_scores, axis=1)
 train_scores_std = np.std(train_scores, axis=1)
 test_scores_mean = np.mean(test_scores, axis=1)
 test_scores_std = np.std(test_scores, axis=1)
 plt.grid()

 plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
 train_scores_mean + train_scores_std, alpha=0.1,
 color=""r"")
 plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
 test_scores_mean + test_scores_std, alpha=0.1, color=""g"")
 plt.plot(train_sizes, train_scores_mean, 'o-', color=""r"",
 label=""Training score"")
 plt.plot(train_sizes, test_scores_mean, 'o-', color=""g"",
 label=""Cross-validation score"")

 plt.legend(loc=""best"")
 return plt

g = plot_learning_curve(gsRFC.best_estimator_,""RF mearning curves"",X_train,Y_train,cv=kfold)
g = plot_learning_curve(gsExtC.best_estimator_,""ExtraTrees learning curves"",X_train,Y_train,cv=kfold)
g = plot_learning_curve(gsSVMC.best_estimator_,""SVC learning curves"",X_train,Y_train,cv=kfold)
g = plot_learning_curve(gsadaDTC.best_estimator_,""AdaBoost learning curves"",X_train,Y_train,cv=kfold)
g = plot_learning_curve(gsGBC.best_estimator_,""GradientBoosting learning curves"",X_train,Y_train,cv=kfold)

",titanic-top-4-with-ensemble-modeling.ipynb
"6.1.4 Feature importance of tree based classifiersIn order to see the most informative features for the prediction of passengers survival, i displayed the feature importance for the 4 tree based classifiers.","nrows = ncols = 2
fig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=""all"", figsize=(15,15))

names_classifiers = [(""AdaBoosting"", ada_best),(""ExtraTrees"",ExtC_best),(""RandomForest"",RFC_best),(""GradientBoosting"",GBC_best)]

nclassifier = 0
for row in range(nrows):
 for col in range(ncols):
 name = names_classifiers[nclassifier][0]
 classifier = names_classifiers[nclassifier][1]
 indices = np.argsort(classifier.feature_importances_)[::-1][:40]
 g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])
 g.set_xlabel(""Relative importance"",fontsize=12)
 g.set_ylabel(""Features"",fontsize=12)
 g.tick_params(labelsize=9)
 g.set_title(name + "" feature importance"")
 nclassifier += 1",titanic-top-4-with-ensemble-modeling.ipynb
"I plot the feature importance for the 4 tree based classifiers Adaboost, ExtraTrees, RandomForest and GradientBoosting .We note that the four classifiers have different top features according to the relative importance. It means that their predictions are not based on the same features. Nevertheless, they share some common important features for the classification , for example Fare , Title 2 , Age and Sex .Title 2 which indicates the Mrs Mlle Mme Miss Ms category is highly correlated with Sex.We can say that: Pc 1, Pc 2, Pc 3 and Fare refer to the general social standing of passengers. Sex and Title 2 Mrs Mlle Mme Miss Ms and Title 3 Mr refer to the gender. Age and Title 1 Master refer to the age of passengers. Fsize, LargeF, MedF, Single refer to the size of the passenger family. According to the feature importance of this 4 classifiers, the prediction of the survival seems to be more associated with the Age, the Sex, the family size and the social standing of the passengers more than the location in the boat.","test_Survived_RFC = pd.Series(RFC_best.predict(test), name = ""RFC"") ",titanic-top-4-with-ensemble-modeling.ipynb
Concatenate all classifier results,"ensemble_results = pd.concat ([test_Survived_RFC , test_Survived_ExtC , test_Survived_AdaC , test_Survived_GBC , test_Survived_SVMC], axis = 1) ",titanic-top-4-with-ensemble-modeling.ipynb
6.2 Ensemble modeling 6.2.1 Combining modelsI choosed a voting classifier to combine the predictions coming from the 5 classifiers.I preferred to pass the argument soft to the voting parameter to take into account the probability of each vote.,"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),
('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)

votingC = votingC.fit(X_train, Y_train)",titanic-top-4-with-ensemble-modeling.ipynb
6.3 Prediction 6.3.1 Predict and Submit results,"test_Survived = pd.Series(votingC.predict(test), name=""Survived"")

results = pd.concat([IDtest,test_Survived],axis=1)

results.to_csv(""ensemble_python_voting.csv"",index=False)",titanic-top-4-with-ensemble-modeling.ipynb
linear algebra,import numpy as np ,titanic-tutorial.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,titanic-tutorial.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,titanic-tutorial.ipynb
"This shows us where the competition data is stored, so that we can load the files into the notebook. We ll do that next.Load the dataThe second code cell in your notebook now appears below the three lines of output with the file locations.Type the two lines of code below into your second code cell. Then, once you re done, either click on the blue play button, or hit Shift Enter . ","train_data = pd.read_csv(""/kaggle/input/titanic/train.csv"")
train_data.head()",titanic-tutorial.ipynb
"Your code should return the output above, which corresponds to the first five rows of the table in train.csv. It s very important that you see this output in your notebook before proceeding with the tutorial! If your code does not produce this output, double check that your code is identical to the two lines above. And, make sure your cursor is in the code cell before hitting Shift Enter . The code that you ve just written is in the Python programming language. It uses a Python module called pandas abbreviated as pd to load the table from the train.csv file into the notebook. To do this, we needed to plug in the location of the file which we saw was kaggle input titanic train.csv . If you re not already familiar with Python and pandas , the code shouldn t make sense to you but don t worry! The point of this tutorial is to quickly! make your first submission to the competition. At the end of the tutorial, we suggest resources to continue your learning. At this point, you should have at least three code cells in your notebook. Copy the code below into the third code cell of your notebook to load the contents of the test.csv file. Don t forget to click on the play button or hit Shift Enter !","test_data = pd.read_csv(""/kaggle/input/titanic/test.csv"")
test_data.head()",titanic-tutorial.ipynb
"As before, make sure that you see the output above in your notebook before continuing. Once all of the code runs successfully, all of the data in train.csv and test.csv is loaded in the notebook. The code above shows only the first 5 rows of each table, but all of the data is there all 891 rows of train.csv and all 418 rows of test.csv! Part 3: Your first submissionRemember our goal: we want to find patterns in train.csv that help us predict whether the passengers in test.csv survived.It might initially feel overwhelming to look for patterns, when there s so much data to sort through. So, we ll start simple.Explore a patternRemember that the sample submission file in gender submission.csv assumes that all female passengers survived and all male passengers died . Is this a reasonable first guess? We ll check if this pattern holds true in the data in train.csv .Copy the code below into a new code cell. Then, run the cell.","women = train_data.loc[train_data.Sex == 'female'][""Survived""]
rate_women = sum(women)/len(women)

print(""% of women who survived:"", rate_women)",titanic-tutorial.ipynb
"Before moving on, make sure that your code returns the output above. The code above calculates the percentage of female passengers in train.csv who survived.Then, run the code below in another code cell:","men = train_data.loc[train_data.Sex == 'male'][""Survived""]
rate_men = sum(men)/len(men)

print(""% of men who survived:"", rate_men)",titanic-tutorial.ipynb
"The code above calculates the percentage of male passengers in train.csv who survived.From this you can see that almost 75 of the women on board survived, whereas only 19 of the men lived to tell about it. Since gender seems to be such a strong indicator of survival, the submission file in gender submission.csv is not a bad first guess!But at the end of the day, this gender based submission bases its predictions on only a single column. As you can imagine, by considering multiple columns, we can discover more complex patterns that can potentially yield better informed predictions. Since it is quite difficult to consider several columns at once or, it would take a long time to consider all possible patterns in many different columns simultaneously , we ll use machine learning to automate this for us.Your first machine learning modelWe ll build what s known as a random forest model. This model is constructed of several trees there are three trees in the picture below, but we ll construct 100! that will individually consider each passenger s data and vote on whether the individual survived. Then, the random forest model makes a democratic decision: the outcome with the most votes wins!The code cell below looks for patterns in four different columns Pclass , Sex , SibSp , and Parch of the data. It constructs the trees in the random forest model based on patterns in the train.csv file, before generating predictions for the passengers in test.csv. The code also saves these new predictions in a CSV file submission.csv.Copy this code into your notebook, and run it in a new code cell.","from sklearn.ensemble import RandomForestClassifier

y = train_data[""Survived""]

features = [""Pclass"", ""Sex"", ""SibSp"", ""Parch""]
X = pd.get_dummies(train_data[features])
X_test = pd.get_dummies(test_data[features])

model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)
model.fit(X, y)
predictions = model.predict(X_test)

output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})
output.to_csv('submission.csv', index=False)
print(""Your submission was successfully saved!"")",titanic-tutorial.ipynb
Scale learning rate to 8 TPU s,LR = 2e-5 * xm.xrt_world_size () ,tpu-inference-pytorch-nlp-xlmroberta.ipynb
encoding,def convert_to_features(batch): ,tpu-inference-pytorch-nlp-xlmroberta.ipynb
upgrading pip,! pip install - q - - upgrade pip ,tpu-sherlocked-one-stop-for-with-tf.ipynb
installing the latest transformers version from pip,! pip install - - use - feature = 2020 - resolver - q transformers == 3.0 .2 ,tpu-sherlocked-one-stop-for-with-tf.ipynb
installing Google Translator package,! pip install - q googletrans ,tpu-sherlocked-one-stop-for-with-tf.ipynb
importing packages,import gc ,tpu-sherlocked-one-stop-for-with-tf.ipynb
defining configuration,class Configuration (): ,tpu-sherlocked-one-stop-for-with-tf.ipynb
data preparation functions,def translate_text_to_english(text): ,tpu-sherlocked-one-stop-for-with-tf.ipynb
building model,"def build_model(model_name , max_len , learning_rate , metrics): ",tpu-sherlocked-one-stop-for-with-tf.ipynb
stratified k fold over language and label,def run_model(config): ,tpu-sherlocked-one-stop-for-with-tf.ipynb
Final Model: XLM Roberta Large,"config_1 = Configuration(""jplu/tf-xlm-roberta-large"" , translation = False , max_length = 84 , batch_size = 64 , epochs = 16 , train_splits = 4) ",tpu-sherlocked-one-stop-for-with-tf.ipynb
Submission Creating the submission file by predicting the label with highest probability.,"df_test = pd.read_csv(config_1.PATH_TEST)

df_submission = pd.DataFrame({""id"": df_test.id.values, ""prediction"": np.argmax(preds_test_1, axis = 1)})
df_submission.to_csv(""submission.csv"", index = False)

df_submission.prediction.value_counts()
",tpu-sherlocked-one-stop-for-with-tf.ipynb
Check TPU is available,"import tensorflow as tf
try:
 tpu = tf.distribute.cluster_resolver.TPUClusterResolver() 
 print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
except ValueError:
 tpu = None
if tpu:
 tf.config.experimental_connect_to_cluster(tpu)
 tf.tpu.experimental.initialize_tpu_system(tpu)
 strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
 strategy = tf.distribute.get_strategy()",tpu-training-pytorch-nlp-xlmroberta.ipynb
Setup Dependencies,"!pip install nlp
!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev",tpu-training-pytorch-nlp-xlmroberta.ipynb
Data Files,"train = pd.read_csv('../input/contradictory-my-dear-watson/train.csv')
test = pd.read_csv('../input/contradictory-my-dear-watson/test.csv')
sample_submission = pd.read_csv('../input/contradictory-my-dear-watson/sample_submission.csv')",tpu-training-pytorch-nlp-xlmroberta.ipynb
CONFIG,TRAIN_BATCH_SIZE = 16 ,tpu-training-pytorch-nlp-xlmroberta.ipynb
Scale learning rate to 8 TPU s,LR = 2e-5 * xm.xrt_world_size () ,tpu-training-pytorch-nlp-xlmroberta.ipynb
mnli data,"mnli = nlp.load_dataset(path = 'glue' , name = 'mnli' , split = 'train[:5%]') ",tpu-training-pytorch-nlp-xlmroberta.ipynb
xnli data,xnli = nlp.load_dataset(path = 'xnli') ,tpu-training-pytorch-nlp-xlmroberta.ipynb
snli data,"snli = nlp.load_dataset(path = 'snli' , split = 'train[:5%]') ",tpu-training-pytorch-nlp-xlmroberta.ipynb
External Data Statistics,"print(""#""*25)
print("" MNLI""); print(""#""*25)
print(""Shape: "", mnli.shape)
print(""Num of Samples: "", mnli.num_rows)
print(""Num of Columns: "", mnli.num_columns)
print(""Column Names: "", mnli.column_names)
print(""Features: "", mnli.features)
print(""Num of Classes: "", mnli.features['label'].num_classes)
print(""Split: "", mnli.split)
print(""Description: "", mnli.description)
print(f""Labels Count - 0's:{len(mnli.filter(lambda x: x['label']==0))}, 1's:{len(mnli.filter(lambda x: x['label']==1))}, 2's: 0's:{len(mnli.filter(lambda x: x['label']==2))}"")
print()
print(""#""*25)
print("" XNLI""); print(""#""*25)
print(""Shape: "", xnli.shape)
print(""Num of Samples: "", xnli.num_rows)
print(""Num of Columns: "", xnli.num_columns)
print(""Column Names: "", xnli.column_names)
print(""Features: "", xnli.features)
print(""Split: "", xnli.split)
print(""Description: "", xnli.description)
print(f""Labels Count - 0's:{len(xnli.filter(lambda x: x['label']==0))}, 1's:{len(xnli.filter(lambda x: x['label']==1))}, 2's: 0's:{len(xnli.filter(lambda x: x['label']==2))}"")
print()
print(""#""*25)
print("" SNLI""); print(""#""*25)
print(""Shape: "", snli.shape)
print(""Num of Samples: "", snli.num_rows)
print(""Num of Columns: "", snli.num_columns)
print(""Column Names: "", snli.column_names)
print(""Features: "", snli.features)
print(""Num of Classes: "", snli.features['label'].num_classes)
print(""Split: "", snli.split)
print(""Description: "", snli.description)
print(f""Labels Count - 0's:{len(snli.filter(lambda x: x['label']==0))}, 1's:{len(snli.filter(lambda x: x['label']==1))}, 2's: 0's:{len(snli.filter(lambda x: x['label']==2))}"")",tpu-training-pytorch-nlp-xlmroberta.ipynb
encoding,def convert_to_features(batch): ,tpu-training-pytorch-nlp-xlmroberta.ipynb
function to preprocess special structure of xnli,def preprocess_xnli(example): ,tpu-training-pytorch-nlp-xlmroberta.ipynb
encode mnli and convert to torch tensor,"mnli_encoded = mnli.map(convert_to_features , batched = True , remove_columns =['idx' , 'premise' , 'hypothesis']) ",tpu-training-pytorch-nlp-xlmroberta.ipynb
"preprocess xnli, encode and convert to torch tensor","xnli_processed = xnli.map(preprocess_xnli , batched = True) ",tpu-training-pytorch-nlp-xlmroberta.ipynb
encode snli and convert to torch tensor,"snli_encoded = snli.map(convert_to_features , batched = True , remove_columns =['premise' , 'hypothesis']) ",tpu-training-pytorch-nlp-xlmroberta.ipynb
Encoded Data Statistics,"print(mnli_encoded.column_names)
print(snli_encoded.column_names)
print(xnli_encoded.column_names)

print(mnli_encoded.num_rows)
print(snli_encoded.num_rows)
print(xnli_encoded.num_rows)",tpu-training-pytorch-nlp-xlmroberta.ipynb
Competitions Data Convert Encode,"train_dataset = nlp.load_dataset('csv', data_files=['../input/contradictory-my-dear-watson/train.csv'])['train']

print(train_dataset.num_rows)
print(train_dataset.column_names)
drop_columns = train_dataset.column_names[:-1]

encoded_train_dataset = train_dataset.map(convert_to_features, batched=True, remove_columns=drop_columns)
encoded_train_dataset.set_format(""torch"", columns=['attention_mask', 'input_ids', 'token_type_ids', 'label']) 
print(encoded_train_dataset.num_rows)
print(encoded_train_dataset.column_names)",tpu-training-pytorch-nlp-xlmroberta.ipynb
Cocatenate and shuffle all datasets,"train_dataset = nlp.concatenate_datasets([mnli_encoded, 
 xnli_encoded, 
 snli_encoded,
 encoded_train_dataset
 ])

print(train_dataset.num_rows)
print(train_dataset.column_names)",tpu-training-pytorch-nlp-xlmroberta.ipynb
Dataset Factory,"class DatasetRetriever(Dataset):
 def __init__(self, dataset:nlp.arrow_dataset.Dataset):
 self.dataset = dataset
 self.ids = self.dataset['input_ids']
 self.mask = self.dataset['attention_mask']
 self.type_ids = self.dataset['token_type_ids']
 self.targets = self.dataset[""label""]
 
 def __len__(self):
 return self.dataset.num_rows
 
 def __getitem__(self, index): 
 ids = self.ids[index]
 mask = self.mask[index]
 type_ids = self.type_ids[index]
 targets = self.targets[index]
 return {
 'ids':torch.tensor(ids),
 'mask':torch.tensor(mask),
 'type_ids':torch.tensor(type_ids),
 'targets':targets
 }",tpu-training-pytorch-nlp-xlmroberta.ipynb
Model Factory,class XLMRoberta(nn.Module): ,tpu-training-pytorch-nlp-xlmroberta.ipynb
Metrics Factory,"class AverageMeter(object):
 def __init__(self, name, fmt=':f'):
 self.name = name
 self.fmt = fmt
 self.reset()

 def reset(self):
 self.val = 0
 self.avg = 0
 self.sum = 0
 self.count = 0

 def update(self, val, n=1):
 self.val = val
 self.sum += val * n
 self.count += n
 self.avg = self.sum / self.count

 def __str__(self):
 fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'
 return fmtstr.format(**self.__dict__)

class ProgressMeter(object):
 def __init__(self, num_batches, meters, prefix=""""):
 self.batch_fmtstr = self._get_batch_fmtstr(num_batches)
 self.meters = meters
 self.prefix = prefix

 def display(self, batch):
 entries = [self.prefix + self.batch_fmtstr.format(batch)]
 entries += [str(meter) for meter in self.meters]
 print('\t'.join(entries))

 def _get_batch_fmtstr(self, num_batches):
 num_digits = len(str(num_batches // 1))
 fmt = '{:' + str(num_digits) + 'd}'
 return '[' + fmt + '/' + fmt.format(num_batches) + ']'

def accuracy(output, target, topk=(1,)):
 with torch.no_grad():
 maxk = max(topk)
 batch_size = target.size(0)

 _, pred = output.topk(maxk, 1, True, True)
 pred = pred.t()
 correct = pred.eq(target.view(1, -1).expand_as(pred))

 res = []
 for k in topk:
 correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)
 res.append(correct_k.mul_(100.0 / batch_size))
 return res",tpu-training-pytorch-nlp-xlmroberta.ipynb
Optimizer Factory,def get_model_optimizer(model): ,tpu-training-pytorch-nlp-xlmroberta.ipynb
Differential Learning Rate, def is_backbone(name): ,tpu-training-pytorch-nlp-xlmroberta.ipynb
Loss Factory,"def loss_fn(outputs, targets):
 return nn.CrossEntropyLoss()(outputs, targets)",tpu-training-pytorch-nlp-xlmroberta.ipynb
Training,"def train_loop_fn(train_loader , model , optimizer , device , scheduler , epoch = None): ",tpu-training-pytorch-nlp-xlmroberta.ipynb
Train," batch_time = AverageMeter('Time' , ':6.3f') ",tpu-training-pytorch-nlp-xlmroberta.ipynb
Evaluation,"def eval_loop_fn(validation_loader , model , device): ",tpu-training-pytorch-nlp-xlmroberta.ipynb
Validation, model.eval () ,tpu-training-pytorch-nlp-xlmroberta.ipynb
Model and Dataset Config,"WRAPPED_MODEL = xmp.MpModelWrapper(XLMRoberta(num_labels=3, multisample=False))

dataset = train_dataset.train_test_split(test_size=0.1)
train_dataset = dataset['train']
valid_dataset = dataset['test']
train_dataset.set_format(""torch"", columns=['attention_mask', 'input_ids', 'token_type_ids', 'label']) 
valid_dataset.set_format(""torch"", columns=['attention_mask', 'input_ids', 'token_type_ids', 'label']) ",tpu-training-pytorch-nlp-xlmroberta.ipynb
Run,def _run (): ,tpu-training-pytorch-nlp-xlmroberta.ipynb
torch.set default tensor type torch.FloatTensor , _run () ,tpu-training-pytorch-nlp-xlmroberta.ipynb
linear algebra,import numpy as np ,transfering-style.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,transfering-style.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,transfering-style.ipynb
import resources,% matplotlib inline ,transfering-style.ipynb
get the features portion of VGG19 we will not need the classifier portion ,vgg = models.vgg19(pretrained = True). features ,transfering-style.ipynb
freeze all VGG parameters since we re only optimizing the target image,for param in vgg.parameters (): ,transfering-style.ipynb
"move the model to GPU, if available","device = torch.device(""cuda"" if torch.cuda.is_available ()else ""cpu"") ",transfering-style.ipynb
 Load in Content and Style ImagesLoad the content image and style image to be used for style transfer. The load image function transforms the image and loads it in the form of normalized Tensors. ,"def load_image(img_path , max_size = 128 , shape = None): ",transfering-style.ipynb
load in content and style image,content = load_image('/kaggle/input/images-for-style-transfer/Data/Artworks/856047.jpg'). to(device) ,transfering-style.ipynb
"Resize style to match content, makes code easier",style = load_image('/kaggle/input/neural-style-transfer/Style Images/starry_night.jpg'). to(device) ,transfering-style.ipynb
and converting it from a Tensor image to a NumPy image for display,def im_convert(tensor): ,transfering-style.ipynb
display the images,"fig ,(ax1 , ax2)= plt.subplots(1 , 2 , figsize =(20 , 10)) ",transfering-style.ipynb
content and style ims side by side,ax1.imshow(im_convert(content)) ,transfering-style.ipynb
" Gram MatrixPicture Credit: The matrix expressing the correlation of this Channel is called Gram Matrix. Loss is minimized by defining the difference between this Gram Matrix and the Gram Matrix of the newly created image as a Loss Function. Next, in order to reflect the content, the loss function is calculated in units of pixels from the feature map spit out from each pre trained CNN. In this way, a new image is created that minimizes the Loss calculated from Style and Loss calculated from Content. ",def gram_matrix(tensor): ,transfering-style.ipynb
get content and style features only once before training,"content_features = get_features(content , vgg) ",transfering-style.ipynb
calculate the gram matrices for each layer of our style representation,style_grams = { layer : gram_matrix(style_features[layer]) for layer in style_features } ,transfering-style.ipynb
then iteratively change its style,target = content.clone (). requires_grad_(True). to(device) ,transfering-style.ipynb
"for displaying the target image, intermittently",show_every = 500 ,transfering-style.ipynb
iteration hyperparameters,"optimizer = optim.Adam ([target], lr = 0.003) ",transfering-style.ipynb
decide how many iterations to update your image 5000 ,steps = 5001 ,transfering-style.ipynb
get the features from your target image," target_features = get_features(target , vgg) ",transfering-style.ipynb
the content loss, content_loss = torch.mean(( target_features['conv4_2']- content_features['conv4_2']) ** 2) ,transfering-style.ipynb
initialize the style loss to 0, style_loss = 0 ,transfering-style.ipynb
then add to it for each layer s gram matrix loss, for layer in style_weights : ,transfering-style.ipynb
get the target style representation for the layer, target_feature = target_features[layer] ,transfering-style.ipynb
get the style style representation, style_gram = style_grams[layer] ,transfering-style.ipynb
"the style loss for one layer, weighted appropriately", layer_style_loss = style_weights[layer]* torch.mean(( target_gram - style_gram)** 2) ,transfering-style.ipynb
add to the style loss, style_loss += layer_style_loss /(d * h * w) ,transfering-style.ipynb
calculate the total loss, total_loss = content_weight * content_loss + style_weight * style_loss ,transfering-style.ipynb
update your target image, optimizer.zero_grad () ,transfering-style.ipynb
display intermediate images and print the loss, if ii % show_every == 0 : ,transfering-style.ipynb
"display content and final, target image","fig ,(ax1 , ax2)= plt.subplots(1 , 2 , figsize =(20 , 10)) ",transfering-style.ipynb
"Engineering TrendOnce we ve identified the shape of the trend, we can attempt to model it using a time step feature. We ve already seen how using the time dummy itself will model a linear trend:target a time bWe can fit many other kinds of trend through transformations of the time dummy. If the trend appears to be quadratic a parabola , we just need to add the square of the time dummy to the feature set, giving us: target a time 2 b time c Linear regression will learn the coefficients a, b, and c.The trend curves in the figure below were both fit using these kinds of features and scikit learn s LinearRegression: Top: Series with a linear trend. Below: Series with a quadratic trend. If you haven t seen the trick before, you may not have realized that linear regression can fit curves other than lines. The idea is that if you can provide curves of the appropriate shape as features, then linear regression can learn how to combine them in the way that best fits the target.Example Tunnel TrafficIn this example we ll create a trend model for the Tunnel Traffic dataset.",from pathlib import Path ,trend.ipynb
ignore warnings to clean up output cells,"simplefilter(""ignore"") ",trend.ipynb
Set Matplotlib defaults,"plt.style.use(""seaborn-whitegrid"") ",trend.ipynb
"In Lesson 1, we engineered our time dummy in Pandas directly. From now on, however, we ll use a function from the statsmodels library called DeterministicProcess. Using this function will help us avoid some tricky failure cases that can arise with time series and linear regression. The order argument refers to polynomial order: 1 for linear, 2 for quadratic, 3 for cubic, and so on.",from statsmodels.tsa.deterministic import DeterministicProcess ,trend.ipynb
"A deterministic process, by the way, is a technical term for a time series that is non random or completely determined, like the const and trend series are. Features derived from the time index will generally be deterministic. We create our trend model basically as before, though note the addition of the fit intercept False argument.",from sklearn.linear_model import LinearRegression ,trend.ipynb
the target,"y = tunnel[""NumVehicles""] ",trend.ipynb
"features, so we need to be sure to exclude it here.",model = LinearRegression(fit_intercept = False) ,trend.ipynb
"The trend discovered by our LinearRegression model is almost identical to the moving average plot, which suggests that a linear trend was the right decision in this case.","
ax = tunnel.plot(style=""."", color=""0.5"", title=""Tunnel Traffic - Linear Trend"")
_ = y_pred.plot(ax=ax, linewidth=3, label=""Trend"")",trend.ipynb
"To make a forecast, we apply our model to out of sample features. Out of sample refers to times outside of the observation period of the training data. Here s how we could make a 30 day forecast:","X = dp.out_of_sample(steps=30)

y_fore = pd.Series(model.predict(X), index=X.index)

y_fore.head()",trend.ipynb
Let s plot a portion of the series to see the trend forecast for the next 30 days:,"
ax = tunnel[""2005-05"":].plot(title=""Tunnel Traffic - Linear Trend Forecast"", **plot_params)
ax = y_pred[""2005-05"":].plot(ax=ax, linewidth=3, label=""Trend"")
ax = y_fore.plot(ax=ax, linewidth=3, label=""Trend Forecast"", color=""C3"")
_ = ax.legend()",trend.ipynb
linear algebra,import numpy as np ,tutorial-notebook.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,tutorial-notebook.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,tutorial-notebook.ipynb
to silence warning,"os.environ[""WANDB_API_KEY""]= ""0"" ",tutorial-notebook.ipynb
Let s set up our TPU.,try : ,tutorial-notebook.ipynb
for CPU and single GPU, strategy = tf.distribute.get_strategy () ,tutorial-notebook.ipynb
"The training set contains a premise, a hypothesis, a label 0 entailment, 1 neutral, 2 contradiction , and the language of the text. For more information about what these mean and how the data is structured, check out the data page: ","train = pd.read_csv(""../input/contradictory-my-dear-watson/train.csv"")",tutorial-notebook.ipynb
We can use the pandas head function to take a quick look at the training set.,train.head(),tutorial-notebook.ipynb
Let s look at one of the pairs of sentences.,train.premise.values[1],tutorial-notebook.ipynb
"These statements are contradictory, and the label shows that.Let s look at the distribution of languages in the training set.","labels, frequencies = np.unique(train.language.values, return_counts = True)

plt.figure(figsize = (10,10))
plt.pie(frequencies,labels = labels, autopct = '%1.1f%%')
plt.show()",tutorial-notebook.ipynb
"To start out, we can use a pretrained model. Here, we ll use a multilingual BERT model from huggingface. For more information about BERT, see: we download the tokenizer.","model_name = 'bert-base-multilingual-cased'
tokenizer = BertTokenizer.from_pretrained(model_name)",tutorial-notebook.ipynb
Tokenizers turn sequences of words into arrays of numbers. Let s look at an example:,"def encode_sentence(s):
 tokens = list(tokenizer.tokenize(s))
 tokens.append('[SEP]')
 return tokenizer.convert_tokens_to_ids(tokens)",tutorial-notebook.ipynb
"BERT uses three kind of input data input word IDs, input masks, and input type IDs.These allow the model to know that the premise and hypothesis are distinct sentences, and also to ignore any padding from the tokenizer.We add a CLS token to denote the beginning of the inputs, and a SEP token to denote the separation between the premise and the hypothesis. We also need to pad all of the inputs to be the same size. For more information about BERT inputs, see: we re going to encode all of our premise hypothesis pairs for input into BERT.","def bert_encode(hypotheses, premises, tokenizer):
 
 num_examples = len(hypotheses)
 
 sentence1 = tf.ragged.constant([
 encode_sentence(s)
 for s in np.array(hypotheses)])
 sentence2 = tf.ragged.constant([
 encode_sentence(s)
 for s in np.array(premises)])

 cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]
 input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)

 input_mask = tf.ones_like(input_word_ids).to_tensor()

 type_cls = tf.zeros_like(cls)
 type_s1 = tf.zeros_like(sentence1)
 type_s2 = tf.ones_like(sentence2)
 input_type_ids = tf.concat(
 [type_cls, type_s1, type_s2], axis=-1).to_tensor()

 inputs = {
 'input_word_ids': input_word_ids.to_tensor(),
 'input_mask': input_mask,
 'input_type_ids': input_type_ids}

 return inputs",tutorial-notebook.ipynb
"Now, we can incorporate the BERT transformer into a Keras Functional Model. For more information about the Keras Functional API, see: model was inspired by the model in this notebook: which is a wonderful introduction to NLP!","max_len = 50

def build_model():
 bert_encoder = TFBertModel.from_pretrained(model_name)
 input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=""input_word_ids"")
 input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=""input_mask"")
 input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=""input_type_ids"")
 
 embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
 output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])
 
 model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)
 model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
 
 return model",tutorial-notebook.ipynb
Generating Submitting Predictions,predictions = [np.argmax(i) for i in model.predict(test_input)],tutorial-notebook.ipynb
"The submission file will consist of the ID column and a prediction column. We can just copy the ID column from the test file, make it a dataframe, and then add our prediction column.","submission = test.id.copy().to_frame()
submission['prediction'] = predictions",tutorial-notebook.ipynb
"Main idea: shared layers are disturbed with two different losses and discriminators cant overfit quicklyRelated works: Nguyen, T. et al. Dual Discriminator Generative Adversarial Nets. NIPS 2017 . but discriminators have no shared layers there.Hoang, Q., Nguyen, T., Le, T., Phung, D.Q. 2017 . Multi Generator Generative Adversarial Nets. ArXiv, abs 1708.02556 discriminator and classificator have shared layers, but they are not both discriminators with different losses","BATCH_SIZE = 32

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa

import matplotlib.pyplot as plt
import numpy as np
import re
try:
 from kaggle_datasets import KaggleDatasets
except:
 pass",two-objective-discriminator.ipynb
photo ds photo ds.shuffle 2048 ," monet_ds = monet_ds.batch(batch_size , drop_remainder = True) ",two-objective-discriminator.ipynb
photo ds photo ds.cache , monet_ds = monet_ds.prefetch(AUTOTUNE) ,two-objective-discriminator.ipynb
return 0," covmean = tf.cast(tf.math.real(covmean), tf.float32) ",two-objective-discriminator.ipynb
Headless discriminator shared part ,def Discriminator (): ,two-objective-discriminator.ipynb
" size, 128, 128, 64 "," down1 = down_sample(64 , 4 , False)( x) ",two-objective-discriminator.ipynb
" size, 64, 64, 128 "," down2 = down_sample(128 , 4)( down1) ",two-objective-discriminator.ipynb
" size, 32, 32, 256 "," down3 = down_sample(256 , 4)( down2) ",two-objective-discriminator.ipynb
" size, 34, 34, 256 ", zero_pad1 = layers.ZeroPadding2D ()( down3) ,two-objective-discriminator.ipynb
" size, 31, 31, 512 "," conv = layers.Conv2D(512 , 4 , strides = 1 , kernel_initializer = initializer , use_bias = False)( zero_pad1) ",two-objective-discriminator.ipynb
" size, 33, 33, 512 ", zero_pad2 = layers.ZeroPadding2D ()( leaky_relu) ,two-objective-discriminator.ipynb
Head for two objective discriminator for Monet,def DHead (): ,two-objective-discriminator.ipynb
" size, 30, 30, 1 "," last = layers.Conv2D(1 , 4 , strides = 1 , kernel_initializer = initializer)( x) ",two-objective-discriminator.ipynb
Discriminator for Photos unchanged ,def DiscriminatorP (): ,two-objective-discriminator.ipynb
" size, 128, 128, 64 "," down1 = down_sample(64 , 4 , False)( x) ",two-objective-discriminator.ipynb
" size, 64, 64, 128 "," down2 = down_sample(128 , 4)( down1) ",two-objective-discriminator.ipynb
" size, 32, 32, 256 "," down3 = down_sample(256 , 4)( down2) ",two-objective-discriminator.ipynb
" size, 34, 34, 256 ", zero_pad1 = layers.ZeroPadding2D ()( down3) ,two-objective-discriminator.ipynb
" size, 31, 31, 512 "," conv = layers.Conv2D(512 , 4 , strides = 1 , kernel_initializer = initializer , use_bias = False)( zero_pad1) ",two-objective-discriminator.ipynb
" size, 33, 33, 512 ", zero_pad2 = layers.ZeroPadding2D ()( leaky_relu) ,two-objective-discriminator.ipynb
" size, 30, 30, 1 "," last = layers.Conv2D(1 , 4 , strides = 1 , kernel_initializer = initializer)( zero_pad2) ",two-objective-discriminator.ipynb
transforms photos to Monet esque paintings, monet_generator = Generator () ,two-objective-discriminator.ipynb
transforms Monet paintings to be more like photos, photo_generator = Generator () ,two-objective-discriminator.ipynb
differentiates real Monet paintings and generated Monet paintings, monet_discriminator = Discriminator () ,two-objective-discriminator.ipynb
differentiates real photos and generated photos, photo_discriminator = DiscriminatorP () ,two-objective-discriminator.ipynb
Head for BCE, dHead1 = DHead () ,two-objective-discriminator.ipynb
Head for hinge loss, dHead2 = DHead () ,two-objective-discriminator.ipynb
CycleGAN with DiffAugment and two objective discriminator,class CycleGan(keras.Model): ,two-objective-discriminator.ipynb
for TPU,with strategy.scope (): ,two-objective-discriminator.ipynb
from ," def DiffAugment(x , policy = '' , channels_first = False): ",two-objective-discriminator.ipynb
30 epochs with lr 2e 4,"with strategy.scope():

 cycle_gan_model.compile(
 m_gen_optimizer = monet_generator_optimizer,
 p_gen_optimizer = photo_generator_optimizer,
 m_disc_optimizer = monet_discriminator_optimizer,
 p_disc_optimizer = photo_discriminator_optimizer,
 gen_loss_fn1 = generator_loss1,
 gen_loss_fn2 = generator_loss2,
 disc_loss_fn1 = discriminator_loss1,
 disc_loss_fn2 = discriminator_loss2,
 cycle_loss_fn = calc_cycle_loss,
 identity_loss_fn = identity_loss,
 aug_fn = aug_fn ,


 )",two-objective-discriminator.ipynb
8 epochs with lr 1e 4,"with strategy.scope():
 monet_generator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)
 photo_generator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)

 monet_discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)
 photo_discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)",two-objective-discriminator.ipynb
8 epochs with lr 1e 5,"with strategy.scope():
 monet_generator_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5)
 photo_generator_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5)

 monet_discriminator_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5)
 photo_discriminator_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5)",two-objective-discriminator.ipynb
"At the end of this step, you will understand the concepts of underfitting and overfitting, and you will be able to apply these ideas to make your models more accurate.Experimenting With Different ModelsNow that you have a reliable way to measure model accuracy, you can experiment with alternative models and see which gives the best predictions. But what alternatives do you have for models?You can see in scikit learn s documentation that the decision tree model has many options more than you ll want or need for a long time . The most important options determine the tree s depth. Recall from the first lesson in this course that a tree s depth is a measure of how many splits it makes before coming to a prediction. This is a relatively shallow treeIn practice, it s not uncommon for a tree to have 10 splits between the top level all houses and a leaf. As the tree gets deeper, the dataset gets sliced up into leaves with fewer houses. If a tree only had 1 split, it divides the data into 2 groups. If each group is split again, we would get 4 groups of houses. Splitting each of those again would create 8 groups. If we keep doubling the number of groups by adding more splits at each level, we ll have 210 groups of houses by the time we get to the 10th level. That s 1024 leaves. When we divide the houses amongst many leaves, we also have fewer houses in each leaf. Leaves with very few houses will make predictions that are quite close to those homes actual values, but they may make very unreliable predictions for new data because each prediction is based on only a few houses .This is a phenomenon called overfitting, where a model matches the training data almost perfectly, but does poorly in validation and other new data. On the flip side, if we make our tree very shallow, it doesn t divide up the houses into very distinct groups. At an extreme, if a tree divides houses into only 2 or 4, each group still has a wide variety of houses. Resulting predictions may be far off for most houses, even in the training data and it will be bad in validation too for the same reason . When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is called underfitting. Since we care about accuracy on new data, which we estimate from our validation data, we want to find the sweet spot between underfitting and overfitting. Visually, we want the low point of the red validation curve in the figure below.Example There are a few alternatives for controlling the tree depth, and many allow for some routes through the tree to have greater depth than other routes. But the max leaf nodes argument provides a very sensible way to control overfitting vs underfitting. The more leaves we allow the model to make, the more we move from the underfitting area in the above graph to the overfitting area.We can use a utility function to help compare MAE scores from different values for max leaf nodes:","from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor

def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
 model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
 model.fit(train_X, train_y)
 preds_val = model.predict(val_X)
 mae = mean_absolute_error(val_y, preds_val)
 return(mae)",underfitting-and-overfitting.ipynb
Data Loading Code Runs At This Point,import pandas as pd ,underfitting-and-overfitting.ipynb
Load data,melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv' ,underfitting-and-overfitting.ipynb
Filter rows with missing values,filtered_melbourne_data = melbourne_data.dropna(axis = 0) ,underfitting-and-overfitting.ipynb
Choose target and features,y = filtered_melbourne_data.Price ,underfitting-and-overfitting.ipynb
compare MAE with differing values of max leaf nodes,"for max_leaf_nodes in[5 , 50 , 500 , 5000]: ",underfitting-and-overfitting.ipynb
"Introduction and SetupFor this tutorial, we will be using the TFRecord dataset. Import the following packages and change the accelerator to TPU. Because TPUs are pretty awesome.","import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa
import tensorflow_datasets as tfds

from kaggle_datasets import KaggleDatasets
import matplotlib.pyplot as plt
import numpy as np

from functools import partial
from albumentations import (
 Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip,
 Rotate
)

try:
 tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
 print('Device:', tpu.master())
 tf.config.experimental_connect_to_cluster(tpu)
 tf.tpu.experimental.initialize_tpu_system(tpu)
 strategy = tf.distribute.experimental.TPUStrategy(tpu)
except:
 strategy = tf.distribute.get_strategy()
print('Number of replicas:', strategy.num_replicas_in_sync)

AUTOTUNE = tf.data.experimental.AUTOTUNE

print(tf.__version__)",understanding-and-improving-cyclegans-tutorial.ipynb
"Load in the dataWe want to keep our photo dataset and our Monet dataset separate. First, load in the filenames of the TFRecords. We ll load both for the CycleGAN. For the first GAN we only need the Monets as training data.All the images for the competition are already sized to 256 x 256. As these images are RGB images, set the channel to 3. Additionally, we need to scale the images to a 1, 1 scale. Because we are building a generative model, we don t need the labels or the image id so we ll only return the image from the TFRecord.","GCS_PATH = KaggleDatasets().get_gcs_path()

MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))
print('Monet TFRecord Files:', len(MONET_FILENAMES))

PHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))
print('Photo TFRecord Files:', len(PHOTO_FILENAMES))",understanding-and-improving-cyclegans-tutorial.ipynb
"You can see I put down a bit of augmentation using random jitter and flip to increase our data set, because we simply don t have enough data for","IMAGE_SIZE =[256 , 256] ",understanding-and-improving-cyclegans-tutorial.ipynb
"image tf.reshape image, 256, 256, 3 "," image = tf.image.decode_jpeg(image , channels = 3) ",understanding-and-improving-cyclegans-tutorial.ipynb
"image tf.cast image, tf.float32 127.5 1"," image = tf.reshape(image ,[* IMAGE_SIZE , 3]) ",understanding-and-improving-cyclegans-tutorial.ipynb
"Then load the data and display the first images to see if it all worked out. Which of course it does, because it s taken directly from the tutorial.","monet_ds = load_dataset(MONET_FILENAMES, labeled=True, repeats=50).batch(100, drop_remainder=True)
photo_ds = load_dataset(PHOTO_FILENAMES, labeled=True, repeats=2 ).batch(100, drop_remainder=True)",understanding-and-improving-cyclegans-tutorial.ipynb
extract 1 batch from the dataset, image = next(iter(ds)) ,understanding-and-improving-cyclegans-tutorial.ipynb
"Build the DCGAN Network Upsample and DownsampleThis one is the same from part1.The downsample, as the name suggests, reduces the 2D dimensions, the width and height, of the image by the stride. The stride is the length of the step the filter takes. Since the stride is 2, the filter is applied to every other pixel, hence reducing the weight and height by 2.We ll be using an instance normalization instead of batch normalization. As the instance normalization is not standard in the TensorFlow API, we ll use the layer from TensorFlow Add ons.","OUTPUT_CHANNELS = 3
LATENT_DIM = 1024

def downsample(filters, size, apply_instancenorm=True):
 initializer = tf.random_normal_initializer(0., 0.02)
 gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)

 result = keras.Sequential()
 result.add(layers.Conv2D(filters, size, padding='same',
 kernel_initializer=initializer, use_bias=False))
 result.add(layers.MaxPool2D())

 if apply_instancenorm:
 result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))

 result.add(layers.LeakyReLU())

 return result",understanding-and-improving-cyclegans-tutorial.ipynb
Upsample does the opposite of downsample and increases the dimensions of the of the image. Conv2DTranspose does basically the opposite of a Conv2D layer.,"def upsample(filters, size, apply_dropout=False):
 initializer = tf.random_normal_initializer(0., 0.02)
 gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)

 result = keras.Sequential()
 result.add(layers.Conv2DTranspose(filters, size, strides=2,
 padding='same',
 kernel_initializer=initializer,
 use_bias=False))

 result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))

 if apply_dropout:
 result.add(layers.Dropout(0.5))

 result.add(layers.LeakyReLU())

 return result",understanding-and-improving-cyclegans-tutorial.ipynb
Build Network The generator first downsamples the input image and then upsample while establishing long skip connections. Skip connections are a way to help bypass the vanishing gradient problem by concatenating the output of a layer to multiple layers instead of only one. Here we concatenate the output of the downsample layer to the upsample layer in a symmetrical fashion. Unets are pretty versatile and help out in our Generator to distill the input image to a lower dimension and then back to the full size at the target. Source,"EPOCHS = 25

LR_G = 2e-4
LR_D = 2e-4
beta_1 = .5

real_label = .9
fake_label = 0",understanding-and-improving-cyclegans-tutorial.ipynb
"The discriminator does not need a Unet, just a nice simple downsample to get a simple fake or real represented in numbers.",def CycleDiscriminator (): ,understanding-and-improving-cyclegans-tutorial.ipynb
" bs, 128, 128, 64 "," down1 = downsample(64 , 4 , False)( x) ",understanding-and-improving-cyclegans-tutorial.ipynb
" bs, 64, 64, 128 "," down2 = downsample(128 , 4)( down1) ",understanding-and-improving-cyclegans-tutorial.ipynb
" bs, 32, 32, 256 "," down3 = downsample(256 , 4)( down2) ",understanding-and-improving-cyclegans-tutorial.ipynb
" bs, 34, 34, 256 ", zero_pad1 = layers.ZeroPadding2D ()( down3) ,understanding-and-improving-cyclegans-tutorial.ipynb
transforms photos to Monet esque paintings, monet_cycle_generator = CycleGenerator () ,understanding-and-improving-cyclegans-tutorial.ipynb
transforms Monet paintings to be more like photos, photo_cycle_generator = CycleGenerator () ,understanding-and-improving-cyclegans-tutorial.ipynb
differentiates real Monet paintings and generated Monet paintings, monet_cycle_discriminator = CycleDiscriminator () ,understanding-and-improving-cyclegans-tutorial.ipynb
differentiates real photos and generated photos, photo_cycle_discriminator = CycleDiscriminator () ,understanding-and-improving-cyclegans-tutorial.ipynb
"Build the CycleGAN modelWe will subclass a tf.keras.Model so that we can run fit later to train our model. During the training step, the model transforms a photo to a Monet painting and then back to a photo. The difference between the original photo and the twice transformed photo is the cycle consistency loss. We want the original photo and the twice transformed photo to be similar to one another.The way this works is by having one GAN for the forwards transformation and one GAN for the backwards transformation. So from image domain and backwards . The resulting images are each evaluated by the standard discriminators of the GANs. The losses are defined in the next section.",class CycleGan(keras.Model): ,understanding-and-improving-cyclegans-tutorial.ipynb
"Define loss functionsThe discriminator loss function below compares real images to a matrix of 1s and fake images to a matrix of 0s. The perfect discriminator will output all 1s for real images and all 0s for fake images. The discriminator loss outputs the average of the real and generated loss.The generator wants to fool the discriminator into thinking the generated image is real. The perfect generator will have the discriminator output only 1s. Thus, it compares the generated image to a matrix of 1s to find the loss.We could probably check if the Least Squares would also perform better for the Cycle GAN, so instead of just stupidly copying the starter, let s see if we can get this to improve the solution!","with strategy.scope():
 def discriminator_loss(predictions_real, predictions_gen, labels_real):
 return (tf.reduce_mean((predictions_gen - tf.reduce_mean(predictions_real) + labels_real) ** 2) +
 tf.reduce_mean((predictions_real - tf.reduce_mean(predictions_gen) - labels_real) ** 2))/2
 
 def generator_loss(predictions_real, predictions_gen, labels_real):
 return (tf.reduce_mean((predictions_real - tf.reduce_mean(predictions_gen) + labels_real) ** 2) +
 tf.reduce_mean((predictions_gen - tf.reduce_mean(predictions_real) - labels_real) ** 2)) / 2",understanding-and-improving-cyclegans-tutorial.ipynb
"More Loss Functions We want our original photo and the twice transformed photo to be similar to one another. Thus, we can calculate the cycle consistency loss be finding the average of their difference.","with strategy.scope():
 def calc_cycle_loss(real_image, cycled_image, LAMBDA):
 loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))

 return LAMBDA * loss1",understanding-and-improving-cyclegans-tutorial.ipynb
"The identity loss compares the image with its generator i.e. photo with photo generator . If given a photo as input, we want it to generate the same image as the image was originally a photo. The identity loss compares the input with the output of the generator.","with strategy.scope():
 def identity_loss(real_image, same_image, LAMBDA):
 loss = tf.reduce_mean(tf.abs(real_image - same_image))
 return LAMBDA * 0.5 * loss",understanding-and-improving-cyclegans-tutorial.ipynb
"Train the CycleGANLet s compile our model. Since we used tf.keras.Model to build our CycleGAN, we can just ude the fit function to train our model. You know the drill already.","with strategy.scope():
 monet_generator_optimizer = tf.keras.optimizers.Adam(LR_G, beta_1=0.5)
 photo_generator_optimizer = tf.keras.optimizers.Adam(LR_G, beta_1=0.5)

 monet_discriminator_optimizer = tf.keras.optimizers.Adam(LR_D, beta_1=0.5)
 photo_discriminator_optimizer = tf.keras.optimizers.Adam(LR_D, beta_1=0.5)",understanding-and-improving-cyclegans-tutorial.ipynb
Double the optimizers double the fun!,"with strategy.scope():
 cycle_gan_model = CycleGan(
 monet_cycle_generator, photo_cycle_generator, monet_cycle_discriminator, photo_cycle_discriminator, real_label=0.66
 )

 cycle_gan_model.compile(
 m_gen_optimizer = monet_generator_optimizer,
 p_gen_optimizer = photo_generator_optimizer,
 m_disc_optimizer = monet_discriminator_optimizer,
 p_disc_optimizer = photo_discriminator_optimizer,
 gen_loss_fn = generator_loss,
 disc_loss_fn = discriminator_loss,
 cycle_loss_fn = calc_cycle_loss,
 identity_loss_fn = identity_loss
 )",understanding-and-improving-cyclegans-tutorial.ipynb
And finally we get to train!,"cycle_gan_model.fit(
 tf.data.Dataset.zip((monet_ds, photo_ds)),
 epochs=EPOCHS
)",understanding-and-improving-cyclegans-tutorial.ipynb
"Visualize our Monet esque photos And now the CycleGAN with the augmented training data and LS Gan. Probably not that much different from before, but it s worth a try, right?","_, ax = plt.subplots(2, 5, figsize=(25, 5))
for i, img in enumerate(photo_ds.take(5)):
 prediction = monet_cycle_generator(img, training=False)[0].numpy()
 prediction = (prediction * 127.5 + 127.5).astype(np.uint8)
 img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)

 ax[0, i].imshow(img)
 ax[1, i].imshow(prediction)
 ax[0, i].set_title(""Input Photo"")
 ax[1, i].set_title(""Monet-esque"")
 ax[0, i].axis(""off"")
 ax[1, i].axis(""off"")
plt.show()",understanding-and-improving-cyclegans-tutorial.ipynb
"Create submission files We ll create the second submission file, as this is more of a tutorial so people can have a look at the outputs. Definitely make sure to play around with it!","import PIL
! mkdir ../images",understanding-and-improving-cyclegans-tutorial.ipynb
Install UPITYou can install UPIT with pip as follows:,"!pip install --upgrade fastai
!pip install --upgrade wandb
!pip install --upgrade torch
!pip install git+https://github.com/tmabraham/UPIT.git",upit-a-package-for-unpaired-img2img-translation.ipynb
"UPIT has various modules: Data: Dataloading functionality. We support standard unpaired image to image translation datasets. Models: The different models are available under this module. Currently the CycleGAN, DualGAN, and GANILLA models are available. Train: The training loop functionality is available here. Most models will be fine with the CycleGAN training loop, but models that do special stuff will get a separate training loop Metrics: An API for common metrics used to evaluate unpaired image to image translation models. Currently only has support for Frechet Inception Distance, but more are coming soon. Inference: Inference interface for the models trained with UPIT. Tracking: Experiment tracking functionality goes here. Currently only Weights and Biases is supported.Let s import them now:","from upit.data.unpaired import *
from upit.models.cyclegan import *
from upit.train.cyclegan import *
from upit.inference.cyclegan import *
from upit.metrics import *
from upit.tracking.wandb import *
from fastai.vision.all import *
import wandb",upit-a-package-for-unpaired-img2img-translation.ipynb
Load dataLet s define the paths for our dataset:,"photo2monet = Path('../input/gan-getting-started')
trainA_path = photo2monet/'photo_jpg'
trainB_path = photo2monet/'monet_jpg'
print(f""There are {len(trainA_path.ls())} photos"")
print(f""There are {len(trainB_path.ls())} Monet paintings"")",upit-a-package-for-unpaired-img2img-translation.ipynb
"In the package, I provide a get dls function that loads the dataset in the fastai specific DataLoaders format given just the domain A and B folder paths. There is also a subset functionality just the number of desired files are provided . See documentation for more information.","dls = get_dls(trainA_path, trainB_path,load_size=256,crop_size=256)",upit-a-package-for-unpaired-img2img-translation.ipynb
"Model definitionI provide the CycleGAN class, which is just an nn.Module for generating converted images. See documentation for more information. Note that the original paper uses gen blocks 9 but let s try a smaller model of gen blocks 3.","cycle_gan = CycleGAN(3,3,64,gen_blocks=3)",upit-a-package-for-unpaired-img2img-translation.ipynb
"We need to instantiate a fastai Learner object used to train a model bundles the data, model, and optimizer with callbacks, allowing for training and inference . For standard CycleGAN training, I provide the cycle learner function to do this. It will add the necessary CycleGANTrainer callback defined in my library, allowing for the generator discriminator training. See the documentation for further details.Note that I added the Frechet Inception Distance, a standard GAN metric and currently the only one implemented in UPIT, but more are coming soon , to allow us to better evaluate our model. It is also the metric for this competition!I also add the Weights and Biases callback that is included in the package which allows us to better track experiments and compare results. There is a cell to load the API key which I have stored as a secret to W B.Also note the show img interval parameter. This allows us to display images from training every show img interval iterations. It does this through another special callback I have written.",from kaggle_secrets import UserSecretsClient ,upit-a-package-for-unpaired-img2img-translation.ipynb
If you use some other Label make sure to change the same below.,"wandb_api = user_secrets.get_secret(""wandb_api"") ",upit-a-package-for-unpaired-img2img-translation.ipynb
"If we want, we can use the learn.lr find command to find the best learning rate. We will find it is often around 1 2e 4, which was also the recommended learning rate in the original CycleGAN paper.",learn.lr_find(),upit-a-package-for-unpaired-img2img-translation.ipynb
"The original CycleGAN paper uses a constant learning rate for half of training, and then linearly decaying to zero for the remainder of training. I added the fit flat lin method to implement this schedule. Note that the original paper runs for 200 epochs in total, but for now we run for 14 epochs.","learn.fit_flat_lin(7,7,2e-4)",upit-a-package-for-unpaired-img2img-translation.ipynb
cuda,b = dls.one_batch () ,upit-a-package-for-unpaired-img2img-translation.ipynb
"Finally, I provide get preds cyclegan to generate predictions from images in testA path and save to pred path. See documentation.","testA_path = '../input/gan-getting-started/photo_jpg/'
pred_path = '../images/'",upit-a-package-for-unpaired-img2img-translation.ipynb
"Thankfully, all that s needed for submission is a zipped folder of those predicted images:","import shutil
shutil.make_archive(""/kaggle/working/images"", 'zip', ""/kaggle/images"")",upit-a-package-for-unpaired-img2img-translation.ipynb
Import basic libraries,import numpy as np ,useful-python-libraries-for-data-science.ipynb
List files available,"print(os.listdir(""../input/"")) ",useful-python-libraries-for-data-science.ipynb
Installing and loading the library,! pip install dabl ,useful-python-libraries-for-data-science.ipynb
"1.1 Automated Preprocessing with dablAs part of the preprocessing, dabl will attempt to identify missing values, feature types and erroneous data. if the detection of semantic types continuous, categorical, ordinal, text, etc fails, the user can provide type hints. Let s demo the library with the help of the titanic dataset.",titanic_df = pd.read_csv('../input/titanic/train.csv') ,useful-python-libraries-for-data-science.ipynb
A first look at data,titanic_df.shape ,useful-python-libraries-for-data-science.ipynb
"1.2 Exploratory Data analysis with dabldabl provides a high level interface that summarizes several common high level plots. For low dimensional datasets, all features are shown for high dimensional datasets, only the most informative features for the given task are shown","dabl.plot(titanic_df, target_col=""Survived"")",useful-python-libraries-for-data-science.ipynb
Initial Model Building with dabl We can find an initial model for our data. The SimpleClassifier implements the familiar scikit learn API of fit and predict. ,"ec = dabl.SimpleClassifier(random_state=0).fit(titanic_df, target_col=""Survived"") ",useful-python-libraries-for-data-science.ipynb
installation and importing the library,! pip install missingno ,useful-python-libraries-for-data-science.ipynb
Let s check out the missing values first with the train.info method,titanic_df.info () ,useful-python-libraries-for-data-science.ipynb
2.1 MatrixThe matrix is a data dense display which lets you quickly visually pick out patterns in data completion.,msno.matrix(titanic_df),useful-python-libraries-for-data-science.ipynb
We can clearly see that Cabin indeed has a lot of missing values. There is also a bar chart on the right.It summarizes the general shape of the data completeness and points out the rows with the maximum and minimum nullity in the dataset.We can also sample the data to only show few data points.,msno.matrix(titanic_df.sample(50)),useful-python-libraries-for-data-science.ipynb
"2.2 Bar ChartAlternatively, you can also plot a barchart to show the missing values",msno.bar(titanic_df),useful-python-libraries-for-data-science.ipynb
2.3 Heatmapmissingno.heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:,msno.heatmap(titanic_df),useful-python-libraries-for-data-science.ipynb
installation and importing the library,! pip install emot ,useful-python-libraries-for-data-science.ipynb
installation and importing the library,! pip install flashtext ,useful-python-libraries-for-data-science.ipynb
"The dataset is from the competition : Real or Not? NLP with Disaster Tweets where our job is to create a ML model to predict whether the test set tweets belong to a disaster or not, in the form of 1 or 0.This is a classic case of a Binary Classification problem.","twitter_df = pd.read_csv('../input/nlp-getting-started/train.csv')
twitter_df.head()",useful-python-libraries-for-data-science.ipynb
Lets create a corpus of all the tweets in the training set,"corpus = ', '.join(twitter_df.text)
corpus[:1000]",useful-python-libraries-for-data-science.ipynb
How many times does the word flood appear in the corpus?,processor = KeywordProcessor () ,useful-python-libraries-for-data-science.ipynb
Replacing all occurences of word forest fire case insensitive with fire,processor = KeywordProcessor(case_sensitive = False) ,useful-python-libraries-for-data-science.ipynb
installing and importing the library,! pip install pyflux ,useful-python-libraries-for-data-science.ipynb
Convert string to datetime64,maruti['Date']= maruti['Date']. apply(pd.to_datetime) ,useful-python-libraries-for-data-science.ipynb
5.1 Visualise the data,"maruti_df = maruti[[ 'Date' , 'VWAP']] ",useful-python-libraries-for-data-science.ipynb
Set Date column as the index column.,"maruti_df.set_index('Date' , inplace = True) ",useful-python-libraries-for-data-science.ipynb
"5.2 Modelling Let s run an ARIMA Model. We can build an ARIMA model as follows, specifying the order of model we want, as well as a pandas DataFrame or numpy array carrying the data.","my_model = pf.ARIMA(data=maruti_df, ar=4, ma=4, family=pf.Normal())
print(my_model.latent_variables)

result = my_model.fit(""MLE"")
result.summary()

my_model.plot_z(figsize=(15,5))
my_model.plot_fit(figsize=(15,10))
my_model.plot_predict_is(h=50, figsize=(15,5))
my_model.plot_predict(h=20,past_values=20,figsize=(15,5))",useful-python-libraries-for-data-science.ipynb
Installing the library,! pip install autoviz ,useful-python-libraries-for-data-science.ipynb
Instantiate the library,from autoviz.AutoViz_Class import AutoViz_Class ,useful-python-libraries-for-data-science.ipynb
Reading the dataset,house_price = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv') ,useful-python-libraries-for-data-science.ipynb
 Numerizer Numerizer is a Python module for converting natural language numbers into ints and floats. It is a port of the Ruby gem numerizer. This could be really useful when preprocessing text data. ,!pip install numerizer,useful-python-libraries-for-data-science.ipynb
" ppscore ppscorebrought to you by the makers of Bamboolib, is a Python implementation of the Predictive Power Score PPS . The PPS is an asymmetric, data type agnostic score that can detect linear or non linear relationships between two columns. The score ranges from 0 no predictive power to 1 perfect predictive power . It can be used as an alternative to the correlation matrix .Let us see it demo through the Titanic Dataset ","!pip install ppscore
",useful-python-libraries-for-data-science.ipynb
Single Predictive Power ScoreHow well can Sex predict the Survival probability?,"titanic_df_subset = titanic_df[[""Survived"", ""Pclass"", ""Sex"", ""Age"", ""Ticket"", ""Fare"", ""Embarked""]]
pps.score(titanic_df_subset, ""Sex"", ""Survived"")",useful-python-libraries-for-data-science.ipynb
PPS matrixwhich predictive patterns exist between the columns?,"
matrix = pps.matrix(titanic_df_subset)
heatmap(matrix)",useful-python-libraries-for-data-science.ipynb
Correlation Matrix,"f = plt.figure(figsize =(16 , 8)) ",useful-python-libraries-for-data-science.ipynb
Read the data,import pandas as pd ,using-categorical-data-with-one-hot-encoding.ipynb
Drop houses where the target is missing,"train_data.dropna(axis = 0 , subset =['SalePrice'], inplace = True) ",using-categorical-data-with-one-hot-encoding.ipynb
Pandas assigns a data type called a dtype to each column or Series. Let s see a random sample of dtypes from our prediction data:,train_predictors.dtypes.sample(10),using-categorical-data-with-one-hot-encoding.ipynb
"Object indicates a column has text there are other things it could be theoretically be, but that s unimportant for our purposes . It s most common to one hot encode these object columns, since they can t be plugged directly into most models. Pandas offers a convenient function called get dummies to get one hot encodings. Call it like this:",one_hot_encoded_training_predictors = pd.get_dummies(train_predictors),using-categorical-data-with-one-hot-encoding.ipynb
"Alternatively, you could have dropped the categoricals. To see how the approaches compare, we can calculate the mean absolute error of models built with two alternative sets of predictors: 1. One hot encoded categoricals as well as numeric predictors 2. Numerical predictors, where we drop categoricals.One hot encoding usually helps, but it varies on a case by case basis. In this case, there doesn t appear to be any meaningful benefit from using the one hot encoded variables.",from sklearn.model_selection import cross_val_score ,using-categorical-data-with-one-hot-encoding.ipynb
"Applying to Multiple FilesSo far, you ve one hot encoded your training data. What about when you have multiple files e.g. a test dataset, or some other data that you d like to make predictions for ? Scikit learn is sensitive to the ordering of columns, so if the training dataset and test datasets get misaligned, your results will be nonsense. This could happen if a categorical had a different number of values in the training data vs the test data.Ensure the test data is encoded in the same manner as the training data with the align command:","one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)
one_hot_encoded_test_predictors = pd.get_dummies(test_predictors)
final_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,
 join='left', 
 axis=1)

",using-categorical-data-with-one-hot-encoding.ipynb
"NLP Augmentation Offline In computer vision problems, there is a virtual infinitude of techniques you can use to augment your images ranging from simple techniques like randomly flipping images to blending images together with CutMix or MixUp. In natural language processing, it is not as easy to come up with similar augmentation strategies we must be a little more creativeThe first idea I had was to randomly replace words with their synonyms or to randomly add word synonyms to the sequence, but then I saw this kernel which is based on this discussion thread and realized we can do better: we can use translation to augment our data and do several things: We can experiment and see if training our model on one language is better worse than training on multiple languages We can change the distribution of languages in our dataset, perhaps translating sentences to low resource languages like Swahili and Urdu We can randomly translate sentences to another language and then translate them back to the original like so: Image from can also apply this augmentation in two ways: offline augmentation or online augmentation. In the first, we augment before we feed to the model, adding to our dataset size. This is preferable for smaller datasets where we are not worried about training taking too long. When you can t afford an increase in size, you resort to online augmentation where augment the data every epoch. We will use offline augmentation in this commit for more see For now, we will only translate to languages currently present in our dataset, but translating to languages outside of our dataset might give us better performance. Please note that some of these language codes are slightly different within the googletrans Python API. See here for more","GEN_BACK_TR = True

GEN_UPSAMPLE = False

GEN_EN_ONLY = False",using-google-translate-for-nlp-augmentation.ipynb
python basics,from matplotlib import pyplot as plt ,using-google-translate-for-nlp-augmentation.ipynb
nlp augmentation,! pip install - - quiet googletrans ,using-google-translate-for-nlp-augmentation.ipynb
model evaluation,"from sklearn.model_selection import train_test_split , StratifiedKFold ",using-google-translate-for-nlp-augmentation.ipynb
for fast parallel processing,"from dask import bag , diagnostics ",using-google-translate-for-nlp-augmentation.ipynb
applies above define function with Dask,def back_translate_parallel(dataset): ,using-google-translate-for-nlp-augmentation.ipynb
pair premises and hypothesis," dataset[[ 'premise' , 'hypothesis']] = list(zip(prems , hyps)) ",using-google-translate-for-nlp-augmentation.ipynb
now we apply translation augmentation, train_thrice_aug = twice_train_aug.pipe(back_translate_parallel) ,using-google-translate-for-nlp-augmentation.ipynb
offline loading,"train = pd.read_csv(""../input/contradictory-my-dear-watson/train.csv"") ",using-google-translate-for-nlp-augmentation.ipynb
view original,print(train.shape) ,using-google-translate-for-nlp-augmentation.ipynb
view first aug,print(train_aug.shape) ,using-google-translate-for-nlp-augmentation.ipynb
view second aug,print(train_twice_aug.shape) ,using-google-translate-for-nlp-augmentation.ipynb
view third aug,print(train_thrice_aug.shape) ,using-google-translate-for-nlp-augmentation.ipynb
check most undersampled languages in training dataset,train['language']. value_counts () ,using-google-translate-for-nlp-augmentation.ipynb
check most undersampled languages in test dataset,test['language']. value_counts () ,using-google-translate-for-nlp-augmentation.ipynb
"We have a choose to make here: do we translate based on language prevalence in the training, testing dataset, or based on the languages that XLM R was trained on? I will base my translation on the test set languages, so I will create a Vietnamese, Hindi, and Bulgarian training datasets, for now:","def translation(sequence , lang): ",using-google-translate-for-nlp-augmentation.ipynb
instantiate translator, translator = Translator () ,using-google-translate-for-nlp-augmentation.ipynb
translate to new language and back to original," translated = translator.translate(sequence , dest = lang). text ",using-google-translate-for-nlp-augmentation.ipynb
pair premises and hypothesis," dataset[[ 'premise' , 'hypothesis']] = list(zip(prems , hyps)) ",using-google-translate-for-nlp-augmentation.ipynb
translate to Vietnamese,"prem_bag_vi = bag.from_sequence(train['premise']. tolist ()).map(lambda x : translation(x , lang = 'vi')) ",using-google-translate-for-nlp-augmentation.ipynb
translate to Hindi,"prem_bag_hi = bag.from_sequence(train['premise']. tolist ()).map(lambda x : translation(x , lang = 'hi')) ",using-google-translate-for-nlp-augmentation.ipynb
translate to Bulgarian,"prem_bag_bg = bag.from_sequence(train['premise']. tolist ()).map(lambda x : translation(x , lang = 'bg')) ",using-google-translate-for-nlp-augmentation.ipynb
and compute,if GEN_UPSAMPLE : ,using-google-translate-for-nlp-augmentation.ipynb
sanity check, train_vi = train ,using-google-translate-for-nlp-augmentation.ipynb
sanity check, train_hi = train ,using-google-translate-for-nlp-augmentation.ipynb
sanity check, train_bg = train ,using-google-translate-for-nlp-augmentation.ipynb
translate to English,"prem_bag_en = bag.from_sequence(train['premise']. tolist ()).map(lambda x : translation(x , lang = 'en')) ",using-google-translate-for-nlp-augmentation.ipynb
sanity check, train_en = train ,using-google-translate-for-nlp-augmentation.ipynb
sanity check,train_en.head () ,using-google-translate-for-nlp-augmentation.ipynb
Gradient Boosting Classifier Model,"GBCModel = GradientBoostingClassifier(n_estimators=250, max_depth=6, learning_rate=0.1, random_state=0)
GBCModel.fit(X_train, y_train)
print('GBCModel Train Score is : ' , GBCModel.score(X_train, y_train))
print('GBCModel Test Score is : ' , GBCModel.score(X_test, y_test))",using-some-models-in-classification-accuracy-95.ipynb
"it can be also linear,poly,sigmoid,precomputed","y_pred_SVC = SVCModel.predict(X_test)
CM_SVC = confusion_matrix(y_test, y_pred_GB)

sns.heatmap(CM_SVC, center=True)
plt.show()

print('Confusion Matrix is\n', CM_SVC)",using-some-models-in-classification-accuracy-95.ipynb
it can be distance,"y_pred_KNN = KNNClassifierModel.predict(X_test)
CM_KNN = confusion_matrix(y_test, y_pred_KNN)

sns.heatmap(CM_KNN, center=True)
plt.show()

print('Confusion Matrix is\n', CM_SVC)",using-some-models-in-classification-accuracy-95.ipynb
"can be also identity , logistic , relu","y_pred_MLP = MLPClassifierModel.predict(X_test)
CM_MLP = confusion_matrix(y_test, y_pred_MLP)

sns.heatmap(CM_MLP, center=True)
plt.show()

print('Confusion Matrix is\n', CM_MLP)
",using-some-models-in-classification-accuracy-95.ipynb
Voting Model,"VotingClassifierModel = VotingClassifier(estimators=[('GBCModel',GBCModel),
 ('SVModel',SVCModel),
 ('KNNModel',KNNClassifierModel),
 ('MLPModel',MLPClassifierModel)],
 voting='soft')
VotingClassifierModel.fit(X_train, y_train)
print('VotingClassifierModel Train Score is : ' , VotingClassifierModel.score(X_train, y_train))
print('VotingClassifierModel Test Score is : ' , VotingClassifierModel.score(X_test, y_test))",using-some-models-in-classification-accuracy-95.ipynb
python basics,from matplotlib import pyplot as plt ,watson-kfold-xlm-r-translation-augmentation.ipynb
deep learning basics,import tensorflow as tf ,watson-kfold-xlm-r-translation-augmentation.ipynb
nlp augmentation,! pip install - - quiet googletrans ,watson-kfold-xlm-r-translation-augmentation.ipynb
easy way to shuffle rows,from sklearn.utils import shuffle ,watson-kfold-xlm-r-translation-augmentation.ipynb
ignore warnings,import warnings ,watson-kfold-xlm-r-translation-augmentation.ipynb
get current TensorFlow version fo,"print(""Currently using Tensorflow version "" + tf.__version__) ",watson-kfold-xlm-r-translation-augmentation.ipynb
"It s Elementary, My Dear WatsonNatural Language Inference NLI is a specific type of NLP task where we must determine whether or not a hypothesis is true based on a premise. Specifically, given a pair of sentences, can we classify them into three different classes: 0 entailment, 1 contradiction, 2 neutral?The current leading model in this field is RoBERTa, described by its creators as a robustly optimized BERT pretraining approach . It changes some of the key hyperparameters of BERT and removes the next sentence pretraining objective all together. The original paper can be found here and the source code hereNow, we have 15 different languages in our dataset, so we cannot use the standard pre trained RoBERTa model as it has only been trained on English sequences. Luckily, there is XLM RoBERTa original paper can be found here which has been trained on 2.5TB of filtered CommonCrawl data in 100 different languages. The implementation procedude is the same as RoBERTa s, so it is easy enough to deploy. Let s see how:","SEED = 34

def seed_everything(seed):
 os.environ['PYTHONHASHSEED']=str(seed)
 tf.random.set_seed(seed)
 np.random.seed(seed)
 random.seed(seed)
 
seed_everything(SEED)",watson-kfold-xlm-r-translation-augmentation.ipynb
choose batch size will depend on cores of our device,BATCH_SIZE = 16 * REPLICAS ,watson-kfold-xlm-r-translation-augmentation.ipynb
get CSV files,"train = pd.read_csv(""../input/contradictory-my-dear-watson/train.csv"") ",watson-kfold-xlm-r-translation-augmentation.ipynb
peek at a premise hypothesis pair and their label,"print(f""Premise: {train['premise'].values[0]}"") ",watson-kfold-xlm-r-translation-augmentation.ipynb
peek at a premise hypothesis pair and their label,"print(f""Premise: {train['premise'].values[1]}"") ",watson-kfold-xlm-r-translation-augmentation.ipynb
explore the distribution of classes and languages,"fig , ax = plt.subplots(figsize =(15 , 10)) ",watson-kfold-xlm-r-translation-augmentation.ipynb
for maximum aesthetics,"palette = sns.cubehelix_palette(8 , start = 2 , rot = 0 , dark = 0 , light = .95 , reverse = True) ",watson-kfold-xlm-r-translation-augmentation.ipynb
set title,graph1.set_title('Distribution of Languages and Labels') ,watson-kfold-xlm-r-translation-augmentation.ipynb
"Back TranslationIn computer vision problems, there is a virtual infinitude of techniques you can use to augment your images ranging from simple techniques like randomly flipping images to blending images together with CutMix or MixUp. In natural language processing, it is not as easy to come up with similar augmentation strategies because it is hard to determine which transformations will preserve the meaning of the original words: Image from on his excellent post on NLP augmentation here The first thought I had was to randomly replace words with their synonyms or to randomly add word synonyms to the sequence, but then I saw this kernel which is based on this discussion thread and realized we can do better: we can use translation to augment our data and do try several things: We can experiment and see if training our model on one language is better worse than training on multiple languages We can change the distribution of languages in our dataset, perhaps translating sentences to low resource languages like Swahili and Urdu We can randomly translate sentences to another language and then translate them back to the original like so: Image from on his excellent post on NLP augmentation herePlease note that some of these language codes are slightly different within the googletrans Python API. See here for more","def back_translate(sequence , PROB = 1): ",watson-kfold-xlm-r-translation-augmentation.ipynb
"I have already created augmented datasets with the above translation method in my kernel here, and the datasets can be found found here and here. Let s quickly compare the three separate datasets:",train.head(),watson-kfold-xlm-r-translation-augmentation.ipynb
offline loading of augmented datasets,train_aug = pd.read_csv('../input/contradictorywatsontwicetranslatedaug/translation_aug_train.csv') ,watson-kfold-xlm-r-translation-augmentation.ipynb
offline loading of augmented datasets,train_twice_aug = pd.read_csv('../input/contradictorywatsontwicetranslatedaug/twice_translated_aug_train.csv') ,watson-kfold-xlm-r-translation-augmentation.ipynb
offline loading of augmented datasets,train_thrice_aug = pd.read_csv('../input/contradictorywatsontwicetranslatedaug/thrice_translation_aug_train.csv') ,watson-kfold-xlm-r-translation-augmentation.ipynb
get CSV files,"train_vi = pd.read_csv(""../input/contradictorytranslatedtrain/train_vi.csv"") ",watson-kfold-xlm-r-translation-augmentation.ipynb
sanity check,train_vi.head () ,watson-kfold-xlm-r-translation-augmentation.ipynb
sanity check,train_hi.head () ,watson-kfold-xlm-r-translation-augmentation.ipynb
sanity check,train_bg.head () ,watson-kfold-xlm-r-translation-augmentation.ipynb
get HuggingFace transformers,! pip install - - quiet transformers ,watson-kfold-xlm-r-translation-augmentation.ipynb
import model and Tokenizer,"from transformers import TFAutoModel , AutoTokenizer ",watson-kfold-xlm-r-translation-augmentation.ipynb
get paths to TensorFlow XLM RoBERTa base and large models,"roberta_base = ""jplu/tf-xlm-roberta-base"" ",watson-kfold-xlm-r-translation-augmentation.ipynb
offline load back translated test samples,test_bt = pd.read_csv('../input/contradictorywatsontwicetranslatedaug/translation_aug_test.csv') ,watson-kfold-xlm-r-translation-augmentation.ipynb
Below is a function that covers the 2 step process where we tokenize our text data with a HuggingFace object TOKENIZER and then convert it into a tf.data.Dataset object for use with TPU:,TOKENIZER = AutoTokenizer.from_pretrained(roberta_large) ,watson-kfold-xlm-r-translation-augmentation.ipynb
function to encode text and convert dataset to tensor dataset,"def to_tf_dataset(dataset , max_len , repeat = False , shuffle = False , labeled = True , batch_size = BATCH_SIZE): ",watson-kfold-xlm-r-translation-augmentation.ipynb
Configuration,LR_START = 1e-6 ,watson-kfold-xlm-r-translation-augmentation.ipynb
stepwise schedule,def lrfn_step(epoch): ,watson-kfold-xlm-r-translation-augmentation.ipynb
smoothish schedule,def lrfn_smooth(epoch): ,watson-kfold-xlm-r-translation-augmentation.ipynb
visualize learning rate schedule,rng =[i for i in range(25 )] ,watson-kfold-xlm-r-translation-augmentation.ipynb
helper function to create our model,"def build_model(transformer_layer , max_len , learning_rate): ",watson-kfold-xlm-r-translation-augmentation.ipynb
must use this to send to TPU cores, with strategy.scope (): ,watson-kfold-xlm-r-translation-augmentation.ipynb
define input s ," input_ids = tf.keras.Input(shape =(max_len ,), dtype = tf.int32) ",watson-kfold-xlm-r-translation-augmentation.ipynb
insert roberta layer, roberta = TFAutoModel.from_pretrained(transformer_layer) ,watson-kfold-xlm-r-translation-augmentation.ipynb
"only need token here, so we extract it now"," out = roberta[: , 0 , :] ",watson-kfold-xlm-r-translation-augmentation.ipynb
add our softmax layer," out = tf.keras.layers.Dense(3 , activation = 'softmax')( out) ",watson-kfold-xlm-r-translation-augmentation.ipynb
assemble model and compile," model = tf.keras.Model(inputs = input_ids , outputs = out) ",watson-kfold-xlm-r-translation-augmentation.ipynb
Configuration,LR_RATE = 5e-6 ,watson-kfold-xlm-r-translation-augmentation.ipynb
Training,"preds = np.zeros(( len(test), 3)) ",watson-kfold-xlm-r-translation-augmentation.ipynb
to clear TPU memory, if DEVICE == 'TPU' : ,watson-kfold-xlm-r-translation-augmentation.ipynb
build model, K.clear_session () ,watson-kfold-xlm-r-translation-augmentation.ipynb
Submission,USE_TTA = False,watson-kfold-xlm-r-translation-augmentation.ipynb
sanity check,submission.head () ,watson-kfold-xlm-r-translation-augmentation.ipynb
"Let s add the libraries where they are really needed, not all of them at the first line",import pandas as pd,watson-nli-with-tensorflow-and-transformers.ipynb
our data frames,"train_df = pd.read_csv(""../input/contradictory-my-dear-watson/train.csv"")
test_df = pd.read_csv(""../input/contradictory-my-dear-watson/test.csv"")
sample_df = pd.read_csv('../input/contradictory-my-dear-watson/sample_submission.csv')",watson-nli-with-tensorflow-and-transformers.ipynb
Initiative knowledge about our data,train_df,watson-nli-with-tensorflow-and-transformers.ipynb
Modeling,import tensorflow as tf,watson-nli-with-tensorflow-and-transformers.ipynb
Our prediction output,"predictions = model.predict(test_dataset, verbose=1)
sample_df['prediction'] = predictions.argmax(axis=1)",watson-nli-with-tensorflow-and-transformers.ipynb
"Hello!This is inference notebook from this train notebook.Also, I interpeted results and showed why the accuracy in this competition is so high.","!pip install --upgrade pip > /dev/null
!pip install --upgrade transformers > /dev/null
!pip install nlp > /dev/null",watson-xlm-r-nli-inference.ipynb
NN,"from tensorflow.keras.layers import Dense , Input , GlobalAveragePooling1D , GlobalMaxPooling1D ",watson-xlm-r-nli-inference.ipynb
for CPU and single GPU, strategy = tf.distribute.get_strategy () ,watson-xlm-r-nli-inference.ipynb
convert transformer encoding to vector," if head == ""cls"" : ",watson-xlm-r-nli-inference.ipynb
using first token as encoder feature map," features = encoder_output[: , 0 , :] ",watson-xlm-r-nli-inference.ipynb
3class softmax," out = Dense(3 , activation = 'softmax')( features) ",watson-xlm-r-nli-inference.ipynb
define model," model = Model(inputs = input_ids , outputs = out) ",watson-xlm-r-nli-inference.ipynb
load data,train = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv') ,watson-xlm-r-nli-inference.ipynb
preprocess,"x , y = preprocess(train) ",watson-xlm-r-nli-inference.ipynb
load external datasets for interpretation purpose,mnli = load_mnli () ,watson-xlm-r-nli-inference.ipynb
Let s build toy search engine: it s looking for a full match of query and knowledge base.,"import re
import string
punct = '[' + ''.join([c for c in string.punctuation if c != ""'""]) + ']'

def preprocess_query(q):
 q = q.lower()
 q = re.sub(punct, ' ', q)
 q = re.sub('[ ]{2,}', ' ', q)
 return q

def search_in_base(q, kb):
 q = preprocess_query(q)
 return int(q in kb)
",watson-xlm-r-nli-inference.ipynb
save results,strategy = init_strategy () ,watson-xlm-r-nli-inference.ipynb
linear algebra,import numpy as np ,welcome-to-deep-learning-cnn-99.ipynb
convert to one hot encoding,from keras.utils.np_utils import to_categorical ,welcome-to-deep-learning-cnn-99.ipynb
"As always, we split the data into a training set and a validation set, so that we can evaluate the performance of our model.","raw_data = np.loadtxt(train_file, skiprows=1, dtype='int', delimiter=',')
x_train, x_val, y_train, y_val = train_test_split(
 raw_data[:,1:], raw_data[:,0], test_size=0.1)",welcome-to-deep-learning-cnn-99.ipynb
"Each data point consists of 784 values. A fully connected net just treats all these values the same, but a CNN treats it as a 28x28 square. Thes two graphs explain the difference: It s easy to understand why a CNN can get better results.","fig, ax = plt.subplots(2, 1, figsize=(12,6))
ax[0].plot(x_train[0])
ax[0].set_title('784x1 data')
ax[1].imshow(x_train[0].reshape(28,28), cmap='gray')
ax[1].set_title('28x28 data')",welcome-to-deep-learning-cnn-99.ipynb
"We now reshape all data this way. Keras wants an extra dimension in the end, for channels. If this had been RGB images, there would have been 3 channels, but as MNIST is gray scale it only uses one.This notebook is written for the tensorflow channel ordering. If you have Keras installed for Theano backend, you might start seeing some error message soon related to channel ordering. This can easily be solved 1 .","x_train = x_train.reshape(-1, 28, 28, 1)
x_val = x_val.reshape(-1, 28, 28, 1)",welcome-to-deep-learning-cnn-99.ipynb
"It would be possible to train the net on the original data, with pixel values 0 to 255. If we use the standard initialization methods for weights, however, data between 0 and 1 should make the net converge faster. ","x_train = x_train.astype(""float32"")/255.
x_val = x_val.astype(""float32"")/255.",welcome-to-deep-learning-cnn-99.ipynb
"The labels were given as integers between 0 and 9. We need to convert these to one hot encoding, i.e. a 10x1 array with one 1 and nine 0:s, with the position of the 1 showing us the value. See the example, with the position of the 1 showing the correct value for the digit in the graph above.",y_train = to_categorical(y_train) ,welcome-to-deep-learning-cnn-99.ipynb
example:,print(y_train[0]) ,welcome-to-deep-learning-cnn-99.ipynb
"Train the modelKeras offers two different ways of defining a network. We will the Sequential API, where you just add on one layer at a time, starting from the input.The most important part are the convolutional layers Conv2D. Here they have 16 32 filters that use nine weights each to transform a pixel to a weighted average of itself and its eight neighbors. As the same nine weights are used over the whole image, the net will pick up features that are useful everywhere. As it is only nine weights, we can stack many convolutional layers on top of each other without running out of memory time. The MaxPooling layers just look at four neighboring pixels and picks the maximal value. This reduces the size of the image by half, and by combining convolutional and pooling layers, the net be able to combine its features to learn more global features of the image. In the end we use the features in two fully connected Dense layers.Batch Normalization is a technical trick to make training faster. Dropout is a regularization method, where the layer randomly replaces a proportion of its weights to zero for each training sample. This forces the net to learn features in a distributed way, not relying to much on a particular weight, and therefore improves generalization. relu is the activation function x max x,0 .",model = Sequential () ,welcome-to-deep-learning-cnn-99.ipynb
"Another important method to improve generalization is augmentation. This means generating more training data by randomly perturbing the images. If done in the right way, it can force the net to only learn translation invariant features. If you train this model over hundreds of epochs, augmentation will definitely improve your performance. Here in the Kernel, we will only look at each image 4 5 times, so the difference is smaller. We use a Keras function for augmentation.","datagen = ImageDataGenerator(zoom_range = 0.1,
 height_shift_range = 0.1,
 width_shift_range = 0.1,
 rotation_range = 10)",welcome-to-deep-learning-cnn-99.ipynb
"The model needs to be compiled before training can start. As our loss function, we use logloss which is called categorical crossentropy in Keras. Metrics is only used for evaluation. As optimizer, we could have used ordinary stochastic gradient descent SGD , but Adam is faster.","model.compile(loss='categorical_crossentropy', optimizer = Adam(lr=1e-4), metrics=[""accuracy""])",welcome-to-deep-learning-cnn-99.ipynb
"We train once with a smaller learning rate to ensure convergence. We then speed things up, only to reduce the learning rate by 10 every epoch. Keras has a function for this: ",annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x),welcome-to-deep-learning-cnn-99.ipynb
"We only used a subset of the validation set during training, to save time. Now let s check performance on the whole validation set.","final_loss, final_acc = model.evaluate(x_val, y_val, verbose=0)
print(""Final loss: {0:.4f}, final accuracy: {1:.4f}"".format(final_loss, final_acc))",welcome-to-deep-learning-cnn-99.ipynb
"To easily get to the top half of the leaderboard, just follow these steps, go to the Kernel s output, and submit submission.csv ","mnist_testset = np.loadtxt(test_file, skiprows=1, dtype='int', delimiter=',')
x_test = mnist_testset.astype(""float32"")
x_test = x_test.reshape(-1, 28, 28, 1)/255.",welcome-to-deep-learning-cnn-99.ipynb
y hat consists of class probabilities corresponding to the one hot encoding of the training labels . I now select the class with highest probability,"y_pred = np.argmax(y_hat,axis=1)",welcome-to-deep-learning-cnn-99.ipynb
"Example Concrete FormulationsTo illustrate these ideas we ll see how adding a few synthetic features to a dataset can improve the predictive performance of a random forest model.The Concrete dataset contains a variety of concrete formulations and the resulting product s compressive strength, which is a measure of how much load that kind of concrete can bear. The task for this dataset is to predict a concrete s compressive strength given its formulation.","import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

df = pd.read_csv(""../input/fe-course-data/concrete.csv"")
df.head()",what-is-feature-engineering.ipynb
"You can see here the various ingredients going into each variety of concrete. We ll see in a moment how adding some additional synthetic features derived from these can help a model to learn important relationships among them.We ll first establish a baseline by training the model on the un augmented dataset. This will help us determine whether our new features are actually useful.Establishing baselines like this is good practice at the start of the feature engineering process. A baseline score can help you decide whether your new features are worth keeping, or whether you should discard them and possibly try something else.",X = df.copy () ,what-is-feature-engineering.ipynb
Train and score baseline model,"baseline = RandomForestRegressor(criterion = ""mae"" , random_state = 0) ",what-is-feature-engineering.ipynb
"If you ever cook at home, you might know that the ratio of ingredients in a recipe is usually a better predictor of how the recipe turns out than their absolute amounts. We might reason then that ratios of the features above would be a good predictor of CompressiveStrength.The cell below adds three new ratio features to the dataset.",X = df.copy () ,what-is-feature-engineering.ipynb
Create synthetic features,"X[""FCRatio""]= X[""FineAggregate""]/ X[""CoarseAggregate""] ",what-is-feature-engineering.ipynb
Train and score model on dataset with additional ratio features,"model = RandomForestRegressor(criterion = ""mae"" , random_state = 0) ",what-is-feature-engineering.ipynb
linear algebra,import numpy as np ,word2vec.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,word2vec.ipynb
stopwords are removed from text to keep just useful info,from nltk.corpus import stopwords ,word2vec.ipynb
"For example, running this by clicking run or pressing Shift Enter will list the files in the input directory",import os ,word2vec.ipynb
Any results you write to the current directory are saved as output.,"raw_train_data_labeled = pd.read_csv(""../input/word2vec-nlp-tutorial/labeledTrainData.tsv"", header=0, delimiter=""\t"", quoting=3)
raw_train_data_unlabeled = pd.read_csv(""../input/word2vec-nlp-tutorial/unlabeledTrainData.tsv"", header=0, delimiter=""\t"", quoting=3)
raw_test_data = pd.read_csv(""../input/word2vec-nlp-tutorial/testData.tsv"", header=0, delimiter=""\t"", quoting=3)",word2vec.ipynb
"w o this the max accuracy was around 88 , but using this validation set acc. increased to around 93 ","imdb_data = pd.read_csv('../input/imdb-review-dataset/imdb_master.csv' , encoding = ""latin-1"") ",word2vec.ipynb
len sequence ,leng = 0 ,word2vec.ipynb
"if words are more than max length then they are skipped, if less than padding with 0 is done",print(avg_length) ,word2vec.ipynb
max len math.ceil avg length this is used to decide how man words in seq to keep,max_len = math.ceil(avg_length) ,word2vec.ipynb
Preparing embedding matrix,vocab_size = len(tokenizer.word_index)+ 1 ,word2vec.ipynb
" 1 is done because i starts from 1 instead of 0, and goes till len vocab ","for word , i in tokenizer.word_index.items (): ",word2vec.ipynb
predicting test data,"y_pred = model.predict(train_test_data[ntrain : ntrain + 25000 , :]) ",word2vec.ipynb
linear algebra,import numpy as np ,xgboost-feat-imp-acc-93.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,xgboost-feat-imp-acc-93.ipynb
"For example, running this by clicking run or pressing Shift Enter will list all files under the input directory",import os ,xgboost-feat-imp-acc-93.ipynb
"You can also write temporary files to kaggle temp , but they won t be saved outside of the current session","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt",xgboost-feat-imp-acc-93.ipynb
Information about Data,train.info(),xgboost-feat-imp-acc-93.ipynb
Unique Values in EACH Feature,train.nunique(),xgboost-feat-imp-acc-93.ipynb
Considering Infinite to NAN throughout the Notebook,"pd.set_option('mode.use_inf_as_na', True)",xgboost-feat-imp-acc-93.ipynb
Columns having Null Values,"null_train=train.columns[train.isnull().any()]
null_train",xgboost-feat-imp-acc-93.ipynb
"Impute Missing ValuesMissing values are one of the most common problems you can encounter when you try to prepare your data for machine learning. The reason for the missing values might be human errors,interruptions in the data flow, privacy concerns, and so on. Whatever is the reason, missing values affect the performance of the machine learning models.","from sklearn.impute import SimpleImputer

imp = SimpleImputer(strategy='most_frequent')

train[null_train] = imp.fit_transform(train[null_train])

train.isnull().sum().sum()",xgboost-feat-imp-acc-93.ipynb
BoxplotVizualize the Distribution of Features,"import plotly.express as px
for col in train.columns:
 fig = px.box(train, y=train[col], color=y['0'],points=""all"")
 fig.show()",xgboost-feat-imp-acc-93.ipynb
Using Isolation Forest,from sklearn.ensemble import IsolationForest ,xgboost-feat-imp-acc-93.ipynb
select all rows that are not outliers,train[out != - 1] ,xgboost-feat-imp-acc-93.ipynb
select all rows that are not outliers,test[out != - 1] ,xgboost-feat-imp-acc-93.ipynb
2D Histogram To see relation between EACH Feature and Target Variable,"import plotly.express as px
for col in train.columns:
 fig = px.density_heatmap(train, x=train[col],y=y['0'], marginal_x=""histogram"", marginal_y=""histogram"")
 fig.show()",xgboost-feat-imp-acc-93.ipynb
Heatmap Checks the Correlation among ALL Features,"import plotly.express as px
fig = px.imshow(train.corr(), text_auto=True,aspect=""auto"")
fig.show()",xgboost-feat-imp-acc-93.ipynb
"Train Test Split It splits the train data into 4 parts, X train, X test, y train, y test.X train, y train first used to train the algorithm. X test is used in that trained algorithms to predict outcomes. Once we get the outcomes, we compare it with y test",X = train ,xgboost-feat-imp-acc-93.ipynb
Train Fit the XGBoost Model To know more about XGB HyperParameters check ,"from xgboost import XGBClassifier
model = XGBClassifier(
 booster='gbtree', 
 objective='binary:logistic', 
 eval_metric='logloss',
 n_estimators=1000,
 max_depth=15,
 min_split_loss=0.1,
 base_score=0.5,
 learning_rate=0.08,
 reg_alpha=0.5,
 reg_lambda=0.5,
 gamma=0.2)

model.fit(X_train, y_train)

model.get_params()",xgboost-feat-imp-acc-93.ipynb
Predicting from XGB Model,"pred=model.predict(X_test)
pred",xgboost-feat-imp-acc-93.ipynb
Scoring Confusion Matrix,"from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,pred)

from sklearn.metrics import ConfusionMatrixDisplay
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)
cm",xgboost-feat-imp-acc-93.ipynb
Scoring F1 Score,"from sklearn.metrics import f1_score
f1_score(y_test, pred)",xgboost-feat-imp-acc-93.ipynb
get importance,importance = model.feature_importances_ ,xgboost-feat-imp-acc-93.ipynb
summarize feature importance,"for i , v in enumerate(importance): ",xgboost-feat-imp-acc-93.ipynb
plot,import matplotlib.pyplot as plt ,xgboost-feat-imp-acc-93.ipynb
Submission,"submission=pd.DataFrame({'Predictions': model.predict(test)})
submission
submission.to_csv('submission.csv', index=False)",xgboost-feat-imp-acc-93.ipynb
Importing Necessary Modules and Functions,"import numpy as np 
import pandas as pd 
import os
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
import warnings
import json
from sklearn import manifold

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import OrdinalEncoder

import xgboost as xgb

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.model_selection import StratifiedKFold

from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import confusion_matrix
from xgboost import plot_tree

warnings.filterwarnings('ignore')",xgboost-wrangling-with-hyperparameters-guide.ipynb
Preparing the Data,"df= pd.read_csv(""/kaggle/input/spaceship-titanic/train.csv"")
df.head()",xgboost-wrangling-with-hyperparameters-guide.ipynb
"Base parameters : booster gbtree , as it is a classification problem objective binary:logistic , as per the problem tree method gpu hist using GPU ","def xgb_helper(PARAMETERS,V_PARAM_NAME=False,V_PARAM_VALUES=False,BR=10):
 
 temp_dmatrix =xgb.DMatrix(data=X_train, label=y_train)
 
 if V_PARAM_VALUES==False:
 cv_results = xgb.cv(dtrain=temp_dmatrix, nfold=5,num_boost_round=BR,params=PARAMETERS, as_pandas=True, seed=123 )
 return cv_results
 
 else:
 results=[]
 
 for v_param_value in V_PARAM_VALUES:
 PARAMETERS[V_PARAM_NAME]=v_param_value
 cv_results = xgb.cv(dtrain=temp_dmatrix, nfold=5,num_boost_round=BR,params=PARAMETERS, as_pandas=True, seed=123)
 results.append((cv_results[""train-auc-mean""].tail().values[-1],cv_results[""test-auc-mean""].tail().values[-1]))
 
 data = list(zip(V_PARAM_VALUES, results))
 print(pd.DataFrame(data,columns=[V_PARAM_NAME,""auc""]))
 
 return cv_results",xgboost-wrangling-with-hyperparameters-guide.ipynb
Create a general base model and evaluate performance,"PARAMETERS={""objective"":'binary:logistic',""eval_metric"":""auc""}
xgb_helper(PARAMETERS)",xgboost-wrangling-with-hyperparameters-guide.ipynb
Create the DMatrix: housing dmatrix,"housing_dmatrix = xgb.DMatrix(data = X_train , label = y_train) ",xgboost-wrangling-with-hyperparameters-guide.ipynb
Create the parameter dictionary for each tree: params,"params = { ""objective"" : ""binary:logistic"" , ""max_depth"" : 5 } ",xgboost-wrangling-with-hyperparameters-guide.ipynb
Create list of number of boosting rounds,"num_rounds =[5 , 10 , 15 , 20 , 25] ",xgboost-wrangling-with-hyperparameters-guide.ipynb
Empty list to store final round rmse per XGBoost model,final_rmse_per_round = [] ,xgboost-wrangling-with-hyperparameters-guide.ipynb
Iterate over num rounds and build one model per num boost round parameter,for curr_num_rounds in num_rounds : ,xgboost-wrangling-with-hyperparameters-guide.ipynb
Perform cross validation: cv results," cv_results = xgb.cv(dtrain = housing_dmatrix , params = params , nfold = 5 , num_boost_round = curr_num_rounds , metrics = ""auc"" , as_pandas = True , seed = 123) ",xgboost-wrangling-with-hyperparameters-guide.ipynb
Append final round RMSE," final_rmse_per_round.append(cv_results[""test-auc-mean""]. tail (). values[- 1]) ",xgboost-wrangling-with-hyperparameters-guide.ipynb
Print the resultant DataFrame,"num_rounds_rmses = list(zip(num_rounds , final_rmse_per_round)) ",xgboost-wrangling-with-hyperparameters-guide.ipynb
 Choose the learning rate. Maybe you can start with a higher one. 0.5 0.1 is ok for starting in most cases. ,"PARAMETERS={""objective"":'binary:logistic',""eval_metric"":""auc"",""learning_rate"": 0.5}
xgb_helper(PARAMETERS)",xgboost-wrangling-with-hyperparameters-guide.ipynb
2.1. Tuning max depth.Tips: Keep it around 3 10.,"PARAMETERS={""objective"":'binary:logistic',""eval_metric"":""auc"",""learning_rate"": 0.5}
V_PARAM_NAME=""max_depth""
V_PARAM_VALUES=range(3,10,1)

data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);",xgboost-wrangling-with-hyperparameters-guide.ipynb
"2.2. Tuning min child weigh.Tips: Keep it small for imbalanced datasets,good for balanced","PARAMETERS={""objective"":'binary:logistic',""eval_metric"":""auc"",""learning_rate"": 0.5,""max_depth"":5}
V_PARAM_NAME=""min_child_weight""
V_PARAM_VALUES=range(0,5,1)

data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);",xgboost-wrangling-with-hyperparameters-guide.ipynb
 Its time for gamma.Tips: Keep it small like 0.1 0.2 forstarting. Will be tuned later. ,"PARAMETERS={""objective"":'binary:logistic',""eval_metric"":""auc"",""learning_rate"": 0.5,""max_depth"":5,""min_child_weight"":1}
V_PARAM_NAME = ""gamma""
V_PARAM_VALUES = [0.1,0.2,0.5,1,1.5,2]

data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);",xgboost-wrangling-with-hyperparameters-guide.ipynb
4.1. Tuning subsample.Tips: Keep it small in range 0.5 0.9.,"PARAMETERS={""objective"":'binary:logistic',""eval_metric"":""auc"",""learning_rate"": 0.5,""max_depth"":5,""min_child_weight"":1,""gamma"":1}
V_PARAM_NAME = ""subsample""
V_PARAM_VALUES = [.4,.5,.6,.7,.8,.9]

data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);",xgboost-wrangling-with-hyperparameters-guide.ipynb
4.2. Tune colsample bytree.Tips: Keep it small in range 0.5 0.9.,"PARAMETERS={""objective"":'binary:logistic',""eval_metric"":""auc"",""learning_rate"": 0.5,""max_depth"":5,""min_child_weight"":1,""gamma"":1,""subsample"":0.7}
V_PARAM_NAME = ""colsample_bytree""
V_PARAM_VALUES = [.4,.5,.6,.7,.8,.9]

data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);",xgboost-wrangling-with-hyperparameters-guide.ipynb
4.3. Tune scale pos weight.Tips: Based on class imbalance.,"PARAMETERS={""objective"":'binary:logistic',""eval_metric"":""auc"",""learning_rate"": 0.5,""max_depth"":5,""min_child_weight"":1,
 ""gamma"":1,""subsample"":0.7,""colsample_bytree"":.8}

V_PARAM_NAME = ""scale_pos_weight""
V_PARAM_VALUES = [.5,1,2]

data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);",xgboost-wrangling-with-hyperparameters-guide.ipynb
5.1. Tune alpha.Tips: Based on class imbalance.,"PARAMETERS={""objective"":'binary:logistic',""eval_metric"":""auc"",""learning_rate"": 0.5,""max_depth"":5,""min_child_weight"":1,
 ""gamma"":1,""subsample"":0.7,""colsample_bytree"":.8, ""scale_pos_weight"":1}

V_PARAM_NAME = ""reg_alpha""
V_PARAM_VALUES = np.linspace(start=0.001, stop=1, num=20).tolist()

data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);",xgboost-wrangling-with-hyperparameters-guide.ipynb
5.2. Tune lambda.Tips: Based on class imbalance.,"PARAMETERS={""objective"":'binary:logistic',""eval_metric"":""auc"",""learning_rate"": 0.5,""max_depth"":5,""min_child_weight"":1,
 ""gamma"":1,""subsample"":0.7,""colsample_bytree"":.8, ""scale_pos_weight"":1,""reg_alpha"":0.15}

V_PARAM_NAME = ""reg_lambda""
V_PARAM_VALUES = np.linspace(start=0.001, stop=1, num=20).tolist()

data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);",xgboost-wrangling-with-hyperparameters-guide.ipynb
6.1. Reduce Learning rate.,"PARAMETERS={""objective"":'binary:logistic',""eval_metric"":""auc"",""max_depth"":5,""min_child_weight"":1,
 ""gamma"":1,""subsample"":0.7,""colsample_bytree"":.8, ""scale_pos_weight"":1,""reg_alpha"":0.15,
 ""reg_lambda"":1}

V_PARAM_NAME = ""learning_rate""
V_PARAM_VALUES = np.linspace(start=0.01, stop=0.3, num=10).tolist()

data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);",xgboost-wrangling-with-hyperparameters-guide.ipynb
Full Model,"PARAMETERS={""objective"":'binary:logistic',""eval_metric"":""auc"",""max_depth"":5,""min_child_weight"":1,
 ""gamma"":1,""subsample"":0.7,""colsample_bytree"":.8, ""scale_pos_weight"":1,""reg_alpha"":0.15,
 ""reg_lambda"":1,""learning_rate"": 0.3}

clf = xgb.XGBClassifier( tree_method=""gpu_hist"",objective=""binary:logistic"",eval_metric=""auc"",max_depth=5,min_child_weight=1,
 gamma=1,subsample=0.7,colsample_bytree=.8, scale_pos_weight=1,reg_alpha=0.15,
 reg_lambda=1,learning_rate= 0.3,n_estimators=800)

clf.fit(X_train,y_train)

clf.save_model(""categorical-model.json"")",xgboost-wrangling-with-hyperparameters-guide.ipynb
Get a graph,"graph = xgb.to_graphviz(clf , num_trees = 1) ",xgboost-wrangling-with-hyperparameters-guide.ipynb
Or get a matplotlib axis,"ax = xgb.plot_tree(clf , num_trees = 1) ",xgboost-wrangling-with-hyperparameters-guide.ipynb
Get feature importances,plt.show () ,xgboost-wrangling-with-hyperparameters-guide.ipynb
Normal Condition,"PARAMETERS={""objective"":'binary:logistic',""eval_metric"":""auc""}
xgb_helper(PARAMETERS)",xgboost-wrangling-with-hyperparameters-guide.ipynb
Overfitting Controlled,"PARAMETERS={""objective"":'binary:logistic',""eval_metric"":""auc"", ""max_depth"":2 , ""min_child_weight"":3, ""gamma"":2}
xgb_helper(PARAMETERS)",xgboost-wrangling-with-hyperparameters-guide.ipynb
Control Overfitting Code Example : Method 2,"PARAMETERS = { ""objective"" : 'binary:logistic' , ""eval_metric"" : ""auc"" , ""subsample"" : 0.3 , ""colsample_bytree"" : 0.3 , ""eta"" : .05 } ",xgboost-wrangling-with-hyperparameters-guide.ipynb
increasing num bossting round to 15,"xgb_helper(PARAMETERS , 25) ",xgboost-wrangling-with-hyperparameters-guide.ipynb
"We will start with the data pre loaded into train X, test X, train y, test y.","import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import Imputer

data = pd.read_csv('../input/train.csv')
data.dropna(axis=0, subset=['SalePrice'], inplace=True)
y = data.SalePrice
X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])
train_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)

my_imputer = Imputer()
train_X = my_imputer.fit_transform(train_X)
test_X = my_imputer.transform(test_X)
",xgboost.ipynb
We build and fit a model just as we would in scikit learn.,from xgboost import XGBRegressor ,xgboost.ipynb
Add silent True to avoid printing out updates with each cycle,"my_model.fit(train_X , train_y , verbose = False) ",xgboost.ipynb
make predictions,predictions = my_model.predict(test_X) ,xgboost.ipynb
"Model TuningXGBoost has a few parameters that can dramatically affect your model s accuracy and training speed. The first parameters you should understand are:n estimators and early stopping rounds n estimators specifies how many times to go through the modeling cycle described above. In the underfitting vs overfitting graph, n estimators moves you further to the right. Too low a value causes underfitting, which is inaccurate predictions on both training data and new data. Too large a value causes overfitting, which is accurate predictions on training data, but inaccurate predictions on new data which is what we care about . You can experiment with your dataset to find the ideal. Typical values range from 100 1000, though this depends a lot on the learning rate discussed below.The argument early stopping rounds offers a way to automatically find the ideal value. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren t at the hard stop for n estimators. It s smart to set a high value for n estimators and then use early stopping rounds to find the optimal time to stop iterating.Since random chance sometimes causes a single round where validation scores don t improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. early stopping rounds 5 is a reasonable value. Thus we stop after 5 straight rounds of deteriorating validation scores.Here is the code to fit with early stopping:","my_model = XGBRegressor(n_estimators=1000)
my_model.fit(train_X, train_y, early_stopping_rounds=5, 
 eval_set=[(test_X, test_y)], verbose=False)
",xgboost.ipynb
"When using early stopping rounds, you need to set aside some of your data for checking the number of rounds to use. If you later want to fit a model with all of your data, set n estimators to whatever value you found to be optimal when run with early stopping.learning rate Here s a subtle but important trick for better XGBoost models:Instead of getting predictions by simply adding up the predictions from each component model, we will multiply the predictions from each model by a small number before adding them in. This means each tree we add to the ensemble helps us less. In practice, this reduces the model s propensity to overfit.So, you can use a higher value of n estimators without overfitting. If you use early stopping, the appropriate number of trees will be set automatically.In general, a small learning rate and large number of estimators will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle.Modifying the example above to include a learing rate would yield the following code:","my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)
my_model.fit(train_X, train_y, early_stopping_rounds=5, 
 eval_set=[(test_X, test_y)], verbose=False)
",xgboost.ipynb
"Selecting Data for Modeling Your dataset had too many variables to wrap your head around, or even to print out nicely. How can you pare down this overwhelming amount of data to something you can understand?We ll start by picking a few variables using our intuition. Later courses will show you statistical techniques to automatically prioritize variables.To choose variables columns, we ll need to see a list of all columns in the dataset. That is done with the columns property of the DataFrame the bottom line of code below .","import pandas as pd

melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path) 
melbourne_data.columns",your-first-machine-learning-model.ipynb
dropna drops missing values think of na as not available ,melbourne_data = melbourne_data.dropna(axis = 0) ,your-first-machine-learning-model.ipynb
"There are many ways to select a subset of your data. The Pandas course covers these in more depth, but we will focus on two approaches for now. Dot notation, which we use to select the prediction target Selecting with a column list, which we use to select the features Selecting The Prediction Target You can pull out a variable with dot notation. This single column is stored in a Series, which is broadly like a DataFrame with only a single column of data. We ll use the dot notation to select the column we want to predict, which is called the prediction target. By convention, the prediction target is called y. So the code we need to save the house prices in the Melbourne data is",y = melbourne_data.Price,your-first-machine-learning-model.ipynb
"Choosing Features The columns that are inputted into our model and later used to make predictions are called features. In our case, those would be the columns used to determine the home price. Sometimes, you will use all columns except the target as features. Other times you ll be better off with fewer features. For now, we ll build a model with only a few features. Later on you ll see how to iterate and compare models built with different features.We select multiple features by providing a list of column names inside brackets. Each item in that list should be a string with quotes .Here is an example:","melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']",your-first-machine-learning-model.ipynb
"By convention, this data is called X.",X = melbourne_data[melbourne_features],your-first-machine-learning-model.ipynb
"Let s quickly review the data we ll be using to predict house prices using the describe method and the head method, which shows the top few rows.",X.describe(),your-first-machine-learning-model.ipynb
"Building Your ModelYou will use the scikit learn library to create your models. When coding, this library is written as sklearn, as you will see in the sample code. Scikit learn is easily the most popular library for modeling the types of data typically stored in DataFrames. The steps to building and using a model are: Define: What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too. Fit: Capture patterns from provided data. This is the heart of modeling. Predict: Just what it sounds like Evaluate: Determine how accurate the model s predictions are.Here is an example of defining a decision tree model with scikit learn and fitting it with the features and target variable.",from sklearn.tree import DecisionTreeRegressor ,your-first-machine-learning-model.ipynb
Define model. Specify a number for random state to ensure same results each run,melbourne_model = DecisionTreeRegressor(random_state = 1) ,your-first-machine-learning-model.ipynb
Fit model,"melbourne_model.fit(X , y) ",your-first-machine-learning-model.ipynb
"Many machine learning models allow some randomness in model training. Specifying a number for random state ensures you get the same results in each run. This is considered a good practice. You use any number, and model quality won t depend meaningfully on exactly what value you choose.We now have a fitted model that we can use to make predictions.In practice, you ll want to make predictions for new houses coming on the market rather than the houses we already have prices for. But we ll make predictions for the first few rows of the training data to see how the predict function works.","print(""Making predictions for the following 5 houses:"")
print(X.head())
print(""The predictions are"")
print(melbourne_model.predict(X.head()))",your-first-machine-learning-model.ipynb
linear algebra,import numpy as np ,your-first-nlp-competition-submission.ipynb
"data processing, CSV file I O e.g. pd.read csv ",import pandas as pd ,your-first-nlp-competition-submission.ipynb
regex applies a regular expression to a string and returns the matching substrings.,import re ,your-first-nlp-competition-submission.ipynb
Import Dataset ,"train_data = pd.read_csv('../input/nlp-getting-started/train.csv')
test_data = pd.read_csv('../input/nlp-getting-started/test.csv')
",your-first-nlp-competition-submission.ipynb
Let s display one the tweets existed in the text column,train_data['text'][ 11] ,your-first-nlp-competition-submission.ipynb
"Before we begin with anything else,let s check the class distribution.There are only two classes 0 and 1.","x=train_data.target.value_counts()
sns.barplot(x.index,x)
plt.gca().set_ylabel('samples')",your-first-nlp-competition-submission.ipynb
Number of characters in tweets,"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
tweet_len=train_data[train_data['target']==1]['text'].str.len()
ax1.hist(tweet_len,color='blue')
ax1.set_title('disaster tweets')
tweet_len=train_data[train_data['target']==0]['text'].str.len()
ax2.hist(tweet_len,color='CRIMSON')
ax2.set_title('Not disaster tweets')
fig.suptitle('Characters in tweets')
plt.show()",your-first-nlp-competition-submission.ipynb
Number of words in a tweet,"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
tweet_len=train_data[train_data['target']==1]['text'].str.split().map(lambda x: len(x))
ax1.hist(tweet_len,color='blue')
ax1.set_title('disaster tweets')
tweet_len=train_data[train_data['target']==0]['text'].str.split().map(lambda x: len(x))
ax2.hist(tweet_len,color='CRIMSON')
ax2.set_title('Not disaster tweets')
fig.suptitle('Words in a tweet')
plt.show()
",your-first-nlp-competition-submission.ipynb
Average word length in a tweet,"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
word=train_data[train_data['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')
ax1.set_title('disaster')
word=train_data[train_data['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')
ax2.set_title('Not disaster')
fig.suptitle('Average word length in each tweet')",your-first-nlp-competition-submission.ipynb
Frequencies Now we want to count the frequency of each word in our corpus.,"corpus=[]
 
for x in train_data['text'].str.split():
 for i in x:
 corpus.append(i)
",your-first-nlp-competition-submission.ipynb
the output is hidden.,"dic=defaultdict(int)
dic=defaultdict(int)
for word in corpus:
 if word not in stop:
 dic[word]+=1

top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:30] 
 


x,y=zip(*top)
plt.rcParams[""figure.figsize""] = (20,10)
plt.bar(x,y , color ='red')",your-first-nlp-competition-submission.ipynb
"So in case you are using plt.plot for example, you can set a tuple with width and height.","plt.bar(x , y , color = 'green') ",your-first-nlp-competition-submission.ipynb
Let s take a look to the punctuations in our tweets : ,"plt.figure(figsize=(10,5))
import string
dic=defaultdict(int)
special = string.punctuation
for i in (corpus):
 if i in special:
 dic[i]+=1
 
x,y=zip(*dic.items())
plt.barh(x,y ,color = 'purple')",your-first-nlp-competition-submission.ipynb
In details about each target ,"from collections import Counter

words = train_data[train_data.target==0].text.apply(lambda x: [word.lower() for word in x.split()])
h_words = Counter()

for text_ in words:
 h_words.update(text_)
 
print(h_words.most_common(50))",your-first-nlp-competition-submission.ipynb
"Missing Values Both training and test set have same ratio of missing values in keyword and location.0.8 of keyword is missing in both training and test set 33 of location is missing in both training and test set Since missing value ratios between training and test set are too close, they are most probably taken from the same sample. Missing values in those features are filled with no keyword and no location respectively.","missing_cols = ['keyword', 'location']

fig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)

sns.barplot(x=train_data[missing_cols].isnull().sum().index, y=train_data[missing_cols].isnull().sum().values, ax=axes[0])
sns.barplot(x=train_data[missing_cols].isnull().sum().index, y=train_data[missing_cols].isnull().sum().values, ax=axes[1])

axes[0].set_ylabel('Missing Value Count', size=15, labelpad=20)
axes[0].tick_params(axis='x', labelsize=15)
axes[0].tick_params(axis='y', labelsize=15)
axes[1].tick_params(axis='x', labelsize=15)
axes[1].tick_params(axis='y', labelsize=15)

axes[0].set_title('Training Set', fontsize=13)
axes[1].set_title('Test Set', fontsize=13)

plt.show()


for df in [train_data, test_data]:
 for col in ['keyword','location']:
 df[col] = df[col].fillna(f'no_{col}')
",your-first-nlp-competition-submission.ipynb
"Cardinality and Target Distribution Locations are not automatically generated, they are user inputs. That s why location is very dirty and there are too many unique values in it. It shouldn t be used as a feature.Fortunately, there is signal in keyword because some of those words can only be used in one context. Keywords have very different tweet counts and target means. keyword can be used as a feature by itself or as a word added to the text. Every single keyword in training set exists in test set. If training and test set are from the same sample, it is also possible to use target encoding on keyword.","print(f'Number of unique values in keyword = {train_data[""keyword""].nunique()} (Training) - {test_data[""keyword""].nunique()} (Test)')
print(f'Number of unique values in location = {train_data[""location""].nunique()} (Training) - {test_data[""location""].nunique()} (Test)')",your-first-nlp-competition-submission.ipynb
"The regular expressionabove is meant to find any four digits at the beginning of a string, which suffices for our case. The above is a raw string meaning that a backslash is no longer an escape character , which is standard practice with regular expressions. regex r d 4 ","def clean_text(df , text_field , new_text_field_name): ",your-first-nlp-competition-submission.ipynb
Convert strings in the Series Index to lowercase., df[new_text_field_name]= df[text_field]. str.lower () ,your-first-nlp-competition-submission.ipynb
remove numbers," df[new_text_field_name]= df[new_text_field_name]. apply(lambda elem : re.sub(r""\d+"" , """" , elem)) ",your-first-nlp-competition-submission.ipynb
remove url," df[new_text_field_name]= df[new_text_field_name]. apply(lambda elem : re.sub(r""https?://\S+|www\.\S+"" , """" , elem)) ",your-first-nlp-competition-submission.ipynb
remove HTML tags," df[new_text_field_name]= df[new_text_field_name]. apply(lambda elem : re.sub(r""<.*?>"" , """" , elem)) ",your-first-nlp-competition-submission.ipynb
"What are Stop words?Stop Words: A stop word is a commonly used word such as the , a , an , in that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.We would not want these words to take up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to stop words. NLTK Natural Language Toolkit in python has a list of stopwords stored in 16 different languages. You can find them in the nltk data directory. home pratima nltk data corpora stopwords is the directory address. Do not forget to change your home directory name ","from nltk.corpus import stopwords
stop = stopwords.words('english')
data_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
data_clean.head()",your-first-nlp-competition-submission.ipynb
"Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph","
from nltk.tokenize import sent_tokenize, word_tokenize
data_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))
data_clean.head()",your-first-nlp-competition-submission.ipynb
Stemming,import nltk ,your-first-nlp-competition-submission.ipynb
Lemmatization,import nltk ,your-first-nlp-competition-submission.ipynb
Lemmatization ,"
from nltk.stem import WordNetLemmatizer
def word_lemmatizer(text):
 lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]
 return lem_text
data_clean['text_clean_tokens'] = data_clean['text_tokens'].apply(lambda x: word_lemmatizer(x))
data_clean.head()",your-first-nlp-competition-submission.ipynb
Splitting the data ,"X_train, X_test, Y_train, Y_test = train_test_split(data_clean['text_clean'], 
 
 data_clean['target'], 
 test_size = 0.2,
 random_state = 10)",your-first-nlp-competition-submission.ipynb
CountVectorizerCountVectorizer converts a collection of text documents to a matrix of token counts: the occurrences of tokens in each document. This implementation produces a sparse representation of the counts.,"vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))
vectorized = vectorizer.fit_transform(X_train)
pd.DataFrame(vectorized.toarray(), 
 index=['sentence '+str(i) 
 for i in range(1, 1+len(X_train))],
 columns=vectorizer.get_feature_names())",your-first-nlp-competition-submission.ipynb
Tfidftransformer ,"from sklearn.feature_extraction.text import (CountVectorizer, 
 TfidfVectorizer,
 TfidfTransformer)
vectorizer = CountVectorizer(analyzer='word', 
 token_pattern=r'\b[a-zA-Z]{3,}\b', 
 ngram_range=(1, 1) 
 ) 
count_vectorized = vectorizer.fit_transform(X_train)
tfidf = TfidfTransformer(smooth_idf=True, use_idf=True)
train_features = tfidf.fit_transform(count_vectorized).toarray()

pd.DataFrame(train_features, 
 index=['sentence '+str(i) 
 for i in range(1, 1+len(X_train))],
 columns=vectorizer.get_feature_names())",your-first-nlp-competition-submission.ipynb
Convert a collection of text documents to a matrix of token counts,train_features = tfidf.fit_transform(X_train).toarray(),your-first-nlp-competition-submission.ipynb
"In order to know the position of a certain word, we can look it up in the vocabulary:","word = ""forest"" ",your-first-nlp-competition-submission.ipynb
meaning that the Nth feature of the matrix is that word.,"test_features = tfidf.transform(X_test).toarray()
print(test_features.shape)",your-first-nlp-competition-submission.ipynb
"in order to reduce the dimensionality of our matrix ! Feature matrix shape: Number of documents x Length of vocabulary we can carry out some Feature Selection, the process of selecting a subset of relevant variables. I will proceed as follows:treat each category as binary for example, the Tech category is 1 for the Tech news and 0 for the others perform a Chi Square test to determine whether a feature and the binary target are independent keep only the features with a certain p value from the Chi Square test.","""""""from sklearn import feature_selection 
y = data_clean['target']
X_names = tfidf.get_feature_names()
p_value_limit = 0.95
dtf_features = pd.DataFrame()
for cat in np.unique(y):
 chi2, p = feature_selection.chi2(X_train, y==cat)
 dtf_features = dtf_features.append(pd.DataFrame(
 {""feature"":X_names, ""score"":1-p, ""y"":cat}))
 dtf_features = dtf_features.sort_values([""y"",""score""], 
 ascending=[True,False])
 dtf_features = dtf_features[dtf_features[""score""]>p_value_limit]
X_names = dtf_features[""feature""].unique().tolist()
len(X_names)""""""",your-first-nlp-competition-submission.ipynb
Model : Multinomial NB ,"import pandas as pd
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score",your-first-nlp-competition-submission.ipynb
Visualizing scikit model performance,"training_accuracy = accuracy_score(train_labels, mnb_classifier.predict(train_features))
print(training_accuracy)",your-first-nlp-competition-submission.ipynb
Fitting the Test data for submission,test_vectorizer =tfidf.transform( data_clean_test['text_clean']).toarray(),your-first-nlp-competition-submission.ipynb
