{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from transformers import PLBartForConditionalGeneration, PLBartTokenizer\n",
    "from transformers import TrainingArguments,Trainer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_1 = \"../data/data_pairs_1.csv\"\n",
    "DATA_PATH_2 = \"../data/data_pairs_2.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>markdown</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Prices: Advanced Regression Techniques Predict sales prices and practice feature engineering, RFs, and gradient boosting Weekly updateCompetition DescriptionAsk a home buyer to describe their dream house, and they probably won t begin with the height of the basement ceiling or the proximity to an east west railroad. But this playground competition s dataset proves that much more influences price negotiations than the number of bedrooms or a white picket fence.With 79 explanatory variables describing almost every aspect of residential homes in Ames, Iowa, this competition challenges y...</td>\n",
       "      <td>import numpy as np</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data processing, CSV file I O e.g. pd.read csv</td>\n",
       "      <td>import pandas as pd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Input data files are available in the .. input directory. For example, running this by clicking run or pressing Shift Enter will list the files in the input directory</td>\n",
       "      <td>from datetime import datetime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>for some statistics</td>\n",
       "      <td>from scipy.stats import skew  from scipy.special import boxcox1p  from scipy.stats import boxcox_normmax  from sklearn.linear_model import ElasticNetCV , LassoCV , RidgeCV  from sklearn.ensemble import GradientBoostingRegressor  from sklearn.svm import SVR  from sklearn.pipeline import make_pipeline  from sklearn.preprocessing import RobustScaler  from sklearn.model_selection import KFold , cross_val_score  from sklearn.metrics import mean_squared_error  from mlxtend.regressor import StackingCVRegressor  from xgboost import XGBRegressor  from lightgbm import LGBMRegressor  import matplotli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Any results you write to the current directory are saved as output. Load data</td>\n",
       "      <td>train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\\r\\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\\r\\nprint (\"Data is loaded!\") print (\"Train: \",train.shape[0],\"sales, and \",train.shape[1],\"features\")\\r\\nprint (\"Test: \",test.shape[0],\"sales, and \",test.shape[1],\"features\") train.head() test.head()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7567</th>\n",
       "      <td>If the word exists in the vocabulary, this command prints a number N, meaning that the Nth feature of the matrix is that word.</td>\n",
       "      <td>test_features = tfidf.transform(X_test).toarray()\\r\\nprint(test_features.shape) train_labels = Y_train\\r\\ntest_labels = Y_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7568</th>\n",
       "      <td>reduce the matrix dimensionality  in order to reduce the dimensionality of our matrix ! Feature matrix shape: Number of documents x Length of vocabulary we can carry out some Feature Selection, the process of selecting a subset of relevant variables. I will proceed as follows:treat each category as binary for example, the Tech category is 1 for the Tech news and 0 for the others perform a Chi Square test to determine whether a feature and the binary target are independent keep only the features with a certain p value from the Chi Square test.</td>\n",
       "      <td>\"\"\"from sklearn import feature_selection \\r\\ny = data_clean['target']\\r\\nX_names = tfidf.get_feature_names()\\r\\np_value_limit = 0.95\\r\\ndtf_features = pd.DataFrame()\\r\\nfor cat in np.unique(y):\\r\\n chi2, p = feature_selection.chi2(X_train, y==cat)\\r\\n dtf_features = dtf_features.append(pd.DataFrame(\\r\\n {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\\r\\n dtf_features = dtf_features.sort_values([\"y\",\"score\"], \\r\\n ascending=[True,False])\\r\\n dtf_features = dtf_features[dtf_features[\"score\"]&gt;p_value_limit]\\r\\nX_names = dtf_features[\"feature\"].unique().tolist()\\r\\nlen(X_names)\"\"\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7569</th>\n",
       "      <td>Model training  Model : Multinomial NB</td>\n",
       "      <td>import pandas as pd\\r\\nfrom sklearn.naive_bayes import MultinomialNB\\r\\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score mnb_classifier = MultinomialNB() mnb_classifier.fit(train_features,train_labels) mnb_prediction = mnb_classifier.predict(test_features)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7570</th>\n",
       "      <td>Visualizing scikit model performance</td>\n",
       "      <td>training_accuracy = accuracy_score(train_labels, mnb_classifier.predict(train_features))\\r\\nprint(training_accuracy) testing_accuracy = accuracy_score(test_labels, mnb_prediction)\\r\\nprint(testing_accuracy) print(classification_report(test_labels, mnb_prediction)) conf_matrix = confusion_matrix(test_labels, mnb_prediction)\\r\\nprint(conf_matrix) import seaborn as sns\\r\\nsns.heatmap(conf_matrix/np.sum(conf_matrix),annot=True, fmt='.2%', cmap='Blues')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7571</th>\n",
       "      <td>from sklearn.neighbors import KNeighborsClassifier knn KNeighborsClassifier .fit train features, train labels  predicts knn.predict test features  print classification report test labels, predicts  Fitting the Test data for submission</td>\n",
       "      <td>test_vectorizer =tfidf.transform( data_clean_test['text_clean']).toarray() test_vectorizer.shape final_predictions = mnb_classifier.predict(test_vectorizer) final_predictions submission_df = pd.DataFrame() submission_df['id'] = data_clean_test['id']\\r\\nsubmission_df['target'] = final_predictions submission_df submission_df['target'].value_counts() submission = submission_df.to_csv('Result.csv',index = False)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     markdown  \\\n",
       "0     House Prices: Advanced Regression Techniques Predict sales prices and practice feature engineering, RFs, and gradient boosting Weekly updateCompetition DescriptionAsk a home buyer to describe their dream house, and they probably won t begin with the height of the basement ceiling or the proximity to an east west railroad. But this playground competition s dataset proves that much more influences price negotiations than the number of bedrooms or a white picket fence.With 79 explanatory variables describing almost every aspect of residential homes in Ames, Iowa, this competition challenges y...   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            data processing, CSV file I O e.g. pd.read csv     \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                     Input data files are available in the .. input directory. For example, running this by clicking run or pressing Shift Enter will list the files in the input directory    \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        for some statistics    \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Any results you write to the current directory are saved as output. Load data    \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n",
       "7567                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          If the word exists in the vocabulary, this command prints a number N, meaning that the Nth feature of the matrix is that word.    \n",
       "7568                                                    reduce the matrix dimensionality  in order to reduce the dimensionality of our matrix ! Feature matrix shape: Number of documents x Length of vocabulary we can carry out some Feature Selection, the process of selecting a subset of relevant variables. I will proceed as follows:treat each category as binary for example, the Tech category is 1 for the Tech news and 0 for the others perform a Chi Square test to determine whether a feature and the binary target are independent keep only the features with a certain p value from the Chi Square test.    \n",
       "7569                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Model training  Model : Multinomial NB     \n",
       "7570                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Visualizing scikit model performance    \n",
       "7571                                                                                                                                                                                                                                                                                                                                                                              from sklearn.neighbors import KNeighborsClassifier knn KNeighborsClassifier .fit train features, train labels  predicts knn.predict test features  print classification report test labels, predicts  Fitting the Test data for submission    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         code  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        import numpy as np    \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       import pandas as pd    \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             from datetime import datetime    \n",
       "3     from scipy.stats import skew  from scipy.special import boxcox1p  from scipy.stats import boxcox_normmax  from sklearn.linear_model import ElasticNetCV , LassoCV , RidgeCV  from sklearn.ensemble import GradientBoostingRegressor  from sklearn.svm import SVR  from sklearn.pipeline import make_pipeline  from sklearn.preprocessing import RobustScaler  from sklearn.model_selection import KFold , cross_val_score  from sklearn.metrics import mean_squared_error  from mlxtend.regressor import StackingCVRegressor  from xgboost import XGBRegressor  from lightgbm import LGBMRegressor  import matplotli...  \n",
       "4                                                                                                                                                                                                                                       train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\\r\\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\\r\\nprint (\"Data is loaded!\") print (\"Train: \",train.shape[0],\"sales, and \",train.shape[1],\"features\")\\r\\nprint (\"Test: \",test.shape[0],\"sales, and \",test.shape[1],\"features\") train.head() test.head()   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...  \n",
       "7567                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          test_features = tfidf.transform(X_test).toarray()\\r\\nprint(test_features.shape) train_labels = Y_train\\r\\ntest_labels = Y_test   \n",
       "7568                     \"\"\"from sklearn import feature_selection \\r\\ny = data_clean['target']\\r\\nX_names = tfidf.get_feature_names()\\r\\np_value_limit = 0.95\\r\\ndtf_features = pd.DataFrame()\\r\\nfor cat in np.unique(y):\\r\\n chi2, p = feature_selection.chi2(X_train, y==cat)\\r\\n dtf_features = dtf_features.append(pd.DataFrame(\\r\\n {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\\r\\n dtf_features = dtf_features.sort_values([\"y\",\"score\"], \\r\\n ascending=[True,False])\\r\\n dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\\r\\nX_names = dtf_features[\"feature\"].unique().tolist()\\r\\nlen(X_names)\"\"\"   \n",
       "7569                                                                                                                                                                                                                                                                                                                     import pandas as pd\\r\\nfrom sklearn.naive_bayes import MultinomialNB\\r\\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score mnb_classifier = MultinomialNB() mnb_classifier.fit(train_features,train_labels) mnb_prediction = mnb_classifier.predict(test_features)   \n",
       "7570                                                                                                                                                    training_accuracy = accuracy_score(train_labels, mnb_classifier.predict(train_features))\\r\\nprint(training_accuracy) testing_accuracy = accuracy_score(test_labels, mnb_prediction)\\r\\nprint(testing_accuracy) print(classification_report(test_labels, mnb_prediction)) conf_matrix = confusion_matrix(test_labels, mnb_prediction)\\r\\nprint(conf_matrix) import seaborn as sns\\r\\nsns.heatmap(conf_matrix/np.sum(conf_matrix),annot=True, fmt='.2%', cmap='Blues')   \n",
       "7571                                                                                                                                                                                             test_vectorizer =tfidf.transform( data_clean_test['text_clean']).toarray() test_vectorizer.shape final_predictions = mnb_classifier.predict(test_vectorizer) final_predictions submission_df = pd.DataFrame() submission_df['id'] = data_clean_test['id']\\r\\nsubmission_df['target'] = final_predictions submission_df submission_df['target'].value_counts() submission = submission_df.to_csv('Result.csv',index = False)   \n",
       "\n",
       "[7572 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH_1).drop(['title'],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the pretrained model \"PLBART\" and its tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plbarttokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-python-en_XX\", src_lang=\"python\", tgt_lang=\"en_XX\")\n",
    "plbart_model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-python-en_XX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_tokenize(data):\n",
    "    new_df = pd.DataFrame(columns=['input_ids_markdown','attention_mask_markdown','input_ids_code','attention_mask_code'])\n",
    "    for i in data.index:\n",
    "        new_df.loc[len(new_df)] = [plbarttokenizer(data.loc[i]['markdown'])['input_ids'],plbarttokenizer(data.loc[i]['markdown'])['attention_mask'],plbarttokenizer(data.loc[i]['code'])['input_ids'],plbarttokenizer(data.loc[i]['code'])['attention_mask']]\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids_markdown</th>\n",
       "      <th>attention_mask_markdown</th>\n",
       "      <th>input_ids_code</th>\n",
       "      <th>attention_mask_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[26343, 5544, 1118, 33475, 18938, 30816, 21498, 15521, 15485, 573, 13333, 18047, 135, 6387, 2118, 3478, 16069, 33463, 246, 12544, 33463, 135, 8169, 12380, 45, 20077, 185, 915, 2489, 13559, 9427, 32187, 14, 5072, 15328, 10, 71, 6734, 2614, 36, 562, 15476, 33463, 135, 1534, 2925, 2482, 7, 2995, 215, 57, 1636, 153, 57, 1314, 228, 33298, 255, 57, 23359, 30784, 71, 197, 21476, 23965, 622, 89, 59, 110, 33455, 1160, 143, 1772, 1550, 718, 13559, 11, 4092, 1426, 29, 208, 2449, 1010, 3134, 7191, 2210, 4530, 8789, 97, 15529, 1433, 57, 1019, 153, 25397, 27152, 255, 14, 5890, 3366, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "      <td>[662, 2988, 268, 865, 2, 50002]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[272, 5037, 33463, 6423, 375, 34, 261, 168, 33455, 33461, 33455, 3880, 33455, 474, 3771, 2, 50002]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[662, 4112, 268, 3880, 2, 50002]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2782, 272, 1139, 395, 2519, 55, 57, 1269, 641, 2139, 33455, 1276, 812, 33463, 1826, 143, 389, 6148, 521, 255, 14630, 23549, 7645, 481, 410, 57, 1139, 55, 57, 641, 2139, 2, 50002]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[320, 2220, 662, 2220, 2, 50002]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[126, 632, 11182, 2, 50002]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[320, 9021, 33455, 6300, 662, 24130, 320, 9021, 33455, 11716, 662, 2534, 33449, 1268, 33485, 33453, 320, 9021, 33455, 6300, 662, 2534, 33449, 1268, 33456, 8717, 2411, 320, 13446, 33455, 11244, 33456, 1856, 662, 20642, 7760, 8665, 16, 282, 19168, 8665, 16, 246, 33340, 8665, 320, 13446, 33455, 2682, 33457, 136, 662, 19077, 20064, 45, 22709, 320, 13446, 33455, 28146, 662, 6737, 33487, 320, 13446, 33455, 15645, 662, 797, 33456, 15645, 320, 13446, 33455, 1800, 12626, 662, 12054, 347, 28267, 320, 13446, 33455, 1856, 33456, 10086, 662, 1202, 18639, 16, 5542, 33456, 465, 33456, 5034, 320, 13446, 3...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1916, 1487, 144, 661, 71, 57, 759, 2139, 395, 3410, 268, 699, 33455, 5012, 272, 2, 50002]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[3370, 24, 3880, 33455, 474, 33456, 4424, 27716, 1216, 33488, 9118, 33489, 32425, 33489, 26087, 33489, 24018, 33489, 28579, 15521, 33488, 5259, 33455, 4424, 1156, 242, 24, 3880, 33455, 474, 33456, 4424, 27716, 1216, 33488, 9118, 33489, 32425, 33489, 26087, 33489, 24018, 33489, 28579, 15521, 33488, 953, 33455, 4424, 1156, 597, 6368, 588, 96, 3331, 23669, 597, 6368, 22012, 33475, 8248, 5259, 33455, 2788, 9892, 33474, 21167, 33463, 135, 8248, 5259, 33455, 2788, 13911, 33474, 6160, 1191, 597, 6368, 866, 33475, 8248, 953, 33455, 2788, 9892, 33474, 21167, 33463, 135, 8248, 953, 33455, 2788, 1391...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7567</th>\n",
       "      <td>[708, 57, 1677, 2189, 55, 57, 18686, 33463, 143, 1130, 6078, 14, 1019, 171, 33463, 7464, 208, 57, 171, 155, 2118, 153, 57, 3767, 96, 208, 1677, 33455, 2, 50002]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[242, 33456, 6160, 24, 2730, 20667, 33455, 5427, 33460, 33522, 33456, 953, 426, 545, 1701, 292, 597, 33460, 953, 33456, 6160, 33455, 2788, 33459, 3370, 33456, 6228, 24, 460, 33456, 5259, 242, 33456, 6228, 24, 460, 33456, 953, 2, 50002]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7568</th>\n",
       "      <td>[4888, 57, 3767, 6306, 4075, 55, 1193, 71, 4888, 57, 6306, 4075, 153, 4057, 3767, 340, 8930, 3767, 1945, 33475, 3637, 153, 7868, 309, 10151, 153, 18686, 1124, 252, 17458, 330, 632, 8930, 11869, 33463, 57, 1034, 153, 11079, 14, 9334, 153, 6493, 2177, 33455, 34, 481, 10201, 268, 3959, 33475, 33440, 12421, 1022, 3346, 268, 3654, 126, 812, 33463, 57, 108, 8637, 3346, 96, 124, 126, 57, 108, 8637, 11520, 135, 142, 126, 57, 5584, 2272, 14, 946, 33443, 21613, 242, 71, 5884, 3978, 14, 2118, 135, 57, 3654, 1002, 395, 12835, 2110, 732, 57, 3993, 215, 14, 3907, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "      <td>[2133, 1721, 13446, 662, 2118, 33456, 10086, 123, 24, 272, 33456, 6778, 1620, 3143, 1211, 764, 33456, 2322, 24, 2730, 20667, 33455, 300, 33456, 4418, 33456, 2322, 292, 31, 33456, 854, 33456, 6060, 24, 28850, 3531, 33458, 33456, 6160, 24, 3880, 33455, 8235, 292, 126, 5905, 55, 865, 33455, 8175, 33460, 33464, 988, 25780, 1819, 31, 24, 2118, 33456, 10086, 33455, 30856, 12102, 33522, 33456, 5259, 33463, 123, 1601, 2093, 33459, 3531, 33458, 33456, 6160, 24, 3531, 33458, 33456, 6160, 33455, 2688, 33460, 468, 33455, 8235, 33460, 5601, 4418, 907, 33522, 33456, 2322, 33463, 40, 5034, 907, 13045, 33...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7569</th>\n",
       "      <td>[3051, 6959, 3051, 54, 7510, 27603, 23987, 2, 50002]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[662, 4112, 268, 3880, 320, 13446, 33455, 32485, 33456, 33465, 26021, 662, 7510, 27603, 26153, 320, 13446, 33455, 13810, 662, 14311, 33456, 6767, 33463, 14303, 33456, 5744, 33463, 10603, 33456, 5034, 53, 7850, 33456, 22804, 24, 7510, 27603, 26153, 292, 53, 7850, 33456, 22804, 33455, 5560, 33460, 5259, 33456, 6160, 33463, 5259, 33456, 6228, 33459, 53, 7850, 33456, 27850, 24, 53, 7850, 33456, 22804, 33455, 12026, 33460, 953, 33456, 6160, 33459, 2, 50002]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7570</th>\n",
       "      <td>[9475, 6420, 20052, 826, 4591, 2, 50002]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[6959, 33456, 21042, 24, 10603, 33456, 5034, 33460, 5259, 33456, 6228, 33463, 53, 7850, 33456, 22804, 33455, 12026, 33460, 5259, 33456, 6160, 1580, 597, 33460, 13621, 33456, 21042, 33459, 3854, 33456, 21042, 24, 10603, 33456, 5034, 33460, 953, 33456, 6228, 33463, 53, 7850, 33456, 27850, 33459, 597, 33460, 13917, 33456, 21042, 33459, 597, 33460, 26385, 33456, 6767, 33460, 953, 33456, 6228, 33463, 53, 7850, 33456, 27850, 1580, 1463, 33456, 5744, 24, 14303, 33456, 5744, 33460, 953, 33456, 6228, 33463, 53, 7850, 33456, 27850, 33459, 597, 33460, 3905, 33456, 5744, 33459, 662, 26966, 268, 23540,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7571</th>\n",
       "      <td>[320, 13446, 33455, 17339, 662, 1202, 26672, 11175, 354, 6896, 1202, 26672, 11175, 9, 5560, 3370, 3993, 33463, 3370, 3470, 6197, 33442, 354, 6896, 33455, 12026, 242, 3993, 597, 14311, 2723, 242, 3470, 33463, 6197, 33442, 24986, 567, 57, 749, 272, 126, 10436, 2, 50002]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[242, 33456, 8164, 2701, 24, 5348, 20667, 33455, 5427, 33460, 272, 33456, 6778, 33456, 953, 1620, 715, 33456, 6778, 22080, 545, 1701, 292, 242, 33456, 8164, 2701, 33455, 2788, 405, 33456, 20031, 24, 53, 7850, 33456, 22804, 33455, 12026, 33460, 953, 33456, 8164, 2701, 33459, 405, 33456, 20031, 10436, 33456, 1616, 24, 3880, 33455, 8235, 292, 10436, 33456, 1616, 1620, 82, 1211, 24, 272, 33456, 6778, 33456, 953, 1620, 82, 1211, 10436, 33456, 1616, 1620, 3143, 1211, 24, 405, 33456, 20031, 10436, 33456, 1616, 10436, 33456, 1616, 1620, 3143, 6526, 854, 33456, 10826, 292, 10436, 24, 10436, 33456, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7572 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        input_ids_markdown  \\\n",
       "0     [26343, 5544, 1118, 33475, 18938, 30816, 21498, 15521, 15485, 573, 13333, 18047, 135, 6387, 2118, 3478, 16069, 33463, 246, 12544, 33463, 135, 8169, 12380, 45, 20077, 185, 915, 2489, 13559, 9427, 32187, 14, 5072, 15328, 10, 71, 6734, 2614, 36, 562, 15476, 33463, 135, 1534, 2925, 2482, 7, 2995, 215, 57, 1636, 153, 57, 1314, 228, 33298, 255, 57, 23359, 30784, 71, 197, 21476, 23965, 622, 89, 59, 110, 33455, 1160, 143, 1772, 1550, 718, 13559, 11, 4092, 1426, 29, 208, 2449, 1010, 3134, 7191, 2210, 4530, 8789, 97, 15529, 1433, 57, 1019, 153, 25397, 27152, 255, 14, 5890, 3366, ...]   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [272, 5037, 33463, 6423, 375, 34, 261, 168, 33455, 33461, 33455, 3880, 33455, 474, 3771, 2, 50002]   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                      [2782, 272, 1139, 395, 2519, 55, 57, 1269, 641, 2139, 33455, 1276, 812, 33463, 1826, 143, 389, 6148, 521, 255, 14630, 23549, 7645, 481, 410, 57, 1139, 55, 57, 641, 2139, 2, 50002]   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              [126, 632, 11182, 2, 50002]   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [1916, 1487, 144, 661, 71, 57, 759, 2139, 395, 3410, 268, 699, 33455, 5012, 272, 2, 50002]   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ...   \n",
       "7567                                                                                                                                                                                                                                                                                                                                                                                                                                      [708, 57, 1677, 2189, 55, 57, 18686, 33463, 143, 1130, 6078, 14, 1019, 171, 33463, 7464, 208, 57, 171, 155, 2118, 153, 57, 3767, 96, 208, 1677, 33455, 2, 50002]   \n",
       "7568                      [4888, 57, 3767, 6306, 4075, 55, 1193, 71, 4888, 57, 6306, 4075, 153, 4057, 3767, 340, 8930, 3767, 1945, 33475, 3637, 153, 7868, 309, 10151, 153, 18686, 1124, 252, 17458, 330, 632, 8930, 11869, 33463, 57, 1034, 153, 11079, 14, 9334, 153, 6493, 2177, 33455, 34, 481, 10201, 268, 3959, 33475, 33440, 12421, 1022, 3346, 268, 3654, 126, 812, 33463, 57, 108, 8637, 3346, 96, 124, 126, 57, 108, 8637, 11520, 135, 142, 126, 57, 5584, 2272, 14, 946, 33443, 21613, 242, 71, 5884, 3978, 14, 2118, 135, 57, 3654, 1002, 395, 12835, 2110, 732, 57, 3993, 215, 14, 3907, ...]   \n",
       "7569                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [3051, 6959, 3051, 54, 7510, 27603, 23987, 2, 50002]   \n",
       "7570                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              [9475, 6420, 20052, 826, 4591, 2, 50002]   \n",
       "7571                                                                                                                                                                                                                                                                                                                          [320, 13446, 33455, 17339, 662, 1202, 26672, 11175, 354, 6896, 1202, 26672, 11175, 9, 5560, 3370, 3993, 33463, 3370, 3470, 6197, 33442, 354, 6896, 33455, 12026, 242, 3993, 597, 14311, 2723, 242, 3470, 33463, 6197, 33442, 24986, 567, 57, 749, 272, 126, 10436, 2, 50002]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                attention_mask_markdown  \\\n",
       "0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]   \n",
       "1                                                                                                                                                                                                                                                                   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "2                                                                                                                                                                                                                   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "3                                                                                                                                                                                                                                                                                                       [1, 1, 1, 1, 1]   \n",
       "4                                                                                                                                                                                                                                                                   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "...                                                                                                                                                                                                                                                                                                                 ...   \n",
       "7567                                                                                                                                                                                                                         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "7568  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]   \n",
       "7569                                                                                                                                                                                                                                                                                        [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "7570                                                                                                                                                                                                                                                                                              [1, 1, 1, 1, 1, 1, 1]   \n",
       "7571                                                                                                                                                                            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               input_ids_code  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [662, 2988, 268, 865, 2, 50002]   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [662, 4112, 268, 3880, 2, 50002]   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [320, 2220, 662, 2220, 2, 50002]   \n",
       "3     [320, 9021, 33455, 6300, 662, 24130, 320, 9021, 33455, 11716, 662, 2534, 33449, 1268, 33485, 33453, 320, 9021, 33455, 6300, 662, 2534, 33449, 1268, 33456, 8717, 2411, 320, 13446, 33455, 11244, 33456, 1856, 662, 20642, 7760, 8665, 16, 282, 19168, 8665, 16, 246, 33340, 8665, 320, 13446, 33455, 2682, 33457, 136, 662, 19077, 20064, 45, 22709, 320, 13446, 33455, 28146, 662, 6737, 33487, 320, 13446, 33455, 15645, 662, 797, 33456, 15645, 320, 13446, 33455, 1800, 12626, 662, 12054, 347, 28267, 320, 13446, 33455, 1856, 33456, 10086, 662, 1202, 18639, 16, 5542, 33456, 465, 33456, 5034, 320, 13446, 3...   \n",
       "4     [3370, 24, 3880, 33455, 474, 33456, 4424, 27716, 1216, 33488, 9118, 33489, 32425, 33489, 26087, 33489, 24018, 33489, 28579, 15521, 33488, 5259, 33455, 4424, 1156, 242, 24, 3880, 33455, 474, 33456, 4424, 27716, 1216, 33488, 9118, 33489, 32425, 33489, 26087, 33489, 24018, 33489, 28579, 15521, 33488, 953, 33455, 4424, 1156, 597, 6368, 588, 96, 3331, 23669, 597, 6368, 22012, 33475, 8248, 5259, 33455, 2788, 9892, 33474, 21167, 33463, 135, 8248, 5259, 33455, 2788, 13911, 33474, 6160, 1191, 597, 6368, 866, 33475, 8248, 953, 33455, 2788, 9892, 33474, 21167, 33463, 135, 8248, 953, 33455, 2788, 1391...   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n",
       "7567                                                                                                                                                                                                                                                                                                                                                                              [242, 33456, 6160, 24, 2730, 20667, 33455, 5427, 33460, 33522, 33456, 953, 426, 545, 1701, 292, 597, 33460, 953, 33456, 6160, 33455, 2788, 33459, 3370, 33456, 6228, 24, 460, 33456, 5259, 242, 33456, 6228, 24, 460, 33456, 953, 2, 50002]   \n",
       "7568  [2133, 1721, 13446, 662, 2118, 33456, 10086, 123, 24, 272, 33456, 6778, 1620, 3143, 1211, 764, 33456, 2322, 24, 2730, 20667, 33455, 300, 33456, 4418, 33456, 2322, 292, 31, 33456, 854, 33456, 6060, 24, 28850, 3531, 33458, 33456, 6160, 24, 3880, 33455, 8235, 292, 126, 5905, 55, 865, 33455, 8175, 33460, 33464, 988, 25780, 1819, 31, 24, 2118, 33456, 10086, 33455, 30856, 12102, 33522, 33456, 5259, 33463, 123, 1601, 2093, 33459, 3531, 33458, 33456, 6160, 24, 3531, 33458, 33456, 6160, 33455, 2688, 33460, 468, 33455, 8235, 33460, 5601, 4418, 907, 33522, 33456, 2322, 33463, 40, 5034, 907, 13045, 33...   \n",
       "7569                                                                                                                                                 [662, 4112, 268, 3880, 320, 13446, 33455, 32485, 33456, 33465, 26021, 662, 7510, 27603, 26153, 320, 13446, 33455, 13810, 662, 14311, 33456, 6767, 33463, 14303, 33456, 5744, 33463, 10603, 33456, 5034, 53, 7850, 33456, 22804, 24, 7510, 27603, 26153, 292, 53, 7850, 33456, 22804, 33455, 5560, 33460, 5259, 33456, 6160, 33463, 5259, 33456, 6228, 33459, 53, 7850, 33456, 27850, 24, 53, 7850, 33456, 22804, 33455, 12026, 33460, 953, 33456, 6160, 33459, 2, 50002]   \n",
       "7570  [6959, 33456, 21042, 24, 10603, 33456, 5034, 33460, 5259, 33456, 6228, 33463, 53, 7850, 33456, 22804, 33455, 12026, 33460, 5259, 33456, 6160, 1580, 597, 33460, 13621, 33456, 21042, 33459, 3854, 33456, 21042, 24, 10603, 33456, 5034, 33460, 953, 33456, 6228, 33463, 53, 7850, 33456, 27850, 33459, 597, 33460, 13917, 33456, 21042, 33459, 597, 33460, 26385, 33456, 6767, 33460, 953, 33456, 6228, 33463, 53, 7850, 33456, 27850, 1580, 1463, 33456, 5744, 24, 14303, 33456, 5744, 33460, 953, 33456, 6228, 33463, 53, 7850, 33456, 27850, 33459, 597, 33460, 3905, 33456, 5744, 33459, 662, 26966, 268, 23540,...   \n",
       "7571  [242, 33456, 8164, 2701, 24, 5348, 20667, 33455, 5427, 33460, 272, 33456, 6778, 33456, 953, 1620, 715, 33456, 6778, 22080, 545, 1701, 292, 242, 33456, 8164, 2701, 33455, 2788, 405, 33456, 20031, 24, 53, 7850, 33456, 22804, 33455, 12026, 33460, 953, 33456, 8164, 2701, 33459, 405, 33456, 20031, 10436, 33456, 1616, 24, 3880, 33455, 8235, 292, 10436, 33456, 1616, 1620, 82, 1211, 24, 272, 33456, 6778, 33456, 953, 1620, 82, 1211, 10436, 33456, 1616, 1620, 3143, 1211, 24, 405, 33456, 20031, 10436, 33456, 1616, 10436, 33456, 1616, 1620, 3143, 6526, 854, 33456, 10826, 292, 10436, 24, 10436, 33456, ...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    attention_mask_code  \n",
       "0                                                                                                                                                                                                                                                                                                    [1, 1, 1, 1, 1, 1]  \n",
       "1                                                                                                                                                                                                                                                                                                    [1, 1, 1, 1, 1, 1]  \n",
       "2                                                                                                                                                                                                                                                                                                    [1, 1, 1, 1, 1, 1]  \n",
       "3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]  \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]  \n",
       "...                                                                                                                                                                                                                                                                                                                 ...  \n",
       "7567                                                                                                                                                                                           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "7568  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]  \n",
       "7569                                                                                        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "7570  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]  \n",
       "7571  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]  \n",
       "\n",
       "[7572 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_1 = fn_tokenize(df_1)\n",
    "new_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_1 = new_df_1.sample(frac = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_1[5600:].to_csv(\"../data/test_tokenized_data_1.csv\",index=False)\n",
    "new_df_1[:5600].to_csv(\"../data/train_tokenized_data_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_1.to_csv(\"../data/tokenized_data_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-07e9f75700f59e54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\zdahmani\\.cache\\huggingface\\datasets\\csv\\default-07e9f75700f59e54\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b84f34a164b4e70a7b4ef143b4d6c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356fc3ae921949299c4e382a4ce5d993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203b1282ae324219a14a0d2584ed9c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b6d4d60b8947d5b7a1e75a5463157d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\zdahmani\\.cache\\huggingface\\datasets\\csv\\default-07e9f75700f59e54\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8881752cc0014358b3c90c4c19ebe049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_1 =  load_dataset(\"csv\", data_files={\"train\": [\"../data/train_tokenized_data_1.csv\"], \"test\": \"../data/test_tokenized_data_1.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\zdahmani\\.cache\\huggingface\\datasets\\csv\\default-07e9f75700f59e54\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\\cache-f59cb5662487984f.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\zdahmani\\.cache\\huggingface\\datasets\\csv\\default-07e9f75700f59e54\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\\cache-130bd719c2a31e29.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = dataset_1[\"train\"].shuffle(seed=42)\n",
    "small_eval_dataset = dataset_1[\"test\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=plbart_model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "     #eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `PLBartForConditionalGeneration.forward` and have been ignored: attention_mask_markdown, input_ids_code, attention_mask_code, input_ids_markdown. If attention_mask_markdown, input_ids_code, attention_mask_code, input_ids_markdown are not expected by `PLBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\zdahmani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5600\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe9208fc07f4ed996d2ed99b601600c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zdahmani\\Documents\\Documentation_generator_project\\NotebookCodeGen\\DocGen_Task\\notebooks\\07-finetune_plbart.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zdahmani/Documents/Documentation_generator_project/NotebookCodeGen/DocGen_Task/notebooks/07-finetune_plbart.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\zdahmani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1498\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1495\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1497\u001b[0m )\n\u001b[1;32m-> 1498\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1499\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1500\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1501\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1502\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1503\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zdahmani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1714\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[0;32m   1713\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1714\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1715\u001b[0m \n\u001b[0;32m   1716\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[0;32m   1717\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1718\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\zdahmani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\zdahmani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\zdahmani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\zdahmani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\zdahmani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:2165\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2164\u001b[0m     \u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2165\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(\n\u001b[0;32m   2166\u001b[0m         key,\n\u001b[0;32m   2167\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\zdahmani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:2149\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[0;32m   2147\u001b[0m format_kwargs \u001b[39m=\u001b[39m format_kwargs \u001b[39mif\u001b[39;00m format_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m   2148\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures, decoded\u001b[39m=\u001b[39mdecoded, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[1;32m-> 2149\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, key, indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m   2150\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[0;32m   2151\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39mformatter, format_columns\u001b[39m=\u001b[39mformat_columns, output_all_columns\u001b[39m=\u001b[39moutput_all_columns\n\u001b[0;32m   2152\u001b[0m )\n\u001b[0;32m   2153\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\zdahmani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\formatting\\formatting.py:491\u001b[0m, in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    489\u001b[0m     pa_subtable \u001b[39m=\u001b[39m _query_table(table, key)\n\u001b[0;32m    490\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     pa_subtable \u001b[39m=\u001b[39m _query_table_with_indices_mapping(table, key, indices\u001b[39m=\u001b[39;49mindices)\n\u001b[0;32m    492\u001b[0m \u001b[39mreturn\u001b[39;00m pa_subtable\n",
      "File \u001b[1;32mc:\\Users\\zdahmani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\formatting\\formatting.py:57\u001b[0m, in \u001b[0;36m_query_table_with_indices_mapping\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mint\u001b[39m):\n\u001b[0;32m     56\u001b[0m     key \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mfast_slice(key \u001b[39m%\u001b[39m indices\u001b[39m.\u001b[39mnum_rows, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcolumn(\u001b[39m0\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mas_py()\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m _query_table(table, key)\n\u001b[0;32m     58\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m):\n\u001b[0;32m     59\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39mkey\u001b[39m.\u001b[39mindices(indices\u001b[39m.\u001b[39mnum_rows))\n",
      "File \u001b[1;32mc:\\Users\\zdahmani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\formatting\\formatting.py:81\u001b[0m, in \u001b[0;36m_query_table\u001b[1;34m(table, key)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[39mQuery a pyarrow Table to extract the subtable that correspond to the given key.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mint\u001b[39m):\n\u001b[1;32m---> 81\u001b[0m     \u001b[39mreturn\u001b[39;00m table\u001b[39m.\u001b[39mfast_slice(key \u001b[39m%\u001b[39;49m table\u001b[39m.\u001b[39;49mnum_rows, \u001b[39m1\u001b[39m)\n\u001b[0;32m     82\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m):\n\u001b[0;32m     83\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39mkey\u001b[39m.\u001b[39mindices(table\u001b[39m.\u001b[39mnum_rows))\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-python-en_XX\", src_lang=\"python\", tgt_lang=\"en_XX\")\n",
    "MAX_LEN = 512\n",
    "SUMMARY_LEN = 150\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.markdown = self.data.markdown\n",
    "        self.code = self.data.code\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.markdown)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        code = str(self.code[index])\n",
    "        code = ' '.join(code.split())\n",
    "\n",
    "        markdown = str(self.markdown[index])\n",
    "        markdown = ' '.join(markdown.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([code], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([markdown], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (7572, 2)\n",
      "TRAIN Dataset: (6058, 2)\n",
      "TEST Dataset: (1514, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset=df.sample(frac=train_size,random_state=42).reset_index(drop=True)\n",
    "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-python-en_XX\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    c = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        #lm_labels = y[:, 1:].clone().detach()\n",
    "        #lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if _%500==0:\n",
    "            #print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "            c+=1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        # xm.optimizer_step(optimizer)\n",
    "        # xm.mark_step()\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zdahmani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2323: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writer(predictions, actuals):\n",
    "    # output_path = r'/content/drive/My Drive/Colab Notebooks/'\n",
    "    # pred_file = output_path+'preds.txt'\n",
    "    # actual_file = output_path+'actuals.txt'\n",
    "    # with open(pred_file, 'w') as f:\n",
    "    #     for pred in predictions:\n",
    "    #         f.write('%s\\n' % pred)\n",
    "\n",
    "    # with open(actual_file, 'w') as f:\n",
    "    #     for actual in actuals:\n",
    "    #         f.write('%s\\n' % actual)\n",
    "    my_dict = {\n",
    "        'Actual Text': actuals,\n",
    "        'Prediction': predictions       \n",
    "        }\n",
    "    final_output = pd.DataFrame(my_dict)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length=150, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            if _%100==0:\n",
    "                print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    predictions, actuals = validate(epoch)\n",
    "    final_df = writer(predictions, actuals)\n",
    "    final_df.to_csv('../data/data.csv')\n",
    "    print('Output Files generated for review')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5093ef3c9eba5a2350b58945b72c1f122b8b5551b9ae00db57837a16e1175c4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
